{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/punnoose-1620/masters-thesis-sensor-data/blob/main/LiteratureReviewHelper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0moeaSAjzjp"
      },
      "source": [
        "# Perform Relevance analysis for all papers related to an idea."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhIvxhXbj7u_"
      },
      "source": [
        "### Expected File Structure :\n",
        "```\n",
        "papers_folder\n",
        "  |---subfolder\n",
        "      |---paper1\n",
        "      |---paper2\n",
        "      |---notepad\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0r-iO7iqjFe"
      },
      "source": [
        "## Imports and Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2ayBYGYK4D09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: google-generativeai in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (0.8.6)\n",
            "Requirement already satisfied: pdfplumber in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (0.11.9)\n",
            "Requirement already satisfied: tqdm in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (4.67.3)\n",
            "Requirement already satisfied: pydantic in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (2.12.5)\n",
            "Collecting dotenv\n",
            "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from google-generativeai) (2.29.0)\n",
            "Requirement already satisfied: google-api-python-client in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from google-generativeai) (2.189.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from google-generativeai) (2.49.0.dev0)\n",
            "Requirement already satisfied: protobuf in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from google-generativeai) (5.29.6)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.27.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from google-api-core->google-generativeai) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from google-api-core->google-generativeai) (2.32.5)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: cryptography>=38.0.3 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from google-auth>=2.15.0->google-generativeai) (46.0.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.6.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2026.1.4)\n",
            "Requirement already satisfied: pdfminer.six==20251230 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from pdfplumber) (20251230)\n",
            "Requirement already satisfied: Pillow>=9.1 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from pdfplumber) (12.1.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from pdfplumber) (5.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from tqdm) (0.4.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from pydantic) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from pydantic) (0.4.2)\n",
            "Collecting python-dotenv (from dotenv)\n",
            "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from cryptography>=38.0.3->google-auth>=2.15.0->google-generativeai) (2.0.0)\n",
            "Requirement already satisfied: pycparser in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from cffi>=2.0.0->cryptography>=38.0.3->google-auth>=2.15.0->google-generativeai) (3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.2)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from google-api-python-client->google-generativeai) (0.31.2)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from google-api-python-client->google-generativeai) (0.3.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: pyparsing<4,>=3.1 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.3.2)\n",
            "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
            "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: python-dotenv, dotenv\n",
            "\n",
            "   ---------------------------------------- 0/2 [python-dotenv]\n",
            "   ---------------------------------------- 0/2 [python-dotenv]\n",
            "   ---------------------------------------- 0/2 [python-dotenv]\n",
            "   ---------------------------------------- 2/2 [dotenv]\n",
            "\n",
            "Successfully installed dotenv-0.9.9 python-dotenv-1.2.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.3 -> 26.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install google-generativeai pdfplumber tqdm pydantic dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lkqiHktNjrj5"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, ConfigDict\n",
        "import google.generativeai as genai\n",
        "# from google.colab import userdata\n",
        "from typing import Type, Optional\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from tqdm import tqdm\n",
        "import pdfplumber\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcVrKA9ZnrR_"
      },
      "source": [
        "## Declare class for return types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Status(BaseModel):\n",
        "    status:str\n",
        "    date:str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PK15GmKV2VXZ"
      },
      "outputs": [],
      "source": [
        "class ContextualSummary(BaseModel):\n",
        "  summary: str\n",
        "  abstract: str\n",
        "  conclusion: str\n",
        "  published: bool\n",
        "  status_updates: list[Status]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7tGDRoQvqb-w"
      },
      "outputs": [],
      "source": [
        "class Author(BaseModel):\n",
        "  name: str\n",
        "  institution: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNPEuVOJoSuz"
      },
      "outputs": [],
      "source": [
        "class Paper(BaseModel):\n",
        "  title: str\n",
        "  methodology: str\n",
        "  relevance: float\n",
        "  relevant_pages: list\n",
        "  conflict: str\n",
        "  difference: str\n",
        "  citation: str\n",
        "  gap: str\n",
        "  paperType: str\n",
        "  authors: list[Author]\n",
        "\n",
        "  model_config = ConfigDict(extra='allow')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivvLF87Q0vz6"
      },
      "source": [
        "## Declare Static Queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CITATION_MAPS = \"\"\"\n",
        "Citations must follow one of the following patterns based on the current/latest status of the paper : \n",
        "1. Pre-Print: A. Author, \"Title,\" arXiv, vol. abs/1234.567, 2026.\n",
        "2. In-Press/Accepted : B. Author, \"Title,\" Journal Name, to be published.\n",
        "3. Submitted : C. Author, \"Title,\" submitted for publication.\n",
        "4. Published in Journal : J. K. Author, \"Name of paper,\" Abbrev. Title of Periodical, vol. x, no. x, pp. xxx–xxx, Abbrev. Month, year, doi: xxx.\n",
        "5. Published in Conference : J. K. Author, \"Name of paper,\" in Abbrev. Name of Conf., City of Conf., Abbrev. State (if relevant), year, pp. xxx–xxx.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQPlWqzl2ReU"
      },
      "outputs": [],
      "source": [
        "CLASS_DETAILS = {\n",
        "    \"title\": \"Title of the Paper\",\n",
        "    \"methodology\": \"What is done in the paper and how it is done, including relevant technical details. Use proper technical terms on what methods are used. Do not miss technical keywords or details.\",\n",
        "    \"relevance\": \"Relevance score (0-1) for how relevant this paper is to our context.\",\n",
        "    \"relevant_pages\": \"List of pages that have content relevant to our topic.\",\n",
        "    \"conflict\": \"How this paper conflicts our idea\",\n",
        "    \"difference\": \"How this paper is different from our idea\",\n",
        "    \"gap\" : \"The project/literature gap found by this paper in regards to this domain.\",\n",
        "    \"citation\": \"String to cite this paper\",\n",
        "    \"paperType\": \"What type of paper is this (qualitative/quantitative)?\",\n",
        "    \"authors\": [\n",
        "        {\n",
        "            \"name\": \"Author Name\",\n",
        "            \"institution\": \"Institution of Author\"\n",
        "        }\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hCUAq1VfucPQ"
      },
      "outputs": [],
      "source": [
        "SYSTEM_QUERY_SUMMARIZER = \"\"\"\n",
        "You are an academically profound individual well versed in the domain of the reference paper. \n",
        "Your only task is to summarise all papers given to you. \n",
        "Do not miss any technical terms that might be relevant to this domain. \n",
        "Isolate Abstract, Conclusion and Status from this paper.\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_QUERY_RELEVANCE = f\"\"\"\n",
        "You are an academically profound individual well versed in the domain of both papers. \n",
        "Do not miss any technical terms that might be relevant to this domain. \n",
        "Output must be Strictly in this format :\n",
        "{CLASS_DETAILS}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h1uwFXEvSDB"
      },
      "source": [
        "## Declare Models Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Declare Model Names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HlAjbxPFvVWa"
      },
      "outputs": [],
      "source": [
        "SUMMARIZATION_MODEL = \"gemini-2.5-flash-lite\"\n",
        "RELEVANCE_MODEL = \"gemini-2.5-pro\"\n",
        "MODEL_API_KEY = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Populate Model API Key based on Colab/Local Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "T-7CYUQRIUQ4"
      },
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "try:\n",
        "    MODEL_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "except Exception:\n",
        "    pass  # Not running in Colab\n",
        "\n",
        "if not MODEL_API_KEY:\n",
        "    MODEL_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "#print(f\"Model Key : {MODEL_API_KEY}\\nKey in Env : {os.getenv('GOOGLE_API_KEY')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSgIgLilz6re"
      },
      "source": [
        "## Configure API Key for LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "kZNd4AZdz_Xz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model API Key has been initialized....\n"
          ]
        }
      ],
      "source": [
        "if MODEL_API_KEY is not None:\n",
        "    print('Model API Key has been initialized....')\n",
        "else:\n",
        "    print('Unable to initialize Model API Key')\n",
        "    raise RuntimeError(\"GOOGLE_API_KEY not found in Colab userdata or environment variables\")\n",
        "genai.configure(api_key=MODEL_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGw0xZHKjWUo"
      },
      "source": [
        "## Declare Project Based Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Folder Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "FOLDER_PATH = \"Literature_Review_Papers/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Project Context/Description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_CONTEXT_MINIATURE = \"\"\"\"\n",
        "This project proposes a stakeholder-facing chatbot built on a hybrid LLM-deterministic architecture. The system is designed to answer user queries by combining probabilistic language understanding with controlled, rule-based execution. The primary goal is to maintain interpretability, predictability, and low hallucination risk while still benefiting from large language models.\n",
        "\n",
        "A user submits a natural language query, which is first converted into a structured input representation. This representation includes the raw query text, metadata such as domain, geography, and time scope, and explicit indicators of ambiguity and confidence expectations. This structured form provides semantic grounding and supports comparison with intent-based and slot-filling dialogue systems.\n",
        "\n",
        "Intent identification is handled by a large language model such as Gemini, GPT, or Claude. The model is used only to classify intent and related attributes, not to select tools or execute actions. Its output is a structured intent label with confidence and grouping information, enabling downstream deterministic processing.\n",
        "\n",
        "The identified intent is passed to a rule-based function selector. This component maps intents to predefined backend functions using a static function map. Each function definition specifies required and optional inputs, domain constraints, and execution preconditions. If preconditions are not satisfied, the system triggers a fallback path that requests clarification from the user. This ensures that critical decisions remain deterministic and auditable.\n",
        "\n",
        "Once a function is selected, a code executor retrieves and processes data from structured sources such as databases or domain-specific datasets. The executor applies business logic and returns raw, structured outputs. No natural language generation occurs at this stage, and all data access is explicitly defined.\n",
        "\n",
        "The raw execution results are then passed to a second LLM acting as a data interpreter. This model translates structured outputs into human-readable responses tailored to the original user query. The interpreter has no control over execution and operates only on provided data and context, reducing the risk of fabricated information.\n",
        "\n",
        "The final response is delivered to the end user or integrated into external platforms such as Microsoft Teams. The architecture enforces a clear separation between understanding, decision-making, execution, and explanation, making the system comparable to task-oriented dialogue systems, hybrid neuro-symbolic approaches, and modern LLM agent frameworks while remaining controlled and explainable.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "1xjEaO-ejPNu"
      },
      "outputs": [],
      "source": [
        "PROJECT_CONTEXT = \"\"\"\n",
        "1. Overall System Goal\n",
        "The proposed system is a stakeholder-facing chatbot designed to answer user queries by combining:\n",
        "- LLM-based intent understanding\n",
        "- Deterministic function selection\n",
        "- Structured data retrieval and execution\n",
        "- LLM-based result interpretation\n",
        "The core design principle is to separate reasoning from execution, ensuring:\n",
        "- Predictability and controllability\n",
        "- Reduced hallucination risk\n",
        "- Easier comparison with traditional dialog systems and agent-based LLM workflows\n",
        "\n",
        "2. High-Level Workflow Overview\n",
        "- An end user (stakeholder) submits a natural language query.\n",
        "- The query is first processed by an Intent Identifier (LLM).\n",
        "- Identified intent is passed to a deterministic Function Selector.\n",
        "- A Code Executor retrieves and processes relevant data from structured sources.\n",
        "- Raw outputs are sent to a Data Interpreter (LLM).\n",
        "- The interpreted response is returned to the user or optionally integrated with external platforms (e.g., Microsoft Teams).\n",
        "Purpose\n",
        "- Enables explicit semantic grounding\n",
        "- Allows comparison with:\n",
        "- Slot-filling approaches\n",
        "- Semantic frame parsing\n",
        "- Ontology-driven dialog systems\n",
        "- Supports ambiguity detection and confidence-aware handling\n",
        "\n",
        "3. Intent Identification Layer (LLM)\n",
        "Role\n",
        "The Intent Identifier uses a Large Language Model (e.g., Gemini, GPT, Claude) to:\n",
        "- Classify the user's intent\n",
        "- Assign confidence levels\n",
        "- Group intents into higher-level categories\n",
        "Key Characteristics\n",
        "- Used only for semantic understanding, not execution\n",
        "- Model choice is configurable based on empirical benchmarking\n",
        "- Output is structured, not free-form text\n",
        "Research Comparison Angle\n",
        "Comparable to:\n",
        "- Neural intent classifiers\n",
        "- Zero-shot / few-shot intent detection\n",
        "- LLM-based semantic parsing (without tool execution)\n",
        "\n",
        "4. Deterministic Function Selector\n",
        "Role\n",
        "- The Function Selector is a non-LLM, rule-based component that:\n",
        "- Maps detected intents to predefined backend functions\n",
        "- Validates required inputs and preconditions\n",
        "- Decides whether execution is possible or clarification is required\n",
        "\n",
        "5. Code Executor and Data Layer\n",
        "Code Executor\n",
        "Responsible for:\n",
        "- Executing the selected function\n",
        "- Fetching data from structured sources\n",
        "- Applying business logic or transformations\n",
        "Data Sources\n",
        "- Databases\n",
        "- Predefined datasets\n",
        "- Domain-specific structured data\n",
        "These are represented as a Map of Available Data, ensuring:\n",
        "- Explicit data provenance\n",
        "- No implicit model assumptions\n",
        "Output\n",
        "- Produces raw, structured data\n",
        "- No natural language generation occurs at this stage\n",
        "\n",
        "6. Data Interpretation Layer (LLM)\n",
        "Role\n",
        "The Data Interpreter (LLM) converts raw execution results into:\n",
        "- Human-readable explanations\n",
        "- Summaries\n",
        "- Contextual insights appropriate for stakeholders\n",
        "Characteristics\n",
        "- No access to execution logic\n",
        "- Operates only on:\n",
        "   - Raw data\n",
        "   - Original user query\n",
        "   - Optional metadata\n",
        "Research Alignment\n",
        "Comparable to:\n",
        "- Neural natural language generation layers\n",
        "- Explainable AI interfaces\n",
        "- Post-hoc summarization systems\n",
        "\n",
        "8. Response Delivery and Integration\n",
        "- The final response is returned to the end user\n",
        "- Optionally routed through external integrations (e.g., Microsoft Teams)\n",
        "- Maintains separation between:\n",
        "   - Internal system logic\n",
        "   - Presentation layer\n",
        "\n",
        "9. Key Architectural Principles\n",
        "- Separation of Concerns\n",
        "   - Understanding ≠ Decision ≠ Execution ≠ Explanation\n",
        "- Controlled Use of LLMs\n",
        "   - LLMs are used only where probabilistic reasoning is beneficial\n",
        "- Determinism and Safety\n",
        "   - Critical system decisions are rule-based\n",
        "- Explainability and Comparability\n",
        "   - Every stage is inspectable and replaceable\n",
        "\n",
        "10. Positioning for Paper Comparison\n",
        "This system can be directly compared with:\n",
        "- Task-oriented dialogue systems (TOD)\n",
        "- LLM agent frameworks (ReAct, Toolformer, AutoGPT)\n",
        "- Hybrid neuro-symbolic architectures\n",
        "- Enterprise conversational AI platforms\n",
        "Key differentiators:\n",
        "- Dual-LLM design (intent + interpretation)\n",
        "- Deterministic execution core\n",
        "- Explicit input and function schemas\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyMRTyP0jZKK"
      },
      "source": [
        "## Function to get paths for files and notepads from a folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "49qGZE5Cjczd"
      },
      "outputs": [],
      "source": [
        "def getFilesAndNotepads(folderPath:str):\n",
        "    file_paths = []\n",
        "    notepad_paths = []\n",
        "\n",
        "    for root, _, files in os.walk(folderPath):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            if file.lower().endswith(('.pdf')):\n",
        "                file_paths.append(file_path)\n",
        "            elif file.lower().endswith(('.txt')):\n",
        "                notepad_paths.append(file_path)\n",
        "\n",
        "    return file_paths, notepad_paths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2zRpniTlkCZ"
      },
      "source": [
        "## Functions to read content from Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "a3ca2108"
      },
      "outputs": [],
      "source": [
        "def read_txt_file_content(file_path: str) -> str:\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "        return content\n",
        "    except FileNotFoundError:\n",
        "        return f\"Error: The file at {file_path} was not found.\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred while reading the file: {e}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-cud_EAInSS3"
      },
      "outputs": [],
      "source": [
        "def read_pdf_contents(pdf_path, detect_columns=True):\n",
        "    \"\"\"\n",
        "    Read all contents from a PDF file, handling both single and multi-column layouts.\n",
        "\n",
        "    Args:\n",
        "        pdf_path: Path to the PDF file\n",
        "        detect_columns: Whether to automatically detect and handle multi-column layouts\n",
        "\n",
        "    Returns:\n",
        "        Extracted text as a string\n",
        "\n",
        "    Example:\n",
        "        text = read_pdf_contents(\"research_paper.pdf\")\n",
        "        print(text)\n",
        "    \"\"\"\n",
        "    all_text = []\n",
        "\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page_num, page in enumerate(pdf.pages, 1):\n",
        "            if detect_columns:\n",
        "                # Get page dimensions\n",
        "                page_width = page.width\n",
        "                page_height = page.height\n",
        "                words = page.extract_words()\n",
        "\n",
        "                # Detect multi-column layout\n",
        "                is_multi_column = False\n",
        "                if len(words) >= 20:\n",
        "                    midpoint = page_width / 2\n",
        "                    left_words = sum(1 for w in words if (w['x0'] + w['x1']) / 2 < midpoint)\n",
        "                    right_words = sum(1 for w in words if (w['x0'] + w['x1']) / 2 > midpoint)\n",
        "                    total = len(words)\n",
        "                    is_multi_column = (left_words / total >= 0.3 and right_words / total >= 0.3)\n",
        "\n",
        "                # Extract based on column detection\n",
        "                if is_multi_column:\n",
        "                    split_point = page_width * 0.5\n",
        "                    left_text = page.crop((0, 0, split_point, page_height)).extract_text() or \"\"\n",
        "                    right_text = page.crop((split_point, 0, page_width, page_height)).extract_text() or \"\"\n",
        "                    page_text = f\"{left_text}\\n\\n{right_text}\".strip()\n",
        "                else:\n",
        "                    page_text = page.extract_text()\n",
        "            else:\n",
        "                page_text = page.extract_text()\n",
        "\n",
        "            if page_text:\n",
        "                all_text.append(f\"=== Page {page_num} ===\\n{page_text}\")\n",
        "\n",
        "    return \"\\n\\n\".join(all_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D71M2DEnqpK_"
      },
      "source": [
        "## Function to invoke LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "GbX59Vmvqsmi"
      },
      "outputs": [],
      "source": [
        "def invoke_gemini(\n",
        "    query: str,\n",
        "    responseClass: Type[BaseModel],\n",
        "    modelName: str,\n",
        "    system_query: Optional[str] = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Invokes Gemini and parses the response into responseClass.\n",
        "    The user query is passed EXACTLY as-is.\n",
        "    \"\"\"\n",
        "\n",
        "    model = genai.GenerativeModel(\n",
        "        model_name=modelName,\n",
        "        system_instruction=system_query,\n",
        "\n",
        "    )\n",
        "\n",
        "    response = model.generate_content(\n",
        "        query,  # <-- query is untouched\n",
        "        generation_config={\n",
        "            \"response_mime_type\": \"application/json\",\n",
        "            \"response_schema\": responseClass\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Gemini already validates against the schema\n",
        "    #return response.parsed\n",
        "    text = response.text\n",
        "    return responseClass.model_validate_json(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTqPLoo2qm-Q"
      },
      "source": [
        "## Start Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "sSO28Sel5xe4"
      },
      "outputs": [],
      "source": [
        "papersWithRelevance = []\n",
        "errorsList = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "6MmL3uHaqmlI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Analyzing reference papers....: 100%|██████████| 28/28 [00:42<00:00,  1.51s/it]\n"
          ]
        }
      ],
      "source": [
        "# Summarize each paper\n",
        "# Analyze each summary with reference to project idea\n",
        "files, notepads = getFilesAndNotepads(FOLDER_PATH)\n",
        "for paper in tqdm(files, desc=\"Analyzing reference papers....\"):\n",
        "  paper_content = read_pdf_contents(paper)\n",
        "  errorFlag = False\n",
        "\n",
        "  try:\n",
        "    # Get Summary and abstract of the paper\n",
        "    summary = invoke_gemini(paper_content, ContextualSummary, SUMMARIZATION_MODEL, SYSTEM_QUERY_SUMMARIZER)\n",
        "  except Exception as e:\n",
        "    errorsList.append({paper:e})\n",
        "    errorFlag = True\n",
        "    continue\n",
        "\n",
        "  # Create Query for Relevance Calculation\n",
        "  relevance_query = f\"\"\"\n",
        "  Here is my current project idea : \n",
        "  {PROJECT_CONTEXT_MINIATURE}\n",
        "\n",
        "  Calculate relevance of this paper with context to my project. Here is the summary of the paper :\n",
        "  {summary.summary}\n",
        "  \"\"\"\n",
        "\n",
        "  if not errorFlag:\n",
        "    try:\n",
        "      # Calculate Relevance values for this Paper\n",
        "      relevance = invoke_gemini(relevance_query, Paper, RELEVANCE_MODEL, SYSTEM_QUERY_RELEVANCE)\n",
        "    except Exception as e:\n",
        "      errorsList.append({paper:e})\n",
        "      errorFlag = True\n",
        "      continue\n",
        "\n",
        "  # Add Paper's local path, abstract and conclusion to result\n",
        "  if not errorFlag:\n",
        "    relevance.paper_path = paper\n",
        "    relevance.abstract = summary.abstract\n",
        "    relevance.conclusion = summary.conclusion\n",
        "    relevance.summary = summary.summary\n",
        "    relevance.status = summary.status_updates\n",
        "    relevance.published = summary.published\n",
        "    papersWithRelevance.append(relevance.model_dump())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU1UvQ0qLFJZ"
      },
      "source": [
        "## Display Relevance Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "MUEgEkIk8dyj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analysis for papers failed. Please check the below cell for list of errors....\n"
          ]
        }
      ],
      "source": [
        "# Print all relevances\n",
        "if len(papersWithRelevance)==0:\n",
        "  print(\"Analysis for papers failed. Please check the below cell for list of errors....\")\n",
        "else:\n",
        "  print(\"Here are the analysis Results : \")\n",
        "  for paper in papersWithRelevance:\n",
        "    print(json.dumps(paper, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Display Error Papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28 papers of 28 faced the following errors when being analyzed : \n",
            "\tPaper: Literature_Review_Papers/(document question answering OR long document QA OR RAG)\\Enhancing_Large_Model_Document_Question_Answering_Through_Retrieval_Augmentation.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/(relevance scoring OR cross-encoder OR re-ranking)\\A_distributed_search_engine_based_on_a_re-ranking_algorithm_model.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/(relevance scoring OR cross-encoder OR re-ranking)\\A_re-ranking_method_based_on_cloud_model.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/(relevance scoring OR cross-encoder OR re-ranking)\\Enhancing_Retrieval_and_Re-ranking_in_RAG_A_Case_Study_on_Tax_Law.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/(relevance scoring OR cross-encoder OR re-ranking)\\The_Research_on_Re-ranking_Algorithm_for_FAQ-based_Systems_in_the_Petroleum_Domain.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/(semantic chunk OR section segmentation)\\A_Sentence-Level_Semantic_Annotated_Corpus_Based_on_HNC_Theory.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/(semantic chunk OR section segmentation)\\Comparison_of_Chunking_Techniques_Across_Diverse_Document_Types_in_NLP_Retrieval_Tasks.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/agentic + large language model + automotive\\BetterCheck - Towards Safeguarding VLMs.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/agentic + large language model + automotive\\Cleaning Maintenance Logs with LLM Agents for Improved Predictive Maintenance.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/agentic + large language model + automotive\\GateLens - A Reasoning-Enhanced LLM Agent for Automotive Software Release Analytics.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/agentic + large language model + automotive\\Optimizing RAG Techniques for Automotive Industry PDF Chatbots.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/agentic + large language model + automotive\\Querying Large Automotive Software Models - Agentic vs Direct LLM Approaches.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/function calling OR tool use\\Enhancing_LLM_Function_Calling_with_Structured_Outputs.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/function calling OR tool use\\Simple_Action_Model_Enabling_LLM_to_Sequential_Function_Calling_Tool_Chain.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/function calling OR tool use\\The_Splitting_and_Matching_Algorithm_of_Dynamic_Path_Oriented_the_Function_Calling_Relationship.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/intent_classification\\A_New_Data_Augmentation_Method_for_Intent_Classification_Enhancement_and_its_Application_on_Spoken_Conversation_Datasets.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/intent_classification\\Comparative_Analysis_of_Intent_Classification_in_Indonesian_Chatbots_Using_BERT_and_RoBERTa_Models.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/intent_classification\\Comparison_Of_Multinomial_Naive_Bayes_Algorithm_And_Logistic_Regression_For_Intent_Classification_In_Chatbot.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/intent_classification\\Integrating_Model-Agnostic_Meta-Learning_with_Advanced_Language_Embeddings_for_Few-Shot_Intent_Classification.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/intent_classification\\Intent_Classification_French_Recruitment_Chatbot_Use_Case.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/intent_classification\\intent_prediction.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/intent_classification\\Leveraging_BERT_for_Next-Generation_Spoken_Language_Understanding_with_Joint_Intent_Classification_and_Slot_Filling.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/intent_classification\\Logistic_Regression-Based_Example_Selection_for_Enhanced_Few-Shot_Learning_in_Intent_Classification.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/retrieval augmented generation OR All MetadataRAG\\BERT_GPT-2_and_Retrieval-Augmented_Generation_RAG.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/retrieval augmented generation OR All MetadataRAG\\Integrating_Retrieval-Augmented_Generation.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/retrieval augmented generation OR All MetadataRAG\\RAGRouter_Learning to Route Queries to Multiple Retrieval‑Augmented Language Models.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/retrieval augmented generation OR All MetadataRAG\\Retrieval-Augmented_Generation_RAG_and_LLM_Integration.pdf\n",
            "\tERROR: 429\n",
            "\tPaper: Literature_Review_Papers/retrieval augmented generation OR All MetadataRAG\\Retrieval_Augmented_Generation_RAG_using_LLMs.pdf\n",
            "\tERROR: 429\n"
          ]
        }
      ],
      "source": [
        "print(f\"{len(errorsList)} papers of {len(files)} faced the following errors when being analyzed : \")\n",
        "for paper in errorsList:\n",
        "    path = list(paper.keys())[0]\n",
        "    err = paper[path]\n",
        "    print(f\"\\tPaper: {path}\\n\\tERROR: {err.code}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOu+kM2pgcB8dVwj4kZ9wvM",
      "collapsed_sections": [
        "-h1uwFXEvSDB",
        "wSgIgLilz6re",
        "rGw0xZHKjWUo",
        "GyMRTyP0jZKK",
        "k2zRpniTlkCZ",
        "D71M2DEnqpK_"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
