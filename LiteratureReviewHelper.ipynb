{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "dcVrKA9ZnrR_",
        "ivvLF87Q0vz6",
        "-h1uwFXEvSDB",
        "wSgIgLilz6re",
        "GyMRTyP0jZKK",
        "k2zRpniTlkCZ",
        "D71M2DEnqpK_"
      ],
      "authorship_tag": "ABX9TyPBUzVrAHBytn65DWUHQq62",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/punnoose-1620/masters-thesis-sensor-data/blob/main/LiteratureReviewHelper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform Relevance analysis for all papers related to an idea."
      ],
      "metadata": {
        "id": "K0moeaSAjzjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Expected File Structure :\n",
        "```\n",
        "papers_folder\n",
        "  |---subfolder\n",
        "      |---paper1\n",
        "      |---paper2\n",
        "      |---notepad\n",
        "```"
      ],
      "metadata": {
        "id": "xhIvxhXbj7u_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Installs"
      ],
      "metadata": {
        "id": "l0r-iO7iqjFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai\n",
        "!pip install pdfplumber"
      ],
      "metadata": {
        "id": "2ayBYGYK4D09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, ConfigDict\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "from typing import Type, Optional\n",
        "\n",
        "from tqdm import tqdm\n",
        "import pdfplumber\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "lkqiHktNjrj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Declare class for return types"
      ],
      "metadata": {
        "id": "dcVrKA9ZnrR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContextualSummary(BaseModel):\n",
        "  summary: str"
      ],
      "metadata": {
        "id": "PK15GmKV2VXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Author(BaseModel):\n",
        "  name: str\n",
        "  institution: str"
      ],
      "metadata": {
        "id": "7tGDRoQvqb-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Paper(BaseModel):\n",
        "  title: str\n",
        "  abstract: str\n",
        "  methodology: str\n",
        "  conclusion: str\n",
        "  relevance: float\n",
        "  relevant_pages: list\n",
        "  citation: str\n",
        "  paperType: str\n",
        "  authors: list[Author]\n",
        "\n",
        "  model_config = ConfigDict(extra='allow')"
      ],
      "metadata": {
        "id": "BNPEuVOJoSuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Declare Static Queries"
      ],
      "metadata": {
        "id": "ivvLF87Q0vz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLASS_DETAILS = {\n",
        "    \"title\": \"Title of the Paper\",\n",
        "    \"abstract\": \"Abstract section from the paper\",\n",
        "    \"methodology\": \"What is done in the paper and how it is done, including relevant technical details?\",\n",
        "    \"conclusion\": \"What was the results of this paper with regard to our context?\",\n",
        "    \"relevance\": \"Relevance score (0-1) for how relevant this paper is to our context.\",\n",
        "    \"relevant_pages\": \"List of pages that have content relevant to our topic.\",\n",
        "    \"citation\": \"String to cite this paper\",\n",
        "    \"paperType\": \"What type of paper is this (qualitative/quantitative)?\",\n",
        "    \"authors\": [\n",
        "        {\n",
        "            \"name\": \"Author Name\",\n",
        "            \"institution\": \"Institution of Author\"\n",
        "        }\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "CQPlWqzl2ReU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_QUERY_SUMMARIZER = \"\"\"\n",
        "You are an academically profound individual well versed in the domain of the reference paper. Do not miss any technical terms that might be relevant to this domain. Summarize this paper.\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_QUERY_RELEVANCE = f\"\"\"\n",
        "You are an academically profound individual well versed in the domain of both papers. Do not miss any technical terms that might be relevant to this domain. Output must be Strictly in this format :\n",
        "{CLASS_DETAILS}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hCUAq1VfucPQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Declare Models for each purpose"
      ],
      "metadata": {
        "id": "-h1uwFXEvSDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SUMMARIZATION_MODEL = \"gemini-2.5-flash-lite\"\n",
        "RELEVANCE_MODEL = \"gemini-2.5-pro\"\n",
        "MODEL_API_KEY = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "HlAjbxPFvVWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure API Key for LLM"
      ],
      "metadata": {
        "id": "wSgIgLilz6re"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=MODEL_API_KEY)"
      ],
      "metadata": {
        "id": "kZNd4AZdz_Xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Declare Folder Path"
      ],
      "metadata": {
        "id": "rGw0xZHKjWUo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xjEaO-ejPNu"
      },
      "outputs": [],
      "source": [
        "FOLDER_PATH = \"\"\n",
        "PROJECT_CONTEXT = \"\"\"\n",
        "High-level overview\n",
        "The proposed system represents a hybrid stakeholder chatbot architecture that combines traditional machine-learning classifiers, deterministic control logic, and optional LLM-based components to handle user queries in a controlled, auditable, and extensible manner.\n",
        "The workflow is divided into three major subsystems:\n",
        "- Intent Identification and Function Selection\n",
        "- Data Retrieval and Code Execution\n",
        "- Data Interpretation and Response Generation\n",
        "Each subsystem is modular, allowing selective replacement of components (e.g., replacing classical ML models with LLMs) without changing the overall control flow.\n",
        "\n",
        "1. Input and Intent Classification\n",
        "\n",
        "Input structure\n",
        "User input is structured and enriched before processing. Each query may include:\n",
        "- Raw query text\n",
        "- Optional annotations\n",
        "- Intent label and confidence\n",
        "- Metadata such as domain, geography, time scope\n",
        "- An ambiguity level indicator\n",
        "This structured representation ensures traceability and supports downstream confidence handling.\n",
        "\n",
        "Intent classifier pipeline\n",
        "The Intent Identifier subsystem processes the user query in two stages:\n",
        "- Intent Classification\n",
        "  -> Determines the most likely intent class for the user query.\n",
        "  -> Implemented using traditional ML models such as:\n",
        "     > Logistic Regression\n",
        "     > Linear Support Vector Machines\n",
        "- Intent Confidence Calculation\n",
        "  -> Computes a confidence score for the predicted intent.\n",
        "  -> Confidence thresholds are used to determine whether the system proceeds automatically or requests clarification.\n",
        "This design explicitly separates intent prediction from confidence assessment, improving interpretability and control.\n",
        "\n",
        "Intent identifier responsibilities\n",
        "The intent identifier outputs:\n",
        "- An intent class\n",
        "- A confidence score\n",
        "These outputs are then passed to the Function Selector.\n",
        "\n",
        "2. Function Selection and Code Execution\n",
        "\n",
        "Function selector\n",
        "The Function Selector acts as a deterministic decision layer. Based on:\n",
        "- The identified intent\n",
        "- Confidence score\n",
        "- Predefined preconditions\n",
        "It selects an appropriate function from a Function Map, which contains:\n",
        "- Function name\n",
        "- Description\n",
        "- Domain\n",
        "- Required and optional inputs\n",
        "- Preconditions\n",
        "- Fallback behavior if preconditions are unmet\n",
        "If preconditions fail, a fallback function is selected, often leading to clarification prompts for the user.\n",
        "This ensures:\n",
        "- Predictable behavior\n",
        "- Reduced hallucination risk\n",
        "- Clear operational boundaries\n",
        "\n",
        "Data and code execution\n",
        "Once a function is selected:\n",
        "- Required data sources are identified using a Map of Available Data\n",
        "- The Code Executor:\n",
        "  -> Retrieves relevant data (e.g., from databases or APIs)\n",
        "  -> Executes deterministic business logic\n",
        "- The output of this stage is raw data, not user-ready text\n",
        "This separation ensures that:\n",
        "- Business logic remains auditable\n",
        "- AI components do not directly manipulate core data\n",
        "\n",
        "3. Data Interpretation and Translation\n",
        "Purpose of the data interpreter\n",
        "Raw outputs from the code executor are not directly exposed to the user. Instead, they are passed to a Data Interpreter and Translator module, whose responsibility is to:\n",
        "- Interpret raw or structured data\n",
        "- Determine whether the data contains textual content\n",
        "- Convert results into a human-readable response\n",
        "\n",
        "Interpreter workflow\n",
        "- Content type check\n",
        "  -> If the data contains textual content, it proceeds to semantic analysis.\n",
        "  -> Otherwise, it is interpreted directly.\n",
        "- Semantic chunk classification\n",
        "  -> Breaks text into meaningful sections.\n",
        "  -> Uses ML models (e.g., XGBoost, Random Forest) to classify relevance.\n",
        "- Section relevance scoring\n",
        "  -> Scores each chunk to filter noise and prioritize key information.\n",
        "- Confidence estimation\n",
        "  -> Assigns confidence levels to interpreted outputs.\n",
        "- Final data interpretation\n",
        "  -> Produces a structured, processed response ready for user consumption.\n",
        "This layered interpretation ensures explainability and confidence awareness in the final output.\n",
        "\n",
        "4. Response Delivery and Integration\n",
        "The processed response is:\n",
        "- Returned to the end user\n",
        "- Optionally integrated with external systems (e.g., Microsoft Teams)\n",
        "This allows the chatbot to function both as a standalone interface and as part of enterprise workflows.\n",
        "\n",
        "5. Design Philosophy and Extensibility\n",
        "Hybrid AI approach\n",
        "- Red components represent AI/ML models.\n",
        "- Blue dashed components indicate areas where LLMs can replace classical models.\n",
        "- Green components represent deterministic data sources or mappings.\n",
        "\n",
        "This makes the architecture:\n",
        "- LLM-optional, not LLM-dependent\n",
        "- Easier to validate for compliance and correctness\n",
        "- Suitable for enterprise or regulated environments\n",
        "\n",
        "6. Relevance for paper-checking / evaluation algorithms\n",
        "For a paper-checking or validation algorithm, this architecture provides:\n",
        "- Clear separation of concerns\n",
        "- Explicit confidence handling at multiple stages\n",
        "- Traceable decision points (intent, function selection, interpretation)\n",
        "- Deterministic execution paths with AI-assisted interpretation\n",
        "\n",
        "This makes it particularly suitable for:\n",
        "- Automated consistency checks\n",
        "- Hallucination detection\n",
        "- Confidence-aware evaluation\n",
        "- Explainability analysis\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to get paths for files and notepads from a folder"
      ],
      "metadata": {
        "id": "GyMRTyP0jZKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getFilesAndNotepads(folderPath:str):\n",
        "    file_paths = []\n",
        "    notepad_paths = []\n",
        "\n",
        "    for root, _, files in os.walk(folderPath):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            if file.lower().endswith(('.pdf')):\n",
        "                file_paths.append(file_path)\n",
        "            elif file.lower().endswith(('.txt')):\n",
        "                notepad_paths.append(file_path)\n",
        "\n",
        "    return file_paths, notepad_paths"
      ],
      "metadata": {
        "id": "49qGZE5Cjczd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to read content from Documents"
      ],
      "metadata": {
        "id": "k2zRpniTlkCZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3ca2108"
      },
      "source": [
        "def read_txt_file_content(file_path: str) -> str:\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "        return content\n",
        "    except FileNotFoundError:\n",
        "        return f\"Error: The file at {file_path} was not found.\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred while reading the file: {e}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pdf_contents(pdf_path, detect_columns=True):\n",
        "    \"\"\"\n",
        "    Read all contents from a PDF file, handling both single and multi-column layouts.\n",
        "\n",
        "    Args:\n",
        "        pdf_path: Path to the PDF file\n",
        "        detect_columns: Whether to automatically detect and handle multi-column layouts\n",
        "\n",
        "    Returns:\n",
        "        Extracted text as a string\n",
        "\n",
        "    Example:\n",
        "        text = read_pdf_contents(\"research_paper.pdf\")\n",
        "        print(text)\n",
        "    \"\"\"\n",
        "    all_text = []\n",
        "\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page_num, page in enumerate(pdf.pages, 1):\n",
        "            if detect_columns:\n",
        "                # Get page dimensions\n",
        "                page_width = page.width\n",
        "                page_height = page.height\n",
        "                words = page.extract_words()\n",
        "\n",
        "                # Detect multi-column layout\n",
        "                is_multi_column = False\n",
        "                if len(words) >= 20:\n",
        "                    midpoint = page_width / 2\n",
        "                    left_words = sum(1 for w in words if (w['x0'] + w['x1']) / 2 < midpoint)\n",
        "                    right_words = sum(1 for w in words if (w['x0'] + w['x1']) / 2 > midpoint)\n",
        "                    total = len(words)\n",
        "                    is_multi_column = (left_words / total >= 0.3 and right_words / total >= 0.3)\n",
        "\n",
        "                # Extract based on column detection\n",
        "                if is_multi_column:\n",
        "                    split_point = page_width * 0.5\n",
        "                    left_text = page.crop((0, 0, split_point, page_height)).extract_text() or \"\"\n",
        "                    right_text = page.crop((split_point, 0, page_width, page_height)).extract_text() or \"\"\n",
        "                    page_text = f\"{left_text}\\n\\n{right_text}\".strip()\n",
        "                else:\n",
        "                    page_text = page.extract_text()\n",
        "            else:\n",
        "                page_text = page.extract_text()\n",
        "\n",
        "            if page_text:\n",
        "                all_text.append(f\"=== Page {page_num} ===\\n{page_text}\")\n",
        "\n",
        "    return \"\\n\\n\".join(all_text)"
      ],
      "metadata": {
        "id": "-cud_EAInSS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to invoke LLM"
      ],
      "metadata": {
        "id": "D71M2DEnqpK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def invoke_gemini(\n",
        "    query: str,\n",
        "    responseClass: Type[BaseModel],\n",
        "    modelName: str,\n",
        "    system_query: Optional[str] = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Invokes Gemini and parses the response into responseClass.\n",
        "    The user query is passed EXACTLY as-is.\n",
        "    \"\"\"\n",
        "\n",
        "    model = genai.GenerativeModel(\n",
        "        model_name=modelName,\n",
        "        system_instruction=system_query,\n",
        "\n",
        "    )\n",
        "\n",
        "    response = model.generate_content(\n",
        "        query,  # <-- query is untouched\n",
        "        generation_config={\n",
        "            \"response_mime_type\": \"application/json\",\n",
        "            \"response_schema\": responseClass\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Gemini already validates against the schema\n",
        "    return response.parsed\n"
      ],
      "metadata": {
        "id": "GbX59Vmvqsmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Analysis"
      ],
      "metadata": {
        "id": "cTqPLoo2qm-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "papersWithRelevance = []"
      ],
      "metadata": {
        "id": "sSO28Sel5xe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize each paper\n",
        "# Analyze each summary with reference to project idea\n",
        "files, notepads = getFilesAndNotepads(FOLDER_PATH)\n",
        "for paper in tqdm(files, desc=\"Analyzing reference papers....\"):\n",
        "  paper_content = read_pdf_contents(paper)\n",
        "\n",
        "  try:\n",
        "    summary = invoke_gemini(paper_content, ContextualSummary, SUMMARIZATION_MODEL, SYSTEM_QUERY_SUMMARIZER)\n",
        "  except Exception as e:\n",
        "    print(\"ERROR: Summary generation faced an error : \", e)\n",
        "    break\n",
        "\n",
        "  relevance_query = f\"\"\"\n",
        "  Here is my current project idea :\n",
        "  {PROJECT_CONTEXT}\n",
        "\n",
        "  Calculate relevance of this paper with context to my project. Here is the summary of the paper :\n",
        "  {summary.summary}\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    relevance = invoke_gemini(relevance_query, Paper, RELEVANCE_MODEL, SYSTEM_QUERY_RELEVANCE)\n",
        "  except Exception as e:\n",
        "    print(\"ERROR: Relevance calculation faced an error : \", e)\n",
        "    break\n",
        "  relevance.paper_path = paper\n",
        "  papersWithRelevance.append(relevance.model_dump())"
      ],
      "metadata": {
        "id": "6MmL3uHaqmlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print all relevances\n",
        "for paper in papersWithRelevance:\n",
        "  print(json.dumps(paper, indent=2))"
      ],
      "metadata": {
        "id": "MUEgEkIk8dyj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}