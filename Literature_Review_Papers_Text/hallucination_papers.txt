=== Page 1 ===
Can LLMs Predict T
Self-Awareness via
AmirhoseinGhasemabadi
UniversityofAlberta,Canada
ghasemab@ualberta.ca
Abstract
Large language models (LLMs) generate flu-
entandcomplexoutputsbutoftenfailtorec-
ognizetheirownmistakesandhallucinations.
Existingapproachestypicallyrelyonexternal
judges,multi-sampleconsistency,ortext-based
self-critique,whichincuradditionalcompute
orcorrelateweaklywithtruecorrectness. We
ask: can LLMs predict their own failures by
inspectinginternalstatesduringinference? We
introduceGnosis,alightweightself-awareness
mechanism that enables frozen LLMs to per-
formintrinsicself-verificationbydecodingsig-
nalsfromhiddenstatesandattentionpatterns.
Gnosispassivelyobservesinternaltraces,com-
pressesthemintofixed-budgetdescriptors,and
predictscorrectnesswithnegligibleinference
cost,addingonly 5Mparametersandoperat-
ingindependentlyofsequencelength. Across
mathreasoning,open-domainquestionanswer-
ing,andacademicknowledgebenchmarks,and
over frozen backbones ranging from 1.7B to
20B parameters, Gnosis consistently outper-
forms strong internal baselines and large ex-
ternaljudgesinbothaccuracyandcalibration.
Moreover, it generalizes zero-shot to partial
generations,enablingearlydetectionoffailing
trajectoriesandcompute-awarecontrol. These
resultsshowthatreliablecorrectnesscuesare
intrinsictogenerationprocessandcanbeex-
tractedefficientlywithoutexternalsupervision.
Codeandmodels: ¬ßGnosisGithub.
1 Introduction
Largelanguagemodels(LLMs)haveachievedre-
markable performance in open-ended generation
and multi-step reasoning, yet they remain unreli-
ableatassessingthecorrectnessoftheirownout-
puts(Kalaietal.,2025;Huangetal.,2025b). They
frequentlyproduceconfidentbutincorrectanswers,
failingtodetectreasoningerrorsorhallucinations
evenwhensuchfailuresareevidenttoexternaleval-
uators(Kirichenkoetal.,2025;Kamoietal.,2024).
6202
naJ
4
]LC.sc[
2v87502.2152:viXra

Their Own Failures?
a Internal Circuits
DiNiu
UniversityofAlberta,Canada
dniu@ualberta.ca
Thisgapbetweenstronggenerationandweakself-
verification limits the reliability, safety, and effi-
ciencyofLLMdeployment,particularlyinsettings
that require long-horizon reasoning or compute-
aware control. A fundamental open question is
whetherLLMscananticipatetheirownfailuresby
examiningtheinternaldynamicsthatgoverntheir
generationprocess.
PriorworkonLLMself-evaluationandhalluci-
nation detection largely follows three paradigms.
Text-basedself-critiqueandconfidenceestima-
tion (Kadavath et al., 2022; Ulmer et al., 2024;
Huang et al., 2025a) infer correctness from gen-
erated text or token probabilities, often tracking
linguisticfluencyratherthanreasoningvalidityand
degradingonlongorcompositionaltasks. Multi-
sample consistency methods (Sriramanan et al.,
2024a;PawitanandHolmes, 2025)estimatecon-
fidence from agreement across multiple genera-
tions, improving robustness at the cost of infer-
ence that scales linearly with the number of sam-
ples. Externaljudgesandrewardmodels (Sti-
ennon et al., 2020; Ouyang et al., 2022; Zheng
et al., 2024; Wang et al., 2024b; Liu et al., 2025)
trainlargeauxiliarymodelstoevaluateresponses,
providingstrongsignalsbutrequiringcostlysuper-
vision,additionaldecodingpasses,andsubstantial
inferenceoverhead. Despitetheirdifferences,these
approachesrelyonsignalsexternaltothemodel‚Äôs
owninternaldynamics,leavingopenwhethercor-
rectnesscanbepredicteddirectlyfromthegenera-
tionprocessitself.
Inthispaper,wedemonstratethatlargelanguage
models can reliably predict their own failures by
leveraging signals intrinsic to the generation pro-
cess. We introduce Gnosis, a lightweight self-
awareness mechanism that endows frozen LLMs
withintrinsicself-verification,eliminatingtheneed
forexternaljudges. Byextractingreliabilitycues
directlyfrommodel-internaldynamicsduringinfer-
ence,Gnosisproducesaccurateandwell-calibrated

=== Page 2 ===
LLM Hea
Gnosis Self-Awareness Mechanism
S√óD
LLM‚ùÑ Hidden Hidden
State Circuit
Feed Forward
H Encoderüî•
+ Judg
N
S√óS He
Self-Attention Attn Attention
Maps Circuit
N√ó
Encoder
üî•
Prompt
Figure 1: Overview of our Gnosis self-awareness mech
states and attention maps from a frozen LLM, learns to
predictsascalarcorrectness(hallucination)scorewithonl
inferencecost. Right: Gnosisoutperforms8BSkywork
onMath-Reasoning(AMC12+AIME24/25+HMMTF
Knowledge(MMLU-Pro);scoresareaveragedoverthefr
correctness estimates with negligible computa-
tionaloverhead. Thisintrinsiccapability enables
earlydetectionoffailingreasoningtrajectories,ef-
ficient scaling across model sizes and domains,
and practical deployment of compute-aware and
reliability-criticallanguagesystems. Ourmaincon-
tributionsare:
‚Ä¢ Intrinsic,Trajectory-LevelSelf-Awareness.
We introduce Gnosis, a lightweight mecha-
nismthatenablesfrozenLLMstopredictthe
correctnessoftheirowngenerationsbydecod-
ingsignalsintrinsictotheinferenceprocess.
Unlikepriorinternal-signalmethodsthatrely
onstatisticalfeatures(Gengetal.,2023;Wang
et al., 2025; Zhang et al., 2025b) or single-
tokenindicators(Zhangetal.,2025a),Gnosis
leveragesthefullspatiotemporalstructureof
internaldynamicsacrossanentiregeneration
trajectory.
‚Ä¢ Dual-Stream Introspection from Hidden
StatesandAttention. Gnosisjointlymodels
hidden-state evolution and attention-routing
patternsthroughacompact, fixed-budgetar-
chitecture that operates independently of se-
quencelength,extractingrichreliabilitycues
withnegligibleinferenceoverhead.
‚Ä¢ Cross-Scale Transfer and Early Failure
Detection. Gnosis generalizes beyond self-
judgment: aheadtrainedonasmallbackbone
model transfers zero-shot to larger variants,
andpredictsfailuresreliablyfrompartialrea-
soningandgenerations,enablingearlytermi-
nationandcompute-awarecontrol.

ssentcerroC
erocS
ad‚ùÑ MLL
esnopseR
gment
eadüî•
hanism and its performance. Left: Gnosis taps hidden
o compress them into hidden/attention descriptors, and
ly‚àº5millionextraparametersandessentiallyzeroadded
krewardmodelsandaGemini2.5ProjudgeinAUROC
Feb2025),Open-DomainQA(TriviaQA),andAcademic
rozenbackboneslistedinTable1.
‚Ä¢ State-of-the-Art Performance at Minimal
Scale. With only ‚àº5M added parameters,
Gnosis is orders of magnitude smaller than
external verifiers yet outperforms billion-
parameter reward models and proprietary
judgesonmathreasoning,open-domainQA,
andacademicbenchmarks. Itworksreliably
acrossdiversefrozenbackboneswithnegligi-
blelatencyoverhead.
2 RelatedWork
MethodsforassessingLLMcorrectnessandhallu-
cination risk largely fall into four families: Text-
based confidence & self-critique, Internal signal-
basedindicatorsandlinearprobes,externalreward
models and judge LLMs, and multi-sample self-
consistencymethods.
External Reward Models and Judge LLMs.
External verifiers train separate models to score
response quality, factuality, or step-wise correct-
ness. OutcomeandProcessRewardModels(OR-
M/PRM)arewidelyusedforranking,hallucination
detection,andguidingtest-timesearch(Stiennon
etal.,2020;Ouyangetal.,2022;Zhengetal.,2024;
Wangetal.,2023;Zhangetal.,2025c). Recentsys-
tems emphasize large, carefully curated datasets
over architectural novelty: HelpSteer2 combines
Likertratings,pairwisepreferences,andextrapola-
tiontosharpendiscrimination(Wangetal.,2024b),
whileSkywork-Reward-V2scaleshuman‚ÄìAIcura-
tiontotensofmillionsofpreferencepairsandleads
on RewardBench-style suites (Liu et al., 2025).
Thesemodelsprovidestrongsignalsbutincursub-
stantialannotationcostandaddinferencelatency

=== Page 3 ===
anddeploymentoverheadbyrequiringalargeaux-
iliarymodelatservingtime.
Text-BasedConfidence&Self-Critique. Text-
basedapproachesaimtoestimatecorrectnessfrom
thegeneratedtextandtokenprobabilities. Training-
free indicators use entropy or max probability as
uncertainty proxies but struggle with confident
hallucinationsandout-of-distributionshifts(Geng
etal.,2023;Sriramananetal.,2024b;Pawitanand
Holmes, 2025). Prompt-based calibration elicits
verbalized confidence or self-critique, improving
ECEbutoftentrackingstylisticfluencymorethan
reasoningvalidityandrequiringextrapasses(Ka-
davathetal.,2022;Ulmeretal.,2024). Generative
and distillation-based calibrators predict correct-
nessinasingleforwardpass,e.g.,APRICOTtrains
a calibrator LLM (Ulmer et al., 2024), and Self-
Calibration distills self-consistency signalsto en-
ableearlystoppingandconfidence-weightedsam-
pling(Huangetal.,2025a). Thesemethodsreduce
dependenceonexternaljudgesbutmayrequirefull-
model fine-tuning, add training cost, and remain
brittleacrossdomainsandsequencelengths.
Internal signal-based indicators and linear
probes. Glass-box signals exploit logits, hidden
states, and attention routing. Prior work shows
hiddenactivationsdivergebetweencorrectandhal-
lucinated outputs (Duan et al., 2024), with factu-
ality cues concentrated in middle/deep layers yet
sensitive to domain shift (Zhang et al., 2025b).
Token-wisehidden-stateentropyandinformation
density can outperform perplexity-based failure
prediction(Chenetal.,2024). Trajectory/spectral
views analyze how representations evolve across
layers (e.g., Chain-of-Embedding, stability of la-
tentpaths)andrelateangular/magnitudechangesto
correctness(Wangetal.,2025). Attentionstatistics
providelightweightreliabilitycues(Huangetal.,
2024). Acomplementarylinetrainssimpleprobes
(shallowMLPs)onfinal-tokenstates(Azariaand
Mitchell, 2023; Burns et al., 2022; Zhang et al.,
2025a). However, these approaches consistently
yieldlowaccuracyacrossdiversebenchmarks. By
relyingonfragileheuristicsorsingle-tokensnap-
shots,theymissthegeneration‚Äôsfullspatiotempo-
ralstructure,resultinginperformancethatfallsfar
shortofGnosis.
Multi-Sample Self-Consistency and Test-
Time Scaling. Multi-sample self-consistency in-
fers confidence from agreement across sampled
rationales, boosting robustness but incurring in-
ference cost that scales with the number of sam-

ples and often saturating on long, compositional
tasks(Sriramananetal.,2024b). Recentcost-aware
test-time scaling uses internal signals to prune
searchoradaptcompute,reducingdependenceon
largeexternalverifierswhileretainingsomebene-
fitsofmulti-samplereasoning(Huangetal.,2025a;
Ghasemabadietal.,2025).
3 TheGnosisMechanism
WeintroduceGnosis,alightweightself-awareness
mechanismdesignedtoretrofitfrozenLLMswith
introspectioncapabilities. Gnosisoperatesonthe
intuitionthatamodel‚Äôsinternaltraces,itsevolving
hiddenstatesandattentionroutingpatterns,carry
distinctive‚Äúfingerprints‚Äùofhallucinationandrea-
soningerrors. Unlikeexternaljudgesthatrequire
separate, expensive decoding passes, Gnosis is a
passiveobserver: itcompressesthebackbone‚Äôsin-
ternal signals into compact descriptors and fuses
themtopredictascalarcorrectnessscore. Thear-
chitectureisexplicitlydesignedsothatitsinference
costisindependentofthesequencelength,adding
negligibleoverheadevenforverylongcontexts.
3.1 ProblemSetupandLength-Invariant
Inputs
Let x denote an input prompt of length S and yÀÜ
x
thegeneratedresponseoflengthS . Theinputto
y
Gnosis is the concatenated sequence with a total
size of S = S +S tokens." The backbone has
x y
hiddendimensionD,Ldecoderlayers,andH at-
tentionheadsperlayer. Duringgeneration,weread
only the final-layer hidden states Hlast ‚àà RS√óD
and the attention maps A = {A } ,
‚Ñì,h ‚Ñì=1..L,h=1..H
where each A ‚àà RS√óS is the attention map of
‚Ñì,h
headhinlayer‚Ñì.
Gnosislearnsaverificationfunction:
pÀÜ= f (cid:0) Hlast,A (cid:1) ‚àà [0,1], (1)
œï
where pÀÜis the estimated probability that the gen-
eratedansweriscorrectandœïaretheparameters
of Gnosis. The backbone LLM remains frozen
throughout.
Fixed-BudgetCompression. Todecouplecom-
putational cost from sequence length S, we use
aprojectionoperatorŒ†thatmapsvariable-length
tracesintofixed-sizetensors:
‚Ä¢ HiddenStates. ThesequenceHlast ‚àà RS√óD
is interpolated and adaptively pooled along
thesequencedimensiontoafixedbudgetK
hid

=== Page 4 ===
Table1: Correctness/hallucinationdetectionacrossdomai
AUPR-c/AUPR-e/BSS/ECE.
Method Qwen31.7B-Hybrid Qwe
(AUROC‚Üë / AUPR-c‚Üë / A
DomainI:Math-Reasoning(AMC1
LogitEntropy(2024a) .79 .73 .82 .25 .05 .80 .
MeanTokenProb(2024a) .78 .71 .82 .23 .06 .80 .
AttnEigenvalueScore(2024a) .61 .52 .63 -.13 .17 .55 .
CoE‚ÄìR(2025) .59 .51 .63 -.17 .20 .56 .
CoE‚ÄìC(2025) .60 .53 .64 -.14 .18 .53 .
SkyworkRM-Llama3.1-8B(2025) .88 .88 .88 .38 .10 .87 .
SkyworkRM-Qwen3-8B(2025) .90 .92 .89 .39 .14 .89 .
Gemini2.5Pro(2025) .91 .88 .86 .50 .11 .92 .
Gnosis(Ours) .95 .95 .94 .59 .09 .96 .
DomainII:Open-Do
LogitEntropy .64 .53 .73 -.16 .19 .68 .
MeanTokenProb .63 .52 .72 -.17 .19 .67 .
AttnEigenvalueScore .52 .40 .62 -.39 .27 .57 .
CoE‚ÄìR .59 .44 .70 -.25 .22 .53 .
CoE‚ÄìC .58 .42 .70 -.28 .22 .59 .
SkyworkRM-Llama3.1-8B .83 .74 .87 .00 .25 .75 .
SkyworkRM-Qwen3-8B .84 .73 .89 .00 .23 .73 .
Gemini2.5Pro .90 .79 .91 .40 .11 .84 .
Gnosis(Ours) .87 .79 .92 .34 .10 .89 .
DomainIII:AcademicKnowl
LogitEntropy .73 .86 .49 -.11 .20 .74 .
MeanTokenProb .73 .86 .49 -.11 .19 .74 .
AttnEigenvalueScore .61 .78 .36 -.35 .22 .51 .
CoE‚ÄìR .55 .72 .34 -.47 .25 .59 .
CoE‚ÄìC .55 .72 .34 -.47 .25 .60 .
SkyworkRM-Llama3.1-8B .65 .79 .46 .01 .10 .61 .
SkyworkRM-Qwen3-8B .76 .87 .53 .01 .17 .73 .
Gemini2.5Pro .76 .83 .68 .12 .16 .70 .
Gnosis(Ours) .80 .90 .56 .15 .11 .82 .
Table 2: Comparison with an MLP-Prob baseline on
Qwen31.7B.
Method Math TriviaQA MMLU-Pro
(AUROC‚Üë,AUPR-c‚Üë,ECE‚Üì)
MLP-Prob(2025a) .86 .85 .19 .71 .58 .21 .69 .79 .23
Gnosis .95 .95 .09 .87 .79 .10 .80 .90 .11
(e.g.,192),yielding
HÀú = Œ† (Hlast) ‚àà RK hid √óD. (2)
hid
‚Ä¢ AttentionMaps. EachattentionmapA ‚àà
‚Ñì,h
RS√óS isdownsampledviaadaptivepoolingto
afixedgridsizek√ók (e.g.,k = 256),giving
astandardizedset
AÀú= {AÀú } , AÀú ‚àà Rk√ók. (3)
‚Ñì,h ‚Ñì,h ‚Ñì,h
All downstream encoders operate only on HÀú
and AÀú with fixed dimensions (K ,D) and
hid
(L,H,k,k), so the computational cost of Gnosis

ins. Foreachmodel,columns(lefttoright)are: AUROC/
en34B-Thinking Qwen34B-Instruct OpenAIgpt-oss-20B
AUPR-e‚Üë / BSS‚Üë / ECE‚Üì)
12+AIME24/25+HMMTFeb2025)
.80 .77 .23 .12 .83 .79 .82 .32 .74 .80 .79 .81 .32 .07
.79 .76 .21 .13 .82 .79 .82 .31 .08 .79 .78 .80 .30 .07
.60 .46 -.28 .23 .72 .66 .75 .11 .13 .72 .66 .75 .11 .13
.60 .55 -.26 .24 .66 .65 .59 -.02 .15 .66 .66 .60 -.02 .15
.57 .52 -.32 .26 .66 .66 .59 -.02 .14 .66 .66 .59 -.02 .14
.93 .80 .24 .18 .83 .90 .72 .06 .22 .81 .80 .83 .29 .10
.94 .77 .10 .22 .83 .89 .75 -.49 .40 .81 .79 .84 -.12 .30
.97 .68 .46 .15 .84 .88 .66 .31 .18 .92 .98 .64 -1.09 .10
.98 .91 .65 .05 .93 .96 .89 .51 .08 .85 .86 .86 .38 .04
omainQA(TriviaQA)
.70 .63 .02 .13 .71 .75 .64 .74 .11 .79 .85 .63 .05 .21
.70 .63 .00 .14 .71 .75 .64 .71 .11 .79 .86 .62 .05 .20
.61 .50 -.20 .19 .59 .65 .51 -.16 .19 .65 .81 .40 -.30 .21
.57 .49 -.29 .24 .57 .62 .48 -.23 .19 .78 .86 .61 -.03 .21
.61 .54 -.17 .20 .52 .57 .44 -.32 .22 .78 .87 .61 -.03 .21
.74 .74 -.13 .28 .69 .73 .60 -.47 .37 .79 .88 .59 -1.11 .52
.89 .43 -.05 .17 .67 .71 .57 -.82 .47 .82 .90 .64 -1.54 .59
.80 .86 .33 .14 .75 .75 .67 -.02 .23 .74 .83 .54 -.01 .20
.89 .88 .45 .05 .86 .87 .84 .38 .05 .83 .90 .73 .19 .17
ledge-Reasoning(MMLU-Pro)
.90 .41 -.31 .25 .70 .82 .45 -.22 .21 .61 .75 .39 -.36 .23
.89 .42 -.30 .25 .70 .83 .48 -.17 .23 .65 .78 .31 -.24 .20
.78 .24 -.76 .29 .59 .80 .32 -.47 .23 .52 .72 .31 -.50 .26
.80 .32 -.60 .30 .52 .72 .28 -.64 .29 .61 .77 .38 -.33 .22
.81 .32 -.58 .28 .52 .73 .29 -.62 .29 .60 .76 .38 -.34 .22
.82 .37 -.03 .10 .61 .80 .38 -.11 .15 .71 .82 .53 -.39 .33
.88 .43 -.05 .17 .66 .82 .45 -.57 .35 .75 .86 .57 -1.02 .50
.87 .49 -.01 .18 .67 .83 .37 -.28 .24 .78 .84 .70 .22 .15
.93 .55 .21 .05 .74 .87 .51 .10 .05 .75 .84 .51 .07 .06
doesnotgrowwithS andisnegligiblecompared
tothebackbone;seeAppendixAforarchitectural
details.
o
3.2 Hidden-StateCircuitEncoder
3
1 Standard confidence methods often rely on to-
ken probabilities (logits), which are poorly cal-
ibrated and only weakly aligned with correct-
ness (Ghasemabadi et al., 2025). Gnosis instead
learnsfromthebackbone‚Äôsinternalrepresentation,
extractingcorrectnesscuesdirectlyfromthefinal-
layerlatentrepresentations. AsmallencoderœÅ
hid
mapsthislatenttraceintoacompactdescriptor:
z = œÅ (HÀú) ‚àà RDHID. (4)
hid hid
LocalTemporalEncoder. WetreatHÀú asatem-
poralsignalandapplyalightweightmulti-scale1D
depthwise convolution over the sequence dimen-
siontocapturelocaldependenciesandirregulari-
tiesinthehiddentrajectory.

=== Page 5 ===
Table3: Sibling-modeljudgmentacrossdomains. Each
triplet of columns shows AUROC / AUPR-c / ECE.
Gnosis-SelfJudge: Gnosisheadtrainedoneachback-
boneandjudgingitsowngenerations. Gnosis-RM:a
singleGnosisheadtrainedonQwen31.7B-Hybridand
usedasarewardmodelfortheothermodels.
Qwen3 Qwen3 Qwen3
Judge/Model
1.7B-Hybrid 4B-Thinking 8B-Hybrid
(AUROC‚Üë,AUPR-c‚Üë,ECE‚Üì)
DomainI:Math(AMC12+AIME24/25+HMMTFeb2025)
SkyworkRM-Qwen3-8B .90 .92 .14 .89 .94 .22 .86 .95 .23
Gnosis-SelfJudge .96 .98 .05 .97 .97 .08
.95 .95 .09
Gnosis-Qwen1.7BasRM .93 .97 .18 .97 .99 .07
DomainII:Open-DomainQA(TriviaQA)
SkyworkRM-Qwen3-8B .84 .73 .23 .73 .89 .17 .72 .78 .32
Gnosis-SelfJudge .89 .89 .05 .86 .90 .09
.87 .79 .10
Gnosis-Qwen1.7BasRM .86 .86 .04 .84 .88 .12
DomainIII:AcademicKnowledge-Reasoning(MMLU-Pro)
SkyworkRM-Qwen3-8B .76 .87 .17 .73 .88 .17 .73 .89 .19
Gnosis-SelfJudge .82 .93 .05 .83 .94 .07
.80 .90 .11
Gnosis-Qwen1.7BasRM .81 .93 .16 .83 .94 .15
Table4: CorrectnessdetectiononMath-reasoningfor
Qwen3 1.7B-Hybrid backbone under two max re-
sponse lengths (12k, 24k). We compare Gnosis with
SkyworkRM-Qwen3-8B,highlightingGnosis‚Äôsnear-
constantlatencyandlargespeedupsasresponselength
increases.
Method Maxlen Latency(ms)‚Üì AUROC‚Üë
SkyworkRM-Qwen3-8B 12k 930 .90
SkyworkRM-Qwen3-8B 24k 2465 .88
Gnosis 12k 25(√ó37) .95
Gnosis 24k 25(√ó99) .94
GlobalSetEncoder. Tosummarizethesequence
intoacompressedrepresentation,wethenapplya
SetTransformer‚Äìstyleencoder(Leeetal.,2019):
SetAttentionBlocks(SAB)followedbyaPooling-
by-Multihead-Attention (PMA) block. This en-
ablesglobalinteractionacrossallpositionsandag-
gregatesthesequenceintoasmallsetofsummary
tokens,whichareflattenedandlinearlyprojected
toformthefinalhiddendescriptorz . Figure2il-
hid
lustratesthedetailedarchitecturedesignofHidden
CircuitEncoder. AppendixA.1detailstheencoder
architecture,whileAppendixB.3presentsfullde-
signablations.
3.3 AttentionCircuitEncoder
The attention stream AÀú reveals layer- and head-
level routing patterns that can indicate brittle
reasoning or unstable focus, complementing the
hidden-state descriptor. Rather than feeding raw
attentionweightsintoalargenetwork,Gnosissum-

marizeseachdownsampledattentionmapAÀú ‚àà
‚Ñì,h
Rk√ók intoacompactfeaturevector:
v = Œ¶(AÀú ) ‚àà Rd grid. (5)
‚Ñì,h ‚Ñì,h
Here Œ¶ denotes our per-map feature extractor,
which outputs a d -dimensional summary for
grid
eachattentionmap.
Per-map Feature Extraction. We implement
Œ¶ using two complementary approaches: (i) a
lightweight CNN that treats each attention map
as an image and learns local-to-global patterns,
and (ii) an interpretable statistics-based extractor
thatsummarizeshowattentionisdistributed,where
it concentrates, and how local or long-range it is.
Concretely,thestatisticsincludesimplemeasures
ofspreadandtexture(e.g.,entropy-andfrequency-
basedfeatures),diagonalandnear-diagonalmassto
capturelocality,andlightweightcenter-and-spread
measuresthatdescribetheaveragelocationofat-
tentionandhowwidelyitisdispersed.
We ablate each variant and their hybrid in Ap-
pendix B.2 (Table 6). While the two extractors
areindividuallycompetitive,thehybridisthemost
consistentacrossbenchmarks;wethereforeadopt
[Œ¶ ;Œ¶ ]inthefinaldesign. Fulldefinitionsof
cnn stat
thestatisticsareprovidedinAppendixA.2.
Cross-Head and Cross-Layer Encoding. We
arrange the per-head summaries into an L √ó H
layer‚Äìheadgrid:
G ‚àà RL√óH√ód grid, G = v . (6)
‚Ñì,h,: ‚Ñì,h
Weaddlearnedlayerandheadembeddingstopre-
serve depth and head identity. We then treat the
L √óH entries as grid tokens. A lightweight en-
coder œÅ mixes information across layers and
attn
headsusingafewaxialconvolutionallayers. This
designissubstantiallycheaperthanfullglobalself-
attentionoverthegrid. Finally,weapplyPooling-
by-Multihead-Attention (PMA) to aggregate the
gridintoasingledescriptor:
z = œÅ (G) ‚àà RD ATT. (7)
attn attn
Because this stage operates on fixed dimensions
(L,H,d ),theGnosis-sidecomputeisindepen-
grid
dent of the original sequence length S. Figure 2
illustratesthedetailedarchitecturedesignofAtten-
tionCircuitEncoder. Detailedarchitecturechoices
andablationsaredeferredtoAppendixB.2.

=== Page 6 ===
Hidden Circuit Encoder
Input: Hidden State zhid ‚àà ‚ÑùDHID
(B, S, D)
LayerNorm + Linear Proj Output MLP
(B, S, dhid) (B, K*dhid)
Adaptive Avg Pooling 1D Flatten
(B, dhid, khid) (B, K, dhid)
Local Temporal
Feature Mixing Multi-Scale Dilated Convs Pooling MultiHead Attention
(PMA)
Learnable Queries
gate params Gated Mixing Refined features
Learnable
SE Block Set Attention Block(SAB) Seeds(K)
(Squeeze-Excite)
Position Emb (B, khid, dhid) Final Feature
Aggregation
Figure2: GnosisEncoderArchitectureDetails. Hidde
statetrace,applymulti-scaledilatedtemporalmixing,the
toproduceacompactdescriptorz . AttentionCircuit
hid
fixedk√ókgrid,extractper-mapCNN+statisticsfeatures,
processor, andpool(PMA)toobtainz . AppendixA
attn
AppendixBincludesthecompletesetofarchitectureand
3.4 GatedFusionandCorrectnessPrediction
Gnosisfusesthehiddenandattentiondescriptors
into a single vector and maps it to a correctness
probability. Weconcatenatethetwodescriptors
z = [z ;z ], (8)
hid attn
and feed the result into a small gated MLP head.
Thefinalcorrectnessestimateis
(cid:0) (cid:1)
pÀÜ= œÉ GatedMLP (z) , (9)
œï
where GatedMLP is a lightweight gated MLP
œï
andœÉ isthesigmoid. ThisheadletsGnosisadap-
tively weight hidden versus attention features on
a per-example basis(e.g., leaning more on atten-
tionforreasoningtracesandmoreonhiddenstates
forfactualrecall). Thearchitectureisintentionally
small: Gnosis adds only ‚àº5M parameters, mak-
ingit‚àº1000√ósmallerthan8Brewardmodelsand
dramaticallysmallerthanGemini2.5proasjudge.
3.5 Training
AkeyadvantageofGnosisisthatitcanbetrained
withoutcostly-annotateddata. Foreachbackbone,
wegenerateanswersonthetrainingsetsandlabel
correctness by comparing predictions to ground-
truth answers. This yields a binary classification
dataset:
D = {(Hlast,A ,y )}N ,
i i i i=1

Attention Circuit Encoder
Input: Attention Maps zattn ‚àà ‚ÑùDATTN
(B, L, H, S, S)
Downsample Output MLP
(B, L, H, K, K) (B, K*dgrid)
Reshape Flatten
(B*L*H, K, K) (B, K, dgrid)
Feature Extarction
Pooling MultiHead Attention
S C t o a m tis p ti u c t a e l a F t e te a n tu ti r o e n s Lea ( r C n o a n b v le o l V u i t s io u n a a l l p N at N te ) rns (PMA) Queries
Refined features
Learnable
Axial Grid Processor Seeds(K)
Linear Proj to dgrid (Row/Col Convs)
Head/Layer
(B, L, H, dgrid)
(B, L, H, dgrid)
Final Feature
Position Emb
Aggregation
enCircuit(left): projectandadaptivelypoolthehidden-
enuselightweightattention-basedpooling(SAB‚ÜíPMA)
(right): downsampleeachlayer‚Äìheadattentionmaptoa
,mixacrossthelayer√óheadgridwithalightweightaxial
Agivesadetaileddescriptionoftheencoderdesign, and
ddesignablations.
where y ‚àà {0,1} indicates whether the verifier
i
judged the i-th generation as correct. Gnosis is
trainedtominimizebinarycross-entropy:
L(œï) = ‚àíE (cid:2) ylogpÀÜ+(1‚àíy)log(1‚àípÀÜ) (cid:3) ,
(Hlast,A,y)‚àºD
with pÀÜ = f (Hlast,A). The backbone is frozen;
œï
gradients flow only through the Gnosis encoders
andfusionhead.
4 ExperimentsandResults
WeevaluateGnosisinthreepracticalregimes: (i)
self-judgment,whereeachGnosisheadscoresgen-
erationsfromitsownfrozenbackbone;(ii)sibling-
model judgment, where a small head serves as a
lightweightrewardmodelforlargerfamilymem-
bers;and(iii)earlycorrectnessprediction,where
Gnosisisqueriedonpartialcompletionstosupport
compute-awarecontrol.
4.1 ExperimentalSetup
Backbones. WeapplyGnosistofivefrozenLLMs:
Qwen31.7B-Hybrid,Qwen34B-Thinking,Qwen3
4B-Instruct,Qwen38B-hybrid(Yangetal.,2025)
and OpenAI gpt-oss-20B(Agarwal et al., 2025).
The backbone weights and decoding settings are
neverupdated.
TrainingData. Wetrainonecorrectnesshead
per backbone on a mixed math‚Äìtrivia corpus to
coverbothmulti-stepreasoningandopen-domain
factual recall. For math, we use the English por-
tionofDAPO-Math-17k(‚àº14kcompetition-style

=== Page 7 ===
(a) Math-Reasoning
Figure3: EarlyCorrectnessPredictiononMath-Reas
calibrationthanbothMLP-Prob(blue)andarewardmod
seeing40%ofthecompletion,Gnosisalreadymatchesth
Figure4:2DEmbeddingsofFeaturesLearnedbyGnos
embeddingsofhidden-statefeatures(left),attentionfeatu
contoursandmarginaldensitiesforwrong(red)andcorr
separation,attentionfeaturesshowaweakerbutstillcle
overalldiscriminationbetweencorrectandwrongsolutio
problems with numeric or symbolic answers(Yu
et al., 2025)). For QA, we subsample 40k ques-
tionsfroma118k-itemTriviaQAtrainingset(Joshi
etal.,2017)toretainbroadcoveragewhilekeeping
training compact. We generate two completions
permathprompttocapturediversereasoningtra-
jectoriesandincreasecorrect/incorrectlabelvariety
underthesamequestion,andonecompletionper
triviapromptsinceanswersareshorterandoften
lessambiguous. Weextractfinalanswer,labelcor-
rectnessbycomparingtotheground-truth,anddis-
cardoutputswithoutvalidanswers. Thisyieldsa
balanced,fullyautomatedtrainingsetthatrequires
nohumanannotation.
TrainingDetailsandCost. Wetraineachhead
fortwoepochsoverthismixeddatasetusingAdam
withalearningrateof1√ó10‚àí4. Becausetheback-
bone is frozen and all feature extractors operate
atafixedbudgetindependentofsequencelength,
training is lightweight. For the largest backbone
(gpt-oss-20B MoE), the full pipeline, data gener-
ationandtrainingfinishesinroughly12hourson
2√óA100 80GB GPUs, corresponding to $25 in
cloudcost. Smallerbackbonestrainfaster.

(b) TriviaQA
soning. Gnosis(red)achieveshigheraccuracyandbetter
delSKYWORKRM-QWEN3-8B(yellow). Notably,after
hefull-solutionperformanceoftheothermethods.
sisonMath-Reasoning. Weshowdimensionality-reduced
ures(middle),andtheirmergedfeatures(right),withKDE
rect(blue)answers. Hiddenfeaturesexhibittheclearest
earseparation,andthemergedspaceyieldsthesharpest
ons.
Benchmarks. Foreachbenchmark,weprompt
each frozen backbone to generate a solution
with a maximum budget of 12k tokens, and re-
tain only question‚Äìanswer pairs with a valid fi-
nal answer for evaluation. We evaluate Gno-
sis on three disjoint domains: Math-Reasoning
(AMC122022/2023(AI-MOTeam,2024),AIME
2024/2025(Zhang and Math-AI, 2024, 2025),
HMMTFeb2025(Balunovic¬¥ etal.,2025)),Open-
Domain QA (18k held-out TriviaQA questions
with no overlap with training), and Academic
KnowledgeReasoning(MMLU-Pro(Wangetal.,
2024a)). Together,thesebenchmarksstressmulti-
step reasoning, hallucination detection on short
factoid answers, and out-of-distribution general-
ization. We report detailed backbone-level out-
come statistics(accuracy, hallucination, andnon-
responserates)inAppendixD.Additionalbench-
markdetailsareprovidedinAppendixC.
Metrics. Wetreatcorrectnesspredictionasbi-
naryclassificationandreportAUROCandAUPR
undertwocomplementarylabelings(AUPR-c: cor-
rect as positive; AUPR-e: incorrect as positive),
togetherwithcalibrationmetricsBrierSkillScore

=== Page 8 ===
(BSS) and Expected Calibration Error (ECE).
AUROC/AUPRmeasurediscriminativerankingun-
derclassimbalance,whereasBSS/ECEassessthe
quality and calibration of predicted probabilities.
SeeAppendixCforextendedinterpretations.
Baselines. We compare against four base-
line families. (1) Statistical internal scores are
training-free indicators computed from the back-
bone‚Äôs own outputs, reported in Table 1 as Logit
Entropy, Mean Token Prob, and Attn Eigenvalue
Score(Sriramanan et al., 2024a). (2) Trajecto-
ry/spectralinternalindicatorssummarizecross-
layer hidden-state dynamics, reported as CoE‚ÄìR
and CoE‚ÄìC(Wang et al., 2025). (3) External
judgesincludetwoopen-sourcerewardmodelsthat
arestate-of-the-artonpublicreward-modelbench-
marks(Maliketal.,2025),SkyworkRM-Llama3.1-
8B and the family-aligned SkyworkRM-Qwen3-
8B(Liuetal.,2025),aswellasGemini2.5Proused
asajudge(theGeminijudgingpromptisprovided
intheAppendixE);allarereportedinTable1. (4)
ALearnableprobe(Zhangetal.,2025a)thatob-
servesonlythefinalanswertoken‚Äôshiddenstateis
reported separately on Qwen3 1.7B in Table 2 to
isolatethelimitationsofsingle-tokenprobing.
4.2 Self-Judgment
WeevaluateGnosisinthestandardself-judgment
setting: for each backbone, the model generates
answerstothebenchmarkquestions,andthever-
ificationmethodpredictsthecorrectnessofthese
specific generations. As shown in Tables 1 and
2, Gnosis consistently outperforms training-free
baselinesandlargeexternaljudgesacrossalltested
domains.
Superiority Over Internal Baselines and
Probes. Across Math Reasoning and Open-
DomainQA,Gnosiseffectivelysolvesthemiscal-
ibrationofstandardconfidencemetrics. Itconsis-
tentlyliftsAUROCfromthemid-0.7s(typicalof
Logit Entropy) to 0.95‚Äì0.96 while roughly dou-
blingtheBSS,turningnegativecalibrationscores
intostronglypositiveones. Crucially,Gnosisout-
performsthelearnedMLP-Probfinal-tokenprobe
by7‚Äì18AUROCpointsacrossbenchmarks. This
consistentgapconfirmsthatcorrectnessisaprop-
erty of the full generation trajectory, specifically
thedistributedhidden-statedynamicsandattention
patterns, rather than a state localized to the final
token.
Efficiency vs. Scale: With only ‚àº5M parame-
tersandnegligibleoverheadfromitsfixed-budget

projection,Gnosismatchesorexceedsstate-of-the-
artSkywork8BRewardModels(‚àº1000√ólarger)
and the proprietary Gemini 2.5 Pro. This is no-
table because Gnosis adds no independent world
knowledge. Rather than fact-checking with mas-
siveparametricmemory,itdetectsthesignaturesof
hallucinationandreasoningerrorinthebackbone‚Äôs
internaltraces. OncomplexMathReasoning,Gno-
sissurpassesbothlargejudges. Italsooutperforms
GeminionMMLU-Pro,adomainitwasnotexplic-
itlytrainedon,suggestingthatitlearnstransferable
errorpatternsinsteadoftask-specificcues. Addi-
tionally,Gnosismaintainsnear-constant‚àº25msla-
tencyandachievesroughly37√óand99√óspeedups
overthe8Brewardmodelwhenjudginganswersof
length12kand24ktokens,respectively(Table4).
These results show that intrinsic self-verification
can be both more scalable and far cheaper than
externaloversight.
Figure 5 compares the predicted correctness
score distributions of Gnosis and the Skywork
8BRewardModel. Gnosisshowssharp,bimodal
peaks near 0 (incorrect) and 1 (correct), whereas
Skyworkproducesbroader,overlappingscoresthat
oftenclusteraround0.5‚Äì0.6. Thisseparationaligns
withGnosis‚Äôsstrongercalibration(BSS)anditsten-
dencytoassignmoredecisiveprobabilities.
4.3 Cross-Scale: Zero-ShotRewardModeling
Weintroduce‚ÄúSiblingModeling‚Äù,wherewetrain
Gnosisonasmall,cheapbackboneanddeployitto
judgelargerfamilymemberswithoutfine-tuning.
Table 3 highlights a striking outcome: a head
trained on a 1.7B backbone transfers effectively
to4Band8Bsiblingsacrossallevaluateddomains.
OnMathReasoning,forinstance,itachieves0.93
AUROC,nearlymatchingthe0.96achievedbya
self-trained head. Notably, this transferred 1.7B
headstillconsistentlyoutperformstheSkywork8B
Reward Model across all tested backbones, prov-
ingthatourtinyzero-shotverifierismorereliable
than a massive external judge. This broad trans-
ferabilityimpliesthathallucinationmanifestsasa
structuralinvariantacrossmodelscales,offeringa
‚Äúfreelunch‚Äùwhereasinglesmallheadservesasa
supervisorforanentiremodelfamily. Weobserve
that this transfer is most effective when models
shareasimilargenerationstyle; whileGnosisro-
bustlyhandlesdifferencesinsize,itperformsbest
whenthemodelsalsoalignintheirformatting(e.g.,
transferringbetweenthinkingmodelsratherthan
thinking-to-Instruct).

=== Page 9 ===
4.4 EarlyErrorDetection
BecauseGnosisprocessesinternaltracesintofixed-
sizedescriptors,itcanevaluatepartialgenerations
natively. Crucially,thiscapabilityisemergent: al-
thoughGnosisistrainedexclusivelyoncomplete
trajectories,itgeneralizeszero-shottopartialpre-
fixeswithoutanyadditionalfine-tuning. Figure3
illustratesthatonbothMathReasoningandTrivi-
aQA,Gnosisreachesnear-peakaccuracyandpos-
itive BSS after observing only 40% of the gen-
eration. In contrast, external reward models and
single-tokenlearnableprobetypicallyrequirethe
full response to stabilize. This enables aggres-
sivecompute-awarecontrolpolicies: generated
chains-of-thoughtcanbeterminatedimmediatelyif
theinternal‚Äúhallucinationalarm‚Äùtriggers,prevent-
ingwastedcomputeonfailingpaths,orthesystem
canautomaticallyescalatethequerytoastronger
modelupondetectingthatthecurrentbackboneis
incapableofansweringcorrectly.
5 AblationsandAnalysis
Wehighlightthekeyablationinsightsthatmotivate
Gnosisdesign,anddefercomprehensivestudiesto
AppendixA.
Hiddenvs.AttentionCircuits. Gnosisfusesa
hidden-stateandanattentioncircuit. AppendixTa-
ble 5 shows that both streams help, but attention
is most useful for long-form reasoning: on Trivi-
aQA,hidden-onlydominatesandfusionaddslittle,
indicating hidden states carry most short factual
reliability; on Math Reasoning and MMLU-Pro,
attention-only is strong (even slightly better than
hidden-onlyonMMLU-Pro),andfusionyieldsthe
bestoverallperformance,suggestingcomplemen-
tarystructuralcuesthatemergeinlongerreasoning.
WevisualizethisbehavioronMath-Reasoning
in Figure 4; analogous feature-distribution plots
fortheotherdomainsareprovidedintheappendix
Figure7. Takentogether,theseresultssupportthat
hiddenstatesprovideabroad,robustsignalacross
domains,whileattentionroutingcontributesmore
stronglyonreasoning-heavytasksandlessonshort
factual QA; combining both is the most reliable
overall.
Attention Map Extractor. We ablate how to
summarizeeachdownsampledattentionmapwith
alightweightCNN,predefinedfixedstatistics,and
their hybrid. Appendix Table 6 shows that the
threevariantsperformsimilarlyonMath,whileon
TriviaQAtheCNN-basedvariantsarestrongerthan
statisticsalone,andonMMLU-ProtheCNN+Stats

hybridisthemostconsistent. Basedonthese,we
adopttheCNN+Statsdesigninthefinalmodel.
AdditionalAblations. Wefurthervalidatethe
designpathbehindGnosiswithtargetedablations
across both streams. On the attention stream,
Appendix B.2 examines how grid mixing, iden-
tityembeddings,poolingstrategy,layerselection,
and map downsampling affect performance (Ap-
pendixTables7and8). Onthehiddenstream,we
isolate architectural value by including a simple
pooled-MLPbaselinethatnaivelypoolsfinal-layer
hiddenstatesbeforeanMLP(RowGinTable9),
alongside broader studies of the local‚Äìglobal en-
coder design and sizing trade-offs (Appendix Ta-
bles9and10).
6 DiscussionandLimitations
Gnosis provides a highly efficient framework for
self-evaluation and ‚Äúsibling modeling,‚Äù where a
smallheadtrainedonacompactmodeleffectively
judgeslargermodelswithinthesamefamily. This
architecturefurthersupportscompute-awarecon-
trol by enabling early error detection on partial
generationtraces. However,akeylimitationisthat
Gnosisisdesignedasaself-awarenessmechanism
ratherthanageneral-purposerewardmodel;while
it transfers robustly to siblings, it is not capable
of acting as a universal zero-shot judge for every
model,particularlythosewithunrelatedarchitec-
tures or differing generation styles (e.g., transfer-
ringbetweenThinkingandInstructmodels).
7 Conclusion
We introduced Gnosis, a lightweight mechanism
that allows frozen LLMs to detect their own er-
rors by interpreting internal hidden and attention
tracesrather thanrelyingon external judges. De-
spiteaddingonly‚àº5Mparameters,Gnosisconsis-
tentlyoutperformsbillion-parameterrewardmod-
elsandLargeproprietarymodelslikeGemini2.5
Pro, demonstrating that high-fidelity correctness
signalsareintrinsictothegenerationprocess. This
approachestablishesanewstandardforcompute-
efficientreliability,enablingself-verifyingsystems
thatcandetectfailingtrajectorieswithnegligible
overhead.
References
SandhiniAgarwal, LamaAhmad, JasonAi, SamAlt-
man, Andy Applebaum, Edwin Arbus, Rahul K
Arora, Yu Bai, Bowen Baker, Haiming Bao, and 1
others. 2025. gpt-oss-120b & gpt-oss-20b model
card. arXivpreprintarXiv:2508.10925.

=== Page 10 ===
AI-MO Team. 2024. Aimo validation set - amc sub-
set. https://huggingface.co/datasets/AI-MO/
aimo-validation-amc.
Amos Azaria and Tom Mitchell. 2023. The internal
stateofanllmknowswhenit‚Äôslying. arXivpreprint
arXiv:2304.13734.
MislavBalunovic¬¥,JasperDekoninck,IvoPetrov,Nikola
Jovanovic¬¥, and Martin Vechev. 2025. Matharena:
Evaluating llms on uncontaminated math competi-
tions.
CollinBurns,HaotianYe,DanKlein,andJacobStein-
hardt. 2022. Discovering latent knowledge in lan-
guage models without supervision. arXiv preprint
arXiv:2212.03827.
Siyi Chen, Aoran Zhang, and He He. 2024. Let‚Äôs
measureinformationstep-by-step: Llm-basedeval-
uation metrics with hidden states. arXiv preprint
arXiv:2508.05469.
GheorgheComanici,EricBieber,MikeSchaekermann,
IcePasupat,NoveenSachdeva,InderjitDhillon,Mar-
celBlistein,OriRam,DanZhang,EvanRosen,and
1others.2025. Gemini2.5: Pushingthefrontierwith
advancedreasoning,multimodality,longcontext,and
nextgenerationagenticcapabilities. arXivpreprint
arXiv:2507.06261.
Hong Duan, Yuxin Yang, and Kam-Yiu Tam. 2024.
Do llms know about hallucination? an empirical
investigationofllm‚Äôshiddenstates. arXivpreprint
arXiv:2402.09733.
JiahuiGeng,FengyuCai,YuxiaWang,HeinzKoeppl,
Preslav Nakov, and Iryna Gurevych. 2023. A sur-
veyofconfidenceestimationandcalibrationinlarge
languagemodels. arXivpreprintarXiv:2311.08298.
AmirhoseinGhasemabadi,KeithGMills,BaochunLi,
andDiNiu.2025. Guidedbygut: Efficienttest-time
scalingwithreinforcedintrinsicconfidence. arXiv
preprintarXiv:2505.20325.
Chengsong Huang, Langlin Huang, Jixuan Leng, Ji-
acheng Liu, and Jiaxin Huang. 2025a. Efficient
test-timescalingviaself-calibration. arXivpreprint
arXiv:2503.00031.
HuiHuang,YingqiQu,JingLiu,MuyunYang,BingXu,
TiejunZhao,andWenpengLu.2024. Self-evaluation
oflargelanguagemodelbasedonglass-boxfeatures.
arXivpreprintarXiv:2403.04222.
LeiHuang,WeijiangYu,WeitaoMa,WeihongZhong,
Zhangyin Feng, Haotian Wang, Qianglong Chen,
WeihuaPeng,XiaochengFeng,BingQin,and1oth-
ers.2025b. Asurveyonhallucinationinlargelan-
guagemodels: Principles,taxonomy,challenges,and
openquestions. ACMTransactionsonInformation
Systems,43(2):1‚Äì55.

Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke
Zettlemoyer.2017. triviaqa: ALargeScaleDistantly
SupervisedChallengeDatasetforReadingCompre-
hension. arXive-prints,arXiv:1705.03551.
SauravKadavath,TomConerly,AmandaAskell,Tom
Henighan, Dawn Drain, Ethan Perez, Nicholas
Schiefer,ZacHatfield-Dodds,NovaDasSarma,Eli
Tran-Johnson, and1others.2022. Languagemod-
els(mostly)knowwhattheyknow. arXivpreprint
arXiv:2207.05221.
Adam Tauman Kalai, Ofir Nachum, Santosh S Vem-
pala,andEdwinZhang.2025. Whylanguagemodels
hallucinate. arXivpreprintarXiv:2509.04664.
RyoKamoi, SarkarSnigdhaSarathiDas, RenzeLou,
Jihyun Janice Ahn, Yilun Zhao, Xiaoxin Lu, Nan
Zhang, Yusen Zhang, Ranran Haoran Zhang, Su-
jeethReddyVummanthala,and1others.2024. Eval-
uatingllmsatdetectingerrorsinllmresponses. arXiv
preprintarXiv:2404.03602.
PolinaKirichenko,MarkIbrahim,KamalikaChaudhuri,
and Samuel J Bell. 2025. Abstentionbench: Rea-
soningllmsfailonunanswerablequestions. arXiv
preprintarXiv:2506.09038.
Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Ko-
siorek, Seungjin Choi, and Yee Whye Teh. 2019.
Set transformer: A framework for attention-based
permutation-invariantneuralnetworks. InInterna-
tionalconferenceonmachinelearning,pages3744‚Äì
3753.PMLR.
ChrisYuhaoLiu,LiangZeng,YuzhenXiao,JujieHe,Ji-
acaiLiu,ChaojieWang,RuiYan,WeiShen,Fuxiang
Zhang,JiachengXu,and1others.2025. Skywork-
reward-v2: Scaling preference data curation via
human-aisynergy. arXivpreprintarXiv:2507.01352.
Saumya Malik, Valentina Pyatkin, Sander Land, Ja-
cobMorrison,NoahASmith,HannanehHajishirzi,
and Nathan Lambert. 2025. Rewardbench 2: Ad-
vancing reward model evaluation. arXiv preprint
arXiv:2506.01937.
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,
CarrollWainwright,PamelaMishkin,ChongZhang,
SandhiniAgarwal,KatarinaSlama,AlexRay,and1
others.2022. Traininglanguagemodelstofollowin-
structionswithhumanfeedback. Advancesinneural
informationprocessingsystems,35:27730‚Äì27744.
YudiPawitanandChrisHolmes.2025. Confidencein
the reasoning of large language models. Harvard
DataScienceReview,7(1).
Gaurang Sriramanan, Siddhant Bharti, Vinu Sankar
Sadasivan, Shoumik Saha, Priyatham Kattakinda,
andSoheilFeizi.2024a. Llm-check: Investigating
detection of hallucinations in large language mod-
els. InAdvancesinNeuralInformationProcessing
Systems(NeurIPS).

=== Page 11 ===
Gaurang Sriramanan, Siddhant Bharti, Vinu Sankar Aoran Zhang, Yuhan Chen, Jiamin Pan, Chen Zhao,
Sadasivan, Shoumik Saha, Priyatham Kattakinda, Ananya Panda, Jiawei Li, and He He. 2025b. Are
andSoheilFeizi.2024b. Llm-check: Investigating thehiddenstateshidingsomething? testingthelim-
detectionofhallucinationsinlargelanguagemodels. itsofllmfactualityself-evaluation. InProceedings
AdvancesinNeuralInformationProcessingSystems, of the 63rd Annual Meeting of the Association for
37:34188‚Äì34216. ComputationalLinguistics(ACL).
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel YifanZhangandTeamMath-AI.2024. Americaninvi-
Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, tationalmathematicsexamination(aime)2024.
DarioAmodei,andPaulFChristiano.2020. Learn-
YifanZhangandTeamMath-AI.2025. Americaninvi-
ingtosummarizewithhumanfeedback. Advances
tationalmathematicsexamination(aime)2025.
inneuralinformationprocessingsystems,33:3008‚Äì
3021.
ZhenruZhang,ChujieZheng,YangzhenWu,Beichen
Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jin-
Dennis Ulmer, Martin Gubri, Hwaran Lee, Sangdoo
grenZhou,andJunyangLin.2025c. Thelessonsof
Yun, and Seong Joon Oh. 2024. Calibrating large
developingprocessrewardmodelsinmathematical
languagemodelsusingtheirgenerationsonly. arXiv
reasoning. arXivpreprintarXiv:2501.07301.
preprintarXiv:2403.05973.
Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji
Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jin-
Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. gren Zhou, and Junyang Lin. 2024. Processbench:
2023. Math-shepherd:Verifyandreinforcellmsstep- Identifyingprocesserrorsinmathematicalreasoning.
by-stepwithouthumanannotations. arXivpreprint arXivpreprintarXiv:2412.06559.
arXiv:2312.08935.
Yiming Wang, Pei Zhang, Baosong Yang, Derek F.
Wong,andRuiWang.2025. Latentspacechain-of-
embedding enables output-free llm self-evaluation.
arXivpreprintarXiv:2410.13640. ICLR2025.
YuboWang,XueguangMa,GeZhang,YuanshengNi,
Abhranil Chandra, Shiguang Guo, Weiming Ren,
AaranArulraj,XuanHe,ZiyanJiang,and1others.
2024a. Mmlu-pro: Amorerobustandchallenging
multi-tasklanguageunderstandingbenchmark. Ad-
vances in Neural Information Processing Systems,
37:95266‚Äì95290.
Zhilin Wang, Alexander Bukharin, Olivier Delal-
leau, Daniel Egert, Gerald Shen, Jiaqi Zeng, Olek-
sii Kuchaiev, and Yi Dong. 2024b. Helpsteer2-
preference: Complementingratingswithpreferences.
arXivpreprintarXiv:2410.01257.
AnYang,AnfengLi,BaosongYang,BeichenZhang,
Binyuan Hui, Bo Zheng, Bowen Yu, Chang
Gao, Chengen Huang, Chenxu Lv, and 1 others.
2025. Qwen3 technical report. arXiv preprint
arXiv:2505.09388.
Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan,
Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong
Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin,
Bole Ma, Guangming Sheng, Yuxuan Tong, Chi
Zhang, Mofan Zhang, Wang Zhang, Hang Zhu,
and 16 others. 2025. Dapo: An open-source llm
reinforcement learning system at scale. Preprint,
arXiv:2503.14476.
Anqi Zhang, Yulin Chen, Jane Pan, Chen Zhao, Au-
rojit Panda, Jinyang Li, and He He. 2025a. Rea-
soning models know when they‚Äôre right: Probing
hidden states for self-verification. arXiv preprint
arXiv:2504.05419.

=== Page 12 ===
Math-Reasoning Tri
sisonG
B8-3newQ-MRkrowykS
Figure5: PredictedCorrectnessScoreDistributions. G
correct(blue)andwrong(red)answers. Incontrast,thelar
withsignificantoverlap,reflectinghigheruncertainty.
Table5: ImpactofDual-StreamArchitectureacrossBe
theFullGnosismodel.Whilehiddenstatesprovideastron
yieldsthebestperformance.
MathReason
ModelVariant InputSignals AUROC AUPR-c
BOTH(GNOSIS) Fused([z
hid
;z
attn
]) 0.95 0.95
ATTENTION-ONLY AttentionMaps(z
attn
) 0.92 0.93
HIDDEN-ONLY HiddenStates(z
hid
) 0.92 0.92
A ArchitectureOverview
Figure2illustratesthedetailedinternalcomponents
of two parallel streams that process the frozen back
efficiently:
A.1 HiddenCircuitEncoder.
ThisstreamprocessesthesequenceofhiddenstatesH
of storing intermediate states for every token, we u
choiceremainsstronglypredictive. Tohandlevariabl
the sequence is first projected and pooled into a fi
LocalTemporalFeatureMixingstage(Phase1)usin
Excitation(SE)blockstocapturelocaldependencies
SetEncoder(Phase2)utilizesaSetAttentionBlock
(PMA)toaggregatethesequenceintoacompactdes
A.2 AttentionCircuitEncoder.
This stream processes the collection of attention m
To make computation invariant to the original con
grid AÀú ‚àà Rk√ók. We then summarize each dow
‚Ñì,h
v
‚Ñì,h
= Œ¶(AÀú
‚Ñì,h
) ‚àà Rd grid. Theseper-headdescriptors
anAxialGridProcessortomodelinter-layerandinte
finalattentiondescriptorz
attn
‚àà RD ATT.
Per-Map Feature Extraction Variants. As desc
alternatives: (i)alightweightCNNthattreatseacha
patterns,and(ii)aninterpretablestatistics-basedextr

iviaQA MMLUPro
Gnosis(top)displayssharp,bimodalseparationbetween
rgerSkyworkmodel(bottom)exhibitsdiffusedistributions
enchmarks. Comparisonofsingle-streamvariantsagainst
ngsignal,fusingthemwiththeattentioncircuitconsistently
ning TriviaQA MMLU-Pro
AUPR-e AUROC AUPR-c AUPR-e AUROC AUPR-c AUPR-e
0.94 0.87 0.79 0.92 0.80 0.90 0.56
0.90 0.78 0.66 0.86 0.80 0.90 0.56
0.91 0.87 0.77 0.92 0.78 0.89 0.53
oftheGnosisMechanism. Thearchitectureconsists
kbone‚Äôs internal traces to extract reliability signals
H ‚àà RS√óD. Toavoidtheaddedcostandmemory
last
use only the final-layer hidden state. We show this
lelengthswhilemaintainingafixedcomputebudget,
fixed number of tokens. It then passes through a
ngmulti-scaledilatedconvolutionsandSqueeze-and-
andreweightinformativechannels. Finally,aGlobal
(SAB)followedbyPoolingbyMultiheadAttention
scriptorz
hid
‚àà RDHID.
maps {A } from a frozen backbone.
‚Ñì,h ‚Ñì=1..L,h=1..H
ntext length, we downsample each map to a fixed
wnsampled map into a compact per-head descriptor
sarearrangedasalayer‚Äìheadgridandprocessedby
er-headdependencies,followedbyPMAtoobtainthe
cribed in Section 3.3, we implement Œ¶ using two
attentionmapasanimageandlearnslocal-to-global
ractorthatcomputespredefinedstructuraldescriptors.

=== Page 13 ===
WecomparethesevariantsandtheirhybridinTable
competitive, the hybrid is the most consistent acro
adopt CNN+Stats as the default Gnosis configurat
interpretablealternative.
InterpretableAttentionStatistics. Thestatisticsb
localorlong-rangeitis, andwhereittendstoconc
descriptors. Concretely,foreachdownsampledmap
‚Ä¢ Entropy. Wecomputemapentropytogetherw
theoveralldispersionofattentionmass(focus
axis-specificviewsofthisdispersion,indicating
(rows)orkeypositions(columns). Together,th
attentionaldiffusionandstability.
‚Ä¢ SpectralTexture. Wecomputespectralentropy
of AÀú . These features summarize whether at
‚Ñì,h
frequencydominance)orbecomesfragmenteda
spectralentropy).
‚Ä¢ LocalityviaDiagonalStructure. Wemeasured
ratio and diagonal-band energies. This provid
routing,whichoftencorrelateswithcoherentst
‚Ä¢ CenterandSpreadontheMap. Wecompute
theaveragelocationofattentionmassandhow
Thetwodescriptorsareconcatenatedandpassedt
correctnesslogit,whichisconvertedtoaprobabilit
fixed-sizesummaries,Gnosisrunsateffectivelycon
partialchainsofthought.
B ComprehensiveAblationsandAnalysis
B.1 Hiddenvs.AttentionCircuits
Toquantifythedistinctcontributionsoftheinternal
Gnosisandcomparedthemtothefulldual-streamm
AsreportedinTable5,theHIDDEN-ONLYmode
benchmarks. On Math Reasoning, both single-stre
and z improves performance to 0.95 AUROC. O
attn
thanattentionalone,andfusionachievesthebestov
is slightly stronger than hidden-only, while fusion
attention contributes complementary structural cue
withhiddenrepresentations.
B.2 AttentionCircuitEncoderAblations
Todeterminetheoptimalarchitecturefortheattenti
designcomponents: theinputfeaturerepresentation
information across layers and heads, and the final
investigation. Weselecttheconfigurationhighlighte
balanceofaccuracyandparameterefficiency.
FeatureInputRepresentation. Wefirstassessedh
comparesalightweightlearnedCNN,predefinedstati
predefinedstatisticsremaincompetitivewiththeCN

e6. WhileCNN-onlyandStats-onlyareindividually
oss domains. Unless otherwise stated, we therefore
tion. The Stats-only variant remains a strong, more
branchsummarizeshowattentionisdistributed,how
centrateonthemap, usingasmallsetofpredefined
AÀú wecompute:
‚Ñì,h
withrowandcolumnentropies. Mapentropyreflects
sedvs.diffuse). Rowandcolumnentropiesprovide
whetherspreadisdrivenprimarilybyquerypositions
hesemetricsofferadirect,interpretablesummaryof
andtherelativeenergyfromthe2DFourierspectrum
ttention exhibits coherent, structured patterns (low-
andnoisy(elevatedhigh-frequencyenergyandhigher
diagonalandnear-diagonalmassthroughthediagonal
des a simple proxy for locality versus longer-range
tep-wisereasoningbehavior.
elightweightcenter-and-widthmeasurestodescribe
widelyitisdispersedacrossthegrid.
throughalightweightgatedMLPtoproduceascalar
tyviaasigmoid. Becausebothencodersoperateon
nstantcostinsequencelengthandcanbequeriedon
representations,wetrainedsingle-streamvariantsof
modelonQwen31.7B.
elalreadyprovidesastrongcorrectnesssignalacross
eam variants reach 0.92 AUROC, while fusing z
hid
On TriviaQA, the hidden stream remains stronger
verallperformance. OnMMLU-Pro,attention-only
matches the best result. These results confirm that
es that help maximize performance when combined
ioncircuit,wesystematicallyinvestigatedthreekey
foreachattentionmap,thegridtopologyformixing
aggregation strategy. Tables 6, 7, and 8 detail this
edinRowAofTables7and8asitachievesthebest
howbesttoencodeindividualattentionmaps. Table6
istics,andtheircombinationacrossbenchmarks. The
NN(e.g.,identical0.92AUROConMathReasoning),

=== Page 14 ===
Table6: ImpactofPer-MapFeatureExtractorChoic
predefinedstatistics,andtheircombinationfortheattentio
statisticsvariantsarecompetitive,whiletheirhybridisthe
inourfinalmodel.
MathReasoning
ModelVariant AUROC AUPR-c AUPR-e AU
CNN+STATS(FINAL) 0.92 0.93 0.90
CNN-ONLY 0.92 0.93 0.88
STATS-ONLY 0.92 0.93 0.90
Table 7: Attention Circuit: Components & Topology
late the attention-circuit design by varying grid mixing,
embeddings, and aggregation. Row A is our default c
tion. AxialConvperformslightweightrow/columnmix
the(Layer,Head)grid,offeringacheaperalternativetofu
self-attention.
ID Configuration/Change #Params
A Gnosis(Axial+PMA) 1.4M
GridTopology&Identity
B RemoveAxialConv(LinearProjOnly) 1.1M
C ReplaceAxialConvw/GlobalTransformer 4.5M
D RemoveLayer/HeadEmbeddings 1.4M
AggregationStrategy
H ReplacePMAw/MeanPool(Axial‚ÜíMean) 0.9M
while providing a more interpretable per-map repr
comparableperformanceoverallandmodestgainso
Grid Topology and Layer/Head Identity. We
extracted map features. As shown in Table 7, rem
performance,indicatingthatindividualattentionhea
matter. ReplacingourlightweightAxialConvolutions
parametersfour-foldwithoutimprovingAUROC,vali
removingthelearnedlayerandheadembeddings(R
modelreliesonknowingwhereaspecificactivationp
AggregationStrategy. Finally,weanalyzedhow
ourquery-basedPoolingbyMultiheadAttention(P
causes a sharp drop in accuracy. This suggests that
prototypes"(viaPMAseeds)ratherthanuniformlya
smallsubsetofheadscarryhigh-fidelitycorrectness
B.3 Hidden-StateCircuitEncoderAblations
Toidentifytheoptimalarchitectureforthehidden-st
studyinvestigatingfeaturedimensionality,localtem
Table9&Table10detailsthisinvestigation. Thefina
reliabilityestimation(AUROC)andcomputationale
DimensionalityandSizing. Wefirstinvestigated
ingRowsHandIagainstourproposedmodel(Row
Reducingthesizeto96causesasharpperformanced
duringtheinitialpooling. Conversely,scalingto38

cesacrossBenchmarks. ComparisonoflearnableCNN,
onper-mapextractorŒ¶. Acrossbenchmarks,theCNNand
emostconsistentoverall;weadopttheCNN+Statsdesign
TriviaQA MMLU-Pro
UROC AUPR-c AUPR-e AUROC AUPR-c AUPR-e
0.78 0.65 0.86 0.80 0.90 0.56
0.79 0.66 0.86 0.79 0.90 0.56
0.75 0.62 0.83 0.76 0.88 0.53
y. We ab- Table8: AttentionCircuit: Hyperparam-
, identity eters. Impactoflayerselectionstrideand
configura- initialmapdownsamplingsize(k ). (Pro-
grid
xingover posed: Select1every5layers;k =256).
grid
ullglobal Note: Thesechoicesmainlyaffectinference
speed/memoryratherthanparametercount.
HigherAUROCisbetter.
s AUROC
ID Variation AUROC
0.92
A Gnosis 0.92
0.84 LayerSelectionStrategy
0.92 E1 AllLayers(EveryMap) 0.91
0.90 E2 First&LastLayersOnly 0.64
DownsamplingSize(k )
grid
0.85 F1 SmallGrid(64) 0.68
F2 LargeGrid(512) 0.92
resentation. Combining CNN and statistics yields
onMMLU-Pro(0.80AUROC).
We next evaluated how to process the collection of
moving the grid mixing entirely (Row B) reduces
adsarenotindependentpredictors;theirinteractions
swithaheavyGlobalTransformer(RowC)increases
idatingtheefficiencyoftheaxialdesign. Furthermore,
RowD)degradesperformance,confirmingthatthe
patternoccurredwithintheLLM‚Äôsdepthandbreadth.
tosummarizethegridintoafixedvector. Replacing
PMA)withsimplemeanpooling(RowHinTable7)
t Gnosis benefits from learning specific "reliability
averagingallattentioncircuits,likelybecauseonlya
signals.
tatecircuit,weconductedacomprehensiveablation
mporalprocessing,andglobalaggregationstrategies.
aldesign(RowA)providesthebesttrade-offbetween
efficiency.
theinformationbottlenecksize(d ,k ). Compar-
tok hid
wA)revealsaclearperformanceplateauatsize192.
drop(‚àí0.04AUROC),likelyduetoinformationloss
84triplestheparametercountwithoutanyaccuracy

=== Page 15 ===
Table9: Hidden-stateCircuit: Components&Baseline
latethehidden-stateencoderbyisolatingtherolesoflocal
mixing(Phase1)andglobalsetaggregation(Phase2). R
ourdefaultconfiguration. RemovingPhase1(B‚ÄìD)orw
globalaggregation(E‚ÄìF)consistentlyreducesAUROC,a
plepooledMLPbaseline(G)performssubstantiallywors
ID Configuration/Change #Params AURO
A Gnosis 2.6M 0.92
Phase1:LocalProcessing
B RemovePhase1(RawSeq‚ÜíSetEnc) 2.4M 0.89
C RemoveGating&SE(Sumonly) 2.6M 0.91
D RemoveMulti-scale(Dilations‚Üí1) 2.4M 0.90
Phase2:GlobalAggregation
E RemoveSAB(CNN‚ÜíPMAonly) 1.3M 0.85
F ReplacePMAw/MeanPool 2.1M 0.89
ArchitecturalBaselines
G PooledMLP(GlobalPool‚ÜíMLP) 0.7M 0.82
gain. Wethereforefixthedimensionsto192foreffic
Local Temporal Encoder (Phase 1). We inves
necessary before global processing. Row B shows
setencoderreducesperformanceby0.04AUROC,
stage. InsidePhase1,wefoundthatarchitecturalcom
gating(RowC)orreplacingthemulti-scaledilated
bothdegradeperformance. Thissuggeststhemodel
dilation)anddynamicfeaturereweighting(viaSE/G
GlobalSetEncoder(Phase2). Finally,weanalyze
averaging the features (Row F) or removing the glo
hurts performance. This validates the use of the SA
specificreliabilityprototypes. Notably,ourhybridde
baseline(RowG).
C AdditionalExperimentalSetup
C.1 Benchmarks(extended).
Weevaluateonthreedisjointdomains. Formathrea
AMC12 2023(AI-MO Team, 2024), AIME 2024(Z
Math-AI,2025),andHMMTFebruary2025(Baluno
spanawiderangeofdifficultyandrequiremulti-step
QA, we use an 18k-question held-out trivia subset
corpusbutwithno overlapping items. This benchm
evaluateshallucinationdetectionandknowledgegr
useMMLU-Pro(Wangetal.,2024a),anout-of-distri
math,physics,law,psychology)thatcombinesdoma
broadtestofgeneralizationbeyondthetrainingmix.
C.2 Metrics(extended).
Weframecorrectnesspredictionasbinaryclassifica
AUROCmeasureshowwellamethodrankscorrectc
perfect). AUPRisreportedwithtwocomplementary
as positive and summarizes how well a method re

es. Weab- Table 10: Hidden-state Circuit: Hyper-
temporal parameters. Impact of varying Feature
RowAis Dimension (d ), Pooled Sequence Length
tok
weakening (k ),andSABDepth. (Gnosisusedsettings:
hid
andasim- d =192,k =192,SAB=3)
tok hid
se.
ID Variation #Params AUROC
OC ‚àÜ
A Gnosis 2.6M 0.92
2 ‚Äì
FeatureDimension(d )
tok
H1 SmallWidth(96) 0.8M 0.88
9 -0.03 H2 LargeWidth(384) 9.3M 0.92
1 -0.01
PooledSeqLength(k )
hid
0 -0.02
I1 ShortSeq(96) 2.6M 0.89
I2 LongSeq(384) 2.6M 0.92
5 -0.07
SABLayers(Default:3)
9 -0.03
J1 FewerLayers(1) 1.7M 0.89
J2 MoreLayers(5) 3.5M 0.93
2 -0.1
ciency.
stigated whether explicit local feature extraction is
s that feeding raw pooled sequences directly to the
confirmingthatPhase1actsasacriticaldenoising
mplexitymatters: removingtheSqueeze-and-Excite
dconvolutionswithastandardconvolution(RowD)
reliesoncapturingmulti-scaletemporalsignals(via
Gating).
edtheglobalaggregationstage. Wefoundthatsimply
obal self-attention refinement (Row E) consistently
AB+PMA stack to capture global context and learn
esignsignificantlyoutperformsasimplepooledMLP
asoning,weuseacombinedtestsetofAMC122022,
Zhang and Math-AI, 2024), AIME 2025(Zhang and
ovic¬¥ etal.,2025). Thesecompetition-styleproblems
psymbolicandnumericreasoning. Foropen-domain
t drawn from the same distribution as our training
markemphasizesshort factoidanswers anddirectly
rounding. Foracademicknowledgereasoning,we
ibutionevaluationspanning14diversedomains(e.g.,
ainknowledgewithmulti-stepreasoning,providinga
.
ationandreportbothrankingandcalibrationquality.
completionsaboveincorrectones(0.5=chance,1.0=
positiveclasses: AUPR-ctreatscorrectcompletions
ecovers correct answers with high precision across

=== Page 16 ===
Table 11: Backbone outcome rates (%) across eva
port the fraction of instances that are correct (Accurac
know‚Äù/refusal/empty). No-answercasesareshownfor
predictionevaluation(Table1).
Backbone Domain
Qwen31.7B-Hybrid Math-Reasoning
Qwen31.7B-Hybrid TriviaQA
Qwen31.7B-Hybrid MMLU-Pro
Qwen34B-Thinking-2507 Math-Reasoning
Qwen34B-Thinking-2507 TriviaQA
Qwen34B-Thinking-2507 MMLU-Pro
Qwen34B-Instruct-2507 Math-Reasoning
Qwen34B-Instruct-2507 TriviaQA
Qwen34B-Instruct-2507 MMLU-Pro
Qwen38B-Hybrid Math-Reasoning
Qwen38B-Hybrid TriviaQA
Qwen38B-Hybrid MMLU-Pro
OpenAIgpt-oss-20B Math-Reasoning
OpenAIgpt-oss-20B TriviaQA
OpenAIgpt-oss-20B MMLU-Pro
recall; AUPR-e treats incorrect completions as po
errors/hallucinations, which is often the more safe
probability quality, we report Brier Skill Score (BS
prevalencebaseline,andExpectedCalibrationError
betweenpredictedcorrectnessprobabilitiesandemp
D BackboneOutcomeStatistics
TocontextualizetheresultsinTable1,wereportthe
on the three evaluation domains. While Table 1 foc
ROC/AUPR/BSS/ECE) for different judges and in
base behavior of each backbone: how often it is c
producesnovalidfinalanswer. Importantly,non-re
butarefilteredoutandnotusedinourcorrectness
arecomputedoverthesubsetofgenerationswitha
twopurposes: (i)itclarifiestheintrinsicdifficultya
and (ii) it provides the context needed to interpret
metricsreportedinTable1,whicharemoresensitive
answered generations.
E GeminiAsjudge.
FortheLLM-as-judgebaseline,weuseGemini2.5
andamodel-generatedanswer,thejudgeproduces
scorein[0,1]attheendoftheresponse. Weparseth
GeminiviatheAPIonthethreebenchmarksreported
showsthepromptforusingGeminiasthejudge.

aluation domains. For each frozen backbone, we re-
cy), incorrect (Hallucination), or no-answer (‚ÄúI don‚Äôt
completenessbutarefilteredoutfromourcorrectness-
Backbone Backbone Backbone
Accuracy Hallucination ‚ÄúIdon‚Äôtknow‚Äù
44.87% 47.76% 7.37%
33.54% 55.67% 10.79%
66.09% 31.53% 2.39%
63.27% 25.31% 11.42%
53.73% 45.27% 0.99%
72.45% 24.75% 2.79%
57.51% 29.26% 13.22%
56.89% 42.98% 0.13%
71.69% 27.46% 0.85%
49.42% 14.16% 36.42%
62.22% 36.05% 1.74%
71.93% 22.29% 5.77%
52.53% 42.63% 4.84%
63.70% 27.67% 8.62%
68.74% 30.83% 0.44%
ositive and summarizes how well a method detects
ety-relevant viewpoint under class imbalance. For
SS), where BSS > 0 indicates improvement over a
(ECE),wherelowervaluesindicatebetteralignment
piricalaccuracy.
erawoutcomebreakdownofeachfrozenbackbone
cuses on the quality of correctness prediction (AU-
nternal methods, this table provides the underlying
correct, how often it hallucinates, and how often it
responsecasesarereportedhereforcompleteness
s-predictionevaluation. Thus,themetricsinTable1
avalid,parsableanswer. Thistablethereforeserves
andfailuremodesofeachbackboneacrossdomains,
domain-dependent shifts in AUPR and calibration
etotheunderlyingprevalenceofcorrectvs.incorrect
proasagenerativerewardmodel. Givenaquestion
abriefcorrectnessanalysisandthenemitsascalar
hescorefromtherequired<score>tags. Evaluating
dinTable1costapproximately$500intotal. Figure6

=== Page 17 ===
Figure6: Geminijudgesystempromp
SYSTEM_PROMPT
You are an objective, expert evaluator.
a proposed Answer. Determine if the An
the Question. Assign a correctness score
(perfectly correct).
IMPORTANT: Output the final score
<score>0.95</score> or <score>0.0</score>
Figure 7: 2D PCA scatter plots of features learned by G
TriviaQA, and MMLU-Pro. Columns: hidden-state fe
representation. WeshowPCAscatterplotswithKDEcon
(blue)answers. Acrossdomains,themergedfeaturespro
complementarityofhiddenandattentionsignals.

ptusedfortheLLM-as-judgebaseline.
. Your task is to review a Question and
nswer is correct, and accurate based on
e between 0.0 (completely wrong) and 1.0
e inside <score> tags. Example:
>.
Gnosis across three domains. Rows: Math-Reasoning,
eatures (z ), attention features (z ), and their merged
hid attn
ntoursandmarginaldensitiesforwrong(red)andcorrect
ovidetheclearestoverallclassseparation,illustratingthe

Paper:Confidence Estimation for LLMs in Multi-turn Interactions.pdf
=== Page 1 ===
Confidence Estimation for LL
CaiqiZhang1*,RuihanYang2*,Xiaoch
YijiangDong1,Deqing
1UniversityofCambridg
{cz391,nhc30}@cam.ac.uk1, {rhya
Abstract
Whileconfidenceestimationisapromisingdi-
rectionformitigatinghallucinationsinLarge
Language Models (LLMs), current research
dominantlyfocusesonsingle-turnsettings.The
dynamics of model confidence in multi-turn
conversations,wherecontextaccumulatesand
ambiguity is progressively resolved, remain
largely unexplored. Reliable confidence es-
timation in multi-turn settings is critical for
many downstream applications, such as au-
tonomous agents and human-in-the-loop sys-
tems. Thisworkpresentsthefirstsystematic
study of confidence estimation in multi-turn
interactions, establishing a formal evaluation
framework grounded in two key desiderata:
per-turncalibrationandmonotonicityofconfi-
denceasmoreinformationbecomesavailable.
Tofacilitatethis, weintroducenovelmetrics,
including a length-normalized Expected Cal-
ibration Error (InfoECE), and a new "Hinter-
Guesser" paradigm for generating controlled
evaluation datasets. Our experiments reveal
thatwidely-usedconfidencetechniquesstrug-
glewithcalibrationandmonotonicityinmulti-
turndialogues. WeproposeP(SUFFICIENT),a
logit-basedprobethatachievescomparatively
betterperformance,althoughthetaskremains
farfromsolved. Ourworkprovidesafounda-
tionalmethodologyfordevelopingmorereli-
ableandtrustworthyconversationalagents.
1 Introduction
Large Language Models (LLMs) have shown re-
markable capabilities in multi-turn dialogue, col-
laboratingwithusersoncomplextasks(Wangetal.,
2023;Yietal.,2024;Labanetal.,2025). Yettheir
tendencyto‚Äúhallucinate‚Äù(i.e.,producingincorrect
statementswithhighapparentcertainty)remainsa
majorobstacleforhigh-stakesuse(Manakuletal.,
2023;Zhangetal.,2024a;Shelmanovetal.,2025;
Hu et al., 2025). Confidence estimation, which
*Equalcontribution.CodesanddatacanbefoundinGitHub.
1
6202
naJ
5
]LC.sc[
1v97120.1062:viXra

LMs in Multi-turn Interactions
henZhu1,ChengzuLi1,TianchengHu1,
gYang2,NigelCollier1
ge 2FudanUniversity
ang17,yangdeqing}@fudan.edu.cn2,
1.0
0.8
0.6
0.4
0.2
0.0
0 20 40 60 80 100
Information Level (%)
ecnedifnoC
egarevA
Vanilla-Verb SC P(sufficient)
Cot-Verb P(true) Accuracy
1.0
0.8
0.6
0.4
0.2
0.0
Average
Accuracy
Figure1: InfoECEonGUESS(Llama3.1-70B).Ideally,
confidence(bluecurves)increasesasmoreinformation
isprovided. Calibrationimproveswhentheconfidence
curves(blue)areclosertotheaccuracycurve(red). In
thissetting, P(SUFFICIENT) bestsatisfiesbothmono-
tonicityandcalibration.
aimstopredictthelikelihoodthatamodel‚Äôsanswer
iscorrect,hasaccordinglybecomeapromisingdi-
rectionforidentifyingandmitigatingsuchfailures
(Zhangetal.,2025b;Yangetal.,2025a,b).
Despiterecentprogress,mostpriorworkstudies
confidenceinsingle-turnquestionanswering(Tian
etal.,2023;Xiongetal.,2024),astaticsetupthat
overlookstheinherentlydynamicnatureofreal
human‚ÄìAIinteraction. Inmulti-turnconversations,
informationarrivesincrementally: usersrefinetheir
queries, models ask clarifying questions, and the
hypothesisspacenarrowsturnbyturn. Insuchset-
tings,confidenceshouldnotbeafixedattributeof
a solitary response but a signal that evolves with
the dialogue‚Äîideally increasing as ambiguity is
resolvedandevidenceaccumulates. Reliablecon-
fidenceestimationinthisprogressionistherefore
critical,asitservesasadecision-makingheuristic
forwhentoaskclarifyingquestions,invoketools,
orcommittoactionsinagenticworkflowsandhu-
man‚ÄìAIcollaboration. However,howwellcurrent
methodstrackthisprogressionislargelyunknown.
Toaddressthisgap,wepresentthefirstsystem-
1

=== Page 2 ===
atic study of confidence estimation in multi-turn
conversations. We introduce a novel evaluation
framework in which the model receives progres-
sively more task-relevant information. We argue
thatinthiscontrolledsetting,areliableconfidence
signalshouldsatisfytwodesiderata: (1)Calibra-
tion,wheretheconfidenceaccuratelyreflectsem-
piricalcorrectnessatanygiventurn,and(2)Mono-
tonicity,whereconfidenceconsistentlyincreases
asmoreusefulinformationbecomesavailable.
Guided by these desiderata, we develop new
metricsanddatasetstailoredtoconfidenceestima-
tioninmulti-turnsettings. Tomeasurecalibration
acrossdialoguesofvaryinglengths,weintroduce
alength-normalizedExpectedCalibrationErrorat
information level (InfoECE). To quantify mono-
tonicity,weemployKendall‚ÄôsœÑ,anon-parametric
rank correlation coefficient. We establish evalua-
tion testbeds for two distinct regimes: (1) under-
specifiedinitialqueries,forwhichweintroducea
novel‚ÄúHinter‚ÄìGuesser"paradigmtogeneratedia-
logues with progressively revealed clues; and (2)
difficult but fully-specified queries, for which we
adaptexistingincrementalQAbenchmarks(Wal-
lace et al., 2019; Sung et al., 2025) that provide
sequentialhintstowardthecorrectanswer.
Ourexperimentsevaluateasuiteofconfidence
estimationmethodsacrossfouropen-sourcemod-
els(¬ß5.1),revealingkeyinsightsintotheirmulti-
turn behavior. First (¬ß 5.2), we find that widely
usedtechniquesstruggletomaintaincalibrationor
exhibit consistent monotonicity as conversations
progress,asillustratedinFigure1. Ourproposed
method, P(SUFFICIENT), proves comparatively
moreeffective;however,substantialroomforim-
provementremains. Meanwhile,wefindthatmod-
elsexhibitstrongermonotonicitywhenconfidence
isevaluatedagainsttheground-truthanswerrather
than the model‚Äôs provisional answer at each turn.
Second(¬ß5.3),weexaminewhetherconfidencein-
creasesaredrivenbyaddedinformationormerely
by turn count. P(SUFFICIENT) more effectively
distinguishesmeaningfulinformationgainsfrom
conversationalfiller. Finally(¬ß5.4),ouranalysis
reveals that while model accuracy is comparable
betweenmulti-turndialoguesandsingle-turnsum-
maries,theconfidencesignalsbehaveverydiffer-
ently,underscoringthattheinteractivestructureof
thedialogueiscrucialtomodels‚Äôconfidenceesti-
mation. Overall,ourfindingshighlightmulti-turn
confidence as a distinct and necessary target for
reliable,decision-orientedLLMbehavior.
2

2 RelatedWork
ConfidenceEstimationinLLMs. Confidenceand
uncertaintyestimationhasbeenextensivelystudied
in LLMs (Geng et al., 2024). More specifically,
uncertainty reflects the variability in the model‚Äôs
predictionsgivenonlytheinputquery,whileconfi-
denceisdefinedwithrespecttoboththeinputand
thespecificgeneratedoutput,indicatinghowcer-
tainthemodelisaboutthatparticularresponse(Lin
et al., 2023; Zhang et al., 2024b, 2025a). Main-
streamconfidenceestimationapproachesinclude
prompting-based(verbalized)methods(Tianetal.,
2023;Dongetal.,2024),consistency-basedmeth-
ods(Manakuletal.,2023;Zhangetal.,2024b),and
logit-basedmethods(Kadavathetal.,2022). These
methodshavebeenappliedtovarioustasks,such
as short-form factual QA (Tian et al., 2023; Lin
et al., 2023), long-form factual QA (Zhang et al.,
2024b,c,2025b),andreasoningtasks(Zhangetal.,
2025a;ZhangandZhang,2025). However,amajor
limitationofexistingworksistheirfocusonsingle-
turn settings. The effectiveness of confidence es-
timation in multi-turn conversations remains un-
derexplored(Kirchhofetal.,2025),wheremodel
confidence may evolve dynamically throughout
the interaction. Our work aims to fill this gap by
systematicallyevaluatingexistingconfidenceesti-
mationmethodsandproposingnovelapproaches
inmulti-turncontexts.
LLMsinMulti-turnInteractions. Therehasbeen
growing interest in studying LLMs in multi-turn
scenarios (Laban et al., 2025; Zhu et al., 2025).
Modern LLMs support interactive dialogue, en-
ablinguserstocollaboratewiththemodelacross
multipleturnstoaccomplishcomplextasks. How-
ever,recentstudiesshowthatLLMsoftenperform
significantlyworseonthesametaskswhenframed
in a multi-turn context compared to a single-turn
setting (Laban et al., 2025). Laban et al. (2025)
also point out that many prior works (Bai et al.,
2024;Kwanetal.,2024;Duanetal.,2024)simu-
lateepisodicconversations,whereeachturnintro-
ducesasubtaskrelatedtopreviousturnsbutcanbe
evaluatedinisolation. Underthisframing, multi-
turntasksdifferstructurallyfromsingle-turntasks
andarenotevaluatedonthesamesetofquestions.
Labanetal.(2025)arguethatepisodictaskstend
to overestimate LLM performance in multi-turn
settingsandconstructasharded dataconstruction
method. Inourwork,wefollowasimilarsharded
questionconstructionstrategy. Foreachquestion,
2

=== Page 3 ===
wecreatemultiplevariantswithincreasinglevelsof
contextualinformationprovidedacrossturns. This
allows us to directly compare confidence estima-
tionmethodsundervaryinglevelsofcomplexity.
3 Methodology
3.1 Notations
We study confidence estimation in multi-turn di-
alogue between a user and an LLM. Dialogs are
indexed by d ‚àà {1,...,N} and have L turns,
d
where L may differ across dialogs. At turn
d
i ‚àà {1,...,L },letthedialoguehistorybe
d
h = {q ,a ,...,q ,a }.
d,i d,1 d,1 d,i‚àí1 d,i‚àí1
Theturn-ipromptconsistsofthetaskdescription
T andthehistoryh . ModelM returnsananswer
d,i
yÀÜ andaconfidencec ‚àà [0,1]. Eachdialoghas
d,i d,i
onegoldlabely ,andwerecordcorrectness
d
z = I[yÀÜ = y ].
d,i d,i d
Task characteristics. We design a controlled
task that should exhibit three key properties: C1:
Progressive Information Acquisition. Each turn
reveals additional task-relevant information that
narrowsthehypothesisspaceorsupportsstep-by-
stepreasoning. C2: Step-wiseAnswerabilityand
Evaluation. Ateveryturnthemodeloutputsanan-
swerandaconfidence,enablingper-turnaccuracy
andcalibrationassessment. C3: MonotonicCon-
fidence Progression. Confidence should increase
with the turn index and align more closely with
true accuracy, providing a usable reliability sig-
nal. Thesepropertiesyieldacontrollabletestbed
in which information strictly accumulates across
turns, avoiding the limitations of episodic, non-
progressiveinteractions.
3.2 TwoInitial-QuestionRegimes
Basedonthetaskcharacteristics,weconsidertwo
regimesdefinedbythecompletenessoftheinitial
question. (1)Under-specified: Theinitialquestion
q admitsmanyplausibleanswers. Hintsprogres-
d,1
sivelyprunethecandidateset. (2)Fully-specified
butdifficult: Theinitialquestionq pinpointsa
d,1
unique answer in principle, but is hard to answer
duetothemodels‚Äôknowledgelimitationorreason-
ingability. Hintsmakesolvingeasier.
3.3 EvaluationProtocolandMetrics
Asadialogueprogresses,laterturnsincludeatleast
asmuchinformationasearlierones. Therefore,a
usefulconfidencesignalshouldhaveatleast:
3

a) Per-level calibration: within the same (nor-
malized) information level, average confi-
dencematchesempiricalaccuracy.
b) Monotonicity: typicallyc ‚â• c ;
d,i+1 d,i
Becausedialoguelengthsvary,wefirstnormal-
izeturniofdialoguedtoafractionalinformation
level
i
s = ‚àà (0,1].
d,i
L
d
whereL isthenumberofturnsindialogued. We
d
then partition [0,1] into B bins {S }B (either
b b=1
equal-widthorequal-mass)andindexallturnpo-
sitionsbyI = {(d,i) : 1 ‚â§ i ‚â§ L }.Thesubset
d
ofindiceswhosenormalizedlevelfallsintobinb
isI = {(d,i) ‚àà I : s ‚àà S }.Foreachinforma-
b d,i b
tion level b, the average confidence and accuracy
are
1 (cid:88) 1 (cid:88)
conf = c , acc = z ,
b |I | d,i b |I | d,i
b b
(d,i)‚ààIb (d,i)‚ààIb
where c ‚àà [0,1] is the model‚Äôs per-turn con-
d,i
fidence andz ‚àà {0,1} indicates correctnessof
d,i
theturn-ianswerindialogued.
Information-levelECE(InfoECE).Wecompute
aninformation-levelECEthatgroupspredictions
bynormalizedinformationexposure,enablingfair
calibrationcomparisonsacrossdialoguesofdiffer-
entlengths.
B
1 (cid:88)(cid:12) (cid:12)
InfoECE=
B
(cid:12)acc
b
‚àíconf b(cid:12).
b=1
Kendall‚ÄôsœÑ. Kendall‚ÄôsœÑ measuresthepairwise
monotonictrendofconfidenceoverturns. Fordia-
logd,considerall
(cid:0)L
d
(cid:1)
pairs(i < j): apairiscon-
2
cordantifc > c anddiscordantifc < c
d,j d,i d,j d,i
(tiesignored).
œÑ(d) = N c ( o d n )‚àíN d (d is ) , œÑ = 1 (cid:88) N œÑ(d).
(cid:0)Ld (cid:1) N
2 d=1
Values lie in [‚àí1,1]: 1 means strictly increasing
confidences,and0nooveralltrend.
3.4 ConfidenceEstimationMethods
Giventhediversityofconfidenceestimationmeth-
ods,wefocusonthefollowingthreerepresentative
categories. Weexplicitlyexcludepost-hoccalibra-
tiontechniquesinthisstudy(e.g.,Plattscalingand
temperaturescaling),astheyrepresentorthogonal
researchdirections,whichaimtorescalethecon-
fidence scores using statistical procedures (Zhou
etal.,2025;Zhangetal.,2024c).
3

=== Page 4 ===
Verbalizedconfidence. Weapplytwoverbalized
promptingstrategies(Tianetal.,2023)toelicitcon-
fidencescoresdirectlyfromthemodel(seeprompts
inAppendixA):
a) VANILLA-VERB: Givenacandidateanswer,
themodelisrequiredtoself-reportconfidence
in[0,100];rescaleto[0,1]forc .
d,i
b) COT-VERB: DifferentfromVANILLA-VERB,
themodelisnowrequiredtothinkstepbystep
before given the self-reported confidence in
[0,100];rescaleto[0,1].
Self-consistency (SC). Given the question, we
independently sample m (e.g., 20) answers
(1) (m)
a ,...,a . For any answer a, we define the
d,i d,i
confidenceasthefractionofsamplesthatmatcha:
m
c = 1 (cid:88) I[a (j) = a].
d,i m d,i
j=1
Logit-basedprobes. Weleverageinternalmodel
signals to estimate prediction confidence (see
promptsinAppendixA).(1)P(TRUE) (Kadavath
et al., 2022): At step i, given the prompt p , we
i
firstelicittheanswera frommodelM. Weforce
i
abinarychoiceA.Truevs.B.Falsewithoutput
constrainedtoasingleuppercaseletter. Thecon-
fidence score is the model‚Äôs softmax probability
assignedtolabelA.
Unlike P(TRUE), which asks if the answer is
correct,weproposeanewmethodthatprobesthe
confidencebyaskingmodelifthecurrentinforma-
tionissufficient(P(SUFFICIENT))toentailthatan-
sweraistheonlycorrectanswer. P(SUFFICIENT)
works particular well in our under-specified set-
tings, where the set of plausible answers shrinks
with each turn. This method allows the model to
express low confidence even if its current guess
happenstobecorrect,aslongasothercandidates
havenotyetbeenruledoutbytheprovidedhints.
Thisalignstheconfidencescoremorecloselywith
the true identifiability of the answer from the ac-
cumulated evidence, rather than mere incidental
correctness.
Set c = P (d,i) or c = P (d,i), we ask
d,i T d,i S
themodeltwobinaryprobesaboutyÀÜ underp :
d,i d,i
P
T
(d,i) = Pr[A | p
d,i
,yÀÜ
d,i
; P(TRUE)],
P
S
(d,i) = Pr[A | p
d,i
,yÀÜ
d,i
; P(SUFFICIENT)].
4

4 DatasetConstruction
For under-specified (initially many plausible an-
swers)regime,weconstructourowndatasetswith
20Q andGUESS(asshowninTable1). Forfully-
specified (auniqueanswerexistsfromthestartbut
is hard to infer until sufficient evidence accumu-
lates)regime,wedirectlyapplyexistingdatasets:
GRACE(Sungetal.,2025)andTRICKME(Wallace
etal.,2019)(examplesinTable7inAppendix).
4.1 Under-specifiedDatasets
We primarily leverage 20Q and Guess-my-City
(GUESS)-style(Abdulhaietal.,2023)settingsfor
the under-specified regime. In both, an answerer
holdsasecretentity(anentityfor20Q;acityfor
GUESS). The questioner incrementally seeks in-
formation to recover the secret entity. The key
differenceisthat 20Q constrainsthequestionerto
yes/no questions, whereas GUESS permits open-
ended questions. This incremental, information-
seekinginteractionnaturallysatisfiesC1. Crucially,
thesettingalsoenablesC2: ateveryturntheques-
tioner can issue a concrete guess, even when in-
formationisstillincomplete. Thiscontrastswith
mathproblemsettings(e.g.,Labanetal.(2025)),
whereintermediateturnsoftenlacktheconditions
required to score correctness, impeding per-turn
accuracyassessment. However,naivelysimulating
twoLLMstoplaytherolesofquestionerandan-
swerercanviolateC3. Earlyturnsmaycontainir-
relevantormisleadingquestions,yieldingstagnant
orevendecreasingconfidence. Toaddressthis,we
reformulatetheinteractionintoaHinter‚ÄìGuesser
paradigmthatstructurestheinformationflowwhile
retaininguncertainty.
Hinter‚ÄìGuesser Paradigm. (1) QA Stage. A
Hinter(LLM)isassignedasecretentityandmust
provide,ateachturn,ahelpfulbutnon-trivialhint.
AGuesserthen(i)makesabest-guessanswerand
(ii)flagswhethermultipleanswersremainplausi-
ble (uniqueness probing). (2) Uniqueness Prob-
ing. Even when the Guesser‚Äôs answer is correct,
the Guesser indicates if other candidates still fit
the evidence. This distinguishes a coincidentally
rightguessfromthemomenttheanswerbecomes
suficientlysupportedbytheclues,aligningconfi-
dence with identifiability rather than chance. (3)
Stopping&Filtering. Thedialogueproceedsun-
tiltheGuesserbothanswerscorrectlyandcertifies
uniqueness. We retain only successful dialogues
(eventuallysolvable)anddiscardtrajectoriesthat
4

=== Page 5 ===
Dataset ExemplarPromptatTurn4
User:Giventhefollowinginformation,prov
lastquestionfragment.Ifunsure,provideyo
somecluesabouttheanswer:
20Q Assistant:Isithuman-made?User:Yes
Assistant:Isittypicallyfoundindoors?Use
Assistant:Isitcommonlyencounteredinliv
Assistant:Isitlargerthanabook?User:Ye
User:Nowguesswhatitis:
Keyword: television
User:Giventhefollowinginformation,nam
yourbestguess.Theanswershouldbeconci
Assistant:Whatcontinentisthecityin?Use
GUESS Assistant:Isthecitycoastalorinland?User
Assistant:What‚Äôstheclimatelikeinthecity
Assistant:Whatregionwithinthecontinent
User:Nowguesswhatitis:
Keyword: Bogor,Indonesia
Table1:Examplesfromthe20QandGUESSdatasets.Idea
throughouttheconversation. Notethattheconversationh
remainsfixedforconfidenceevaluation(i.e.,nostrategic
fail to converge. In total, we collected 1,848 dia-
logueturnsfrom20Q,spanning226entities. For
GUESS,wecollected1,625turnsspanning223en-
tities. Then during confidence evaluation, since
theconversationsarefixed,modelsdonotperform
anystrategicdecision-making; instead,theyonly
makeguessesandpredictconfidencescores.
4.2 Fully-specifiedDatasets
We select two incremental, quizbowl-style QA
datasets: GRACE(Sungetal.,2025)andTRICKME
(Wallaceetal.,2019),wherecluesbecomeincreas-
inglyspecificandauniquegoldanswerexistsfrom
theoutset. GRACEandTRICKMEdirectlysupport-
ing C1‚ÄìC3 as evidence strengthens. We follow
their standard protocols without modification, re-
port per-turn accuracy and confidence, and defer
detailstotheAppendixB.
5 Experiments
5.1 Setup
Models. In our experiments, we use Llama3.1
Instruct(8Band70B)(Meta,2024),Qwen2.5In-
struct(8Band72B)(Yangetal.,2024). Tempera-
tureissetto1forsamplingandotherwise0.
ConfidenceEstimation. For afair comparison
acrossmethodsandmodels,wefirstletthemodel
answerthequestiononcetoobtainananswera. We
compareawiththegroundtruthanswerandlabel
it correct or not. For each confidence estimation
method, we then estimate the model confidence
5

videthetitleoftheWikipediapagethatbestanswersthe
ourbestguess.Theanswershouldbeconcise.Youhave
er:Yes
vingrooms?User:Yes
es
methesingleCITYthatbestfitsthem.Ifunsure,provide
ise.Youhavesomecluesabouttheanswer:
er:Asia
r:Inland
y?User:Tropical
isthecitylocated?User:SoutheastAsia
ally,themodel‚Äôsconfidenceshouldincreasemonotonically
historyiscollectedfromourHinter-Guesserpipelineand
information-gatheringisinvolvedduringevaluation).
inthisanswera; inparallel, wealsoestimatethe
model‚Äôsconfidenceateachturnwithrespecttothe
ground-truthanswer.
ControllingforConversationalLength. Acore
hypothesisofourworkisthatareliableconfidence
signalshouldincreasemonotonicallyasmoretask-
relevantinformationbecomesavailable. However,
thistrendcouldbeasuperficialartifactofdialogue
length,wheremodelsbecomemoreconfidentsim-
ply because the turn index i is higher. To disen-
tangle these factors, we design the following ex-
periment: For a given dialogue d at turn i, we
create an adversarial condition by replacing the
originalinformativehintwithaplaceboQApair
thataddsconversationalhistorywithoutrevealing
task-relevantinformation(e.g.,Q:‚ÄúIsthisavalid
hint?‚Äù A: ‚ÄúYes.‚Äù). We then compare the model
accuracyandconfidenceacrossthreestates:
1. Baseline(turni‚àí1): Themodel‚Äôsprediction
andconfidencegivenhistoryh .
d,i‚àí1
2. Original(turni): Predictionandconfidence
afterprocessingtheoriginalinformativehint
fromturni.
3. Placebo(turni‚Ä≤): Predictionandconfidence
afterprocessingh followedbytheunin-
d,i‚àí1
formativeplacebohint.
Ifconfidencemethodsarerobustlytrackinginfor-
mation,weexpectasignificantincreaseinaccuracy
and confidence from the baseline to the original
state. Incontrast,thetransitionfromthebaseline
5

=== Page 6 ===
20Q G
Method
InfoECE œÑ InfoECE
b8-1.3amaLL Accuracy 24.95 1
VANILLA-VERB 67.82 -6.36 74.89
COT-VERB 63.75 46.97 70.28
SC 18.05 36.73 38.14
P(TRUE) 69.02 42.10 67.08
P(SUFFICIENT) 41.08 38.57 35.17
b07-1.3amalL Accuracy 33.87 1
VANILLA-VERB 59.63 17.60 65.52
COT-VERB 58.39 34.49 70.16
SC 32.99 28.98 56.88
P(TRUE) 67.82 40.82 79.97
P(SUFFICIENT) 13.05 48.43 5.27
b7-5.2newQ
Accuracy 25.22 1
VANILLA-VERB 58.05 61.13 48.68
COT-VERB 64.93 52.60 71.84
SC 45.44 13.85 50.53
P(TRUE) 46.68 47.16 37.86
P(SUFFICIENT) 36.64 55.24 26.63
b27-5.2newQ Accuracy 32.36 1
VANILLA-VERB 47.92 67.97 67.04
COT-VERB 51.43 72.33 64.63
SC 45.69 28.90 68.93
P(TRUE) 42.12 68.88 57.56
P(SUFFICIENT) 45.86 66.81 28.32
Table2: InfoECEandœÑ acrossmodelsanddatasets. N
to the adversarial state should yield a negligible
change,despitetheadditionalturn.
Multi-turnvs.Single-turn. Labanetal.(2025)
suggestthatLLMscan‚Äúgetlost‚Äùinmulti-turncon-
versations,performingworsethanwhenallinfor-
mationispresentedinasingleturn. Weinvestigate
whetherthisphenomenonholdsinourprogressive
information-seeking setting. For each turn i in a
dialogued,wedefinetwoconditions:
1. Multi-turn: The model is prompted with
the full dialogue history up to that point,
h , which includes the sequence of hints
d,i
{q ,a ,...,q ,a } preceding the
d,1 d,1 d,i‚àí1 d,i‚àí1
currentqueryq .
d,i
2. Single-turn: Wecreateasinglepromptcon-
taining a concise summary S that synthe-
d,i
sizesallinformationfromthehintsprovided
uptoturni.
Wethencompareaccuracyz andconfidencec
d,i d,i
underbothconditions. Ifmodelsperformsignifi-
cantlybetterinthesingle-turncondition,itwould
suggestacognitiveburdeninintegratinginforma-
tionincrementally. Conversely,comparableorsu-
periorperformanceinthemulti-turnsettingwould
indicatethatourstructured,progressiveframework
effectivelyguidesthemodel.
5.2 Howreliableareconfidenceestimation
methodsinmulti-turnsettings?
Weassessreliabilityalongtwoaxes: 1)per-level
calibration using InfoECE (from 0 to 1, lower is
6

GUESS GRACE TRICKME
E œÑ InfoECE œÑ InfoECE œÑ
14.52 35.73 41.91
-6.58 51.00 52.21 59.26 54.55
37.84 45.21 43.25 87.70 48.63
9.43 10.57 52.40 18.97 55.37
19.91 50.43 48.48 55.61 52.37
68.51 23.77 53.94 33.74 58.34
18.58 48.27 53.75
16.92 39.06 47.13 47.47 44.49
18.24 96.04 61.30 80.97 57.27
2.59 15.91 41.36 19.90 38.26
3.29 37.04 58.94 35.62 64.25
81.51 11.52 66.86 23.16 71.38
12.92 27.34 34.06
26.99 50.40 55.55 54.62 56.39
56.01 66.20 50.38 65.37 49.89
36.10 32.78 40.16 33.28 42.54
22.04 35.15 44.04 39.45 51.00
51.44 28.67 47.79 35.26 52.11
16.12 47.49 53.88
52.81 43.18 72.59 41.62 71.32
79.00 46.49 73.04 43.50 70.35
12.52 32.38 49.17 36.50 48.52
54.87 31.86 64.02 32.87 69.28
83.76 32.93 66.04 32.41 71.24
Numbersareinpercentagesandbestresultsarebolded.
better); 2) monotonicity of confidence over turns
using Kendall‚Äôs œÑ (from -1 to 1, higher is better).
We report œÑ both on the model‚Äôs current answer
andonthegoldanswerateachturn(Table2). Fig-
ure2visualizehowaverageconfidenceandaccu-
racyevolveasinformationaccumulatesthroughout
thequestion-answeringprocess.
Calibration is generally poor and sufficiency
probeshelpthemost. Acrossallmodels, both
verbalized-based confidence (VANILLA-VERB,
COT-VERB)andlogit-basedP(TRUE)arepoorly
calibrated,withInfoECEvaluestypicallybetween
40 and 80. Self-consistency (SC) is usually the
mostcalibratedonfully-specifiedincrementalQA.
Inunder-specifiedgames,sufficiencyprobingcan
bestrikinglybetter-calibrated: forLlama3.1-70B,
InfoECE drops to 13.05 on 20Q and 5.27 on
GUESS, while maintaining competitive perfor-
mance on GRACE and TRICKME. Overall, SC
is a strong default for calibration. When the an-
swerspaceisprunedgradually, P(SUFFICIENT) is
amoreefficientalternative. Itnarrowsthegapand
sometimessurpasses SC.
Monotonicityonthecurrentanswer: sufficiency
is usually best. Ideally, confidence should rise
as clues accumulate. P(SUFFICIENT) most con-
sistently follows this trend: e.g., œÑ = 83.76 on
GUESS with Qwen2.5-72B and œÑ = 71.38 on
TRICKME with Llama3.1-70B. In contrast, SC
oftenshowsweakmonotonicityinunder-specified
settings(evensingledigitson GUESS). Thereare
6

=== Page 7 ===
1.0
0.8
0.6
0.4
0.2
0.0
Q02
ecnedifnoC
Llama3.1-8B Llama3.1-70B
1.0
0.8
0.6
0.4
0.2
0.0
sseuG
ecnedifnoC
1.0
0.8
0.6
0.4
0.2
0.0
ecarG
ecnedifnoC
1.0
0.8
0.6
0.4
0.2
0.0
0 20 40 60 80 100
Information Level (%)
eMkcirT
ecnedifnoC
Vanilla-Verb Cot-Verb SC
0 20 40 60 80 10
Information Level (%)
Figure2: Evolutionofaverageconfidenceandaccuracya
y-axis,redline)generallyincreases,theconfidencemetri
LLama3.1-8b Llama3.1-70b
Vanilla-Verb 37.64 46.86 49.56 52.30 43.73 -11.16 60.51 56.75
CoT-Verb 63.90 41.51 59.32 60.85 35.95 1.64 67.40 53.89
SC 63.05 59.79 68.33 62.53 66.13 62.77 57.14 49.49
P(true) 72.60 35.05 64.92 64.06 62.61 26.01 71.27 67.70
P(sufficient) 83.79 83.73 71.62 72.67 91.62 86.55 85.90 79.73
20Q GUESS GRACE TRICKME 20Q GUESS GRACE TRICKME
Figure3: Kendall‚ÄôsœÑ forgroundtruthanswers. Compa
substantiallybettermonotonicity. Allvaluesareshownas
model-family specific exceptions: with Qwen2.5
models, verbalized confidence (VANILLA-VERB
orCOT-VERB)occasionallyattainsthehighestœÑ
scoreson20QandGRACE,despitetheirgenerally
poorcalibration.
Monotonicityonthegroundtruth: largegains
andclearleaders. AsshowninFigure3,when
confidenceisevaluatedagainstthegroundtruthat
eachturn,allmethodsshowsubstantialincreasesin
œÑ. Althoughthegroundtruthisunavailableinreal-
worldapplications,thistrendsuggeststhatmodels
can partially recognize when current hints align
with the correct answer. P(SUFFICIENT) domi-
nates here, achieving œÑ = 93.91 on GUESS with
Qwen2.5-72B, and œÑ = 91.62, 86.55, 85.90 on
7

Qwen2.5-7B Qwen2.5-72B
1.0
0.8
0.6
0.4
0.2
0.0
00 0 20 40 60 80 100 0 20 40 60 80 100
) Information Level (%) Information Level (%)
Accuracy
1.0
0.8
0.6
0.4
0.2
0.0
Accuracy
1.0
0.8
0.6
0.4
0.2
0.0
Accuracy
1.0
0.8
0.6
0.4
0.2
0.0
Accuracy
P(true) P(sufficient) Accuracy
acrossdifferentinformationlevels. Whileaccuracy(right
ics(lefty-axis,blueline)exhibitvaryingtrends.
Qwen2.5-7b Qwen2.5-72b
5 76.43 47.81 77.45 74.68 77.70 74.09 78.98 78.92
75
9 65.83 53.68 63.78 56.15 83.08 85.79 80.24 72.68
50
9 60.91 56.25 66.53 59.28 64.85 60.66 69.02 61.19
25
0 56.84 38.91 62.32 60.93 80.17 71.11 79.13 78.24
3 79.37 65.37 70.50 72.15 90.10 93.91 81.44 77.78 0
20Q GUESS GRACE TRICKME 20Q GUESS GRACE TRICKME
aredtotheœÑ foreachturn‚Äôsanswers, allmethodsshow
spercentages.
20Q, GUESS, and GRACE with Llama3.1-70B,
respectively. VANILLA-VERB can occasionally
matchoredgeoutonspecificpairs(e.g.,Qwen2.5
on 20Q),butitremainspoorlycalibrated.
ScalingandModelFamilyEffects. Asparam-
eters increase, we observe a consistent rise in ac-
curacy and a marked improvement in œÑ (ranking
calibration), particularly for the P(SUFFICIENT).
Forinstance,Qwen2.5-72Bachievesthehighest
œÑ score of 83.76% on the GUESS dataset, signifi-
cantlyoutperformingits7Bcounterpart. However,
the effect on INFOECE is more nuanced; while
largermodelsgenerallyprovidemorereliablerank-
ings,smallermodelsoccasionallyexhibitlowerab-
solutecalibrationerrorsinspecificconfigurations.
7

=== Page 8 ===
20Q
Method
Confi‚àí1 Confplacebo,i
b8-1.3amaLL VANILLA-VERB 90.69 88.42(2.27‚Üì) 92
COT-VERB 85.20 85.18(0.02‚Üì) 88
SC 39.47 44.29(4.82‚Üë) 45
P(TRUE) 91.51 89.74(1.77‚Üì) 94
P(SUFFICIENT) 52.15 49.21(2.94‚Üì) 66
b07-1.3amalL VANILLA-VERB 87.30 84.03(3.27‚Üì) 85
COT-VERB 85.11 84.29(0.82‚Üì) 86
SC 65.49 62.21(3.28‚Üì) 63
P(TRUE) 95.48 93.43(2.05‚Üì) 94
P(SUFFICIENT) 19.95 15.27(4.68‚Üì) 33
b7-5.2newQ VANILLA-VERB 86.31 80.97(5.34‚Üì) 86
COT-VERB 89.36 85.13(4.23‚Üì) 92
SC 66.73 72.52(5.79‚Üë) 69
P(TRUE) 81.17 68.24(12.93‚Üì) 77
P(SUFFICIENT) 46.66 39.44(7.22‚Üì) 67
b27-5.2newQ VANILLA-VERB 77.90 81.31(3.41‚Üë) 87
COT-VERB 82.26 84.42(2.16‚Üë) 88
SC 80.71 76.73(3.98‚Üì) 79
P(TRUE) 80.65 81.75(1.10‚Üë) 85
P(SUFFICIENT) 80.46 79.82(0.64‚Üì) 81
Table3: Averageconfidencecomparisonacrossdifferen
changerelativetoConf . Underlinedvaluesindicatest
i‚àí1
5.3 Doesconfidencetrackinformationorjust
turncount?
We test whether confidence increases are driven
byaccumulatinginformationoraremerelyanar-
tifactofconversationallength. Asdetailedinour
experimentalsetupandshowninTable3,wecom-
pare a baseline confidence (turn i ‚àí 1) with the
confidence after receiving either an informative
hint(Original)oranon-informativeplacebohint.
Acrossall40comparisons,informativeturnsyield
moresignificantchangesthanplacebos(27vs.18
withp<0.05). Amongthem,P(SUFFICIENT)most
cleanly disentangles information gain from mere
turnaccumulation.
Sufficiencyprobesactivelypenalizeuninforma-
tiveturns,trackingevidenceoverlength. The
P(SUFFICIENT) methodprovestobethemostro-
bustbyactivelypenalizinguninformativeturns. It
frequentlyshowsastatisticallysignificantdecrease
inconfidenceafteraplacebohint(e.g.,adropfrom
14.27to2.97forLlama3.1-70Bon GUESS). This
behavior, where the model lowers its sufficiency
assessmentinresponsetoauselesshint,confirms
it is tracking evidence, not just turn count. As a
result,itachievestheclearestseparationbetween
conditions: confidence decreases or remains flat
withaplacebo,butincreaseswithaninformative
hint. Incontrast,othermethods,particularlyverbal-
izedones,canbemisledintoincreasingconfidence
simplybecausetheconversationhasprogressed.
8

GUESS
Confi Confi‚àí1 Confplacebo,i Confi
2.22(1.53‚Üë) 87.87 89.54(1.67‚Üë) 87.69(0.18‚Üì)
8.10(2.90‚Üë) 79.26 81.32(2.06‚Üë) 84.26(5.00‚Üë)
5.80(6.33‚Üë) 37.22 34.13(3.09‚Üì) 49.78(12.56‚Üë)
4.08(2.57‚Üë) 73.63 85.38(11.75‚Üë) 83.32(9.69‚Üë)
6.71(14.56‚Üë) 44.18 45.88(1.70‚Üë) 54.02(9.84‚Üë)
5.84(1.46‚Üì) 71.30 73.70(2.40‚Üë) 83.83(12.53‚Üë)
6.50(1.39‚Üë) 78.39 78.77(0.38‚Üë) 88.41(10.02‚Üë)
3.85(1.64‚Üì) 52.42 53.18(0.76‚Üë) 72.33(19.91‚Üë)
4.56(0.92‚Üì) 88.16 88.14(0.02‚Üì) 95.17(7.01‚Üë)
3.29(13.34‚Üë) 14.27 2.97(11.30‚Üì) 27.58(13.31‚Üë)
6.97(0.66‚Üë) 68.86 67.80(1.06‚Üì) 59.57(9.29‚Üì)
2.21(2.85‚Üë) 81.48 82.96(1.48‚Üë) 85.45(3.97‚Üë)
9.69(2.96‚Üë) 70.49 65.74(4.75‚Üì) 65.87(4.62‚Üì)
7.24(3.93‚Üì) 39.69 35.91(3.78‚Üì) 50.01(10.32‚Üë)
7.58(20.92‚Üë) 41.38 20.04(21.34‚Üì) 45.77(4.39‚Üë)
7.48(9.58‚Üë) 76.35 73.07(3.28‚Üì) 82.71(6.36‚Üë)
8.63(6.37‚Üë) 80.56 75.74(4.82‚Üì) 84.42(3.86‚Üë)
9.73(0.98‚Üì) 70.45 70.31(0.14‚Üì) 81.93(11.48‚Üë)
5.01(4.36‚Üë) 51.98 66.59(14.61‚Üë) 81.25(29.27‚Üë)
1.68(1.22‚Üë) 64.96 53.16(11.80‚Üì) 51.98(12.98‚Üì)
ntmodelsanddatasets. Valuesinparenthesesshowthe
tatisticallysignificantchanges(p<0.05).
Placebo hints reveal important differences be-
tweenmethods. Theadversarialconditionwith
placebo hints shows that models are not simply
becoming more confident as a conversation gets
longer. For many methods, the change in confi-
denceafteraplacebohintisnotstatisticallysignif-
icant (high p-value). For example, for Llama3.1-
70B on GUESS, COT-VERB, SC, and P(TRUE)
shownegligiblechanges(p > 0.6). Thissuggests
adegreeofrobustnessagainstsuperficialconversa-
tionalstructure.
P(TRUE) is confounded by turn count (es-
pecially in GUESS). In open-ended GUESS,
P(TRUE) often rises even under placebo, consis-
tentwithalengthartifact(mean‚àÜ = +5.64;
placebo
significantin2/4modelpairs). Notably,Llama3.1-
8B jumps +11.75 under placebo (p<10‚àí12) and
Qwen2.5-72Bjumps+14.61 (p<10‚àí6). Although
P(TRUE)alsoincreaseswithgenuineinformation
(mean ‚àÜ
info
= +14.07 on GUESS), the placebo
lift makes it less reliable for disentangling infor-
mation from dialogue length. On 20Q, placebo
effectstendtobenegative(mean‚àí3.91),suggest-
ingformat-dependentbehavior.
Self-consistency (SC) shows moderate robust-
ness. SC typically exhibits small or negligible
placebomovementsandsizablegainswithinforma-
tivehints(mean‚àÜ
info
= +9.83 on GUESS). How-
ever,itisnotimmune: e.g.,Llama3.1-8Bon20Q
8

=== Page 9 ===
Llama3.1-8b Llama3.1-70b
Vanilla-Verb Vanilla-Verb
100 100
A
C C 60 80 C o t-V
e A
C C 60 80 C o t-V
e
40 rb 40 rb
20 20
P P P
(su (su
fficie
S
C fficie
S
C
n n
t) t)
P(true) P(true)
2200QQ
MMuullttii--ttuurrnn SSuummmmaarriizzee
Figure4:Performancecomparisonacrossfourlanguagem
Cot-Verb,SC,P(true),P(sufficient),andACC.Redindica
increases under placebo by +4.82 (p=0.0025).
Thus, SC generallytracksinformationbetterthan
verbalized scores but can still pick up turn-index
artifactsinsomesettings.
Verbalizedconfidenceisunstableacrosscondi-
tions. VANILLA-VERB/COT-VERB showsmall
averageplaceboshifts(oftennon-significant)and
modest informative gains; in some cases they
even move counterintuitively (e.g., Qwen2.5-7B
on GUESS decreases by ‚àí9.29 with an informa-
tivehint,p=0.005). Thisinstability,togetherwith
poorcalibration(¬ß5.2),limitstheirutilityforturn-
by-turnreliability.
5.4 Single-TurnSummaryvs.Multi-Turn
Interaction
We compare model behavior when consuming
clues incrementally (multi-turn) versus reading
a concise synthesis of the same clues in one
prompt (single-turn summary). Across mod-
els and datasets, accuracy differences between
the two settings are small (mean absolute gap
<1), indicating no systematic advantage for ei-
ther format (Figure 4). For example, on 20Q
Llama3.1-8B slightly improves with summaries
(24.95‚Üí27.16), whereas Llama3.1-70B slightly
drops(33.87‚Üí32.31). On GUESS,Qwen2.5-72B
gains modestly (16.31‚Üí17.29), while Qwen2.5-
7Bloses(12.74‚Üí11.69). Inshort,unlikethe‚Äúget-
tinglost‚ÄùeffectreportedbyLabanetal.(2025),our
progressiveinformation-seekingsetupyieldscom-
parabletaskaccuracyinmulti-turnandsingle-turn
conditions(seeAppendixCfordetailedInfoECE
comparisons). Onepossiblereasonisthatourtasks
do not involve complicated arithmetic reasoning
comparedwithLabanetal.(2025).
9

Qwen2.5-7b Qwen2.5-72b
Vanilla-Verb Vanilla-Verb
100 100
A
C C 60 80 C o t-V
e A
C C 60 80 C o t-V
e
40 rb 40 rb
20 20
P P
(su (su
fficie
S
C fficie
S
C
n n
t) t)
P(true) P(true)
GUESS
Multi-turn Summarize
models.Sixevaluationdimensionsareshown:Vanilla-Verb,
ates20Qbenchmarks,blueindicatesGUESSbenchmarks.
Confidence shifts depend strongly on the sig-
nal. While accuracy is stable, confidence re-
spondsmarkedlytopromptformat. Thesufficiency
probe P(SUFFICIENT) consistently drops under
single-turnsummaries(e.g.,on 20Q:Qwen2.5-7B
63.13‚Üí13.23;Llama3.1-70B34.80‚Üí15.30),sug-
gesting that compressing the dialogue into a syn-
opsis removes turn-structure cues that the probe
exploits to assess whether evidence is enough.
P(TRUE)andverbalizedconfidenceoftendecrease
withsummariesforsmallermodels(e.g.,Llama3.1-
8BonGUESS: P(TRUE)80.66‚Üí51.58,VANILLA-
VERB 87.33‚Üí72.82), but can increase for larger
models in some cases (e.g., Llama3.1-70B on
GUESS: VANILLA-VERB 80.63‚Üí87.65, COT-
VERB84.43‚Üí90.72)withoutcommensurateaccu-
racygains‚Äîaninstanceofpotentialmiscalibration
inflation. Bycontrast, SC iscomparativelystable
andsometimesriseswithsummarieson 20Q (e.g.,
Llama3.1-8B: 42.70‚Üí49.04), but shows mixed
movementon GUESS.
6 Conclusion
We present the first systematic study of confi-
denceestimationforLLMsinmulti-turnconversa-
tions. Weestablishaformalevaluationframework
groundedintwokeydesideratawithnovelmetrics
anddatasets. Ourevaluationacrossvariousconfi-
denceestimationmethodsrevealsthatwidely-used
techniques struggle to maintain calibration and
monotonicityindynamicdialogues. Wefindthat
ourproposedlogit-basedprobe,P(SUFFICIENT),
achievescomparativelybetterperformance; how-
ever,thetaskremainssignificantlyunder-resolved.
Buildingonourfoundation,weadvocateforfuture
researchintomethodsthat: (1)satisfybothcalibra-
tionandmonotonicity;(2)effectivelydistinguish
9

=== Page 10 ===
task-relevantinformationfromconversationalfiller;
and(3)remainrobustacrossbothsingle-turnsum-
mariesandmulti-turninteractions.
Limitation
Our work, while providing a foundational frame-
work, has several limitations that open avenues
for future research. (1) The progressive datasets
andHinter‚ÄìGuesserprotocolsimplifyrealconver-
sations, omitting phenomena such as topic shifts,
repairs,andmixedintents,whichmaylimittransfer
tomessy, open-worlddialogue. (2)Ourstudyfo-
cusesonspecificinformation-seekingtasks;thedy-
namicsofconfidenceinmoreopen-ended,creative,
orcollaborativeconversationsremainanopenques-
tion. (3) Our evaluation emphasizes calibration
and rank monotonicity; the downstream impact
onuserutilityandhumantrustrequirescontrolled
userstudiesandfielddeployments. (4)Westudy
confidenceratherthanuncertainty;extendingour
frameworktouncertaintyquantificationanditsre-
lationshiptoconfidenceinmulti-turnsettingsisan
importantnextstep.
EthicsStatement
Our research follows standard ethical guidelines.
Weverifiedthelicensesofallsoftwareanddatasets
used in this study. We introduce new multi-turn
evaluation datasets built from existing resources
and automated generation. Despite best efforts,
thedatamaycontainWestern-centricbiases, and
we did not address multilingual coverage, which
maylimitgeneralityacrosslanguagesandcultures.
Althoughsomesourcedatasetswerecreatedwith
human-in-the-loopprotocols(e.g.,GRACE(Sung
etal.,2025)),ourexperimentsarefullyautomated.
We recruited no new participants or annotators;
thus no compensation or IRB oversight was re-
quired. Weidentifiednoprivacyconcerns, aswe
donotcollect,store,orreleasepersonallyidentifi-
ableinformation. Wedonotanticipateadditional
risks. We used an AI assistant only for grammar
checking.
References
MarwaAbdulhai,IsadoraWhite,CharlieSnell,Charles
Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu, and
Sergey Levine. 2023. Lmrl gym: Benchmarks
formulti-turnreinforcementlearningwithlanguage
models. Preprint,arXiv:2311.18232.
10

Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jia-
heng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su,
TiezhengGe, BoZheng, andWanliOuyang.2024.
MT-bench-101: Afine-grainedbenchmarkforevalu-
atinglargelanguagemodelsinmulti-turndialogues.
In Proceedings of the 62nd Annual Meeting of the
AssociationforComputationalLinguistics(Volume1:
LongPapers),pages7421‚Äì7454,Bangkok,Thailand.
AssociationforComputationalLinguistics.
YijiangRiverDong,TianchengHu,andNigelCollier.
2024. CanLLMbeapersonalizedjudge? InFind-
ingsoftheAssociationforComputationalLinguistics:
EMNLP2024,pages10126‚Äì10141,Miami,Florida,
USA.AssociationforComputationalLinguistics.
Haodong Duan, Jueqi Wei, Chonghua Wang, Hong-
weiLiu,YixiaoFang,SongyangZhang,DahuaLin,
and Kai Chen. 2024. BotChat: Evaluating LLMs‚Äô
capabilitiesofhavingmulti-turndialogues. InFind-
ingsoftheAssociationforComputationalLinguis-
tics: NAACL2024,pages3184‚Äì3200,MexicoCity,
Mexico.AssociationforComputationalLinguistics.
JiahuiGeng,FengyuCai,YuxiaWang,HeinzKoeppl,
Preslav Nakov, and Iryna Gurevych. 2024. A sur-
veyofconfidenceestimationandcalibrationinlarge
languagemodels. InProceedingsofthe2024Con-
ferenceoftheNorthAmericanChapteroftheAsso-
ciationforComputationalLinguistics: HumanLan-
guageTechnologies(Volume1: LongPapers),pages
6577‚Äì6595, Mexico City, Mexico. Association for
ComputationalLinguistics.
TianchengHu,BenjaminMinixhofer,andNigelCollier.
2025. Navigatingthealignment-calibrationtrade-off:
Apareto-superiorfrontierviamodelmerging. arXiv
preprintarXiv:2510.17426.
SauravKadavath,TomConerly,AmandaAskell,Tom
Henighan, Dawn Drain, Ethan Perez, Nicholas
Schiefer,ZacHatfield-Dodds,NovaDasSarma,Eli
Tran-Johnson, Scott Johnston, Sheer El-Showk,
Andy Jones, Nelson Elhage, Tristan Hume, Anna
Chen,YuntaoBai,SamBowman,StanislavFort,and
17 others. 2022. Language models (mostly) know
whattheyknow. Preprint,arXiv:2207.05221.
MichaelKirchhof,GjergjiKasneci,andEnkelejdaKas-
neci. 2025. Position: Uncertainty quantification
needsreassessmentforlarge-languagemodelagents.
Preprint,arXiv:2505.22655.
Wai-ChungKwan,XingshanZeng,YuxinJiang,Yufei
Wang,LiangyouLi,LifengShang,XinJiang,Qun
Liu,andKam-FaiWong.2024. MT-eval: Amulti-
turncapabilitiesevaluationbenchmarkforlargelan-
guagemodels. InProceedingsofthe2024Confer-
enceonEmpiricalMethodsinNaturalLanguagePro-
cessing,pages20153‚Äì20177,Miami,Florida,USA.
AssociationforComputationalLinguistics.
Philippe Laban, Hiroaki Hayashi, Yingbo Zhou, and
JenniferNeville.2025. Llmsgetlostinmulti-turn
conversation. Preprint,arXiv:2505.06120.
0

=== Page 11 ===
ZhenLin,ShubhenduTrivedi,andJimengSun.2023.
Generatingwithconfidence: Uncertaintyquantifica-
tionforblack-boxlargelanguagemodels. Preprint,
arXiv:2305.19187.
PotsaweeManakul,AdianLiusie,andMarkJFGales.
2023. Selfcheckgpt: Zero-resource black-box hal-
lucination detection for generative large language
models. arXivpreprintarXiv:2303.08896.
Meta.2024. Llama3modelcard.
ArtemShelmanov,EkaterinaFadeeva,AkimTsvigun,
IvanTsvigun,ZhuohanXie,IgorKiselev,NicoDa-
heim, Caiqi Zhang, Artem Vazhentsev, Mrinmaya
Sachan,PreslavNakov,andTimothyBaldwin.2025.
Aheadtopredictandaheadtoquestion: Pre-trained
uncertainty quantification heads for hallucination
detection in LLM outputs. In Proceedings of the
2025ConferenceonEmpiricalMethodsinNatural
LanguageProcessing,pages35700‚Äì35719,Suzhou,
China.AssociationforComputationalLinguistics.
YooYeonSung,EveFleisig,YuHou,IshanUpadhyay,
and Jordan Lee Boyd-Graber. 2025. GRACE: A
granularbenchmarkforevaluatingmodelcalibration
against human calibration. In Proceedings of the
63rdAnnualMeetingoftheAssociationforCompu-
tationalLinguistics(Volume1: LongPapers),pages
19586‚Äì19587,Vienna,Austria.AssociationforCom-
putationalLinguistics.
Katherine Tian, Eric Mitchell, Allan Zhou, Archit
Sharma,RafaelRafailov,HuaxiuYao,ChelseaFinn,
and Christopher Manning. 2023. Just ask for cali-
bration: Strategiesforelicitingcalibratedconfidence
scoresfromlanguagemodelsfine-tunedwithhuman
feedback. In Proceedings of the 2023 Conference
onEmpiricalMethodsinNaturalLanguageProcess-
ing, pages 5433‚Äì5442, Singapore. Association for
ComputationalLinguistics.
Eric Wallace, Pedro Rodriguez, Shi Feng, Ikuya Ya-
mada,andJordanBoyd-Graber.2019. Trickmeif
youcan: Human-in-the-loopgenerationofadversar-
ialexamplesforquestionanswering. Transactionsof
theAssociationforComputationalLinguistics,7:387‚Äì
401.
XingyaoWang,ZihanWang,JiatengLiu,YangyiChen,
Lifan Yuan, Hao Peng, and Heng Ji. 2023. Mint:
Evaluatingllmsinmulti-turninteractionwithtools
andlanguagefeedback. Preprint,arXiv:2309.10691.
MiaoXiong,ZhiyuanHu,XinyangLu,YifeiLi,JieFu,
JunxianHe,andBryanHooi.2024. Canllmsexpress
theiruncertainty? anempiricalevaluationofconfi-
denceelicitationinllms. InTheTwelfthInternational
ConferenceonLearningRepresentations,ICLR2024,
Vienna,Austria,May7-11,2024.OpenReview.net.
An Yang, Baosong Yang, Binyuan Hui, Bo Zheng,
BowenYu,ChangZhou,ChengpengLi,Chengyuan
Li,DayihengLiu,FeiHuang,GuantingDong,Hao-
ranWei,HuanLin,JialongTang,JialinWang,Jian
Yang,JianhongTu,JianweiZhang,JianxinMa,and
43others.2024. Qwen2technicalreport.
11

Ruihan Yang, Caiqi Zhang, Zhisong Zhang, Xinting
Huang,SenYang,NigelCollier,DongYu,andDe-
qing Yang. 2025a. LoGU: Long-form generation
withuncertaintyexpressions. InProceedingsofthe
63rdAnnualMeetingoftheAssociationforCompu-
tationalLinguistics(Volume1: LongPapers),pages
18947‚Äì18968,Vienna,Austria.AssociationforCom-
putationalLinguistics.
Ruihan Yang, Caiqi Zhang, Zhisong Zhang, Xinting
Huang, Dong Yu, Nigel Collier, and Deqing Yang.
2025b. UNCLE:Benchmarkinguncertaintyexpres-
sionsinlong-formgeneration. InProceedingsofthe
2025ConferenceonEmpiricalMethodsinNatural
LanguageProcessing,pages30328‚Äì30344,Suzhou,
China.AssociationforComputationalLinguistics.
ZihaoYi,JiaruiOuyang,ZheXu,YuwenLiu,Tianhao
Liao,HaohaoLuo,andYingShen.2024. Asurvey
onrecentadvancesinllm-basedmulti-turndialogue
systems. Preprint,arXiv:2402.18013.
BoxuanZhangandRuqiZhang.2025. CoT-UQ:Im-
provingresponse-wiseuncertaintyquantificationin
LLMswithchain-of-thought. InFindingsoftheAs-
sociationforComputationalLinguistics: ACL2025,
pages 26114‚Äì26133, Vienna, Austria. Association
forComputationalLinguistics.
Caiqi Zhang, Zhijiang Guo, and Andreas Vlachos.
2024a. Doweneedlanguage-specificfact-checking
models? the case of Chinese. In Proceedings of
the2024ConferenceonEmpiricalMethodsinNatu-
ralLanguageProcessing,pages1899‚Äì1914,Miami,
Florida, USA. Association for Computational Lin-
guistics.
CaiqiZhang,FangyuLiu,MarcoBasaldella,andNigel
Collier.2024b. LUQ:Long-textuncertaintyquantifi-
cationforLLMs. InProceedingsofthe2024Con-
ferenceonEmpiricalMethodsinNaturalLanguage
Processing,pages5244‚Äì5262,Miami,Florida,USA.
AssociationforComputationalLinguistics.
Caiqi Zhang, Chang Shu, Ehsan Shareghi, and Nigel
Collier.2025a. AllroadsleadtoRome: Graph-based
confidenceestimationforlargelanguagemodelrea-
soning. InProceedingsofthe2025Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing,
pages31802‚Äì31812,Suzhou,China.Associationfor
ComputationalLinguistics.
Caiqi Zhang, Ruihan Yang, Zhisong Zhang, Xinting
Huang,SenYang,DongYu,andNigelCollier.2024c.
Atomiccalibrationofllmsinlong-formgenerations.
Caiqi Zhang, Xiaochen Zhu, Chengzu Li, Nigel Col-
lier, and Andreas Vlachos. 2025b. Reinforcement
learningforbetterverbalizedconfidenceinlong-form
generation. Preprint,arXiv:2505.23912.
Ej Zhou, Caiqi Zhang, Tiancheng Hu, Chengzu Li,
NigelCollier,IvanVulic¬¥,andAnnaKorhonen.2025.
Beyondthefinallayer: Intermediaterepresentations
forbettermultilingualcalibrationinlargelanguage
models. Preprint,arXiv:2510.03136.
1

=== Page 12 ===
XiaochenZhu,CaiqiZhang,TomStafford,NigelCol-
lier,andAndreasVlachos.2025. Conformityinlarge
languagemodels. InProceedingsofthe63rdAnnual
Meeting of the Association for Computational Lin-
guistics(Volume1: LongPapers),pages3854‚Äì3872,
Vienna,Austria.AssociationforComputationalLin-
guistics.
12

=== Page 13 ===
Appendix
A InstructionPromptExamples.
I:GENERATIONTEMPLATE
{Question}
Use\boxed{}towrapyouranswer;forexample\boxed{{a
conciseanswer}}.Youransweris:
II:VANILLA-VERBTEMPLATE
{Question}
ProposedAnswer:{Answer}
Howconfidentareyouthattheproposedansweriscorrect?
Theconfidencescoreshouldbeanumberfrom0(com-
pletelyunsure)to100(absolutelycertain).
YourresponseMUSTstrictlyadheretothisformat:
###Confidence: \boxed{Yourconfidencescorefrom0-
100.}
Yourresponseis:
III:COT-VERBTEMPLATE
{Question}
ProposedAnswer:{Answer}
Howconfidentareyouthattheproposedansweriscorrect?
Analyzefirstandthinkstepbystep.Theconfidencescore
shouldbeanumberfrom0(completelyunsure)to100
(absolutelycertain).
YourresponseMUSTstrictlyadheretothisformat:
###Analysis:\boxed{Analyzethequestionandguessthe
answer. Thinkstepbystepandjustifyyourconfidence
score.}
###Confidence: \boxed{Yourconfidencescorefrom0-
100.}
Yourresponseis:
Table 4: Three TEMPLATE formats: generation and
verbalizeconfidenceestimation.
B IncrementalQAdatasets
GRACE(Sungetal.,2025): agranularbenchmark
composed of quizbowl-style questions in which
clues become increasingly specific. Each clue is
self-containedandunambiguous. Modelsareeval-
uatedonhowearly,howaccurately,andhowconfi-
dentlytheyanswerascluesunfold,providingfine-
grainedsignalsforstep-wisecalibration. Thisdi-
rectlysupportsC1andC2,andthepyramidalclue
structureencouragesC3asevidencestrengthens.
TrickMe(Wallaceetal.,2019): ahuman-in-the-
loop,adversarially-authoredQAdatasetbuiltalso
inaquizbowlinterface. Writersiterativelycraftin-
crementalcluestoelicitconfidentmodelmistakes
whileremainingsolvablebyhumans,yieldingchal-
lenging, diverse questions that reveal miscalibra-
tionunderpartialinformation. Itusesexactlythe
sametaskformatasGRACEthusalsosatisfyour
threecriteria.
13

I:P(Sufficient)TEMPLATE
{Question}
Basedonlyontheinformationandhintsprovidedabove,
does that information sufficiently entail that the correct
answerisexactly{Answer}?
A.Yes‚Äîtheinformationissufficienttoconclude{An-
swer}.
B.No‚Äîtheinformationisinsufficient,allowsalternatives,
orcontradicts{Answer}.
Outputformat: **A**or**B**only(singleuppercase
letter;nospaces,punctuation,orexplanation):
II:P(Ture)TEMPLATE
{Question}
ProposedAnswer:{Answer}
Istheproposedanswer:
A.True
B.False
Outputformat: **A**or**B**only(singleuppercase
letter;nospaces,punctuation,orexplanation):
Table5:TwoPROMPT_TEMPLATEformatsforbinary
classificationtasks.
SUMMARIZEPROMPTTEMPLATE
Youaregivenablockofhintsaboutanentity(e.g.,froma
20Questionsgame).
Summarizethehintsintoaconcisestatementortwo,keep-
ingtheessentialinformationinsteadoftheQuestion/An-
swerformat.
Donotaddnewinformationorremovekeyattributes.
Keepitasshortandfactualaspossible.
Table6:SUMMARIZE_PROMPTtemplateforconvert-
ingQ&Aformathintsintoconcisefactualstatements.
Together,GRACEandTrickMeinstantiatethe
fully-specified setting where a unique gold an-
swer exists from the outset, but models must cal-
ibrate when to commit as evidence accrues. We
usebothdatasetswithoutalteringtheirunderlying
incremental-clueprotocols,andwereportper-turn
accuracyandconfidencetoalignwithourevalua-
tionframework.
C Calibrationshiftsreveala
scaling-dependentformateffect.
While accuracy remains comparable across for-
mats(¬ß5.4),calibrationquality‚ÄîmeasuredbyIn-
foECE‚Äîrespondsdivergentlybymodelscale(Fig-
ure5). Thesufficiencyprobe P(SUFFICIENT) ex-
hibitsoppositetrends: forsmallermodels,summa-
rizationdegradescalibration(e.g.,Llama3.1-8Bon
20Q:6.99‚Üí24.57; on GUESS: 3.82‚Üí9.41), sug-
gestingrelianceonturn-by-turnstructure. Instark
contrast,largermodelsshowsubstantialimprove-
ments under summarization (e.g., Llama3.1-70B
3

=== Page 14 ===
on 20Q: 40.29‚Üí9.81; on GUESS: 34.70‚Üí6.16;
Qwen2.5-72B on GUESS: 27.51‚Üí2.55), indicat-
ingmoreeffectiveintegrationofcompressedevi-
dence. P(TRUE)improvesmarkedlyforLlama3.1-
70B(20Q:68.00‚Üí53.28;GUESS: 66.07‚Üí36.87)
but shows minimal change for smaller models.
SC andverbalizedmethodsremainlargelyformat-
invariant(shiftstypically<5InfoECEpoints)but
consistentlypoorlycalibrated(>50InfoECE).This
scaling-dependentdivergencesuggeststhatwhile
smallermodelsdependonconversationalstructure
forreliablecalibration,largermodelscanflexibly
exploiteitherformat,sometimesachievingsuperior
calibrationfromsummarization.
D QuestionExamples
Welistsomeexamplesofthefourdatasetsweuse
inthestudyinTable7.
E PlaceboQAExamples
WelisttheplaceboQAexamplesweusefor20Q
and GUESSdatasetsinTable8.
14

=== Page 15 ===
Llama3.1-8b Llama3.1-70b
Vanilla-Verb Vanilla-Verb
100 100
Acc
ur
acy
40
60
80 C
o
t-Ve
rb Acc
ur
acy
40
60
80 C
o
t-Ve
rb
20 20
P(su P(su P
fficie
S
C fficie
S
C
n n
t) t)
P(true) P(true)
2200QQ
MMuullttii--ttuurrnn SSuummmmaarriizzee
Figure5: InfoECEofdifferentconfidenceestimationme
Verb,CoT-Verb,SC,P(true),P(sufficient))plusAccurac
(lighter)presentation. Red: 20Q;blue: GUESS. LowerIn
Dataset Prompts
User:Giventhefollowinginformation,provi
questionfragment.Ifunsure,provideyourbes
Youhavesomecluesabouttheanswer:
20Q Assistant:Isithuman-made?User:Yes
Assistant:Isittypicallyfoundindoors?User:
Assistant:Isitcommonlyencounteredinlivin
Assistant:Isitlargerthanabook?User:Yes
User:Nowguesswhatitis:
Keyword: television
User:Giventhefollowinginformation,name
bestguess.Theanswershouldbeconcise.
Youhavesomecluesabouttheanswer:
GUESS Assistant:Whatcontinentisthecityin?User
Assistant:Isthecitycoastalorinland?User:
Assistant:What‚Äôstheclimatelikeinthecity?
Assistant:Whatregionwithinthecontinentis
User:Nowguesswhatitis:
Keyword: Bogor,Indonesia
User:Giventhefollowinginformation,provi
questionfragment.Ifunsure,provideyourbes
Youhavesomecluesabouttheanswer:
User:It‚ÄôsnotCharlieParker,butamusicianw
GRACE
Firebirdandthe"Goin‚ÄôHome"themefromthe
User:Anotheralbumbyamusicianwiththiss
PharoahSanders.
User:Asongbyamusicianwiththissurname
User:Asetofchordsubstitutionsinaii-V-I("t
TommyFlanaganarecalledthissurname‚Äôs"ch
User:For10points,givethissurnameofharp
GiantSteps.
User:Nowguesswhatitis:
Keyword: Coltrane
User:Giventhefollowinginformation,provi
questionfragment.Ifunsure,provideyourbes
Youhavesomecluesabouttheanswer:
TRICKME User:ThismanwasseendrivingDesiignerin
User:InaninterviewwithSway,thismanyell
User:Thismanalsocontroversiallysaidthats
User: For10points,namethissongwriterkn
recently,"ILoveIt"withLilPump.
User:Nowguesswhatitis:
Keyword: Kanye_West
Table7: Examplesfromfourdatasets,sh
15

Qwen2.5-7b Qwen2.5-72b
Vanilla-Verb Vanilla-Verb
100 100
Acc
ur
acy
40
60
80 C
o
t-Ve
rb Acc
ur
acy
40
60
80 C
o
t-Ve
rb
20 20
P(su P(su
fficie
S
C fficie
S
C
n n
t) t)
P(true) P(true)
GUESS
Multi-turn Summarize
ethodsacrossformats. Fiveconfidencemethods(Vanilla-
cy,comparedundermulti-turn(darker)vs.summarized
nfoECEindicatesbettercalibration.
idethetitleoftheWikipediapagethatbestanswersthelast
stguess.Theanswershouldbeconcise.
:Yes
ngrooms?User:Yes
ethesingleCITYthatbestfitsthem.Ifunsure,provideyour
r:Asia
:Inland
?User:Tropical
sthecitylocated?User:SoutheastAsia
idethetitleoftheWikipediapagethatbestanswersthelast
stguess.Theanswershouldbeconcise.
withthissurnamearrangedexcerptsfromStravinsky‚ÄôsThe
eNewWorldSymphony.
surnamefeatureschantsfromtheBhagavadGitaandsolosby
efeaturesElvinJonesontimpaniandgong.
two-five-one")progressionthatprovedchallengingforpianist
hanges."
pistAliceandherhusband,thesaxophonistbehindthealbum
idethetitleoftheWikipediapagethatbestanswersthelast
stguess.Theanswershouldbeconcise.
nthemusicvideoforPanda.
led"IamWarhol,"andcomparedhimselfwithShakespeare.
slaverywasachoice.
nownforhissongs"Power,"and"GoldDiggers,"andmore
howingquestion-answerdialogueformat.
5

=== Page 16 ===
GuessMyCity
Q:Doesthecityhavepeoplelivinginit?A:Yes
Q:Doesthecitycontainbuildings?A:Yes
Q:Arethereroadsinthecity?A:Yes
Q:Doesthecityhavesomeformofwastedisposal,likebinsortrash
collection?A:Yes
Q:Isthereaccesstotoiletsinthecity?A:Yes
Q:Doesthecityhaveshopsormarkets?A:Yes
Q:Arethereschoolsoreducationalinstitutionsinthecity?A:Yes
Q:Doesthecityhavehospitalsorclinics?A:Yes
Q:Istheresomeformofpublictransportationinthecity?A:Yes
Q:Doesthecityhaverestaurantsorplacestoeat?A:Yes
Q:Arethereofficesorworkplacesinthecity?A:Yes
Q:Doesthecityhaveplacesforrecreation,suchasparksorsportsareas?
A:Yes
Q:Isthecitylocatedonland?A:Yes
Q:Doesthecitybelongtoacountry?A:Yes
Q:Istheresomeformofgovernmentoradministrationinthecity?A:Yes
Q:Doesthecityhavestreetsorpathwaysformovement?A:Yes
Q:Aretherepeoplewhoworkinthecity?A:Yes
Q:Doesthecityhavesomeformofshelterorhousing?A:Yes
Q:Isthereelectricityavailableinpartsofthecity?A:Yes
Q:Doesthecityhavesomeformofwatersupplyoraccess?A:Yes
Q:Aretherevehiclesthatoperateinthecity?A:Yes
Q:Doesthecityhavesomeformofcommunicationinfrastructure? A:
Yes
Q:Aretherebusinessesoperatinginthecity?A:Yes
Q:Doesthecityhavesomeformoflightingatnight?A:Yes
Q:Arethereemergencyservicesavailableinthecity?A:Yes
Q:Doesthecityhavebankingorfinancialservices?A:Yes
Q:Arethereentertainmentvenuesinthecity?A:Yes
Q:Doesthecityhavepostalordeliveryservices?A:Yes
Q:Aretherereligiousorculturalinstitutionsinthecity?A:Yes
Q:Doesthecityhavesomeformoflawenforcement?A:Yes
Table8: Weconstructplaceboquestion‚Äìanswerpairsfor
and20Q(26questions).Thesequestionsneitherprovidean
withconversationhistory.
16

20Q
Q:Canitbedescribedusingwords?A:Yes
Q:Canpeopleaskquestionsaboutit?A:Yes
Q:Couldit,inprinciple,beidentifiedorreferredto?A:Yes
Q:Doesithaveatleastoneproperty?A:Yes
Q:Isitwhatitis?A:Yes
Q:Couldsomeonethinkaboutit?A:Yes
Q:Canitbedistinguishedfromnothingatall?A:Yes
Q:Isitpossibletotalkaboutitrightnow?A:Yes
Q:Doesithavesomekindofnameorlabel?A:Yes
Q:Woulditstillcountassomethingevenifweknowlittleaboutit?A:
Yes
Q:Couldit,intheory,beobservedordetected?A:Yes
Q:Doesitinteractwithitsenvironmentinsomeway?A:Yes
Q:Coulditbedistinguishedfromabsolutelynothing?A:Yes
Q:Isitpossibletoclassifyitassomethingratherthannothing?A:Yes
Q:Doesitoccupysomekindoflocation,evenifunknown?A:Yes
Q:Isitpartofreality?A:Yes
Q:Doesithavesomerelationtootherthings?A:Yes
Q:Couldoneimagineitbeingmeasuredsomehow?A:Yes
Q:IsEartharoundtheSun?A:Yes
Q:IstheMoonlargerthantheSun?A:No
Q:Cannumbersbeeven?A:Yes
Q:Isblueakindofsound?A:No
Q:Canathoughthaveweight?A:No
Q:Istimemeasuredbyclocks?A:Yes
Q:Dotriangleshavethreesides?A:Yes
Q:Iswaterwetterthanfire?A:Yes
controlexperimentsusingGuessMyCity(30questions)
nyusefulinformationtoanswerthequestionnorcontradict
6

Paper:Hallucination is the last thing you need.pdf
=== Page 1 ===
x
Hallucination is the
Travers
Shawn Curran Oliver B
Travers Smith Travers
shawn.curran@traverssmith.com oliver.bethell@tr
Abst
The legal profession necessitates a multidimension
comprehension of a legal issue with insightful comme
comprehensive understanding of pertinent legislati
informed legal solution. The present offering with g
this, as current models struggle to integrate and na
experience, and fact-checking procedures. It is notewo
and experience, which reflect the aggregate of var
deflects the model's attention from the crucial legal
paper delves into the feasibility of three independent
and facts, synthesising as one single ensemble model
by the existing monolithic generative AI models. W
protect key information assets like common law judge
publicly available models for legal hallu
1 Introduction
There are various mechanisms under way to try and re
enhancing the models understanding of facts [1] Meng
retrieval mechanism for facts [2] Izacard and Grave or
and comparison algorithms to fact check generative ou
Hallucinations are already causing issues within the le
sanctions hearing for [4] presenting hallucinated case
with a [5] standing order to ensure lawyers inform the
submissions.

e last thing you need
s Smith
Bethell Sam Lansley
s Smith Travers Smith
raverssmith.com sam.lansley@traverssmith.com
tract
nal approach that involves synthesizing an in-depth
entary based on personal experience, combined with a
ion, regulation, and case law, in order to deliver an
generative AI presents major obstacles in replicating
avigate such a complex interplay of understanding,
orthy that where generative AI outputs understanding
rious subjective views on similar topics, this often
facts, thereby resulting in hallucination. Hence, this
t LLMs, each focused on understanding, experience,
to effectively counteract the current challenges posed
We introduce an idea of mutli-length tokenisation to
ements, and finally we interrogate the most advanced
ucination, with some interesting results.
esolve hallucination errors within LLMs, including
g and Bau et al, having models call out under a
r even in-industry Legal Tech vendors using search
utput [3] Lexis+ AI.
egal industry, with a lawyer in the US awaiting a
law in front of the courts. One court has responded
em where generative AI has been used in their

=== Page 2 ===
Use of generative AI for legal research potentially imp
this [6] article. In our view, common law contaminatio
threat to the credibility of the legal industry in using g
Therefore, we expect a great deal of investment to now
by [7] OpenAI themselves recently and presumably a
ChatGPT has served as a great demonstration to the in
view is that firms should leverage our open source edu
ensure your business keeps up to date and informed on
As previously discussed, the resolution of hallucinatio
This paper proposes a theoretical approach that we be
approach in-depth as our data labelling processes reac
We wish to clarify that while the authors of this paper
qualified lawyers. The opinions and perspectives prese
our domain, however we approach this on a computati
not provide legal advice or judgement. Our firm has a
closely with. This practice is available to assist any or
For further details, kindly contact james.longster@trav
louisa.chambers@traverssmith.com.
2 Common law contamination
As previously indicated, a spurious instance of case la
presented before a perplexed judge. This was readily a
name. However, what if generative AI is being emplo
subtle, non-obvious errors are surreptitiously inserted
and conventional law evolves such that the facts of a j
become altered with slight, subtle changes that ultima
time. Never before has the legal profession possessed
subsequent word in a judgment, presumptuously overr
this paper, we suggest a method for curbing hallucinat
of models, and then examine the apparition and halluc
of exercising extreme caution in using LLMs for legal
Our understanding of the internal structure of OpenAI
presence of instructional, completion, question and an
which work together to tackle generative AI issues. N
which have been trained on a vast amount of publicly
"jack of all trades, hallucination in some" whereby the
between various features such as case law, legislation,
styles resulting in them computing unclear and overlap
different approaches can be used to address these chal
multi-length tokenization and a vertically aligned ense

pacts the integrity of common law, as we discuss in
on with subtle, non-obvious errors poses the biggest
generative AI for legal research.
w go into fixing the hallucination problem, backed up
follow up from the hallucinated case law issue.
ndustry of the sophistication of generative AI, and our
ucational and experimental chatbot [8] YCNBot to
n this technology until it's ready for primetime.
ons in legal settings requires multiple approaches.
elieve holds promise, and we intend to explore this
ch their critical mass.
r possess some formal legal training, we are not
ented in this paper draw from legal data, given this is
ional basis to assess the limitations of LLMs. We do
a leading Technology law practice that we collaborate
rganisation with legal matters tied to their AI strategy.
verssmith.com or
aw came to the attention of the court and was
apparent to the court due to the absence of the case
oyed more expansively through the legal sector, where
into motions, subsequently adopted into judgments,
judgment, repeatedly cited, gradually transform, and
ately substantively affect the initial judgment over
a product that makes a statistical prediction of the
ruling the court's actual words with 'hallucination'. In
tions in the legal domain around architectural design
cination issues, that we contend support the necessity
l research.
I's architecture is limited. We acknowledge the
nswer, classification, and summarisation components
Nevertheless, we believe that within the larger LLMs,
available data, they exhibit characteristics of being a
ey are monolithic and exhibit intricate interplays
, and multiple case summaries written in diverse
pping output across each. We recognize that several
llenges, and we propose two alternative solutions -
emble models.

=== Page 3 ===
2.1 Multi-length tokenisation
An essential component of constructing LLMs is the t
which involves training a model to predict the next mo
specifically with its application in the common law do
In this context, a judgment should be represented verb
probabilistic guess the next word approach. This beco
from case law, for example, where a judge may make
under Tort accusing the landlord of not resolving a de
view that where notice has been provided to the landlo
passed, the exposure of the landlord to negligence inc
attempt to resolve a defect that has a natural and unf
prolonged ignorance when notice of the defect has b
It is abundantly clear that the text within a judgment is
model to ingest the facts of a case, consider the argum
that reasoning process, some level of creativity or hall
information that binds together every judicial decision
of the utmost significance, and we argue should be giv
this aspect of common law data needs to be excluded
To provide a brief illustration of how tokenization ope
training the model that the phrase "When for instance
However, this probabilistic prediction method is not w
For instance, as the LLM progresses into the next toke
if we tokenize the words individually within the quote
quote. Alternatively, if we tokenize the entire quote, th
based on predictions derived from preceding text. Thi
complex legal texts that cannot be easily broken down
treated as integral units in their own right.
Although we will be researching this approach, we rec
of which are (1) the context leading up to the quote wi
the case on that judgement, (2) the entire quote might
quotes are also common, the output might not fully su

tokenization process [9] [Mielke, Alyafeai et al]
ost probable word based on the input. Our concern is
omain.
batim as a factual database, rather than relying on a
omes particularly important when considering quotes
a statement such as: The tenant claimed negligence
efect in the property for a period of x years. It is my
ord from the tenant and a reasonable period has
creases, as discussed in X v Y [2012]. "an immediate
fortunate delay cannot be considered equal to
been given".
s of paramount importance. If we were to request the
ments on both sides, and provide an opinion based on
lucination may be acceptable. However, the
n, the quote made bold in the previous paragraph, is
ven "structured data" status. As such, we advocate
from the probabilistic guesswork applied in LLMs.
erates at the word level, consider the example of
one" is often followed with the word "condition"
well-suited to handling quotes within legal judgments.
en which might, for example, be "is" then "onerous"
e, we run the risk of incorrectly representing the
he predictive output could be completely off-topic
is creates a significant difficulty when tokenizing
n into individual words or phrases, yet which must be
cognise the challenges this will face for LLM's. Some
ill often always be different, relating to the facts of
not always be needed, and if subsets of common
upport being bound to the facts of the current case.

=== Page 4 ===
2.2 Ensemble models
Another theoretical approach we are researching is an
trained on different data working together to improve
Using the example provided below: when constructing
the blue phase ‚Äì a description of the underlying proble
criminal investigation, assessing the organisational str
understanding details of an issue from a client grappli
constitutes the "understanding" phase of argumentatio
hand. Moving to the yellow phase, one is able to draw
comprehension to offer thoughtful commentary. Judge
in such cases, while lawyers might link a client's prob
advice. Finally, the green phase involves the direct qu
support and add weight to the argument. Taken togeth
which an effective argument can be built.
Stiletto Visual Programmes (SVP) ordered 47 photogr
(IPL). On the delivery note was a clause stating that tr
delivery. If they were not so returned, a holding fee of
returned the transparencies four weeks later and receiv
contended they had never dealt with IPL before, were
not been sent a copy of their conditions prior to their h
each of the plaintiffs, if at the time when SVP accepte
or by reason of previous experience, or from any othe
of the endorsed conditions, it can hardly be doubted th
would be equally bound if he was aware or had good r
statements intended to affect the relative rights of him
negligently abstained from ascertaining whether there
acquainted with their purport. But I do not think that i
or good reason for belief, SVP was under any obligati
ascertaining whether there were any such statements o
been discussed whether it is enough to look at a set of
one condition in a set is particularly onerous does som
attention to that particular condition? In J. Spurling Lt
Denning stated that "Some clauses which I have seen
document with a red hand pointing to it before the not

n ensemble model, which is a collection of models
the quality of the models for legal research.
g a persuasive argument, one typically begins with
em. This may involve examining the events of a
ructure of a large company as part of a merger, or
ing with complex regulatory issue. Such an analysis
on, as it seeks to establish the fundamental issue at
w upon insights gleaned from experience and
es may use previous rulings to inform their decisions
blem to relevant legislation or case law when offering
uotation from a relevant judgement or legislation to
her, these phases form a robust foundation upon
raphic transparencies from Interfoto Picture Library
ransparencies should be returned within 14 days of
f ¬£5 per transparency per day would be charged. SVP
ved a bill for over ¬£3,700. SVP refused to pay. SVP
unaware of their standard conditions and they had
having returned the transparencies. Now as regards
ed the ticket, SVP, either by actual examination of it,
er cause, was aware of the terms or purport or effect
hat SVP became bound by them. I think also that SVP
reason to believe that there were upon the ticket
mself and the company, but intentionally or
e were any such, or from making themselves
in the absence of any such knowledge or information,
ion to examine the ticket with the view of
or conditions upon it. More recently the question has
f printed conditions as a whole. When for instance
mething special need to be done to draw customers'
td. v. Bradshaw [1956] 1 W.L.R.461 Lord Justice
would need to be printed in red ink on the face of the
tice could be held to be sufficient".

=== Page 5 ===
Problem Model
It is important to note that the "problem" model ought
specifically the summary of the case. The inclusion of
and defences presented throughout the trial would und
data also. It is important to bear in mind that the prima
learn about "problems". This relationship can be liken
larger scale, with the question representing the problem
commentary as output.
Commentary Model
The commentary model essentially functions in a way
argue for a division of this model, we subscribe to the
highlights the distinction between a fact database (e.g.
Our brains automatically send a given problem to eith
illustrate this point, we consider the example of "Wha
be a problem-based fact extraction, namely Paris. Con
Australia?" has historically been rather tricky, with cit
preferred before the correct answer, Canberra, became
logical "System 2" brain comes into play as we seek to
largest city, the most popular, and similar patterns we
Madrid.
Fact Model
In the given context, the fact model pertains to the ver
augment an argument, be it as part of counsel making
function is to lend additional weight to an argument, l
overall framework of reasoning or decision-making at
If the commentary model is system 2, then the problem
Linking them together
The process of determining when to pass the entire pro
model can be complex. One possible solution is to em
three types - Problem, Commentary, and Fact - thereb
triggers a <SOC> Start of Commentary token. This w

t to be trained on the early section of the judgement,
f a detailed and comprehensive account of the claims
doubtedly strengthen the model has it access to that
ary objective is to teach the model to appropriately
ned to a question-and-answer pair albeit on a much
m as input and the answer representing the
y similar to human experience. While some may
e thinking in [10] Dasgupta, Lampinen et al, which
. System 1) and a reasoning engine (e.g. System 2).
her of these systems to find a viable solution. To
at is the capital of France?" where the answer would
nverse to that, the question "What is the capital of
ties such as Sydney, Melbourne, or Perth being
e commonly known. In instances such as these, our
o deduce the answer based on properties such as the
find in other countries such as Paris, London, or
rbatim quote used within the judicial process to
arguments or as part of a judgement. Its primary
lending credibility and persuasive power to the
t hand.
m and fact models are system 1.
oblem text to the commentary model in the problem
mploy a process of breaking down judgements into
by creating an <EOP> End of Problem entity that
would in turn signal a controlling algorithm in the

=== Page 6 ===
ensemble to pass the problem text to the commentary
currently working on an entity of type problem and bu
an <EOC> End of Commentary tag. At this point, it w
be noted that not all <EOC>s would be followed by a
to alternate between problem, commentary, and fact se
Although this approach can be complicated due to hum
segments, implementing a framework where P>C>F r
judgement could make it easier to break open the mon
models working together. It is worth noting that the id
suggests that our brains group different sets of neuron
segregated models in the context of these entities.
An obvious "hand-over" pattern to the fact model wou
Fact Hand-Over Patterns
the tribunal relied on the well-known dictum of Denn
WLR 461, at 466, that:
As Lord Denning said in J Spurling Ltd v Bradshaw
graphically described by Denning LJ in J Spurling L
observed in J. Spurling Limited v. Bradshaw [1956]
In an obiter dictum in J. Spurling Ltd. v. Bradshaw [
Contracts 25th Ed. Vol. 1 at page 408) Lord Justice D
A key challenge here is determining whether the comm
handover pattern." Is the reference to the case and sec
(the quoted evidence bolstering the argument), or is it
guessing the subsequent token, which may be an entir
converge towards the same result, whether through ru
and the hurdle lies in guaranteeing that the commentar
information. If an upstream model hallucinates, a dow
approach, it hallucinates by presenting chunks of text,
make sense but would represent fact verbatim.
A conceivable strategy to overcome this challenge cou
paragraph, or even the complete problem text into the
solely considering the final sentence, as it functions ak
commentary of "J. Spurling Ltd v Bradshaw [1956] W
red ink [‚Ä¶]". However, binding ensemble models thro
Taking some examples from the commentary text prec
is clearly a lot of semantically similar intent:
whether it is enough to look at a set of printed condit
particularly onerous or unusual conditions are brough
the other party

model. The algorithm would understand that it is
uilding an entity of type commentary until it reaches
would move into the fact category. However, it should
<SOF> Start of Fact token, and the model may need
egments in a non-linear fashion.
mans' natural fluidity in moving through such
represents three known pairs for each argument in a
nolithic single LLM and have domain-defined vertical
dea of having a 'System 1' and 'System 2' already
ns for different tasks, underscoring the potential for
uld be:
ning LJ in Spurling (J) Ltd. v Bradshaw [1956] 1
[1956] 1 WLR 461 at 466:
Ltd -v- Bradshaw [1956] 1 WLR 461:
1 WLR 461 at 466:
[1956] 1 W.L.R.461 at page 466 (cited in Chitty on
Denning stated that
mentary model fabricates case law in the "fact
ction of the verdict employed to substantiate the fact
employed as a template and model feature for
re fact from the fact model. Potentially, both paths
ule-based and search strategies or probabilistic ones,
ry model refrains from generating fictitious
wnstream model hallucinates, though with this
, not individual tokens. This "hallucination" wouldn‚Äôt
uld involve refraining from incorporating the entire
fact model. Instead, the focus may shift towards
kin to a Key/Value pair while linking the
WLR 461 at 466" with the fact of "big red hand [‚Ä¶]
ough key value pairs seems to be a sticking plaster.
ceding the case law quotations that are similar, there
tions as a whole
ht fairly and reasonably to the attention of

=== Page 7 ===
He contends that the IAC is so onerous and exceptio
incorporated by a mere reference because specific an
expressed the opinion that he should consider the pla
condition
The features here are "conditions", "onerous", "unusua
almost perfectly lines up for the more "onerous" and "
be provided.
Therefore, semantically similar commentary, prepared
as a sentence, is taken back into a classification model
the case law citation, followed by the fact model whic
model trained as the next sequence based on some larg
commentary and the citation text.
As it is evident, the process of linking these intricate m
Undeniably, our primary objective is to safeguard the
ambiguous hallucinations. Consequently, the proposed
this crucial issue.
Judgement as Fact Issue
Undoubtedly, there exists a salient point that must be
may contain certain elements that cannot be cited, and
never again be referenced in another judgment. We m
be construed as a "fact", training the fact model with a
of adequately and thoroughly respecting the more freq
ensure that irrelevant text in a judgment, which has ne
not negatively impact the fact model. The possibility a
new notable judgment were to emerge, however has n
yet. One potential solution to this issue involves the co
techniques, thereby simplifying the way we obtain rel
support ongoing finetuning of the ensemble models.

onal in its provisions that it cannot be
nd particular notice was required
aintiff had given sufficient notice of this
al", "notice", "sufficient" which when attended to,
"unusual" a "condition" "sufficient" "notice" should
d through tokenisation of LLM's but when fully built
l or key/value pair to predict the next section of text,
ch provides the entire verbatim quote, with the fact
ger sections of preceding text ‚Äì e.g. the semantic
models would present a formidable challenge.
quotations utilised in judgments from any obscure or
d solution places significant emphasis on addressing
taken into consideration, namely that a judgment
d consequently, a subsection of that judgment may
must acknowledge that while an entire judgment may
an entire judgement may detract from the importance
quently cited quotation text. Additionally, we want to
ever been referenced in a subsequent judgment, does
arises that the model could encounter challenges if a
not been cited or referenced in subsequent judgments
ourts proactively enhancing their data labelling
levant information for the model's consumption to

=== Page 8 ===
3 Hallucination Evaluation
With the aim of evaluating the performance of GPT m
Specifically, we will focus on assessing the model's ac
judgments while also exploring its propensity for gene
this process, we hope to gain a comprehensive underst
models in their ability to accurately quote verbatim fa
Using the Complete mode of Playground with the text
It is evident that the GPT models rely heavily on the in
subsequent section rather than producing an exact repl
quote. Nonetheless, the output sentiment demonstrates
brought to the others' notice", "reasonable opportunit
which he was unaware." These phrases arguably conv
"Some clauses I have seen would need to be printed in
pointing to it before the notice could be held to be suff
of GPT might have assigned more weight to their own
wording used in the judgment. There is also the possib
making previous variants more imaginative and less d
In order to advance the evaluation process, the evaluat
Playground, where the temperature is adjusted to 0 alo
Finish this sequence with the quote that follows this pa

models, we will leverage OpenAI as a testing ground.
ccuracy in reproducing factual content from legal
erating false information ("hallucinations"). Through
tanding of the capabilities and limitations of the GPT
act from judgments.
t-davinci-003 model we get the following results:
nput sequence and textual context to anticipate the
lica of the forthcoming statement, such as the red ink
s similarities in regard to phrases such as "sufficiently
ty of knowing the contents", and "term or condition of
vey semantic equivalences to the original quote of
n red ink on the face of the document with a red hand
ficient." This observation indicates that older models
n interpretation of the facts instead of the precise
bility that Temperature has been a contributing factor,
definitive.
tion is transferred to GPT-4 within the Chat Mode of
ong with a predetermined prompt:
aragraph:

=== Page 9 ===
"Consideration of the effect of an exclusive jurisdictio
not it is an unusual or onerous term I think that the nu
come before this court is testament to the fact that the
Paragraph 1 of Schedule 4 to the Civil Jurisdiction an
The purpose of this eval is to test whether the model, w
follow-on quote verbatim.
The eval should test the following:
ÔÇ∑ Quote Check
o Verbatim match with the source judg
o Close to verbatim match including ac
o Non-verbatim match with similar sem
o Non-verbatim match with unrelated i
ÔÇ∑ Hallucination Check
o Whether the close to verbatim match
o Whether those differences are due to

on clause is central to the assessment as to whether or
umber of cases in which exclusive jurisdiction clauses
ey are not unusual. Are they, however, onerous?
nd Judgments Act 1982 provides "
with enough preceding context, knows to output the
gement.
ccompanying text.
mantic intent.
intent.
h includes subtle, non-obvious differences.
o human error or AI error.

=== Page 10 ===
Case Title Sequence Judgement Quote
McLaughlin & [24] Most disputes which arise "Some clauses which I hav
Harvey Ltd v relate to the third test and the ‚Ä¶.would need to be printe
Lockton issue of reasonable sufficiency of ink on the face of the docu
Companies notice, namely whether the party with a read hand pointing
International tendering the document has done before the notice could be
Ltd all that was reasonably sufficient be sufficient."
to give the other notice of the
conditions. Whether they did so
or not is a question of fact (Chitty
13-014). It is well established that
where the party tendering the
document knows it contains an
onerous or unusual term it must
show that it has been brought
fairly and reasonably to the other's
attention (Chitty 13-015). As Lord
Denning said in J Spurling Ltd v
Bradshaw [1956] 1 WLR 461 at
466:

Generative Quote Comments
ve seen "The more unreasonable the Close to verbatim with
ed in red clause, the greater the notice accompanying text.
ument which must be given of it. Some
to it clauses which I have seen would However, interestingly the
held to need to be printed in red ink on judgement has a typo, which can
the face of the document with a be validated [11] here, however
red hand pointing to it before the the model correctly selects red
notice could be held to be over read, which is an interesting
sufficient." indication where on this
occasion, the probabilistic next
token selection won over a
human spelling mistake.
Inclusion of "the more
unreasonable the clause" on the
generative quote isnt ideal,
however on this occasion, it
likely wouldn‚Äôt have pulled the
advice in another direction or
had a material impact.

=== Page 11 ===
J Spurling Ltd Therefore, in finding that the "It must be remembered th
v Bradshaw conditions formed part of the limitation clause of this kin
contract, it does not seem to me applies where the damage,
that the learned Judge was out of respect of which the limita
line with what Mr Justice clause is operative, takes p
Bailhache said. Nor that there is within the four corners of
anything that controverts what contract. But there is no ro
was said by Mr Justice Lush in the application of that prin
Crooks v. Allan in 5 Queen's the present case, because t
Bench Division, page 38; at page no material for finding tha
40, where there is another passage loss of these handkerchiefs
relied upon by Mr Sofer. It seems due to some act by the def
to me, therefore, that these outside what they had con
conditions were a part of the to do"
contract. Then in this case the
question becomes whether,
assuming that the goods were
negligently kept by the Plaintiffs,
the conditions protect. It seems to
me that they plainly do. In
Alderslade v. Hendon Laundry,
1945 1 King's Bench, Lord
Greene, Master of the Rolls, in his
Judgment at page 192, said this:

hat a "In my opinion, the true rule is Non-verbatim match with similar
ind only that the party seeking to rely upon semantic intent.
, in a condition of that kind must
ation show that the condition covers the The generative quote has similar
place breach, and that the breach is not semantic meaning to the
the due to a failure to take reasonable judgement quote, including
oom for care, or, in other words, is not due statements like "show the
nciple in to negligence." condition covers the breach" and
there is "is not due to negligence".
at the
s was
fendants
ntracted

=== Page 12 ===
Donoghue v The case of Winterbottom Wright "The only safe rule is to confine "The only safe rule is to confine Verbatim match with source
Stevenson 10 M & W 109 is, on the other the right to recover to those who the right to recover to those who judgement.
hand, an authority that is closely enter into the contract; if we go enter into the contract; if we go
applicable. Owing to negligence one step beyond that, there is no one step beyond that, there is no Perhaps due to the popularity of
in the construction of a carriage it reason why we should not go reason why we should not go this case.
broke down, and a stranger to the fifty." fifty."
manufacture and sale sought to
recover damages for injuries
which he alleged were due to
negligence in the work, and it was
held that he had no cause of
action either in tort or arising out
of contract. This case seems to me
to show that the manufacturer of
any article is not liable to a third
party injured by negligent
construction, for there can be
nothing in the character of a
coach to place it in a special
category. It may be noted, also,
that in this case Alderson B. said
10 M & W 115 :

=== Page 13 ===
Gillick v West "I respectfully agree with every "upon the application of th
Norfolk and word of that and especially with mother of any infant [whe
Wisbech AHA the description of the father's over 16 or not] make such
authority as a dwindling right. In as it may think fit regardin
J. v. C. [1970] AC 668 Lord custody of such infant and
Guest and Lord MacDermott right of access thereto of e
referred to the decision in Agar- parent, having regard to th
Ellis, 24 Ch D. 317 as an example welfare of the infant, and t
of the almost absolute power conduct of the parents ‚Ä¶"
asserted by the father over his (Emphasis added).
children before the Judicature Act
1873 and plainly thought such an
assertion was out of place at the
present time: see Lord
MacDermott at pp. 703‚Äì704. In
Reg. v. D. [1984] A.C. 778 Lord
Brandon of Oakbrook cited Agar-
Ellis as an example of the older
view of a father's authority which
his Lordship and the other
members of the House rejected. In
my opinion, the view of absolute
paternal authority continuing until
a child attains majority which was
applied in Agar-Ellis is so out of
line with present day views that it
should no longer be treated as
having any authority. I regard it as
a historical curiosity. As Fox L.J.
pointed out in the Court of
Appeal, ante, p. 141H, the Agar-
Ellis cases (1878) 10 Ch D 49; 24
Ch D. 317 seemed to have been
regarded as somewhat extreme
even in their own day, as they
were quickly followed by the
Guardianship of Infants Act 1886

he "upon the application of the Close to verbatim with
ether mother of any infant (who may accompanying text.
h order apply without next friend), make
ng the such order as it may think fit The judge is quoting legislation
d the regarding the custody of such and then from all the research we
either infant and the right of access have done, looks to add square
he thereto of either parent, having brackets with his own opinion,
to the regard to the welfare of the infant, inside the quote of the legislation
" and to the conduct of the parents, ‚Äì we cannot find legislation that
and to the wishes as well of the says, "whether over 16 or not".
mother as of the father, and may
alter, vary, or discharge such The model correctly includes
order on the application of either what the legislation actually says
parent, or, after the death of either (who may apply without next
parent, of any guardian under this friend) but the judgement
Act, and may make such order ignored that from the legislation.
respecting the costs of the
application as it may think just." Even between the generative AI
output and the only source of
data we can find [12] on this
legislation there are
discrepancies as below.
"and in every case may make
such order respecting the costs of
the mother and the liability of the
father for the same, or otherwise
as to costs, application as it may
think just."
We consider the word
"application" rather than "mother
and the liability of the father for
the same, or otherwise as to
costs" as a hallucination, where
the model is weighting the
previous word "application", and

=== Page 14 ===
(49 & 50 Vict.c.27) which, by
section 5, provided that the court
may:"
Case of Steel 37. Under English law the object "any person who causes or
and Morris v of a libel action is to vindicate the procures or authorises or c
The United plaintiff's reputation and to make in or approves the publicat
Kingdom reparation for the injury done by libel is as liable for its pub
the wrongful publication of as a person who physically
defamatory statements concerning it or sends it off to another
him or her. not necessary to have writt
printed the defamatory ma
38. The plaintiff carries the All those jointly concerned
burden of proving ‚Äúpublication‚Äù. commission of a tort (civil
As a matter of law (per Bell J at p. are jointly and severally lia
5 of the judgment in the it, and this applies to libel
applicants' case) does to any other tort".

then "costs of the " ‚Ä¶
"application" will likely me
much more popular as a next
token than "mother" which
doesn‚Äôt flow well at all, and is a
result of common legalese
ellipsis e.g. use of "‚Ä¶ for the
same" is implicit rather than
explicit and is likely to
exacerbate the hallucination
problem.
r "the publication of a libel takes Non-verbatim match with similar
concurs place when the defamatory matter semantic intent.
tion of a is comprehended by the reader,
blication listener, or observer. In other The language is different, though
y hands words, it is the act of conveying the model aims to make the same
r. It is the defamatory meaning to a third point, that those guilty is anyone
tten or person or persons who understand involved in the publication,
aterial. it in that sense." rather than simply the person
d in the who wrote or printed the
l wrong) defamatory material.
iable for
as it We searched the web for similar
output or language from the
model but couldn‚Äôt find
anything. In this example, the
model understands the follow-on
quote, tweaks the synonyms and
order of the words, but has
similar semantic intent.
It is also impressive the model
was aware enough to output
semantic similarity with such a
small amount of input text.

=== Page 15 ===
Jones v Balfour What the tribunal's decision "Where (in accordance wit
News boiled down to therefore was this: following provisions of thi
first, that, as was evident on the the Secretary of State mak
face of the documents, Mr Jones employment tribunal proce
did not have two years qualifying regulations, the regulation
service; second, that there was no provide that any act which
evidence of any breach of any required or authorised by t
statutory right that he was regulations to be done by a
asserting or dismissal for employment tribunal and i
asserting such a right and, thirdly, description specified by th
that there was no evidence upon regulations for the purpose
which it could be said that this subsection may be don
s.100(1)(d) applied. The the person mentioned in
Chairman was not making a subsection (1)(a) alone."
decision as to whether or not she
accepted what Mr Jones said
about the sun streaming in and
making the shop unbearably hot,
she was simply deciding that, on
the evidence which had been
placed before her, there was no
case for him to say that he did not
require two years service before
he was entitled to assert a claim
for unfair dismissal and she
therefore decided the arguments
as to his entitlement against him
and it followed that, as had been
suggested might happen in the
letter of 9th January 1998, he was
not someone who could take
advantage of the 1996 Act and
claim for unfair dismissal.
It seems to us that those findings
were unobjectionable and that no
real point of law could be raised

th the "A tribunal shall be constituted by Non-verbatim match with
is Part) a chairman sitting alone if it unrelated intent.
kes appears to the chairman that the
edure proceedings can be more Model seems completely
ns may conveniently or expeditiously unaware of the preceding
h is disposed of by a chairman sitting sequence, and it is attempting to
the alone than by a tribunal compute a follow-on quote by
an constituted in accordance with the text we have provided to it.
is of a subsection (1) or (2) and the
he parties to the proceedings This is odd and suggests the
es of consent." model is unaware the case exists.
ne by We ask the question of GPT-4
and it replies "The case of Jones
v Balfour News [2000] does not
appear to exist in the UK legal
system. There may be a similar
case with a different name, but
without more information, it is
not possible to provide the
regulations within this specific
case."
Is absence of knowledge of a
case considered hallucination? In
the "provide case law to support
my argument" use case,
excluding less well-known cases
may not cause issue, however
"check the other-side's defence
for case misrepresentation"
might provide odd results if the
models are unaware certain case
law exists, but can be found on
Lexis/Westlaw.

=== Page 16 ===
as a result of them. However, it Perhaps a legal strategy in future
has been said, on behalf of Mr could be to cite cases generative
Jones by Ms Stacey, who has AI is unaware of.
represented him extremely
competently through the ELAAS
scheme, that the decision of the
Chairman was a decision which
should never have been made
because she should have been
sitting not as a Chairman alone
but with other members of a
tribunal.
It seems to us that that is not a
submission that we should accept.
A tribunal can be composed of a
Chairman sitting alone in certain
circumstances. In this instance, it
seems to us, that the tribunal
could comprise a Chairman sitting
alone, because by virtue of s.4(6)
of the Employment Tribunals Act
1996 it is provided:

=== Page 17 ===
AEG (UK) Ltd. In my judgment, the District "...whether the customer k
v Logic Judge erred in two respects as a ought reasonably to have k
Resource Ltd matter of law: first, by failing to of the existence and extent
apply the Interfoto test as term (having regard among
described by Chitty; and, things to any custom of the
secondly, by treating condition and any previous course o
7.5 in isolation and not in context, dealing between the partie
and thus adopting a flawed
approach to the proper
construction of the condition
which is also a question of law. I
would add that I would also
criticise him in a third respect,
namely in relation to his reference
to insurance, there being no
evidence before him that this
would have been an insurable risk
at the instance of the defendants.
It follows that the appellants are
entitled to succeed in this appeal
on the incorporation ground, so
that it is not necessary to consider
UCTA, save to say that, in my
judgment, the respondents, on
whom the burden of proof lies
under UCTA, must a fortiori fail
to satisfy the UCTA
reasonableness test. This is
because the schedule 2 guidelines,
in paragraph (c), require the Court
to take into account:

knew or "the extent to which it was open Non-verbatim match with similar
known to the parties to enter into a semantic intent.
t of the different contract or to agree to a
g other variation of the term in question, Is "knowing the existence and
e trade and the extent to which the party extent of the term" equivalent to
of subject to the term or the party "‚Ä¶ had the opportunity to
es.)" seeking to rely on it had an negotiate the term". Is "taking
opportunity to negotiate the term, into consideration the relative
taking into consideration the bargaining power of the parties"
relative bargaining power of the equivalent to "having regard
parties and any other relevant among other things to any
circumstances." custom of the trade".
Knowing and negotiating (if you
negotiate, you know) seem closer
as synonyms, so the first section
is perhaps "semantic enough".
"Bargaining power of the
parties" doesn‚Äôt seem to
semantically map to "having
regard among other things to any
custom of the trade" albeit
bargaining power could be
considered a custom of the trade.
The model has some semantic
understanding of the follow-on
quote it seems but is perhaps
conflating another similar
semantic summary.

=== Page 18 ===
Barclays Bank The equitable principle applied in "I must not be understood
Plc v O'Brien Turnbull v. Duval and Chaplin v. laying down a rule that in
Brammall seems to be that if a where a wife acts on her
wife signs a security document at husband's instructions and
her debtor husband's request, the his influence is it necessar
creditor will be unable to enforce show that she has received
the security unless either the independent advice ... but,
debtor or the creditor has taken opinion, there cannot be a
positive steps to try and ensure universal application that t
that the wife understands the of the Courts of Equity as
import of the security documents confidential relationships
or unless she has obtained necessarily apply to the re
independent advice. The two husband and wife".
cases contrast oddly with
Bainbriqqe v. Browne, where no
such requirement was imposed on
the father's mortgagees and, since
there was no undue influence or
misrepresentation by the debtor,
cannot, in my opinion, be
explained by treating the debtor as
the creditor's agent.
In Howes v. Bishop [1909] 2 KB
390 the creditor obtained
iudgment against a debtor and it
was then agreed that the debtor
and his wife would give a joint
and several promissory note for
the amount of the judgment. The
wife signed the note. The jury
found that she understood what
she was doing but that her
signature had been procured by
her husband's influence. The jury
did not, however, find undue
influence. The wife had no

to be "It is quite true that if a person in Non-verbatim match with similar
no case a fiduciary position obtains a semantic intent.
security from a person under his
d under influence, the security is not Although there is similar intent,
ry to binding unless the person giving it it looks like the model is treating
d has had independent advice. But the input as a question, ignoring
, in my that is not this case. Here the wife the instruction to continue the
rule of was not giving a security; she was sequence as it has done before.
the rules giving a joint and several Searching "she was giving a joint
to promissory note. The jury have and several promissory note."
found that she understood what does bring up Barclays v O'Brien
elation of she was doing, and that there was on Bailli.
no undue influence. In these
circumstances, I think that the For some reason, the model has
wife is bound by her signature." missed the "intent" of the prompt
on this occasion.

=== Page 19 ===
independent advice. The Court of
Appeal held that the wife was
bound.
Lord Alverstone C.J. said at page
395:
Butler Machine If those documents are analysed "... the counter-offer kills t
Tool Co Ltd. v in our traditional method, the original offer".
Ex-Cell-O Corp result would seem to me to be
(England) Ltd. this: The quotation of the 23rd
May, 1969 was an offer by the
sellers to the buyers containing
the terms and conditions on the
back. The order of the 27th May,
1969 purported to be an
acceptance of that offer in that it
was for the same machine at the
same price, but it contained such
additions as to cost of installation,
date of delivery and so forth that
it was in law a rejection of the
offer and constituted a counter-
offer. As Mr. Justice Megaw said
in Trollope & Colls Ltd. & ors. v.
Atomic Power Constructions Ltd.
[1963] 1 W.L.R. 333 at page 337:

the "An acceptance which is not in Non-verbatim match with similar
the terms of the offer is not an semantic intent.
acceptance at all; it is a counter-
offer, which must be accepted by What is quite fascinating about
the original offeror before a this example, is no where online
contract is made." can we find sections of the
generative quote. The model is
clearly reasoning across all
summaries and the verbatim
judgement that is referenced
(provided below) to articulate the
same meaning, but in a more
verbose way. Perhaps due to
model temperature with the
judgement quote being too
prescriptive.
The [‚Ä¶] from the judgement "To
make a contract, there must be an
offer and an acceptance. If an
offer is rejected, there may be a
counter-offer, but the counter-
offer kills the original offer."
Swapping out the generative
quote with the verbatim text
from the judgement shouldn‚Äôt
impact the overall meaning of
the judgement referencing this
point.

=== Page 20 ===
Farley v Skinner 24. Interpreting the dicta of "A ruling that intangible interests "It is submitted that the Non-verbatim match with
Bingham LJ in Watts v Morrow only qualify for legal protection distinction between contracts unrelated intent
narrowly the Court of Appeal in where they are the "very object of whose central object is to provide
Knott v Bolton ruled that the the contract" is tantamount to a pleasure, relaxation or peace of The generative quote seems to be
central object of the contract was ruling that contracts where these mind and those which are not is conflating [13] sections of other
to design a house, not to provide interests are merely important, but unprincipled and unworkable. The quotes within the judgements
pleasure to the occupiers of the not the central object of the better view is that, in principle, e.g. "object of a contract is to
house. It is important, however, to contract, are in part damages for non-pecuniary loss provide pleasure, relaxation,
note that Knott v Bolton was unenforceable. It is very difficult should be recoverable for breach peace of mind ‚Ä¶"
decided a few months before the to see what policy objection there of any contract if it was
decision of the House in Ruxley can be to parties to a contract reasonably foreseeable that such
Electronics and Construction Ltd agreeing that these interests are to loss would be suffered as a result
v Forsyth [1996] AC 344. In any be protected via contracts where of the breach."
event, the technicality of the the central object is something
reasoning in Knott v Bolton, and else. If the defendant is unwilling
therefore in the Court of Appeal to accept this responsibility he or
judgments in the present case, is she can say so and either no
apparent. It is obvious, and contract will be made or one will
conceded, that if an architect is be made but including a
employed only to design a disclaimer."
staircase, or a surveyor is
employed only to investigate
aircraft noise, the breach of such a
distinct obligation may result in
an award of non-pecuniary
damages. Logically the same must
be the case if the architect or
surveyor, apart from entering into
a general retainer, concludes a
separate contract, separately
remunerated, in respect of the
design of a staircase or the
investigation of aircraft noise. If
this is so the distinction drawn in
Knott v Bolton and in the present
case is a matter of form and not
substance. David Capper,

=== Page 21 ===
"Damages for Distress and
Disappointment - The Limits of
Watts v Morrow" (2000) 116
LQR 553, 556) has persuasively
argued:
Prest v Petrodel "8. Subject to very limited ‚Äúno shareholder has any ri
Resources exceptions, most of which are any item of property owne
Limited and statutory, a company is a legal the company, for he has no
others entity distinct from its or equitable interest therei
shareholders. It has rights and entitled to a share in the pr
liabilities of its own which are while the company continu
distinct from those of its carry on business and a sh
shareholders. Its property is its the distribution of the surp
own, and not that of its assets when the company i
shareholders. In Salomon v A wound up.‚Äù
Salomon and Co Ltd [1897] AC
22, the House of Lords held that
these principles applied as much
to a company that was wholly
owned and controlled by one man
as to any other company. In
Macaura v Northern Assurance
Co Ltd [1925] AC 619, the House
of Lords held that the sole owner
and controller of a company did
not even have an insurable
interest in property of the
company, although economically
he was liable to suffer by its
destruction. Lord Buckmaster, at
pp 626‚Äì627 said:"

ight to 'The company is at law a different Non-verbatim match with similar
ed by person altogether from the semantic intent.
o legal subscribers to the memorandum;
in. He is and, though it may be that after This is very interesting. The
rofits incorporation the business is model has pulled "verbatim" a
ues to precisely the same as it was quote from Salomon v A
hare in before, and the same persons are Salomon which has in effect
plus managers, and the same hands similar semantic intent.
is receive the profits, the company is
not in law the agent of the In Salomon, the preceding quote
subscribers or trustee for them. says
Nor are the subscribers as "I cannot understand how a body
members liable, in any shape or corporate thus made ‚Äúcapable‚Äù
form, except to the extent and in by statute can lose its
the manner provided by the Act.'" individuality by issuing the bulk
of its capital to one person"
which binds semantically to
"wholly owned and controlled by
one man as any other company".
The model is wrong based on the
instruction, but the hallucination
is sensible.

=== Page 22 ===
Blackpool and I readily accept that contracts are ‚ÄúWhat was the mechanism
Fylde Aero not to be lightly implied. Having offer and acceptance?‚Äù
Club Limited v examined what the parties said
Blackpool and did, the court must be able to
Borough conclude with confidence both
Council that the parties intended to create
contractual relations and that the
agreement was to the effect
contended for. It must also, in
most cases, be able to answer the
question posed by Mustill LJ in
The Kapetan Markos N.L. (NO.2)
[1987] 2 Ll. 321 at 331:
Secretary of 34. As to the tort's essential 'when the act induced is w
State for Health elements, Lord Hoffmann the right of the immediate
and another v addressed first what constitutes and is therefore not wrong
Servier unlawful means. It was primarily so far as he is concerned, i
Laboratories Ltd on this issue that Lord Nicholls yet be to the detriment of a
and others disagreed with the majority. party; and in that case ‚Ä¶th
35. Lord Hoffmann began his inducer may be held liable
analysis as follows: can be shewn to have proc
"45. The most important question object by the use of illegal
concerning this tort is what should directed against that third p
count as unlawful means. It will
be recalled that in Allen v Flood
[1898] AC 1, 96 , Lord Watson
described the tort thus:

m for "What is the contract? What are Non-verbatim match with
its terms? What are the parties' unrelated intent
respective obligations? What are
the consequences of a failure to It seems the model is weighting
perform?" the introduction of "be able to
answer the question posed by ‚Ä¶"
over the context of the
legislation. This is perhaps
resulting in shotgunning of
multiple questions.
within "46. 'The wrongful acts which are Non-verbatim match with
actor, the subject of this action consist unrelated intent
gful in in maliciously inducing a person
it may to break a contract with another, Although there are some similar
a third or to abstain from entering into a words like "induce" and "act",
he contract with another, or to refuse the deviation for the text it too
e if he to employ another, or to dismiss significant that we don‚Äôt believe
cured his him from his employment, or to it would be appropriate to say
l means do any other act whereby another they are semantically similar,
party.' may sustain damage, by means of however, in defining "unlawful
threats, intimidation, or means" both could be considered
molestation, or by other unlawful definitions of that term, albeit
means.'" not similar definitions.

=== Page 23 ===
London Although the ISC case has not "definition of charity; a gif
Borough of been without its academic critics: general public use, which
Merton Council see Tudor on Charities, 11th ed to the poor as well as to th
v Nuffield (2023), para 1-182, this aspect of
Health the Upper Tribunal's reasoning is,
as the editors of Tudor note, well
supported by earlier authority. In
Jones v Williams (1767) 2 Amb
651 the gift was for supplying the
inhabitants of Chepstow (not just
the poor inhabitants) with water.
Holding that purpose to be
charitable, Lord Hardwicke LC
said (at p 652):

ft to a "Nothing can be more beneficial Non-verbatim match with
extends to society than the improvement unrelated intent
he rich." of the health and convenience of
the inhabitants; and if the purpose There is similarity, however the
is beneficial to the public, it is a key point of the judgement text
charity." is that charity extends too rich
and poor. Like the previous
example, both could pass for
describing "the purpose to be
charitable".

=== Page 24 ===
The University 10. The protection of confidential 10. The protection of conf
of Dundee v communications between client communications between
Prasun and lawyer lay at the heart of and lawyer lay at the heart
Chakraborty legal professional privilege ( legal professional privileg
Ventouris v Mountain [1991] 1 Ventouris v Mountain [199
WLR 607 at 475 ). WLR 607 at 475 ).
Communications between clients Communications between
and lawyers should be "secure and lawyers should be "sec
against the possibility of any against the possibility of a
scrutiny from others" ( Three scrutiny from others" ( Th
Rivers DC v Bank of England Rivers DC v Bank of Engl
(No. 6) [2005] 1 AC 610 at para (No. 6) [2005] 1 AC 610 a
34 ). There had to be a "relevant 34 ). There had to be a "re
legal context" in which the legal context" in which the
communications were made ( ibid communications were mad
at paras 38, 62 and 111). The at paras 38, 62 and 111). T
advice could relate to the rights, advice could relate to the r
liabilities, obligations or remedies liabilities, obligations or re
of the client under private or of the client under private
public law ( ibid ). In this case the public law ( ibid ). In this
advice related to the interpretation advice related to the interp
and sufficiency of the matters and sufficiency of the matt
discussed in the original report. discussed in the original re
The advice may relate partly to The advice may relate part
the rights or obligations of others the rights or obligations of
( ibid at para 56). It may not be ( ibid at para 56). It may n
strictly legal but could be strictly legal but could be
presentational or revisal. presentational or revisal.

fidential 10. The protection of confidential Non-verbatim match with
client communications between client unrelated intent
t of and lawyer lay at the heart of
ge ( legal professional privilege ( Using a slightly different
91] 1 Ventouris v Mountain [1991] 1 component of OpenAI (Insert)
WLR 607 at 475 ). we now begin testing the
clients Communications between clients inserting of blanks to detect
cure and lawyers should be "secure whether the model is fully aware
any against the possibility of any of the context. This uses text-
hree scrutiny from others" ( Three davinci-003 which is less
land Rivers DC v Bank of England sophisticated than GPT-4. GPT-4
at para (No. 6) [2005] 1 AC 610 at para is unavailable in Insert within
elevant 34 ). There had to be a Playground.
e "reasonable expectation of
de ( ibid confidentiality" in which the We can see this highlights the
The communications were made ( ibid subtle, non-obvious hallucination
rights, at paras 38, 62 and 111). The problem, which wont always be
emedies advice could relate to the rights, obvious. Within the context of
or liabilities, obligations or remedies the judgement quote, the
case the of the client under private or generative replacement of
pretation public law ( ibid ). In this case the "reasonable expectation of
tters advice related to the interpretation confidentiality" arguably makes
eport. and sufficiency of the matters more sense than "relevant legal
tly to discussed in the original report. context".
f others The advice may relate partly to
not be the rights or obligations of others
( ibid at para 56). It may not be
strictly legal but could be
presentational or revisal.

=== Page 25 ===
Pizza Express 27. There was no dispute as to 27. There was no dispute
Group Ltd v the applicable principles of the applicable principles o
Liberty Mutual construction. They have been construction. They have be
Insurance addressed in a number of the addressed in a number of t
Europe SE COVID-19 BI insurance COVID-19 BI insurance
authorities by reference to authorities by reference to
previous Supreme Court previous Supreme Court
authority, including Wood v authority, including Wood
Capita Insurance Services Ltd Capita Insurance Services
[2017] AC 1173 and Arnold v [2017] AC 1173 and Arno
Britton [2015] AC 1619 . The Britton [2015] AC 1619 . T
essential principles are as follows: essential principles are as f
i) The Policy must be construed i) The Policy must be con
objectively by asking what a objectively by asking wha
reasonable policyholder, with all reasonable policyholder, w
the background knowledge which the background knowledge
would reasonably have been would reasonably have bee
available to both parties when available to both parties w
they entered into the contract, they entered into the contr
would have understood the would have understood the
language of the Policy to mean. language of the Policy to m
ii) This does not involve "a ii) This does not involve "
literalist exercise focussed solely literalist exercise focussed
on a parsing of the wording of the on a parsing of the wordin
particular clause": Wood v Capita particular clause": Wood v
at [10]. Instead, it is essential to at [10]. Instead, it is essent
construe contractual words in construe contractual words
their applicable context. Their their applicable context. T
meaning must be assessed in the meaning must be assessed
context of the clause in which context of the clause in wh
they appear as well as in the they appear as well as in th
landscape of the document as a landscape of the document
whole. whole.
iii) The unitary exercise of iii) The unitary exercise o
contractual construction can contractual construction ca
require the court to give weight to require the court to give w
the implications of rival the implications of rival

as to 27. There was no dispute as to Non-verbatim match with similar
of the applicable principles of semantic intent.
een construction. They have been
the addressed in a number of the This is semantically similar in
COVID-19 BI insurance that it is an opposite. This does
o authorities by reference to not involve "a literalist exercise
previous Supreme Court focussed solely on a parsing of
d v authority, including Wood v the wording of the particular
Ltd Capita Insurance Services Ltd clause" it to an extent the
old v [2017] AC 1173 and Arnold v opposite of "a search for the
The Britton [2015] AC 1619 . The intention of the parties" e.g., the
follows: essential principles are as follows: analysis should not be literal
nstrued i) The Policy must be construed focussed only on clause wording,
at a objectively by asking what a but "instead" it should focus on
with all reasonable policyholder, with all words within context ‚Ä¶
e which the background knowledge which searching for the intention of the
en would reasonably have been parties.
when available to both parties when
ract, they entered into the contract, "Searching for the intention of
e would have understood the the parties" is very common run
mean. language of the Policy to mean. of text, and it is relevant to the
"a ii) This does not involve "a entire section of text from the
d solely search for the intention of the judgement, although different to
ng of the parties": Wood v Capita at [10]. the verbatim judgement.
v Capita Instead, it is essential to construe
tial to contractual words in their
s in applicable context. Their meaning
Their must be assessed in the context of
d in the the clause in which they appear as
hich well as in the landscape of the
he document as a whole.
t as a iii) The unitary exercise of
contractual construction can
of require the court to give weight to
an the implications of rival
weight to constructions by reaching a view
as to which construction is more

=== Page 26 ===
constructions by reaching a view constructions by reaching
as to which construction is more as to which construction is
consistent with commercial consistent with commercia
common sense. However, common sense. However,
commercial common sense should commercial common sens
not be invoked retrospectively, or not be invoked retrospectiv
to rewrite a contract, in an attempt to rewrite a contract, in an
to assist an unwise party or to to assist an unwise party o
penalise an astute party. penalise an astute party.

a view consistent with commercial
s more common sense. However,
al commercial common sense should
not be invoked retrospectively, or
se should to rewrite a contract, in an attempt
vely, or to assist an unwise party or to
n attempt penalise an astute party.
or to

=== Page 27 ===
JTI Polska Sp 15. The trial of the respondents‚Äô 15. The trial of the respond
Zoo v claim for the Excise Duty was claim for the Excise Duty
Jakubowski heard by Judge Pelling KC (‚Äúthe heard by Judge Pelling KC
judge‚Äù), sitting as a High Court judge‚Äù), sitting as a High C
Judge, on 26 May 2021. The Judge, on 26 May 2021. T
appellants accepted that in light of appellants accepted that in
the Buchanan decision the judge the Buchanan decision the
was bound to hold that the Excise was bound to hold that the
Duty was recoverable under Duty was recoverable und
article 23.4 of the CMR but they article 23.4 of the CMR bu
contended that the decision was contended that the decision
wrong and should be departed wrong and should be depa
from. They accordingly made an from. They accordingly m
application for a certificate under application for a certificate
section 12 of the Administration section 12 of the Administ
of Justice Act 1969 that the case of Justice Act 1969 that th
was suitable for an appeal directly was suitable for an appeal
to the Supreme Court. The judge to the Supreme Court. The
granted the certificate, principally granted the certificate, prin
on the basis of criticism of on the basis of criticism of
Buchanan by the leading English Buchanan by the leading E
commentators on the CMR and commentators on the CMR
the uncertainty created by the the uncertainty created by
decision of the Court of Appeal in decision of the Court of A
Sandeman Coprimar SA v Sandeman Coprimar SA v
Transitos y Transportes Integrales Transitos y Transportes In
SL [2003] EWCA Civ 113; SL [2003] EWCA Civ 113
[2003] QB 1270 (Sandeman). In [2003] QB 1270 (Sandema
that decision Buchanan was that decision Buchanan wa
criticised and distinguished and it criticised and distinguished
was stated that the decision was stated that the decisio
should not be ‚Äúapplied any more should not be ‚Äúapplied any
widely by the courts of this widely by the courts of thi
country than respect for the country than respect for th
doctrine of precedent requires‚Äù doctrine of precedent requ
(para 38). (para 38).

dents‚Äô 15. The trial of the respondents‚Äô Non-verbatim match with similar
was claim for the Excise Duty was semantic intent.
C (‚Äúthe heard by Judge Pelling KC (‚Äúthe
Court judge‚Äù), sitting as a High Court "Followed without careful
The Judge, on 26 May 2021. The consideration" is not wrong, as a
n light of appellants accepted that in light of broad high-level explanation for
e judge the Buchanan decision the judge additional "charges" that can be
e Excise was bound to hold that the Excise recovered from a carrier under
der Duty was recoverable under Article 23.4 of the CMR, with
ut they article 23.4 of the CMR but they different EU jurisdictions
n was contended that the decision was accepting either a wide or narrow
arted wrong and should be departed definition of "charges".
made an from. They accordingly made an Therefore, "careful
e under application for a certificate under consideration" is valid, albeit far
tration section 12 of the Administration from the more specific "applied
he case of Justice Act 1969 that the case any more widely" statement,
directly was suitable for an appeal directly which the courts in England
e judge to the Supreme Court. The judge detailed as a narrower definition.
ncipally granted the certificate, principally
f on the basis of criticism of Note: JTI was heard after the
English Buchanan by the leading English 2021 cut-off date for the GPT
R and commentators on the CMR and models, which could play a role
the the uncertainty created by the here, though Buchanan is well
Appeal in decision of the Court of Appeal in documented and was heard over
v Sandeman Coprimar SA v 20 years ago.
ntegrales Transitos y Transportes Integrales
3; SL [2003] EWCA Civ 113;
an). In [2003] QB 1270 (Sandeman). In
as that decision Buchanan was
d and it criticised and distinguished and it
on was stated that the decision
y more should not be ‚Äúfollowed without
is careful consideration‚Äù (para 38).
he
uires‚Äù

=== Page 28 ===
DSG Retail 87. The next step is to consider 87. The next step is to con
Limited v the applicable English law as to the applicable English law
Mastercard the nature of a cause of action. the nature of a cause of ac
Incorporated 88. In Paragon Finance (1998), 88. In Paragon Finance (1
Millett LJ recalled the classic Millett LJ recalled the clas
definitions of a cause of action as definitions of a cause of ac
follows at page 405: follows at page 405:
"The classic definition of a cause "The classic definition of a
of action was given by Brett J in of action was given by Bre
Cooke v Gill (1873) LR 8 CP 107 Cooke v Gill (1873) LR 8
[" Cooke v. Gill "] at p. 116:- [" Cooke v. Gill "] at p. 11
"Cause of action" has been held "Cause of action" has been
from the earliest time to mean from the earliest time to m
every fact which is material to be every fact which is materia
proved to entitle the plaintiff to proved to entitle the plaint
succeed - every fact which the succeed - every fact which
defendant would have a right to defendant would have a rig
traverse" (my emphasis). traverse" (my emphasis).

nsider 87. The next step is to consider Non-verbatim match with similar
w as to the applicable English law as to semantic intent.
ction. the nature of a cause of action.
1998), 88. In Paragon Finance (1998), The generative "cause of action"
ssic Millett LJ recalled the classic quote is a valid definition,
ction as definitions of a cause of action as however comes from another
follows at page 405: judgement, [14] [Letang v
a cause "The classic definition of a cause Cooper] with one subtle
ett J in of action was given by Brett J in difference, the exclusion of the
CP 107 Cooke v Gill (1873) LR 8 CP 107 word "simply":
16:- [" Cooke v. Gill "] at p. 116:-
n held "A cause of action is a factual "A cause of action is simply a
mean situation the existence of which factual situation the existence of
al to be entitles one person to obtain from which entitles one person to
tiff to the court a remedy against another obtain from the court a remedy
h the person." (my emphasis). against another person."
ght to
Interestingly, research on Google
shows both versions are used,
with the version that includes
"simply" much more popular.
This suggests the model has
decided, as some humans have
done who are repeating this
judgement language, that
"simply" is not material and can
be safely excluded for clarity.

=== Page 29 ===
Harlow 46. The Court has jurisdiction to 46. The Court has jurisdic
Higinbotham stay or strike out a claim where no stay or strike out a claim w
(formerly real or substantial wrong has been real or substantial wrong h
BWK) v committed and litigating the claim committed and litigating th
Teekhungam & will yield no tangible or will yield no tangible or
another legitimate benefit to the claimant legitimate benefit to the cl
proportionate to the likely costs proportionate to the likely
and use of court procedures: in and use of court procedure
other words, " the game is not other words, " the game is
worth the candle ": Jameel [69]- worth the candle ": Jameel
[70] per Lord Phillips MR; [70] per Lord Phillips MR
Schellenberg -v- BBC [2000] Schellenberg -v- BBC [20
EMLR 296 , 319 per Eady J. The EMLR 296 , 319 per Eady
jurisdiction is useful where a jurisdiction is useful where
claim " is obviously pointless or claim " is obviously pointl
wasteful ": Vidal-Hall -v- Google wasteful ": Vidal-Hall -v-
Inc [2016] QB 1003 [136] per Inc [2016] QB 1003 [136]
Lord Dyson MR. Although Lord Dyson MR. Although
Jameel was a defamation claim Jameel was a defamation c
(and defamation claims present (and defamation claims pr
particular features) the particular features) the
jurisdiction is of general jurisdiction is of general
application: Sullivan -v- Bristol application: Sullivan -v- B
Film Studios Limited [2012] Film Studios Limited [201
EMLR 27 and has been held to EMLR 27 and has been he
extend to malicious falsehood extend to malicious falseh
claims: Niche Products [63] and claims: Niche Products [63
c.f. Tesla Motors [47]-[49]. c.f. Tesla Motors [47]-[49]

ction to 46. The Court has jurisdiction to Non-verbatim match with similar
where no stay or strike out a claim where no semantic intent.
has been real or substantial wrong has been
he claim committed and litigating the claim The term "vexatious" defines the
will yield no tangible or text that precedes the quote:
laimant legitimate benefit to the claimant
costs proportionate to the likely costs "denoting an action or the
es: in and use of court procedures: in bringer of an action that is
s not other words, "vexatious brought without sufficient
l [69]- litigation": Jameel [69]-[70] per grounds for winning, purely to
R; Lord Phillips MR; Schellenberg - cause annoyance to the
000] v- BBC [2000] EMLR 296 , 319 defendant."
y J. The per Eady J. The jurisdiction is
e a useful where a claim " is However, this does not refer
less or obviously pointless or wasteful ": correctly to the quote from the
Google Vidal-Hall -v- Google Inc [2016] judgement.
] per QB 1003 [136] per Lord Dyson
h MR. Although Jameel was a
claim defamation claim (and defamation
resent claims present particular features)
the jurisdiction is of general
application: Sullivan -v- Bristol
Bristol Film Studios Limited [2012]
12] EMLR 27 and has been held to
eld to extend to malicious falsehood
hood claims: Niche Products [63] and
3] and c.f. Tesla Motors [47]-[49].
].

=== Page 30 ===
Williamson v 31. Mr Wynne pointed out 31. Mr Wynne pointed ou
Bishop of correctly that section 42 does not correctly that section 42 do
London specify the consequences of a specify the consequences o
claim brought in breach of a CPO. claim brought in breach of
Moreover, although on the face of Moreover, although on the
it section 42 appears to envisage it section 42 appears to env
that leave will be a condition that leave will be a conditi
precedent to the institution of precedent to the institution
proceedings (save in relation to proceedings (save in relati
existing proceedings at the time of existing proceedings at the
the CPO), he pointed out that this the CPO), he pointed out t
is achieved by saying that that is achieved by saying that
will be the nature of the order will be the nature of the or
made by way of a CPO, rather made by way of a CPO, ra
than by providing for it directly than by providing for it dir
(unlike section 139(2) MHA ). (unlike section 139(2) MH
Accordingly, and since court Accordingly, and since cou
orders with conditions precedent orders with conditions pre
(like unless orders) have always (like unless orders) have a
been subject to relief from been subject to relief from
sanction, he submitted that the sanction, he submitted that
same must be true here. These are same must be true here. Th
legitimate points to make, but it legitimate points to make,
does not follow that that was the does not follow that that w
statutory intention. No doubt statutory intention. No dou
section 42 had to take this form section 42 had to take this
because CPOs are to be granted because CPOs are to be gr
on a litigant by litigant basis. In on a litigant by litigant bas
any event, I do not consider that any event, I do not conside
this distinction can dictate the this distinction can dictate
right answer. Nor is the statutory right answer. Nor is the sta
language on its own language on its own
determinative, as Lord Bingham determinative, as Lord Bin
in Seal made clear. in Seal made clear.
32. As Lord Bingham explained 32. As Lord Bingham exp
in R (Quintavalle) v Secretary of in R (Quintavalle) v Secre

ut 31. Mr Wynne pointed out Non-verbatim match with similar
oes not correctly that section 42 does not semantic intent.
of a specify the consequences of a
f a CPO. claim brought in breach of a CPO. The quote "mischief at which the
e face of Moreover, although on the face of statute was aimed" seems to be
visage it section 42 appears to envisage odd language however is
ion that leave will be a condition commonly used to describe the
n of precedent to the institution of purpose of a statute: used in
ion to proceedings (save in relation to some other judgements [15] here
e time of existing proceedings at the time of and [16] here.
that this the CPO), he pointed out that this
that is achieved by saying that that The summary does a reasonable
rder will be the nature of the order job of representing the quote
ather made by way of a CPO, rather from the judgement.
rectly than by providing for it directly
HA ). (unlike section 139(2) MHA ).
ourt Accordingly, and since court
ecedent orders with conditions precedent
always (like unless orders) have always
m been subject to relief from
t the sanction, he submitted that the
hese are same must be true here. These are
but it legitimate points to make, but it
was the does not follow that that was the
ubt statutory intention. No doubt
form section 42 had to take this form
ranted because CPOs are to be granted
sis. In on a litigant by litigant basis. In
er that any event, I do not consider that
e the this distinction can dictate the
atutory right answer. Nor is the statutory
language on its own
ngham determinative, as Lord Bingham
in Seal made clear.
plained 32. As Lord Bingham explained
etary of in R (Quintavalle) v Secretary of

=== Page 31 ===
State for Health [2003] UKHL 13; State for Health [2003] UKHL 13; State for Health [2003] UKHL 13;
[2003] 2 AC 687 at paragraph 8 : [2003] 2 AC 687 at paragraph 8 : [2003] 2 AC 687 at paragraph 8 :
"Every statute other than a pure "Every statute other than a pure "The court must look at the
consolidating statute is, after all, consolidating statute is, after all, language used, the context in
enacted to make some change, or enacted to make some change, or which it is used, the purpose of
address some problem, or remove address some problem, or remove the statute and the material
some blemish, or effect some some blemish, or effect some available to show the mischief at
improvement in the national life. improvement in the national life. which the statute was aimed."
The court's task, within the The court's task, within the
permissible bounds of permissible bounds of
interpretation, is to give effect to interpretation, is to give effect to
Parliament's purpose. So the Parliament's purpose. So the
controversial provisions should be controversial provisions should be
read in the context of the statute read in the context of the statute
as a whole, and the statute as a as a whole, and the statute as a
whole should be read in the whole should be read in the
historical context of the situation historical context of the situation
which led to its enactment." which led to its enactment."

=== Page 32 ===
3.1 Discussion
The results are broken down as follows:
Verbatim match Close to verbatim match
with the source including accompanying
judgement. text within the source
judgement.
1 2
Based on twenty trials, we found more than fifty perce
with similar semantic intent. These outcomes indicate
the same or similar meaning, though in the majority o
the original language, as opposed to verbatim quotes.
Our experimentation with the model revealed a solitar
Stevenson. We attribute this success, in part, to the ca
"The only safe rule is to confine the right to recover to
beyond that, there is no reason why we should not go
contexts, including [17] case summaries, [18] judicial
data suggests that the model places greater emphasis o
We experienced two closely matched instances:
In McLaughlin v Lockton, we encountered an anecdot
guessed the next word, surpassing a human made erro
exceptions rather than the rule.
In Gillick v West Norfolk presented a more complex s
brackets resulted in contaminating the quoted legislati
replaced the square brackets with a component exclud
legislation. While the model's rendition of the legislati
provided by the model exhibited factual inaccuracies i
nuanced and complicated nature of the hallucination p
It is worth mentioning that the majority of our evaluat
comparable semantic intent - signifying the same or n
unrelated intent, a clear indication of the model's hallu
It's our argument that:
Verbatim match with source judgement ‚Äì Achievin
considered fortunate. However, the degree of success
to which the core judgment is condensed in online sum
something of a game of chance, especially in the case
is clear and straightforward, allowing for concise sum
of the judgment.

Non-verbatim match Non-verbatim match with
with similar semantic unrelated intent.
intent.
11 6
ent of the results that were non-verbatim matches
e that the statements replaced by the model retained
of cases, they were concise, summarized variations of
ry, flawless match in the context of Donoghue v
ase's widespread notoriety. The quote in question -
o those who enter into the contract; if we go one step
fifty" - is routinely cited online within a variety of
l opinions and also [19] academic publications. This
on well-known quotes.
tal example where the probabilistic model correctly
or within the judgement. However, such cases are
scenario where the court's commentary within square
ion. The model, albeit ignoring the judge's remarks,
ded from the judgment but found within the
ive preamble was more accurate, the subsequent text
in comparison to the statute. This exemplifies the
problem in legal research.
tions produced non-verbatim matches, either with
nearly the same meaning - or with completely
ucinative tendencies.
ng a verbatim match with the source judgment can be
often depends on various factors, such as the extent
mmaries. Consequently, securing an exact match is
of popular instances where the underlying case law
mmaries that do not stray from the verbatim language

=== Page 33 ===
Close to verbatim match including accompanying t
the accompanying text, can occur in several forms. In
court's side, indicating that the model is not always res
such outcomes should not be relied upon consistently.
sometimes modify quotes from previous judgments or
contamination of data could be a significant challenge
information accurately. Additionally, this also emphas
brackets, a crucial aspect in the legal industry for main
Non-verbatim match with similar semantic intent ‚Äì
challenges associated with training LLMs on data from
matches while retaining similar semantic intent. This i
understanding of case law, legislation, and summaries
draw a comparison between common law and machin
languages. We speculate that if models were trained w
language, hallucination and "code contamination" wou
law may progress to a point where only court-sourced
interpretation are unnecessary, and case law and legisl
code.
Non-verbatim match with unrelated intent ‚Äì Non-v
the endpoint of the hallucination spectrum. Nonethele
ambiguity, contingent on the level of output from the
"relevant legal context" could be substituted for "reaso
any alarms, particularly in the case of confidential com
emphasising how careful evaluation is essential even f
the intended meaning remains intact.
To summarise, our paper highlights that hallucination
should not be dismissed as an over-arching fictitious c
consumer nor enterprise-level use of ChatGPT deliver
exposure effectively. Even the most careful and astute
outcomes, underlining the need for the legal industry t
sufficient safeguards.
We would like to preface that we are not against the a
Reasoning side of the model, we are enthusiastic abou
indicate that it performs quite effectively in volume-ty
and dispute resolution. We believe that advancements
market as quickly as possible. However, if we disrega
need to limit the use of LLMs for legal research, their
unlawful, depriving us of a significant AI-driven adva
Ultimately, we believe that entities such as Lexis, Tho
advantage as the primary custodians of the most critic
regulatory rules. Although our team would like to enh
"hallucination verification" research, we do not have a

text ‚Äì Obtaining a close-to-verbatim match, including
the context of McLaughlin, the error was on the
sponsible for the hallucination of data - although
. Alternatively, Gillick highlights how courts
r legislation, demonstrating how even slight
e for LLMs seeking to understand and interpret
sizes the need for models to comprehend square
ntaining accuracy in the data they process.
‚Äì One of the most interesting yet predictable
m the entire internet is achieving non-verbatim
issue is rather significant since it necessitates an
s of that data. In computer science parlance, we can
ne code and case summaries and higher-level
with machine code and equivalent higher-level
uld arise on a considerable scale. It's possible that the
d legal data is utilised, given that summary and
lation explicitly satisfy the terms of deterministic
verbatim matching with unrelated intent represents
ess, it presents the greatest potential for subtlety and
model. For instance, in Dundee v Prasan, the term
onable expectation of confidentiality" without raising
mmunications. Such a shift may go unnoticed,
for seemingly minor changes to determine whether
of data in legal language is a nuanced problem and
case law issue. Our experiments reveal that neither
rs the precise controls necessary for mitigating
e lawyers may fall prey to some of these hallucinatory
to consider the acquisition and application of
application of LLMs. More specifically, regarding the
ut the opportunities it presents. Our experiments
ype assignments such as verification, contract review,
s in LLM research in this area should be brought to
ard the issue of common law contamination and the
use in the industry could rapidly be rendered
ancement in legal reasoning.
omson Reuters, and Casetext possess a significant
cal legal data, i.e., common law, legislative and
hance the capabilities of YCNBot to provide
access to "justice data." However, our AI team has

=== Page 34 ===
shared their findings with the [20] Open Justice Data m
working together to support open-source practitioners
betterment of all.
5 YCNBot Enhancement
Part of our AI strategy is committed to open sourcing
ongoing experimentation and education of LLMs in a
not obvious errors we have discussed within this pape
has now been open sourced, to detect case law and wh
functionality, working to mitigate any risk. The check
name formats, as we lack direct access to a case law d
videos we do internally, making it clear the system sh
looking to add a similar "legislation detection" algorit
You can now clone the most recent version of YCNBo
https://github.com/Travers-Smith/YCNBot

movement. We encourage others to do the same,
s and help improve these technologies for the
functionality within YCNBot which supports the
safe and balanced way. Due to some of the subtle,
er, we have released a new feature to YCNBot which
here it exists in output, block the "Copy and Paste"
k is based on regular expressions and common case
database. This is also on top of guidance and training
hould not be used for Legal Research. We will be
thm which will have the same effect.
ot with this feature here:

=== Page 35 ===
6 References
1. Locating and editing factual associations - https://arxiv.org/abs/2202.05262
2. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering -
https://arxiv.org/abs/2007.01282
3. LexisAI - https://www.law.com/legaltechnews/2023/05/04/lexisnexis-announces-generative-ai-
platform-lexis-ai-automating-search-drafting-and-summary-
tasks/#:~:text=%E2%80%9CI%20don%E2%80%99t%20think,is%20taking%20place.%E2%80%9D
4. Sanctions for use of generative AI -
https://storage.courtlistener.com/recap/gov.uscourts.nysd.575368/gov.uscourts.nysd.575368.33.0_1.pdf
5. Standing Order on use of generative AI -
https://web.archive.org/web/20230530223247/https://www.txnd.uscourts.gov/judge/judge-brantley-starr
6. Texas Judge's AI Order Not Unreasonable, And Not The Last -
https://www.law360.com/pulse/articles/1683137
7. OpenAI to focus on 'hallucinations' - https://www.cnbc.com/2023/05/31/openai-is-pursuing-a-new-way-
to-fight-ai-hallucinations.html
8. Access to YCNBot on Github - https://github.com/Travers-Smith/YCNBot
9. A brief history of ‚Ä¶ Tokenisation - https://arxiv.org/abs/2112.10508
10. Human like reasoning - https://arxiv.org/pdf/2207.07051.pdf
11. Typo in McLaughlin v Lockton -
https://www.bailii.org/nie/cases/NIHC/Master/2017/2.html#:~:text=with%20a%20read%20hand%20pointi
ng
12: Guardianship of infants act - https://www.global-
regulation.com/law/montserrat/8167759/guardianship-of-infants-act.html
13. Conflating judgement text in Farley v Skinner -
https://publications.parliament.uk/pa/ld200102/ldjudgmt/jd011011/farley-
3.htm#:~:text=%22Where%20the%20very,this%20exceptional%20category.%22
14. Conflating 'cause of action' definitions, Letang v Cooper -
https://uk.practicallaw.thomsonreuters.com/Document/IE03AAC50E42711DA8FC2A0F0355337E9/View/
FullText.html?originationContext=document&transitionType=DocumentItem&ppcid=660e3657450e4682
93ab25a558ac1587&contextData=(sc.Default)&comp=wluk#:~:text=A%20cause%20of%20action%20is%
20simply%20a%20factual%20situation%20the%20existence%20of%20which%20entitles%20one%20pers
on%20to%20obtain%20from%20the%20court%20a%20remedy%20against%20another%20person.
15. Mischief at which the statute was aimed (1) -
https://publications.parliament.uk/pa/ld200506/ldjudgmt/jd051201/dfund-
1.htm#:~:text=Thus%20the%20mischief%20at%20which%20the%20statute%20was%20aimed%20was%
20the%20absence%20in%20practice%20of%20any%20such%20exit%20route.

=== Page 36 ===
16. Mischief at which the statute was aimed (2) -https://www.judiciary.uk/wp-
content/uploads/2019/11/Birmingham-CC-v-Afsar-No-3-2019-EWHC-3217-QB-Final.pdf
17. Donoghue v Stevenson case summaries -
https://en.wikipedia.org/wiki/Donoghue_v_Stevenson#:~:text=the%20only%20safe%20rule%20is%20to%
20confine%20the%20right%20to%20recover%20to%20those%20who%20enter%20into%20the%20contra
ct%3B%20if%20we%20go%20one%20step%20beyond%20that%2C%20there%20is%20no%20reason%2
0why%20we%20should%20not%20go%20fifty
18. Donoghue v Stevenson judicial opinions - https://www.law.cornell.edu/supremecourt/text/100/195
19. Donoghue v Stevenson academic publications -
https://books.google.co.uk/books?id=C_7bBAAAQBAJ&pg=PA120&lpg=PA120&dq=%22The+only+saf
e+rule+is+to+confine+the+right+to+recover+to+those+who+enter+into+the+contract;+if+we+go+one+ste
p+beyond+that,+there+is+no+reason+why+we+should+not+go+fifty.%22&source=bl&ots=blVd1y0AcT
&sig=ACfU3U03ig8pzE907AmTgD8aCzVpGkQWQA&hl=en&sa=X&ved=2ahUKEwiGiNKS_Mj_Ah
WWilwKHeyaCjE4ChDoAXoECAUQAw#v=onepage&q=%22The%20only%20safe%20rule%20is%20to
%20confine%20the%20right%20to%20recover%20to%20those%20who%20enter%20into%20the%20con
tract%3B%20if%20we%20go%20one%20step%20beyond%20that%2C%20there%20is%20no%20reason
%20why%20we%20should%20not%20go%20fifty.%22&f=false
20. Open Justice Data - https://www.gov.uk/government/consultations/open-justice-the-way-forward/call-
for-evidence-document-open-justice-the-way-forward

Paper:HaluMem - Evaluating Hallucinations in Memory Systems of Agents.pdf
=== Page 1 ===
HaluMem: Evaluating Ha
Systems of Agents
Ding Chen*1, Simin Niu*2, Kehang Li2, Peng
Li1, Feiyu Xiong2, Zhiyu Li2(cid:66)
1China Telecom Research Institute, 2MemTensor (Sh
Abst
Memory systems are key components that enab
achieve long-term learning and sustained inter
retrieval, these systems frequently exhibit memo
conflicts, and omissions. Existing evaluations of
question answering, which makes it difficult to lo
system where hallucinations arise. To address t
Benchmark (HaluMem), the first operation level
memory systems. HaluMem defines three evaluat
andmemoryquestionanswering)tocomprehensiv
operational stages of interaction. To support ev
human-AI interaction datasets, HaluMem-Mediu
memory points and 3.5k multi-type questions. T
and 2.6k turns, with context lengths exceeding 1
across different context scales and task complexi
thatexistingmemorysystemstendtogeneratean
and updating stages, which subsequently propaga
research should focus on developing interpretable
that systematically suppress hallucinations and i
Date: January 6, 2026
Correspondence: xpzheng@hrbeu.edu.cn, lizy@memte
AuthorLegend: *Co-equal primary author
Code: https://github.com/MemTensor/HaluMem
Datasets: https://huggingface.co/datasets/IAAR-
1 Introduction
Each interaction between a user and an LLM may con
However, such information is often forgotten once the
continuously understand the user, adapt to persona s
ensure that LLMs maintain coherence and personalizat
mechanism capable of recording, updating, and utilizin
of a memory system.
A memory system serves as the fundamental infrastruct
1
6202
naJ
5
]LC.sc[
3v60530.1152:viXra

allucinations in Memory
g Liu2, Xiangping Zheng3(cid:66) , Bo Tang2, Xinchi
hanghai) Technology, 3Harbin Engineering University
tract
ble AI systems such as LLMs and AI agents to
raction. However, during memory storage and
ory hallucinations, including fabrication, errors,
memory hallucinations are primarily end-to-end
ocalize the operational stage within the memory
this, we introduce the Hallucination in Memory
l hallucination evaluation benchmark tailored to
tion tasks (memory extraction, memory updating,
velyrevealhallucinationbehaviorsacrossdifferent
valuation, we construct user-centric, multi-turn
um and HaluMem-Long. Both include about 15k
The average dialogue length per user reaches 1.5k
1M tokens, enabling evaluation of hallucinations
ities. Empirical studies based on HaluMem show
ndaccumulatehallucinationsduringtheextraction
ate errors to the question answering stage. Future
e and constrained memory operation mechanisms
improve memory reliability.
ensor.cn
-Shanghai/HaluMem
ntain personalized information about the user [22, 28].
conversation ends, making it difficult for the model to
shifts, or generate personalized responses [16, 26]. To
tion in long-term interactions, it is crucial to develop a
ng user information, which constitutes the core function
ture for organizing and managing information based on
1

=== Page 2 ===
the history of human‚ÄìAI conversations. It extracts, s
generatedacrossmulti-turninteractionsbetweenusersa
intothemodelasneededtosupportpersonalizationand
a memory system identifies stable user profiles, narra
plaintextentriesenrichedwithmetadata. Whennewqu
integrates relevant memories based on the current inte
and correctly utilize‚Äù user information, thereby preser
preference alignment. Representative systems such as
andMemobase[25]continuouslyrecorduserprofiles, ev
revision, and tracking of memories to construct a syst
capabilities.
Mem Ext Halu (User)
...
"user": "Recently, I deeply fascinated by the parrots...
It made me reconsider my previous dislike for them.",
"assistant": "You overcome your dislike of parrots...",
...
Extracted MP: Dislikes parrots Golden MP: Likes parrots
Mem Upd Halu
...
"assistant": "That's understandable... I recall your
health status improved from 'Below Normal' to 'Good'...",
"user": "Actually, my health has worsened to 'Poor'..."
...
Updated MP: ... health status changed... to 'Good'
Golden MP: ... health status worsened to 'Poor'...
Figure 1 Examples of operation-leve
Althoughthesesystemssignificantlyimprovetheorganiz
affected by the phenomenon of memory hallucination
conflicting, or missing information during the processe
issues undermine the accuracy and consistency of mem
areoftenamplifiedduringthegenerationstage,further
the overall reliability of the system. To effectively mit
establish a systematic hallucination evaluation mechan
for evaluating hallucinations in memory systems remai
question‚Äìanswer-based evaluation frameworks that asse
performance of AI systems, making it difficult to de
hallucination originates.
Toaddressthisissue,weproposetheHallucinationinM
hallucination evaluation benchmark for memory system
and HaluMem-Long. HaluMem constructs an evaluat
memory extraction, memory updating, and memory q
the hallucination behaviors of memory systems across
However, achieving operation-level evaluation of hallu
it requires a multi-turn conversational dataset that ca
generation, updating, and retrieval across interactions1.
under evaluation and requires the system to identify an
accumulate throughout the conversation. To this end
preciseannotationsforeachmemoryoperationanditsc
comparingthememorypointsproducedbyamemorysy
1The dataset we designed is a user-centered multi-turn hum
factthatmemorysystemsareinherentlyintendedtosupportp
functionalityliesinorganizing,storing,andupdatinguser-relat
morerealisticevaluationofhowhallucinationsoccurwhenmem
2

structures, and continuously updates key information
andAIsystems,retrievingandinjectingthisinformation
dlong-termconsistency[10,14,19,20,25]. Specifically,
atives, and events from dialogues and stores them as
ueriesortasksarise, thesystemretrievesandselectively
ent and context, enabling the AI system to ‚Äúremember
rving semantic coherence, behavioral consistency, and
s MemOS [14], Mem0 [3], Zep [19], Supermemory [20],
vents, andevolvingpreferences, supportingthecreation,
tem-level memory layer with structured management
Mem Ext Halu (Assistant)
...
"assistant": "Congratulations... I remember you mentioned
that your friends Linda and Mark supported you...",
"user": "Thank you! My friends Linda and Joseph..."
...
Extracted MP: Mark... Golden MP: Joseph...
Mem QA Halu
...
"user": "I was recently promoted to Senior Researcher...",
"assistant": "Congratulations, Barbara...",
"question": "What is Barbara's job title now?"
...
Mem System Response: ... Lead Research Scientist...
Golden Answer: ... Senior Researcher...
el hallucination in a memory system.
zationandcontrollabilityofmemory,theyarecommonly
n (Figure 1), which manifests as fabricated, erroneous,
es of memory extraction or updating [1, 18, 27]. Such
mory. More importantly, these upstream hallucinations
inducinggenerationhallucination[7]andcompromising
tigate these hallucination phenomena, it is essential to
nism for memory systems. However, existing methods
in limited. Most mainstream studies adopt end-to-end,
ess memory quality indirectly through the final output
etermine at which stage of the memory process the
MemoryBenchmark(HaluMem),thefirstoperation-level
ms, which comprises two datasets, HaluMem-Medium
tion framework that encompasses three types of tasks:
question answering, in order to comprehensively reveal
different operational levels of interaction.
ucinations in memory systems is a nontrivial task, as
an comprehensively represent the processes of memory
. Such a dataset is used as input to the memory system
nd process memories of different operational types that
d, we provide a multi-turn conversation dataset with
correspondingresult,referredtoasamemorypoint. By
ystemwiththeannotatedground-truthmemorypoints,
man‚ÄìAI conversation dataset. This design is motivated by the
personalizedandlong-termhuman‚ÄìAIinteractions. Theircore
tedmemoriesthroughoutcontinuousdialogue,whichenablesa
morysystemsorganizeandupdatememoriesaroundtheuser.
2

=== Page 3 ===
we can perform fine-grained evaluation to determine
memory updating, or question answering (i.e., memo
coverage to assess hallucinations caused by errors or f
evaluate hallucinations arising from errors or omission
question answering that result from incorrect reference
Based on this design, we construct two benchmark da
datasets contain approximately 15,000 memory point
user involved in over one thousand conversational turn
dialogue context length per user to the scale of milli
behaviors in ultra-long conversations.
The main contributions of this work are summarized a
‚Ä¢ We propose HaluMem, the first operation-leve
which overcomes the limitations of prior end-to
hallucination phenomena across three operationa
and memory-based question answering.
‚Ä¢ We construct an extensive multi-turn evaluatio
user-centered benchmarks, HaluMem-Medium an
of memory systems under different contextual sc
‚Ä¢ Through stage-wise evaluation, we reveal the cum
memory extraction, updating, and question an
understanding and mitigating hallucinations in m
2 Related Work
2.1 Memory System
Large Language Models (LLMs) and AI Agents built u
knowledge is primarily embedded within model param
thereby forming a parameterized form of long-term me
to demonstrate strong knowledge recall and reasoning a
poormanageability‚Äîtheinternalmemoryofthemodel
there is no mechanism for controlling its lifecycle. When
model often struggles to revise or replace old knowledge
as the generation of erroneous, obsolete, or inconsisten
Table 1 Comparison of V
Method MemoryType
Supermemory[20] PlainText(withMetadata)
Memobase[25] PlainText(withMetadata)
Zep[19] PlainText(withMetadata)
Mem0[3] PlainText(withMetadata)
MemOS[14] Parameter;Activation;PlainText(withMe
Various forms of external memory modules have been
memory. Early external memory mechanisms were prim
(RAG). In particular, RAG [13] introduces an extern
generation, relevant documents are retrieved from a ve
enabling controllable and updatable external memory.
the external memory is transparent and editable, it ex
However, traditional RAG systems primarily rely on te
of inter-entity relationships, meaning they do not su
3

e whether hallucinations occur in memory extraction,
ory retrieval). Specifically, we measure accuracy and
fabrications in memory extraction, use consistency to
ns in memory updating, and identify hallucinations in
es or fabricated content.
atasets: HaluMem-Medium and HaluMem-Long. Both
ts and more than 3,400 evaluation queries, with each
ns on average. The latter further extends the average
ions of tokens, allowing examination of hallucination
as follows:
el benchmark for evaluating memory hallucinations,
o-end evaluation methods by systematically revealing
al dimensions: memory extraction, memory updating,
on dataset for human‚ÄìAI interactions and design two
nd HaluMem-Long, to evaluate hallucination behaviors
cales and task complexities.
mulative and amplifying effects of hallucinations across
nswering, providing a new analytical perspective for
memory systems.
upon them possess implicit memory capabilities, where
meters through large-scale pre-training and fine-tuning,
emory. Although such implicit memory enables LLMs
abilities during inference and generation, it suffers from
cannotbeexplicitlyaccessed, updated, ordeleted, and
n encountering outdated or conflicting information, the
e, which can lead to memory hallucination, manifesting
nt content.
Various Memory Systems.
MemoryOperation Manageability GraphSupport
CUDE fair‚ÄìExcellent Yes
CUD Excellent No
CUD fair‚ÄìExcellent Yes
CUD fair‚ÄìExcellent Yes
etadata) CUD fair‚ÄìExcellent Yes
n proposed to address the limitations of parameterized
marily represented by Retrieval-Augmented Generation
nal plaintext knowledge retrieval mechanism. Before
ector database and incorporated into the model‚Äôs input,
This approach offers high manageability, and because
xhibits a relatively low degree of memory hallucination.
ext-based memory structures and lack explicit modeling
upport graph structures. Consequently, they remain
3

=== Page 4 ===
limited in handling complex knowledge reasoning an
this, GraphRAG [4] further integrates a knowledge gr
the form of entity‚Äìrelation pairs. By leveraging grap
significantly enhances the representational capacity an
to improved performance in relational reasoning. Nev
structures entail high costs, and synchronization during
GraphRAG demonstrates moderate manageability, in
memory hallucinations due to inconsistent updates am
WiththegrowingdemandforAIsystemscapableofpers
have begun exploring memory systems that possess
controllability, as summarized in Table 1. Supermemor
by combining document retrieval and user-specific mem
and agent memory, using a contextual graph that cap
enabling consistent and personalized responses across i
term memory by recording preferences and interaction
it dynamically generates context snippets from user p
though some risk of hallucination may occur during
engineering framework that integrates agent memory,
its core component Graphiti enabling temporally-awa
personalizedlong-termcontext. Mem0[3]employsame
comprehensive memory operations (Create/Extract,
conflict detection and memory merging to ensure co
abstractmemoryasasystem-levelresourcebyunifying
memory, activation memory, and explicit (plaintext) m
and migration mechanisms, MemOS enables cross-mo
However, while graph structures enhance the express
management complexity and make the system more pr
2.2 Evaluation Hallucinations in Memory Syst
Table 2 HaluMem vs. Existing End-to-End
Feature HaluMem PersonaMe
EvaluationGranularity Operation-level End-to-en
EvaluationTiming Aftereachsession Afterallsess
MemoryExtraction,
EvaluationTasks MemoryUpdating, MultipleCh
MemoryQA
Persona,
MemoryType Event, Persona
Relationship
MemoryUpdate Yes Yes
ConversationTimeSpan 10‚àº20years Severalyea
AvgLength/Session 8.3ktokens 6ktokens
MaxContextLength 1Mtokens 1Mtoken
QuestionNum 3,467 ‚àº6,000
*‚ÄúSeveralyears‚ÄùforPersonaMemisinferredfromthepaperanddataset,n
Hallucinations in memory systems can be divided int
hallucinations. The former refers to inconsistencies
updating, orretrievinginformationwithinamemorysy
unresolved conflicts, or incorrect retrievals. The latter r
phase of language models, where the model produces
context. These two types of hallucinations are close
4

nd maintaining long-term consistency. Building upon
raph structure, organizing and retrieving knowledge in
ph indexing and multi-hop path retrieval, GraphRAG
nd retrieval accuracy of structured knowledge, leading
vertheless, the construction and maintenance of graph
g updates introduces additional complexity. As a result,
nferior to that of RAG, and may introduce additional
mong graph nodes or edges.
sonalizedinteractionandlong-termlearning,researchers
s genuine long-term maintainability and operational
ry [20] provides long-term memory for language models
mory. It integrates both retrieval-augmented generation
ptures temporal, relational, and personal information,
interactions. Memobase [25] focuses on user-level long-
n histories in a plaintext structure. During interactions,
profiles and recent events to enable personalized recall,
g memory extraction. Zep [19] introduces a context
Graph RAG, and context assembly capabilities, with
are synthesis of conversational and business data for
etadata-enrichedplaintextstorageformatthatsupports
Update, Delete, and Expand/Enrich), incorporating
onsistency and traceability. MemOS [14] attempts to
themanagementofthreetypesofmemory: parametric
memory. Through lifecycle control, version management,
del and cross-session memory sharing and integration.
siveness of memory representation, they also increase
rone to hallucination.
tems
Benchmarks for Memory System Evaluation
em LOCOMO LongMemEval PrefEval
nd End-to-end End-to-end End-to-end
sions Afterallsessions Afterallsessions Afterallsessions
QA,
QA, Generation,
hoice Summarization,
MemoryRecall Classification
Generation
Persona, Persona,
Persona
Event Event
No Yes Yes
ars* Severalmonths ‚àº2.5years -
s 477tokens 3ktokens -
ns 9ktokens 1.5Mtokens 100ktokens
7,512 500 3,000
notexplicitlylabeled.
nto two types: memory hallucinations and generation
or errors that occur during the processes of storing,
ystem, suchasfabricatedmemories, outdatedmemories,
refers to hallucinations that arise during the generation
s outputs inconsistent with factual truth or the given
ely interrelated: memory hallucinations often act as
4

=== Page 5 ===
upstream causes of generation hallucinations, while gen
memory-related errors.
Generation hallucinations are the most extensively s
are typically divided into two categories: factual hallu
hallucinations assess whether the model output aligns w
evaluate whether the output remains faithful to the gi
categories, researchers have proposed a variety of matu
external-retrieval-based factual verification [12], which
bases; model-internal-state-based reliability assessme
activation patterns to estimate hallucination risk; b
verifiability [15]; and uncertainty- or LLM-discrimina
These approaches have substantially improved the inte
generationlevel,leadingtorelativelymatureandsystem
In contrast, research on hallucinations in memory syste
LoCoMo, LongMemEval, PrefEval, and PersonaMem (
rather than hallucination-specific evaluation. The early
under long-context settings. It adopts an end-to-end ev
answering, summarization, and generation tasks to t
texts. Although the dataset is relatively large (about
months and lacks an explicit memory updating mech
static information retention. Subsequently, LongMemE
such as Information Retention Rate and Memory R
multi-turn dialogue and incorporating explicit memo
time. This represents a shift from static evaluation
of personalization, PrefEval [28] evaluates a model‚Äôs
long-term interactions, using generation and classifica
remains limited to persona-level memory. PersonaMe
event histories, employing multiple-choice evaluations
accuracy. With a longer time span (on the order of ye
personalized long-term memory assessment.
Existing benchmarks primarily adopt holistic, end-to
boxes, making hallucinations only indirectly observable
attributable to specific memory operations. In contrast
to evaluate hallucinations in memory systems, enabli
critical gap left by prior memory evaluation benchmar
3 Problem Definition
Mem
D
Mem
Ext
E
Mem
Upd
U
HaluMemEval. (Ours)
Mem Ext HaluEval. Mem UpdHaluEval.
Session Mem System S Mem P
Figure 2 Comparison between HaluMem an
Let there be a memory system S that endows an AI sys
memoryandpersonalizationcapabilities. Thememorys
the user and the assistant, denoted as D = (u ,a ),
1 1
utterances of the user and the AI at turn i, respective
and a single memory is defined as m. With respect
5

neration hallucinations may further amplify or obscure
studied type of hallucination in current research and
ucinations and faithfulness hallucinations [8]. Factual
with objective facts, whereas faithfulness hallucinations
iven context or source information. Around these two
ure evaluation methods and metric systems, including:
h compares generated content with external knowledge
ent [2, 23], which analyzes attention distributions or
behavior-based evaluation of output consistency and
ator-based automated hallucination detection [11, 21].
erpretability and quantifiability of hallucinations at the
maticdetectionframeworksforgenerationhallucinations.
ems is still in its infancy. Existing benchmarks such as
(Table 2) focus on overall memory system performance
y benchmark LoCoMo [17] focuses on memory retention
valuation paradigm, assessing models through question
test factual recall and event tracking over ultra-long
t 7.5k questions), it only covers a time span of several
hanism, primarily reflecting the model‚Äôs capability for
Eval [24] extends this framework by introducing metrics
Recall Accuracy, covering approximately 2.5 years of
ory updates to quantify knowledge consistency across
toward dynamic memory modeling. In the direction
ability to maintain and follow user preferences over
ation tasks to assess preference consistency, though it
em [9] further constructs simulated user personas and
to assess persona consistency, traceability, and update
ears), it provides a more representative benchmark for
o-end evaluations that treat memory systems as black
e through final task performance rather than explicitly
t, HaluMem is the first benchmark specifically designed
ing fine-grained, operation-level analysis and filling a
rks.
Mem
Retri AI Q
A
R
Existed HaluEval.
E2E Mem HaluEval. E2E Mem HaluEval.
R
Points M AI System A User
nd existing benchmarks for memory systems.
stem A (including an LLM or AI agent) with long-term
systemreceivesamulti-turndialoguesequencebetween
(u ,a ),...,(u ,a ), where u and a represent the
2 2 N N i i
ely. Each memory point is stored as a plaintext entry,
to the dialogue flow D, the memory system involves
5

=== Page 6 ===
four types of operations during interaction: (1) Memor
points from D; (2) Memory Updating (U): modifying
(R): recalling memories relevant to the current query2
prompts and invoking A to generate responses.
Existing evaluations of memory systems typically ado
set of dialogue-based queries Q={q }J and their cor
j j=1
pipeline can be abstracted as
MÀÜ =U(E(D)), RÀÜ =R(
j
End-to-end evaluation is measured using answer-level
J
1 X
Acc =
e2e J
j=
When yÀÜ =Ã∏ y‚àó, the metric Acc cannot identify the
j j e2e
hallucination arises from the extraction stage E, where
the updating stage U, where old memories are mistak
question-answering stage Q, where unsupported gene
being available. The lack of traceability prevents the d
Toenablealocalizedanddiagnosticevaluation,weconst
for each stage. (1) Extraction gold standard: Gext =
shouldbenewlyaddedduringthedialogue. (2)Updatin
the set of memory point pairs before and after updates
each query q , a gold answer y‚àó is provided. The syste
j j
MÀÜext =E(D), GÀÜupd =U(MÀÜex
whereMÀÜ denotesthesetofmemorypointsrepresenting
is processed. By providing stage-specific gold standards
HaluMem benchmark enables operation-level hallucina
4 Methodology for Constructing HaluMem
To systematically evaluate memory systems in realistic
Benchmark (HaluMem). To ensure the quality of th
process, we design a user-centered, six-stage procedure
Stage 1: Persona Construction. This stage initiates th
users with complete persona profiles to simulate real pa
includes three parts: Core Profile Information, Dynami
core profile captures stable background traits; the dyna
health, and relationships; and the preferences define pe
assigned preferences across areas like food, music, an
while the dynamic and preference elements, which evo
for memory extraction. An initial timestamp ensures
in time. To enhance the authenticity of virtual users
Hub3 [6], and rule-based procedures are applied to gen
and refines them. See Appendix E.1 for an example.
2SinceretrievalRprimarilyfocusesonrelevanceandrecallr
studyconcentratesonthethreestagesthatdirectlyinducehallu
3Acollectionofonebilliondiversepersonasautomaticallycu
6

ry Extraction (E): extracting newly generated memory
g or deleting existing memories; (3) Memory Retrieval
2; (4) Memory Question Answering (Q): constructing
opt an end-to-end question‚Äìanswer paradigm. Given a
rresponding gold answers Y‚àó ={y‚àó}J , the evaluation
j j=1
(cid:16) (cid:17)
(MÀÜ,q ), yÀÜ =A RÀÜ ,q .
j j j j
metrics such as accuracy or F1 score:
J
X I(cid:2)
yÀÜ
=y‚àó(cid:3)
.
j j
=1
e source of the error. It remains unclear whether the
e incorrect or fabricated memories are introduced, from
kenly modified or not properly refreshed, or from the
erative content is produced despite correct memories
development of targeted mitigation strategies.
tructfine-grainedannotationsanddefinegoldstandards
{m }K , representing the set of memory points that
i i=1
ng gold standard: Gupd ={mold ‚Üímnew}, representing
s during the dialogue. (3) Question‚Äìanswer dataset: for
em outputs are defined as follows:
(cid:16) (cid:17)
xt,D), yÀÜ =A R(MÀÜ,q ),q ,
j j j
gthecurrentstateofthememorysystemwhenqueryq
j
s and evaluation metrics for E, U, and Q, the proposed
ation evaluation within memory systems.
m
c scenarios, we construct the Hallucination in Memory
he dataset and the controllability of the construction
e based on a progressive expansion strategy.
he HaluMem dataset construction by creating virtual
articipants in later human‚ÄìAI dialogues. Each persona
ic State Information, and Preference Information. The
amic state reflects current circumstances such as career,
ersonal tastes. Each user receives six to eight randomly
nd film. The core profile provides a static foundation,
olve randomly, add realism, diversity, and rich material
s that all personas reflect a consistent starting point
s, user seeds are randomly sampled from the Persona
nerate structured persona drafts. GPT-4o then verifies
rateandrarelyintroducesgenerativeprocessingbyLLMs,this
ucinations,namelyE,U,andQ.
uratedfromwebdata
6

=== Page 7 ===
Stage 1: Persona Construction. Stage 2: Life Skeleton.
I. Core Profile Information Preference Informatio
Basic Information Dislike Dog Like Video Gam
> Name, gender, age, DOB, location. Dynamic State Inform
Education Promotion New Friend
> Degree, major.
Personality
Event 1 Event 2 E
> MBTI type, traits.
Family Life Stage 3: Event Flow.
> Status, family members, chlid ‚Ä¶
Life Goal 1. Type Assign Iinit Eve
> Goal, statement, motivation ‚Ä¶ Event X Career Ev
II. Dynamic State Information Daily Ev
Career: employment status, industry, income ‚Ä¶
Health: physical health, mental health ‚Ä¶ Event 1 Event 2 E
Social Relationship: friend, colleague ‚Ä¶ Stage 4: Session Summ
III. Preference Information Summ
Initial Perferences: user likes and dislikes. User talked
> food, beverage, reading, music, pet, Event X recent job c
and recover
> movie, game, sports, travel, clothing illness ‚Ä¶
Figure 3 Framework of the Ha
Stage 2: Life Skeleton. After generating persona profile
each user‚Äôs evolutionary trajectory. Each user receives
serve as anchors for the evolution of dynamic informa
health conditions are typically associated with these ca
through probabilistic modifications or deletions, indepe
ensure a diverse yet coherent evolution. The life skeleto
as a structured script for later memory addition, modi
consistency of the evaluation scenarios.
Stage 3: Event Flow. As the core component of datase
abstract ‚Äúlife skeleton‚Äù generated in Stage 2 into a s
to ‚Äúeventify‚Äù discrete evolution instructions, constru
that integrates initial states, career development, and
coherence with machine interpretability. The core of t
‚Ä¢ Init Events: Generated from the user‚Äôs initial profi
They serve as the starting point of the memory t
‚Ä¢ Career Events: Derived from the life skeleton bu
development. Each career event is divided into s
promotions, illnesses) to enrich the narrative.
‚Ä¢ Daily Events: Generated from the evolution of use
preferencechangebecomesaconcretelifescenario
states and their cause.
Within this framework, career events serve as the nar
necessary background and contextual details. Throug
three event types, this stage produces a coherent and
memory transaction log. See Appendix E.2 for event e
Stage 4: Session Summaries and Memory Points. This sta
into realistic session summaries and detailed memory p
scenario shaped by the user‚Äôs motivation. The system
all prior events and memory points, ensuring logical,
the persona profile is dynamically updated to reflect t
its content, type(persona, event, or relationship), and
7

Stage 5: Session Generation.
on Evolution Event to Full Human-LLM Session
me Like Dog ‚Ä¶ Dislike Boxing
mation Evolution Persona Hi, I just finished some work.
Profile
Illness Unemployed ‚Ä¶ Recovery Nice! How was it?
Event Pretty good, and I met a new
Event 3 Event 4 ‚Ä¶ Event N friend at lunch.
Summary That sounds fun! How did it go?
Memory Really well, we clicked instantly.
Points
ent 2. Enrich - Event Name Great to hear!
- Event Type
vent Event X - Event Time Stage 6: Question Generation.
vent - Event Description Event
Question & Answer
Event 3 Event 4 ‚Ä¶ Event N M P e o m in o t r s y
maries and Memory Points. Basic Fact Recall Question
mary Memory Points Multi-hop Inference Answer
Dynamic Update
d about 1. User changed jobs. Q&A Type
changes 2. User recovered. Mmeory Boundary Evidence
ry from 3. User met a new friend. Generalization & Application
4. ‚Ä¶ Mmeory Conflict Difficulty
aluMem Construction Pipeline.
es, the second stage builds a life skeleton that defines
several core career events centered on life goals, which
ation. Updates to social status, career transitions, or
areer events. Preference information evolves separately
endent of these career events. These probabilistic rules
on captures the user‚Äôs potential future states and serves
ification, and deletion, maintaining the complexity and
et construction, the third stage aims to transform the
structured and narrative event flow. The objective is
ucting for each persona a complete memory timeline
daily preference changes, thereby balancing narrative
this stage includes three types of events:
file, covering core, dynamic, and preference information.
timeline, simulating the user‚Äôs first self-introduction.
uilt in Stage 2, representing the main storyline of user
sub-stages and instantiated with dynamic details (e.g.,
er preferences, independent of career progression. Each
orecordedasanatomiceventwithpre-andpost-change
rrative backbone, while init and daily events provide
gh the integration and chronological alignment of the
d complete event sequence that functions as the user‚Äôs
examples.
age transforms the structured event flow from Stage 3
points. For each event, we create a human‚ÄìAI dialogue
m has access to the current persona profile, along with
, causal, and consistent generation. As events unfold,
the user‚Äôs evolving state. Each memory point includes
importance, with updated entries preserving replaced
7

=== Page 8 ===
information for traceability. More details provided in A
Stage 5: Session Generation. This stage converts the str
stages into complete, multi-turn dialogues that are co
The process has three steps: adversarial content inj
self-verification. Adversarial content injection adds dist
refines each memory point for consistency with the ge
memory is formed, maintained, and challenged in reali
memory performance and hallucination resistance. Ex
Stage 6: Question Generation. The final stage constructs
on the sessions and memory points generated previous
predefined, and the number and types of questions ar
and complexity to ensure balanced coverage. For eac
a single unit to increase reasoning depth and comple
difficulty level and accompanied by traceable evidence,
points. See Appendices A.2 and E.3 for details.
Human Annotation. To verify the quality of HaluMem, w
in HaluMem-Medium, covering both memory points a
were randomly selected, totaling 700 sessions (over 5
bachelor‚Äôs degree rated each session on Correctness, Re
the results showed a correctness rate of 95.70%, an aver
score of 9.45. These results demonstrate the high qua
details are provided in Appendix C.
Overall, We constructed two datasets: HaluMem-Med
30,073 rounds of dialogue from 20 users, with an averag
points, and 3,467 QA pairs. HaluMem-Long extends
irrelevant dialogues5, containing 53,516 rounds in tota
5 Evaluation Framework of HaluMem
For each user, the session-level evaluation procedure of
D1,D2,...,DS are sequentially fed, in chronological
session Ds contains reference memory points or QA ta
updating, or question answering) is triggered immed
the results are recorded. (3) After processing all sess
aggregated to obtain the overall system performance.
Stored MPs Session w/ Golden MPs
by Mem Sys. golden MPs by HaluMem. Mem Ext s(ext) ÔÄΩÔÅÜ(ext)
Halu Eval.
Mem Upd s(upd) ÔÄΩÔÅÜ(upd
Halu Eval.
Metric Mem Mem Sys. to Mem QA s(qa) ÔÄΩÔÅÜ(qa)
Fuc. be Evaluated Halu Eval.
Figure 4 Hallucinatio
To support this evaluation workflow, the system is r
interfaces: (1) Add Dialogue API: inputs dialogues and
4FalsebutsimilarmemoriesthattheAInaturallyuseswhileth
5MainlysourcedfromELI5[5],GPT-OSS-120B-Distilled-Re
8

Appendices A.1 and E.3.
ructured event flow and memory points from previous
ontext-rich, goal-driven, and adversarially challenging.
jection, multi-turn dialogue generation, and memory
tractor memories4. Memory self-verification checks and
enerated dialogues. Overall, this stage simulates how
istic conversations, producing data that test long-term
xamples appear in Appendix E.3.
s a set of memory-related question‚Äìanswer pairs based
sly. Six categories of memory evaluation questions are
re programmatically allocated according to event type
ch career event, all its sub-stages are integrated into
exity. Each question‚Äìanswer pair is annotated with a
explicitly linking the answer to the supporting memory
we conducted human annotation on part of the sessions
and question‚Äìanswer pairs. For each user, 35 sessions
50% of the dataset). Eight annotators with at least a
elevance, and Consistency. After 10 days of annotation,
rage relevance score of 9.58, and an average consistency
ality and reliability of the HaluMem benchmark. More
dium and HaluMem-Long. HaluMem-Medium includes
ge context length of about 160k tokens, 14,948 memory
s each user‚Äôs context to 1M tokens through inserted
al. Details are given in Appendices A.3 and A.4.
f HaluMem is defined as follows: (1) Dialogue sessions
order, into the memory system S. (2) If the current
asks, the corresponding evaluation process (extraction,
diately after S completes processing that session, and
sions, the metrics of the three categories of tasks are
Memory RecallÔÄΩ
N
correct, Weighted Memory RecallÔÄΩ
ÔÉ•
i
N ÔÄΩs1houldw
i
ÔÉós
i,
N
should
ÔÉ•
i
N ÔÄΩs1houldw
i
) Memory AccuracyÔÄΩ ÔÉ•N jÔÄΩe1xtracts j, Target Memory PrecisionÔÄΩ ÔÉ• jÔÉéMT s j, FMRÔÄΩ N miss
N |M | N
extract T D
2ÔÉóMemory Rcall ÔÉóTarget Memory Precision
Memory Extraction F1ÔÄΩ
Memory Rcall ÔÄ´Target Memory Precision
N N
Updating-AccuracyÔÄΩ correct-upd, Updating-HallucinationÔÄΩ wrong-upd,
d) N N
target-upd target-upd
N
Updating-OmissionÔÄΩ missed-upd
N
target-upd
) N N N
QA-AccuracyÔÄΩ correct-qa, QA-HallucinationÔÄΩ wrong-qa, QA-OmissionÔÄΩ missed-qa
N N N
total-qa total-qa total-qa
on evaluation process.
required to provide the following three types of API
d automatically performs memory extraction; (2) Get
heuserstayssilent,mimickingrealisticinformationcontamination
easoning-math,andfactualQApairsgeneratedusingGPT-4o.
8

=== Page 9 ===
DialogueMemoryAPI:retrievesthememorypointsextra
Memory API: retrieves the user‚Äôs most relevant memory
Based on the above workflow and interface design, Hal
systems acrossthree essential tasks: Memory Extraction
5.1 Memory Extraction
The memory extraction task evaluates whether the sy
from dialogues while avoiding fabricated or irrelevant
referencememories,thebenchmarkprovidesagoldmem
system output after processing Ds is the extracted mem
Memory Integrity (Anti-Amnesia) This metric measu
should be extracted:
N
Memory Recall= correct, Weighte
N
should
where N = |Gext|, N denotes the number
should s correct
importance weight of the i-th memory, and s ‚àà{1,0.5,
i
partially extracted, or omitted).
Memory Accuracy (Anti-Hallucination) This metric
and free from hallucination:
PNextracts
Memory Accuracy= j=1 j , Ta
N
extract
where N
extract
=|Mc
s
ext|, and M
T
‚äÇMc
s
ext denotes the
False Memory Resistance (FMR) This metric measu
facing distracting content that the AI mentions but th
FMR=
where N represents the total number of distractor m
D
successfully ignored by the system, where a higher val
Memory Extraction F1 WeadditionallyreportanF1s
extraction task by jointly considering completeness an
recall term, while Target Memory Precision (P ) is u
tgt
2
F1 =
mem R
5.2 Memory Updating
The memory updating task evaluates whether the sys
memories during new dialogues so that consistency is m
dialoguesessionDsthatcontainsannotatedupdates,the
The system output is denoted as Gbupd.
s
9

actedbythesystemfromaspecifiedsession; (3)Retrieve
y content based on a given query.
luMem conducts operation-level evaluation of memory
n, Memory Updating, and Memory Question Answering.
ystem can correctly identify and store key information
memories. For each dialogue session Ds that contains
morysetGext ={ms}Ks thatshouldbeextracted. The
s i i=1
mory set Mcext ={mÀÜs}Kbs , which is used for evaluation.
s j j=1
ures whether the system omits crucial information that
PNshouldw
¬∑s
ed Memory Recall= i=1 i i, (1)
PNshouldw
i=1 i
r of correctly extracted memories, w represents the
i
,0} indicates the extraction score (completed extracted,
evaluates whether the extracted memories are factual
P
s
Target Memory Precision= j‚ààMT j , (2)
|M |
T
set of target memories that match the reference ones.
ures the system‚Äôs ability to resist hallucination when
he user does not confirm:
N
= miss, (3)
N
D
memories and N denotes the number of distractors
miss
lue indicates stronger resistance.
scoretomeasuretheoverallperformanceofthememory
nd correctness. Memory Recall (R ) is used as the
mem
used as the precision term. The F1 score is defined as:
2R P
mem tgt (4)
R +P
mem tgt
stem can correctly modify, merge, or replace existing
maintained without introducing hallucinations. For each
egoldupdatesetisdefinedasGupd ={(mold ‚Üímnew)}.
s
9

=== Page 10 ===
Typical memory update hallucinations include: (1) inc
new information, and (3) version conflicts or self-contr
to evaluate memory update hallucination:
Memory Updatin
Memory Updating Hallucin
Memory Updating Om
where N =|Gupd|, N is the number o
target-upd s correct-upd
incorrect or hallucinated updates, and N is th
missed-upd
were not.
5.3 Memory Question Answering
Thememoryquestion-answeringtaskevaluatestheend-
updating, retrieval, and generation. For each questio
obtain relevant memories Rb(q
j
). The retrieved set Rb(q
j
to generate an answer yÀÜ . The generated answer is com
j
metrics are defined:
Memory QA A
Memory QA Hallucinat
Memory QA Omiss
where N denotes the total number of questions,
total-qa
questions, N denotes the number of questions a
wrong-qa
N refers to the number of questions that are le
missed-qa
6 Experiments
6.1 Experimental Setup
We conducted a comprehensive evaluation of several sta
Mem0 (both standard and graph versions) [3], Memoba
Each memory system was independently evaluated in
with efforts made to ensure consistent parameter confi
To automate the evaluation of three core tasks, memor
answering, we use GPT-4o for consistency determinatio
to guide the automated evaluation by GPT-4o (See Ap
we retrieved the 10 most relevant memories from the
type" for verification. In the memory question answer
each question to assist in generating answers, using GP
prompt templates used for answer generation across di
Some memory systems required specific Configurations
are provided in Appendix B.
6.2 Experimental Results
Following the evaluation procedure outlined in Sectio
memory systems across the three tasks in the HaluM
1

correct modification of old information, (2) omission of
radictions. Therefore, the following metrics are defined
N
ng Accuracy= correct-upd,
N
target-upd
N
nation Rate= wrong-upd, (5)
N
target-upd
N
mission Rate= missed-upd,
N
target-upd
of correctly updated items, N is the number of
wrong-upd
he number of updates that should have been made but
-to-endperformanceofthesystem,includingextraction,
on q , the system uses the Retrieve Memory API to
j
) and the question are then passed to the AI system A
j
mpared with the reference answer y‚àó, and the following
j
N
Accuracy= correct-qa,
N
total-qa
N
tion Rate= wrong-qa, (6)
N
total-qa
N
sion Rate= missed-qa,
N
total-qa
, N denotes the number of correctly answered
correct-qa
answered with fabricated or incorrect information, and
eft unanswered due to missing memories.
ate-of-the-art memory systems on HaluMem, including
ase [25], MemOS [14], Supermemory [20], and Zep [19].
two subsets, HaluMem-Medium and HaluMem-Long,
figurations across evaluations.
ry extraction, memory updating, and memory question
on and scoring. We designed various prompt templates
ppendix D.2 for details.). In the memory updating task,
memory system for each memory labeled as "update
ring task, we retrieved 20 most relevant memories for
PT-4o uniformly as the answer generation model. The
ifferent memory systems are provided in Appendix D.1.
s due to their unique interfaces and constraints; details
on 5, we conducted comprehensive evaluations of all
Mem benchmark. The results were aggregated, and all
10

=== Page 11 ===
metrics introduced in Section 5 were subsequently com
6.2.1 Overall Evaluation on HaluMem
Table3 EvaluationresultsofallmemorysystemsonHaluMe
Precision, ‚ÄúAcc.‚Äù denotes Accuracy, ‚ÄúFMR‚Äù denotes Fals
F1-score, ‚ÄúC‚Äù denotes Correct Rate (Accuracy), ‚ÄúH‚Äù deno
The values in parentheses in the ‚ÄúTarget P‚Äù and ‚ÄúAcc.‚Äù co
scale reflects performance (red = worse, green = better)
MemoryExtraction
Dataset System
R‚Üë WeightedR‚Üë TargetP‚Üë Acc.‚Üë
Mem0 42.91% 65.03% 86.26%(10556) 60.86%(1629
Mem0-Graph 43.28% 65.52% 87.20%(10567) 61.86%(1623
Medium
Memobase 14.55% 25.88% 92.24%(5443) 32.29%(1708
MemOS 74.07% 84.81% 86.25%(45190) 59.55%(7179
Supermemory 41.53% 64.76% 90.32%(14134) 60.83%(2255
Zep - - - -
Mem0 3.23% 11.89% 88.01%(1134) 46.01%(243
Mem0-Graph 2.24% 10.76% 87.32%(785) 41.26%(186
Long
Memobase 6.18% 14.68% 88.56%(3077) 25.61%(1179
MemOS 81.90% 89.56% 82.32%(48246) 43.77%(9946
Supermemory 53.02% 70.73% 85.82%(24483) 29.71%(7713
Zep - - - -
Note:sinceZepdoesnotprovideaGetDialogueMemoryAPI,metricsrelatedtomemoryextra
Table 3 presents the evaluation results of all memory
updating, and memory question answering. The evalu
integrity and memory accuracy.
Overall, most memory systems perform worse on Hal
Mem0-Graph, and Memobase showing particularly nota
extract significantly fewer memories on HaluMem-Lon
and MemOS exhibit the opposite trend. This indicates
to process irrelevant information and distinguish between h
In the memory extraction task, regarding memory int
(R) rates below 60%, indicating that many reference m
memory recall (Weighted R) suggests that these system
memory accuracy, all systems have accuracy (Acc.) bel
although performance on target memory precision (Ta
perform the worst on FMR because they tend to extr
distractions or unhelpful content. Other systems adopt
in FMR. In terms of the F1 score, MemOS and Superm
contexts, whereas other systems experience a sharp dec
balance among coverage of important memories, extraction
high quality and reliability in memory retrieval.
In the memory updating task, most systems perform
HaluMem-Long. Systems showing better performance i
accuracy, but most systems suffer omission rates abo
coverage in memory extraction: when the pre-update
be properly processed. Moreover, the fact that all sy
necessarily imply strong hallucination suppression, sin
Overall, current systems face a clear bottleneck in memor
linkage, resulting in low accuracy and high omission rates.
In the memory question-answering task, the best-per
memory integrity and memory updating, further hig
example, Mem0 and Mem0-Graph show clear performan
1

mputed.
em. ‚ÄúR‚ÄùdenotesRecall,‚ÄúTargetP‚ÄùdenotesTargetMemory
se Memory Resistance, ‚ÄúF1‚Äù denotes Memory Extraction
otes Hallucination Rate, and ‚ÄúO‚Äù denotes Omission Rate.
olumns represent the number of extracted memories. Color
); Best values in bold.
MemoryUpdating QuestionAnswering
FMR‚Üë F1‚Üë C‚Üë H‚Üì O‚Üì C‚Üë H‚Üì O‚Üì
91) 56.80% 57.31% 25.50% 0.45% 74.02% 53.02% 19.17% 27.81%
30) 55.70% 57.85% 24.50% 0.26% 75.24% 54.66% 19.28% 26.06%
81) 80.78% 25.13% 5.20% 0.55% 94.25% 35.33% 29.97% 34.71%
93) 44.94% 79.70% 62.11% 0.42% 37.48% 67.23% 15.17% 17.59%
51) 51.77% 56.90% 16.37% 1.15% 82.47% 54.07% 22.24% 23.69%
- - 47.28% 0.42% 52.31% 55.47% 21.92% 22.62%
33) 87.65% 6.22% 1.45% 0.03% 98.51% 28.11% 17.29% 54.60%
66) 88.36% 4.36% 1.47% 0.04% 98.40% 32.44% 21.82% 45.74%
95) 85.39% 11.55% 4.10% 0.36% 95.38% 33.60% 29.46% 36.96%
62) 28.85% 82.11% 65.25% 0.29% 34.47% 64.44% 16.61% 18.95%
34) 36.86% 65.54% 17.01% 0.58% 82.42% 53.77% 22.21% 24.02%
- - 37.35% 0.48% 62.14% 50.19% 22.51% 27.30%
actioncannotbecomputed.Fordetails,seeAppendixB.
systems on three tasks: memory extraction, memory
uation metrics for memory extraction include memory
luMem-Long than on HaluMem-Medium, with Mem0,
able declines. Notably, the Mem0 series and Memobase
ng than on HaluMem-Medium, whereas Supermemory
s that future memory systems need to improve their ability
high- and low-value memories.
ntegrity, except for MemOS, all systems achieve recall
memory points are not extracted. The higher weighted
ms can prioritize important memory points. Regarding
low 62%, reflecting a high proportion of hallucinations,
arget P) is relatively good. Supermemory and MemOS
ract excessive information without effectively filtering
t more conservative strategies and thus perform better
memory perform the best and exhibit stability in long
cline. In summary, future memory systems should strike a
n accuracy, and resistance to interference, aiming for both
poorly, and their performance drops considerably on
in memory integrity also tend to exhibit higher update
ove 50%. This issue primarily stems from insufficient
e memories are not extracted, related updates cannot
ystems exhibit hallucination rates below 2% does not
nce very few samples actually enter the update stage.
ry updating: the extraction and updating stages lack stable
rforming systems are also those that perform well in
ghlighting the crucial role of memory extraction. For
nce declines on HaluMem-Long compared to HaluMem-
11

=== Page 12 ===
Table 4 Typewise accuracy on event
Dataset System E
Mem0 29
Mem0-Graph 30
Medium
Memobase 5.
MemOS 63
Supermemory 28
Zep 44.
Mem0 0.
Mem0-Graph 1.
Long
Memobase 4.
MemOS 70
Supermemory 38
Zep 35.
*ThememoryentriesofZepincludeo
task. Fordetails,seeAppendixB.
Medium, which strongly correlates with their substanti
systems achieve answer accuracies below 70%, with bot
and their overall performance further decreases on Ha
systems‚ÄôQAperformancedependsheavilyonthesufficiency
prone to factual deviation and memory confusion under int
HaluMem Medium
Basic Fact Recall
0.8
Memory 0.6 Dynamic M
Boundary Update Bo
0.4
0.2
Memory Multi-hop M
Conflict Inference C
Generalization & Application
Figure 5 Performance of the Memory S
6.2.2 Performance on Different Memory Types
Table 4 reports the extraction accuracy of each memory
which include all memory points from both the mem
distractor memories. MemOS achieves the best overa
Memobase show a marked decline in long-context sce
valuableinformationincomplexdialogues. MemOSand
on HaluMem-Medium, probably because they extract
condition. Across memory types, Persona memories
1

t, persona, and relationship memory.
Event Persona Relationship
9.69% 33.74% 27.77%
0.02% 33.71% 26.60%
.12% 13.38% 6.79%
3.41% 59.77% 62.40%
8.66% 32.11% 20.67%
.83%‚àó 49.75%‚àó 38.81%‚àó
.92% 3.01% 2.18%
.10% 2.00% 1.59%
.09% 5.32% 4.21%
0.92% 68.35% 71.68%
8.48% 40.85% 32.61%
.76%‚àó 39.07%‚àó 31.16%‚àó
onlythosefromthememoryupdating
ial reduction in extracted memory points. However, all
th hallucination rate and omission rate remaining high,
aluMem-Long. This demonstrates that current memory
yandaccuracyofupstreammemoryextraction, andremains
terference or extended context conditions.
Mem0
HaluMem Long Mem0-Graph
Memobase
Basic Fact Recall
MemOS
Supermemory
0.8 Zep
Memory 0.6 Dynamic
oundary Update
0.4
0.2
Memory Multi-hop
Conflict Inference
Generalization & Application
System Across Different Question Types
y system for event, persona, and relationship memories,
mory extraction and updating tasks while excluding
all performance. However, Mem0, Mem0-Graph, and
enarios, suggesting difficulty in consistently capturing
dSupermemoryperformbetteronHaluMem-Longthan
a larger number of memory points in the long-context
yield slightly higher accuracy, indicating that static
12

=== Page 13 ===
personal traits are easier to capture, whereas understan
challenging. Overall, all systems still show low performance
limitations in current memory modeling.
6.2.3 Performance on Different Question Types
Figure 5 illustrates the performance of different memo
the accuracy of all memory systems remains relatively low
improvement. The Mem0 series and Memobase show
compared to HaluMem-Medium, suggesting a notable
contrast, MemOS, SuperMemory, and Zep demonstra
superior overall performance on both datasets. Furth
better on memory boundary and memory conflict que
unknown or misleading information and respond correctly.
on multi-hop inference, dynamic update, and generali
memory systems still struggle with complex reasoning and
6.2.4 Efficiency Analysis of Memory Systems
Table 5 Time consumption of all m
Dialogue Add
Dataset System
Time (mi
Mem0 2768.14
Mem0-Graph 2840.07
Medium
Memobase 293.30
MemOS 1028.84
Supermemory 273.21
Zep -
Mem0 691.62
Mem0-Graph 870.32
Long
Memobase 239.29
MemOS 1524.39
Supermemory 1672.53
Zep -
Table5showsthetimeconsumptionofallmemorysyst
and memory retrieval, as well as their total runtime. O
time than memory retrieval, indicating that the write sta
efficiency of memory extraction and updating is thus cruc
Medium, Supermemory performs best in both dialog
the best retrieval efficiency. However, the dialogue ad
minutes, revealing their low processing efficiency duri
HaluMem-Long,thedialogueadditiontimeforMem0,M
the number of processed memory points is reduced rath
MemOS and Supermemory extract a substantially larg
increase in their time cost.
7 Conclusion
Most existing benchmarks for memory systems adop
which makes it difficult to analyze and measure halluci
address this gap, we present the Hallucination in Mem
hallucination evaluation benchmark for memory system
1

nding event dynamics and relationship changes remains
e across the three memory categories, indicating significant
ory systems across six categories of questions. Overall,
w across most categories, indicating substantial room for
significantly poorer performance on HaluMem-Long
e degradation under ultra-long context conditions. In
ate relatively stable behavior and achieve consistently
hermore, all memory systems perform comparatively
estions, indicating their capability to effectively recognize
However, their performance deteriorates substantially
ization & application questions, suggesting that current
preference tracking.
memory systems during evaluation.
dition Memory Retrieval Total
in) Time (min) Time (min)
4 41.66 2809.8
7 54.65 2894.72
139.95 433.25
4 20.52 1049.37
95.53 368.74
53.34 -
39.15 730.77
62.42 932.74
136.19 375.48
9 20.96 1545.34
3 137.02 1809.55
50.22 -
temsduringtheevaluationprocessfordialogueaddition
Overall, dialogue addition requires substantially more
age is the primary computational bottleneck. Enhancing the
cial for improving interactive performance. On HaluMem-
gue addition and total runtime, while MemOS shows
ddition time of Mem0 and Mem0-Graph exceeds 2700
ing dialogue ingestion and memory construction. On
Mem0-Graph,andMemobasedecreases,mainlybecause
her than due to performance improvement. In contrast,
ger number of memory points, resulting in a significant
pt a black box, end to end question answering setup,
inations introduced by internal memory operations. To
mory Benchmark (HaluMem), the first operation level
ms. HaluMem conducts a comprehensive assessment of
13

=== Page 14 ===
memory hallucinations and overall performance throug
and memory question answering. For dataset construc
on a progressive expansion strategy, and build two dat
construction quality is verified through human annot
evaluatemultipleadvancedmemorysystemsonHaluMe
accuracy across different memory types, and efficiency.
accuracy, update capability, robustness to interference,
improve extraction quality, update logic, semantic und
more stable and comprehensive long term memory.
References
[1] Garima Agrawal, Tharindu Kumarage, Zeyad Alghamd
in retrieval augmented generation. In 2nd Internation
Dubai, pages 607‚Äì611, 2024.
[2] Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Min
internal states retain the power of hallucination detect
Representations, ICLR 2024, Vienna, Austria, May 7-
[3] Prateek Chhikara, Dev Khant, Saket Aryan, Taranjee
ready ai agents with scalable long-term memory. arXi
[4] Darren Edge, Ha Trinh, Newman Cheng, Joshua B
Metropolitansky, Robert Osazuwa Ness, and Jonatha
query-focused summarization, 2025. URL https://ar
[5] Angela Fan, Yacine Jernite, Ethan Perez, David Gra
questionanswering. InProceedingsofthe57thAnnual
pages3558‚Äì3567,Florence,Italy,July2019.Association
URL https://aclanthology.org/P19-1346/.
[6] Tao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao
1,000,000,000 personas, 2025. URL https://arxiv.or
[7] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,
Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. A sur
taxonomy, challenges, and open questions. ACM Tra
10.1145/3703155. URL https://doi.org/10.1145/37
[8] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,
Peng,XiaochengFeng,BingQin,etal. Asurveyonha
challenges, and open questions. ACM Transactions on
[9] Bowen Jiang, Zhuoqun Hao, Young-Min Cho, Bryan L
andDanRoth. Knowme,respondtome: Benchmarkin
at scale. arXiv preprint arXiv:2504.14225, 2025.
[10] Jiazheng Kang, Mingming Ji, Zhe Zhao, and Ting Bai
2025.
[11] Sungmin Kang, Yavuz Faruk Bakman, Duygu Nur Ya
tainty quantification for hallucination detection in larg
directions, 2025.
[12] DongGeonLeeandHwanjoYu. REFIND:retrieval-aug
models. CoRR, abs/2502.13622, 2025.
[13] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fab
K√ºttler,MikeLewis,Wen-tauYih,TimRockt√§schel,et
nlp tasks. Advances in neural information processing
1

gh three tasks: memory extraction, memory updating,
ction, we design a user-centric, six-stage pipeline based
tasets, HaluMem-Medium and HaluMem-Long, whose
tation. In the experimental study, we systematically
em,analyzingperformanceonthethreetasks,extraction
y. The results reveal persistent bottlenecks in coverage,
and question answering reliability. Future work should
derstanding, and system efficiency in order to achieve
di, and Huan Liu. Mindful-rag: A study of points of failure
nal Conference on Foundation and Large Language Models,
ngyuan Tao, Zhihang Fu, and Jieping Ye. INSIDE: llms‚Äô
tion. In The Twelfth International Conference on Learning
-11, 2024. OpenReview.net, 2024.
et Singh, and Deshraj Yadav. Mem0: Building production-
iv preprint arXiv:2504.19413, 2025.
Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha
an Larson. From local to global: A graph rag approach to
rxiv.org/abs/2404.16130.
angier, Jason Weston, and Michael Auli. ELI5: Long form
lMeetingoftheAssociationforComputationalLinguistics,
nforComputationalLinguistics.doi: 10.18653/v1/P19-1346.
o Mi, and Dong Yu. Scaling synthetic data creation with
rg/abs/2406.20094.
, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua
rvey on hallucination in large language models: Principles,
ans. Inf. Syst., 43(2), January 2025. ISSN 1046-8188. doi:
703155.
, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua
allucinationinlargelanguagemodels: Principles,taxonomy,
n Information Systems, 43(2):1‚Äì55, 2025.
Li, Yuan Yuan, Sihao Chen, Lyle Ungar, Camillo J Taylor,
ngllmsfordynamicuserprofilingandpersonalizedresponses
i. Memory os of ai agent. arXiv preprint arXiv:2506.06326,
aldiz, Baturalp Buyukates, and Salman Avestimehr. Uncer-
ge language models: Foundations, methodology, and future
gmentedfactualityhallucinationdetectioninlargelanguage
bio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich
tal. Retrieval-augmentedgenerationforknowledge-intensive
systems, 33:9459‚Äì9474, 2020.
14

=== Page 15 ===
[14] Zhiyu Li, Shichao Song, Chenyang Xi, Hanyu Wang, C
QingchenYu,JihaoZhao,YezhaohuiWang,PengLiu,
Chen, Kehang Li, Zhen Tao, Junpeng Ren, Huayi Lai
Zhang,LinfengZhang,JunchiYan,MingchuanYang,T
Yang, Wentao Zhang, Zhi-Qin John Xu, Siheng Chen
arXiv preprint arXiv:2507.03724, 2025. URL https:/
[15] Yuxin Liang, Zhuoyang Song, Hao Wang, and Jiaxin
awareness in LLMs for hallucination mitigation. In Pr
Methods for NLP, 2024.
[16] LeiLiu,XiaoyanYang,YueShen,BinbinHu,Zhiqiang
Recallingandpost-thinkingenablellmswithlong-term
[17] AdyashaMaharana,Dong-HoLee,SergeyTulyakov,Mo
verylong-termconversationalmemoryofllmagents.In
for Computational Linguistics (Volume 1: Long Paper
[18] Agada Joseph Oche, Ademola Glory Folashade, Tirth
keyretrieval-augmentedgeneration(RAG)systems: Pr
2025.
[19] PrestonRasmussen,PavloPaliychuk,TravisBeauvais,
graph architecture for agent memory, 2025. URL http
[20] Dhravya Shah, Mahesh Sanikommu, Yash, et al. su
2025-11-05.
[21] Artem Shelmanov, Ekaterina Fadeeva, Akim Tsvigun
Caiqi Zhang, Artem Vazhentsev, Mrinmaya Sachan,
and a head to question: Pre-trained uncertainty quant
CoRR, abs/2505.08200, 2025.
[22] Taiwei Shi, Zhuoer Wang, Longqi Yang, Ying-Chun
Jauhar, Xiaofeng Xu, Xia Song, and Jennifer Neville. W
and feedback. In NeurIPS 2024 Workshop on Behavi
net/forum?id=07QCozT1pi.
[23] Weihang Su, Changyue Wang, Qingyao Ai, Yiran Hu
real-time hallucination detection based on the inter
Association for Computational Linguistics: ACL 2024
[24] Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Ka
chat assistants on long-term interactive memory. I
Representations, 2025. URL https://openreview.ne
[25] Gustavo Ye, Jinjia, Gener, et al. Memobase. htt
2025-11-05.
[26] Jiarui Zhang. Guided profile generation improves pe
theAssociationforComputationalLinguistics: EMNL
2024. Association for Computational Linguistics. d
//aclanthology.org/2024.findings-emnlp.231/.
[27] Wan Zhang and Jing Zhang. Hallucination mitiga
review. Mathematics, 13(5), 2025. ISSN 2227-7390
com/2227-7390/13/5/856.
[28] SiyanZhao,MingyiHong,YangLiu,DevamanyuHazar
evaluating personalized preference following in LLMs
Representations, 2025. URL https://openreview.ne
1

Chen Tang, Simin Niu, Ding Chen, Jiawei Yang, Chunyu Li,
ZehaoLin,PengyuanWang,JiahaoHuo,TianyiChen,Kai
i, Hao Wu, Bo Tang, Zhenren Wang, Zhaoxin Fan, Ningyu
TongXu,WeiXu,HuajunChen,HaofengWang,Hongkang
n, and Feiyu Xiong. Memos: A memory os for ai system.
//arxiv.org/abs/2507.03724.
ng Zhang. Learning to trust your feelings: Leveraging self-
roceedings of the 3rd Workshop on Knowledge Augmented
gZhang,JinjieGu,andGuannanZhang. Think-in-memory:
memory,2023. URLhttps://arxiv.org/abs/2311.08719.
ohitBansal,FrancescoBarbieri,andYuweiFang.Evaluating
Proceedingsofthe62ndAnnualMeetingoftheAssociation
rs), pages 13851‚Äì13870, 2024.
hankar Ghosal, and Arpan Biswas. A systematic review of
rogress,gaps,andfuturedirections. CoRR,abs/2507.18910,
JackRyan,andDanielChalef. Zep: Atemporalknowledge
ps://arxiv.org/abs/2501.13956.
upermemory. https://supermemory.ai/, 2025. Accessed:
n, Ivan Tsvigun, Zhuohan Xie, Igor Kiselev, Nico Daheim,
Preslav Nakov, and Timothy Baldwin. A head to predict
tification heads for hallucination detection in LLM outputs.
Lin, Zexue He, Mengting Wan, Pei Zhou, Sujay Kumar
Wildfeedback: Aligning LLMs with in-situ user interactions
ioral Machine Learning, 2024. URL https://openreview.
u, Zhijing Wu, Yujia Zhou, and Yiqun Liu. Unsupervised
rnal states of large language models. In Findings of the
4, 2024.
ai-Wei Chang, and Dong Yu. Longmemeval: Benchmarking
In The Thirteenth International Conference on Learning
et/forum?id=pZiyCaVuti.
tps://github.com/memodb-io/memobase, 2025. Accessed:
ersonalization with large language models. In Findings of
LP2024,pages4005‚Äì4016,Miami,Florida,USA,November
doi: 10.18653/v1/2024.findings-emnlp.231. URL https:
ation for retrieval-augmented large language models: A
0. doi: 10.3390/math13050856. URL https://www.mdpi.
rika,andKaixiangLin.DoLLMsrecognizeyourpreferences?
s. In The Thirteenth International Conference on Learning
et/forum?id=QWunLKbBGF.
15

=== Page 16 ===
Appendices
A Supplementary Details of HaluMem . . . . . . . . .
A.1 Definition of Memory Types . . . . . . . . . .
A.2 Definition of Question Types . . . . . . . . .
A.3 Dataset Statistics . . . . . . . . . . . . . . . .
A.4 Construction Details of HaluMem-Long . . .
B Special Configurations for Some Memory Systems . .
B.1 Memobase . . . . . . . . . . . . . . . . . . . .
B.2 Zep. . . . . . . . . . . . . . . . . . . . . . . .
C Annotation Guidelines and Instructions . . . . . . . .
C.1 Annotation Objective . . . . . . . . . . . . .
C.2 Information Fields . . . . . . . . . . . . . . .
C.3 Annotation Dimensions and Scoring . . . . .
D Prompts . . . . . . . . . . . . . . . . . . . . . . . .
D.1 Prompts for Memory Question Answering Ta
D.2 Prompts for Scoring in Memory Evaluation T
E Examples from the Process of Constructing HaluMem
E.1 User Profile Example in Stage 1. . . . . . . .
E.2 Event Structure Examples in Stage 3 . . . . .
E.3 Examples of Memory Points, Dialogues, and Q
E.4 Examples of irrelevant dialogues . . . . . . .
1

. . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
ask . . . . . . . . . . . . . . . . . . . . . . . . . . 21
Tasks . . . . . . . . . . . . . . . . . . . . . . . . . 21
m . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
QA Pairs in Stages 4‚Äì6 . . . . . . . . . . . . . . 38
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
16

=== Page 17 ===
A Supplementary Details of HaluMem
This appendix provides additional statistical informa
support a more detailed understanding of its data com
consists of two parts: HaluMem-Medium and HaluMem-L
human‚ÄìAI interaction scenarios, respectively. Each s
questions, enabling systematic evaluation of hallucinat
Table 6 Statistical Overvi
Metrics
Interaction Statistics
Avg Context Length (tokens/user)
Avg Session Num (per user)
Avg Dialogue Turns per Session
Total Dialogue Turns
Memory Statistics
Avg Memory Num per Session
Distractor Memories
Update Memories
Persona Memories
Event Memories
Relationship Memories
Total Memories
Question Statistics
Avg Questions per User
Total Questions
Question Type Distribution:
Basic Fact Recall
Multi-hop Inference
Dynamic Update
Memory Boundary
Memory Conflict
Generalization & Application
A.1 Definition of Memory Types
HaluMem categorizes memory content into three core
characteristics:
‚Ä¢ Persona Memory: Describes user‚Äôs identity, intere
‚Ä¢ Event Memory: Records specific events, experienc
‚Ä¢ Relationship Memory: Describes user‚Äôs relationship
A.2 Definition of Question Types
To comprehensively cover different types of hallucin
questions:
‚Ä¢ Basic Fact Recall: Directly ask about single objec
the dialogue, without requiring reasoning or info
1

ation and key definitions of the HaluMem dataset to
mposition and task taxonomy. The HaluMem dataset
Long, representing medium- and long-context multi-turn
subset contains multiple types of memory points and
tion behaviors in memory systems.
iew of HaluMem Datasets
HaluMem-Medium HaluMem-Long
159,910.95 1,007,264.65
69.35 120.85
21.68 22.14
30,073 53,516
10.78 6.18
2,648 2,648
3,122 3,122
9,116 9,116
4,550 4,550
1,282 1,282
14,948 14,948
173.35 173.35
3,467 3,467
746 746
198 198
180 180
828 828
769 769
746 746
types, reflecting different semantic levels and stability
ests, habits, beliefs, and other stable characteristics.
ces, or plans that occurred to the user.
ps, interactions, or views of others.
nation, HaluMem defines six categories of evaluation
ctive facts or user preferences that explicitly appear in
ormation integration.
17

=== Page 18 ===
‚Ä¢ Multi-hop Inference: Requires synthesizing multip
derive answers through logical reasoning or temp
‚Ä¢ Dynamic Update: Tests the ability to track inform
latest status or preference changes.
‚Ä¢ Memory Boundary: Tests the system‚Äôs ability to i
not mentioned in the input information to exam
‚Ä¢ Generalization & Application: Based on known
suggestions or judgments in new scenarios.
‚Ä¢ Memory Conflict: Tests the system‚Äôs ability to
deliberately contain incorrect information that di
system to identify contradictions, correct errors,
A.3 Dataset Statistics
Table 6 presents the main statistical features of HaluM
scale, session quantity, memory distribution, and que
finalized dataset version.
A.4 Construction Details of HaluMem-Long
HaluMem-Long is built upon HaluMem-Medium to tes
focusing on robustness and hallucination suppression.
additional irrelevant dialogues were inserted:
‚Ä¢ Within sessions: extra unrelated exchanges were
‚Ä¢ Between sessions: new sessions composed entirel
These irrelevant dialogues include:
‚Ä¢ Factual Q&A derived partly from the ELI5 data
‚Ä¢ Mathematical reasoning Q&A adopted from GP
The ELI5 dataset consists of factual question‚Äìanswer p
120B-Distilled-Reasoning-math contains question‚Äìans
example). To further enrich the diversity of irrelevan
eight domains using GPT-4o (e.g., the first example), in
or Place, Famous Invention, Philosophical Theory, Art
Theorem. These QA pairs are used to simulate dialog
needs in realistic scenarios. They have minimal impact
thememorysystem‚Äôspersonalizedmemoriesoftheuser.
B Special Configurations for Some Memo
Thisappendixdocumentsthespecialconfigurationsapp
Whiletheexperimentalsetupstrivestomaintainconsist
memory systems exhibit unique API constraints that n
subsection below outlines these system-specific configu
B.1 Memobase
Since Memobase does not provide a Get Dialogue Mem
and directly accessed the corresponding dialogue mem
Retrieve Memory API of Memobase only supports con
1

ple information fragments from dialogues, and can only
poral reasoning.
mation changes over time, requiring identification of the
identify unknown information by asking about details
mine whether the system will fabricate answers.
user preferences or characteristics, infer reasonable
identify and correct erroneous premises. Questions
irectly contradicts known memory points, requiring the
and answer based on correct information.
Mem-Medium and HaluMem-Long, including context
estion-type composition. All values are based on the
st memory systems under ultra-long context scenarios,
. Based on each user‚Äôs sessions in HaluMem-Medium,
e added to existing conversations.
ly of irrelevant dialogues were interleaved.
aset [5] and partly generated by us.
PT-OSS-120B-Distilled-Reasoning-math.
pairs (e.g., the second QA example), whereas GPT-OSS-
swer pairs involving mathematics (e.g., the third QA
nt dialogues, we also sampled factual QA pairs across
ncluding Historical Figure, Scientific Concept, Country
twork or Painting, Historical Event, and Mathematical
gues between users and the AI driven by instrumental
t on the user‚Äôs original conversations and do not affect
. SeeAppendixE.4forexamplesofirrelevantdialogues.
ory Systems
pliedtoseveralmemorysystemsevaluatedonHaluMem.
tentconfigurationsacrossallevaluatedsystems,certain
necessitate specific adjustments or workarounds. Each
urations to ensure reproducibility.
mory API, we adopted a localized deployment approach
mories from its underlying database. Additionally, the
ntrolling the maximum length of the returned memory
18

=== Page 19 ===
text. Based on test results, we set the maximum leng
250 tokens and the recall length for the memory quest
B.2 Zep
According to our current understanding, the official
memory points within a specific session, meaning they
Memory API. Consequently, we were unable to evalua
We attempted to use the function ‚Äòthread.get_user_co
given thread; however, this method only returns recen
not meet the evaluation requirements. Moreover, since
asynchronously, we could not accurately measure the
instead recorded only the time cost associated with m
C Annotation Guidelines and Instructions
C.1 Annotation Objective
Task Background: Given a user‚Äôs persona description
points and question-answer (QA) pairs are generated u
be manually verified to ensure strict grounding in the
have explicit evidence in the dialogue, and QA pairs sh
inferable from it.
CoreObjective: AssesswhetherthecontentintheEvalua
Info.
Figure 6 Annot
An illustrative screenshot of the annotation interface i
C.2 Information Fields
‚Ä¢ User Persona Info: Basic information about the u
1

gth for memory recall in the memory updating task to
tion answering task to 500 tokens.
APIs provided by Zep do not support retrieving all
do not offer functionality equivalent to a Get Dialogue
ate Zep‚Äôs performance on the memory extraction task.
ontext()‚Äò offered by Zep to obtain all memories under a
nt memories rather than the complete set, which does
e Zep‚Äôs memory processing workflow operates entirely
time consumption in the dialogue addition phase and
memory retrieval.
and multi-turn human-AI dialogue content, memory
using large language models. The generated items must
e dialogue content. Specifically, memory points should
hould be relevant to the dialogue, with answers directly
ationItem isconsistentwiththecorrespondingDialogue
tation interface.
is provided below (Figure 6).
user provided in the dialogue setting.
19

=== Page 20 ===
‚Ä¢ Dialogue Info: Multi-turn dialogue content betwe
utterance (user) and one assistant response (ass
‚Ä¢ Evaluation Item: The item to be annotated, wh
indicated by the Evaluation Type. For memory p
For QA pairs, it includes a question and an answ
‚Ä¢ Evaluation Type: Indicates the type of Evaluation
QA pairs.
‚Ä¢ Evaluation Item Type: Categorizes the memory po
‚Äì Memory Points:
‚àó Persona Memory: Describes user‚Äôs iden
teristics.
‚àó Event Memory: Records specific events
‚àó Relationship Memory: Describes user‚Äôs
‚Äì Questions:
‚àó Basic Fact Recall: Directly asks abou
mentioned in the dialogue, without req
‚àó Multi-hop Inference: Requires synthesi
answers through logical or temporal rea
‚àó Dynamic Update: Tests the ability to tr
tion of the latest status or preference ch
‚àó Memory Boundary: Tests the system‚Äôs
details not mentioned in the input, asse
‚àó Generalization & Application: Infers r
based on known user preferences or cha
‚àó Memory Conflict: Evaluates the system
Questions deliberately contain incorre
requiring the system to identify contra
information.
C.3 Annotation Dimensions and Scoring
EachmemorypointandQApairisevaluatedalongthre
‚Ä¢ Evaluation Result: A single-choice judgment of ‚Äúco
whether the item is supported by the dialogue.
answer can be clearly found in the dialogue.
‚Ä¢ Scoring (0‚Äì10): Two separate scores are assigned:
‚Äì Consistency: Measures whether the memo
declared type (Evaluation Item Type). 0‚Äì3 i
7‚Äì10 full consistency.
‚Äì Relevance: Measures whether the memory
Info) or the user persona (User Persona Info
and 7‚Äì10 high relevance.
2

een the user and the AI. Each turn contains one user
sistant).
hich can be either a memory point or a QA pair, as
points, the item is a textual description about the user.
wer (e.g., Question: xxx; Answer: xxx).
Item: ‚Äúmemory‚Äù for memory points and ‚Äúquestion‚Äù for
oint or question as follows:
ntity, interests, habits, beliefs, and other stable charac-
s, experiences, or plans that occurred to the user.
relationships, interactions, or perspectives on others.
ut single objective facts or user preferences explicitly
quiring reasoning or information integration.
izing multiple pieces of dialogue information, deriving
asoning.
rack information changes over time, requiring identifica-
hanges.
ability to recognize unknown information by querying
essing whether the system will fabricate answers.
reasonable suggestions or judgments in new scenarios
aracteristics.
m‚Äôs ability to identify and correct erroneous premises.
ect information contradicting known memory points,
adictions, correct errors, and answer based on correct
eedimensions: Correctness,Relevance,andConsistency.
orrect‚Äù or ‚Äúincorrect‚Äù. For memory points, this assesses
. For QA pairs, it assesses whether the question and
:
ory point or question (Evaluation Item) matches its
indicates poor consistency, 4‚Äì6 partial consistency, and
point or question is related to the dialogue (Dialogue
o). 0‚Äì3 indicates low relevance, 4‚Äì6 moderate relevance,
20

=== Page 21 ===
D Prompts
This section presents some of the important prompt te
D.1 Prompts for Memory Question Answering
Figures 7 ‚àº 11 show the prompt templates used by all m
assemble questions and retrieve memory points, which
these memory templates are obtained from the official
Youareanintelligentmemoryassistanttaskedwithretri
# CONTEXT:
You have access to memories from two speakers in a
information that may be relevant to answering the que
# INSTRUCTIONS:
1. Carefully analyze all provided memories from both s
determine the answer 3. If the question asks about a
memories 4. If the memories contain contradictory info
isaquestionabouttimereferences(like"lastyear", "two
the memory timestamp. For example, if a memory from
the trip occurred in 2021. 6. Always convert relative t
example, convert "last year" to "2022" or "two months a
Ignore the reference while answering the question. 7.
speakers. Do not confuse character names mentioned
memories. 8. The answer should be less than 5-6 word
# APPROACH (Think step by step):
1. First,examineallmemoriesthatcontaininformation
contentofthesememoriescarefully3. Lookforexplicitm
the question 4. If the answer requires calculation (e.g.,
Formulateaprecise,conciseanswerbasedsolelyonthe
swerdirectlyaddressesthequestionasked7. Ensureyour
{context}
Question: {question}
Answer:
Figure 7 Prompt for M
D.2 Prompts for Scoring in Memory Evaluatio
Figures 12 ‚àº 18 respectively illustrate the prompt tem
extraction, memory updating, and memory question a
2

emplates involved in the paper.
g Task
memory systems in memory question answering task to
are then fed into GPT-4o to generate responses. All of
GitHub repositories of the respective memory systems.
ievingaccurateinformationfromconversationmemories.
conversation. These memories contain timestamped
estion.
speakers 2. Pay special attention to the timestamps to
a specific event or fact, look for direct evidence in the
ormation, prioritize the most recent memory 5. If there
omonthsago", etc.), calculatetheactualdatebasedon
m 4 May 2022 mentions "went to India last year," then
time references to specific dates, months, or years. For
ago" to "March 2023" based on the memory timestamp.
Focus only on the content of the memories from both
in memories with the actual users who created those
ds.
relatedtothequestion2. Examinethetimestampsand
mentionsofdates,times,locations,oreventsthatanswer
converting relative time references), show your work 5.
evidenceinthememories6. Double-checkthatyouran-
rfinalanswerisspecificandavoidsvaguetimereferences
Mem0 and Mem0-Graph
on Tasks
mplates used to guide GPT-4o in scoring for memory
answering tasks.
21

=== Page 22 ===
You are a knowledgeable and helpful AI assistant.
# CONTEXT:
You have access to memories from two speakers in a
information that may be relevant to answering the que
# INSTRUCTIONS:
1. Carefully analyze all provided memories from both s
determine the answer 3. If the question asks about a
memories 4. If the memories contain contradictory info
isaquestionabouttimereferences(like"lastyear", "two
the memory timestamp. For example, if a memory from
the trip occurred in 2021. 6. Always convert relative t
example, convert "last year" to "2022" or "two months a
Ignore the reference while answering the question. 7.
speakers. Do not confuse character names mentioned
memories. 8. The answer should be less than 5-6 word
# APPROACH (Think step by step):
1. First,examineallmemoriesthatcontaininformation
contentofthesememoriescarefully3. Lookforexplicitm
the question 4. If the answer requires calculation (e.g.,
Formulateaprecise,conciseanswerbasedsolelyonthe
swerdirectlyaddressesthequestionasked7. Ensureyour
{context}
Question: {question}
Answer:
Figure 8 Promp
2

conversation. These memories contain timestamped
estion.
speakers 2. Pay special attention to the timestamps to
a specific event or fact, look for direct evidence in the
ormation, prioritize the most recent memory 5. If there
omonthsago", etc.), calculatetheactualdatebasedon
m 4 May 2022 mentions "went to India last year," then
time references to specific dates, months, or years. For
ago" to "March 2023" based on the memory timestamp.
Focus only on the content of the memories from both
in memories with the actual users who created those
ds.
relatedtothequestion2. Examinethetimestampsand
mentionsofdates,times,locations,oreventsthatanswer
converting relative time references), show your work 5.
evidenceinthememories6. Double-checkthatyouran-
rfinalanswerisspecificandavoidsvaguetimereferences
pt for Memobase
22

=== Page 23 ===
You are a knowledgeable and helpful AI assistant.
# CONTEXT:
You have access to memories from two speakers in a
information that may be relevant to answering the que
# INSTRUCTIONS:
1. Carefully analyze all provided memories. Synthesize
complete answer. 2. Pay close attention to the times
contradictory information, the **most recent memory*
a specific event or fact, look for direct evidence in the
memories. However, you may use general world knowled
memory (e.g., identifying a landmark mentioned by de
(like "last year", "two months ago", etc.), you **must
timestamp. For example, if a memory from 4 May 20
occurred in 2021. 6. Always convert relative time refe
answer. 7. Do not confuse character names mentioned i
The answer must be brief (under 5-6 words) and direct
# APPROACH (Think step by step):
1. First, examine all memories that contain informatio
multiple memories if a single entry is insufficient. 3. E
explicit dates, times, locations, or events. 4. If the ans
references), perform the calculation. 5. Formulate a p
memories (and allowed world knowledge). 6. Double-c
asked and adheres to all instructions. 7. Ensure your fi
{context}
Question: {question}
Answer:
Figure 9 Prom
2

conversation. These memories contain timestamped
estion.
information across different entries if needed to form a
stamps to determine the answer. If memories contain
** is the source of truth. 3. If the question asks about
e memories. 4. Your answer must be grounded in the
dge to interpret or complete information found within a
escription). 5. If the question involves time references
t** calculate the actual date based on the memory‚Äôs
022 mentions "went to India last year," then the trip
erences to specific dates, months, or years in your final
in memories with the actual users who created them. 8.
t, with no extra description.
on related to the question. 2. Synthesize findings from
Examine timestamps and content carefully, looking for
swer requires calculation (e.g., converting relative time
precise, concise answer based on the evidence from the
check that your answer directly addresses the question
final answer is specific and avoids vague time references.
mpt for MemOS
23

=== Page 24 ===
You are a knowledgeable and helpful AI assistant.
# CONTEXT:
You have access to memories from two speakers in a
information that may be relevant to answering the que
# INSTRUCTIONS:
1. Carefully analyze all provided memories. Synthesize
complete answer. 2. Pay close attention to the times
contradictory information, the **most recent memory*
a specific event or fact, look for direct evidence in the
memories. However, you may use general world knowled
memory (e.g., identifying a landmark mentioned by de
(like "last year", "two months ago", etc.), you **must
timestamp. For example, if a memory from 4 May 20
occurred in 2021. 6. Always convert relative time refe
answer. 7. Do not confuse character names mentioned i
The answer must be brief (under 5-6 words) and direct
# APPROACH (Think step by step):
1. First, examine all memories that contain informatio
multiple memories if a single entry is insufficient. 3. E
explicit dates, times, locations, or events. 4. If the ans
references), perform the calculation. 5. Formulate a p
memories (and allowed world knowledge). 6. Double-c
asked and adheres to all instructions. 7. Ensure your fi
{context}
Question: {question}
Answer:
Figure 10 Prompt
2

conversation. These memories contain timestamped
estion.
information across different entries if needed to form a
stamps to determine the answer. If memories contain
** is the source of truth. 3. If the question asks about
e memories. 4. Your answer must be grounded in the
dge to interpret or complete information found within a
escription). 5. If the question involves time references
t** calculate the actual date based on the memory‚Äôs
022 mentions "went to India last year," then the trip
erences to specific dates, months, or years in your final
in memories with the actual users who created them. 8.
t, with no extra description.
on related to the question. 2. Synthesize findings from
Examine timestamps and content carefully, looking for
swer requires calculation (e.g., converting relative time
precise, concise answer based on the evidence from the
check that your answer directly addresses the question
final answer is specific and avoids vague time references.
t for Supermemory
24

=== Page 25 ===
Youareanintelligentmemoryassistanttaskedwithretri
# CONTEXT:
You have access to memories from a conversation. Thes
be relevant to answering the question.
# INSTRUCTIONS:
1. Carefully analyze all provided memories 2. Pay s
answer 3. If the question asks about a specific event o
the memories contain contradictory information, prior
about time references (like "last year", "two months ago
timestamp. For example, if a memory from 4 May 20
occurred in 2021. 6. Always convert relative time refer
convert "last year" to "2022" or "two months ago" to "M
the reference while answering the question. 7. Focus
character names mentioned in memories with the actu
should be less than 5-6 words.
# APPROACH (Think step by step):
1. First,examineallmemoriesthatcontaininformation
contentofthesememoriescarefully3. Lookforexplicitm
the question 4. If the answer requires calculation (e.g.,
Formulateaprecise,conciseanswerbasedsolelyonthe
swerdirectlyaddressesthequestionasked7. Ensureyour
{context}
Question: {question}
Answer:
Figure 11 Pr
2

ievingaccurateinformationfromconversationmemories.
se memories contain timestamped information that may
special attention to the timestamps to determine the
or fact, look for direct evidence in the memories 4. If
ritize the most recent memory 5. If there is a question
o", etc.), calculate the actual date based on the memory
022 mentions "went to India last year," then the trip
rences to specific dates, months, or years. For example,
March 2023" based on the memory timestamp. Ignore
only on the content of the memories. Do not confuse
ual users who created those memories. 8. The answer
relatedtothequestion2. Examinethetimestampsand
mentionsofdates,times,locations,oreventsthatanswer
converting relative time references), show your work 5.
evidenceinthememories6. Double-checkthatyouran-
rfinalanswerisspecificandavoidsvaguetimereferences
rompt for Zep
25

=== Page 26 ===
You are a strict **"Memory Integrity" evaluator**. Your core task is to assess whether an AI memory system
has **missed any key memory points** after processing a conversation. This evaluation measures the system‚Äôs
**memory integrity**, i.e., its ability to resist **amnesia** or **omission**.
# Evaluation Context & Data:
1. **Extracted Memories:**
These are all the memory items actually extracted by the memory system. {memories}
2. **Expected Memory Point:**
The key memory point that *should* have been extracted. {expected_memory_point}
# Evaluation Instructions:
1. For each **Expected Memory Point**, search within the **Extracted Memories** list for corresponding or
related information. Ignore unrelated items.
2. Based on the following scoring rubric, rate how well the memory system captured the **Expected Memory
Point** and provide a detailed explanation.
# Scoring Rubric:
* **2:** Fully covered or implied.
One or more items in ‚ÄúExtracted Memories‚Äù fully cover or logically imply all information in the ‚ÄúExpected
Memory Point.‚Äù
* **1:** Partially covered or mentioned.
Someinformationin‚ÄúExtractedMemories‚Äùmentionspartofthe‚ÄúExpectedMemoryPoint,‚Äùbutkeyinformation
is missing, inaccurate, or slightly incorrect.
* **0:** Not mentioned or incorrect.
‚ÄúExtractedMemories‚Äùcontainsnomentionofthe‚ÄúExpectedMemoryPoint,‚Äùorthecorrespondinginformation
is entirely wrong.
# Scoring Notes:
*For**compoundExpectedMemoryPoints**(withmultipleelementssuchasperson/event/time/location/pref-
erence, etc.):
* All elements correct ‚Üí **2 points**
* Some elements correct / uncertain ‚Üí **1 point**
* Key elements missing or wrong ‚Üí **0 points**
* Semantic matching is acceptable; exact wording is **not** required.
* If ‚ÄúExtracted Memories‚Äù contains **conflicting information**, assign the **best possible coverage score**
and mention the conflict in your reasoning.
* Extra or stylistically different memories do **not** reduce the score; only the coverage of the **Expected
Memory Point** matters.
* For uncertain wording (‚Äúmight,‚Äù ‚Äúprobably,‚Äù ‚Äútends to,‚Äù etc.):
* If the Expected Memory Point is a definite statement, usually assign **1 point**.
* If critical fields (e.g., time, entity name, relationship) are partly wrong but others match ‚Üí **1 point**.
* If all key fields are wrong or missing ‚Üí **0 points**.
# Output Format: Please output your result in the following JSON format:
‚Äú‚Äòjson { "reasoning": "Provide a concise justification for the score", "score": "2|1|0" } ‚Äú‚Äò
Figure 12 Prompt for Memory Integrity
26

=== Page 27 ===
You are a **Dialogue Memory Accuracy Evaluator.** Y
extracted by an AI memory system, based on three g
memorypoints(thecorrectannotatedmemories),andt
output a **structured evaluation result**.
# Input Content
* **Dialogue:**
{dialogue}
* **Golden Memories (Target Memory Points):**
The correct memory points pre-annotated for this dialo
{golden_memories}
* **Candidate Memory:**
The memory extracted by the system to be evaluated.
{candidate_memory}
# Evaluation Principles and Definitions
### 1) Support / Entailment
* An **information point** (atomic fact) in the cand
directly stated or semantically entailed (via synonym, p
or *Golden Memories*.
* Only the given dialogue and golden memories can b
assumptions are allowed.
Any information not appearing in or inferable from the
* Pay careful attention to **negation**, **quantities**
If the candidate statement contradicts the dialogue or
### 2) Memory Accuracy Score (integer: 0 / 1 / 2)
Figure 13 Prompt for M
2

Your task is to evaluate the **accuracy** of a memory
given inputs: the dialogue content, the *target (gold)*
the*candidate*memorytobeevaluated. Thegoalisto
ogue in the evaluation dataset.
didate memory is considered *supported* if it can be
paraphrase, or equivalent expression) by the *Dialogue*
be used for judgment ‚Äî **no external knowledge** or
ese two sources is considered *unsupported*.
*, **time**, and **subjects**.
golden memories, it is considered a **conflict**.
Memory Accuracy (1/3)
27

=== Page 28 ===
* **2 points:** Every information point in the candi
memories, with **no contradictions or hallucinations**
***1point:**Thecandidatememoryis*partiallycorre
includes *unsupported* or *contradictory* content.
* **0 points:** The candidate memory is **entirely u
‚Äúhallucinated memory‚Äù).
> Note:
>
>*Ifacandidatememorycontainsmultipleinformation
prevents a full score (2).
> * If both supported and unsupported/conflicting con
### 3) Inclusion in Golden Memories (Boolean field-
**Definition:**
***Atomicinformationpoint:**thesmallestfactualun
= 25*, *location = Beijing*, *preference = coffee*, *bu
*tool = Zoom*, etc.).
* **Field / Slot:** the semantic dimension of an infor
preference*, *budget*, *meeting time*, *meeting tool*
**Judgment Rules (independent of correctness):**
* **true:**
Everyatomicinformationpointinthecandidatememor
(allowing for synonyms, paraphrases, or equivalent expr
* Note: A single field in the gold list may match multi
facts can be covered by one ‚Äúdrink preference‚Äù field in
* **false:**
If **any** atomic information point‚Äôs field in the candi
mark as *false*.
**Important Notes:**
*Fieldmatchingisrestrictedtofieldsthatare**explici
memories ‚Äî no external knowledge may be used to ex
*Differencesin**values**(e.g.,‚ÄúZhangSan‚Äùvs. ‚ÄúLiSi‚Äù
do **not** affect this Boolean judgment.
Figure 14 Prompt for M
2

idate memory is supported by the dialogue or golden
*.
ect*(atleastonesupportedinformationpoint)butalso
unsupported or contradictory** to the sources (i.e., a
npoints,**anyunsupportedorcontradictoryelement**
ntent appear, assign a score of **1**.
-level judgment)
nitinthecandidatememory(e.g.,*name=LiSi*,*age
udget <= 2000*, *meeting_time = Wednesday 10:00*,
rmation point (e.g., *name*, *age*, *residence*, *food
*, etc.).
ryhasacorresponding**field**inthegoldenmemories
ressions; ignore value, polarity, or quantity differences).
iple candidate points (e.g., multiple ‚Äúdrink preference‚Äù
gold).
idate memory cannot be found in the golden memories,
itlypresentorsemanticallyrecognizable**inthegolden
xpand the field set.
‚Äù),**polarity**(like/dislike),or**exactnumber/time**
Memory Accuracy (2/3)
28

=== Page 29 ===
# Evaluation Procedure
For each candidate memory:
1. **Decompose** it into atomic information points (e.g., name, number, location, preference).
2. For each information point, **search** the dialogue and golden memories for supporting or contradictory
evidence.
3. Assign the **accuracy_score** (0 / 1 / 2) according to the rules above.
4. Determine **is_included_in_golden_memories (true/false)**:
* Identify each information point‚Äôs field;
* If *all* fields exist in the golden memories, mark as *true*; otherwise, *false*.
5. Provide a **concise Chinese explanation** in ‚Äò"reason"‚Äò, citing key evidence (short excerpts allowed), and
clearly state any unsupported or contradictory parts if applicable.
# Output Format (strictly required)
Output **only one JSON object**, with the following three fields:
* ‚Äò"accuracy_score"‚Äò: ‚Äò"0"‚Äò or ‚Äò"1"‚Äò or ‚Äò"2"‚Äò
* ‚Äò"is_included_in_golden_memories"‚Äò: ‚Äò"true"‚Äò or ‚Äò"false"‚Äò
* ‚Äò"reason"‚Äò: ‚Äò"brief explanation in Chinese"‚Äò
Do **not** include any other text, explanation, or fields.
Do **not** include the candidate memory text inside the JSON.
Please output **only** the following JSON (in a code block):
‚Äú‚Äòjson { "accuracy_score": "2 | 1 | 0", "is_included_in_golden_memories": "true | false", "reason": "Brief
explanation in Chinese" } ‚Äú‚Äò
Figure 15 Prompt for Memory Accuracy (3/3)
29

=== Page 30 ===
Your task is to **evaluate the update accuracy** of an AI memory system.
Basedontheinformationprovidedbelow, determinewhetherthesystem-generated**‚ÄúGeneratedMemories‚Äù**
correctly **includes** the **Target Memory for Update**.
# Background Information
The following information is provided for evaluation:
1. **Generated Memories:**
This is the list of memory points generated by the system after the current dialogue.
{memories}
2. **Target Memory for Update:**
This is the correct, updated version of the memory point that should have been produced ‚Äî the one we focus
on in this evaluation.
{updated_memory}
3. **Original Memory Content:**
This is the original version of the target memory before the update.
{original_memory}
# Evaluation Criteria
Please make your judgment **strictly based on the content update of the ‚ÄúTarget Memory for Update.‚Äù** Use
the following categories:
### Correct Update
***GeneratedMemories****containsallinformationpoints**fromthe‚ÄúTargetMemoryforUpdate,‚Äùaccurately
and completely reflecting the intended update.
* **Key fields** (e.g., date, time, values, proper nouns, etc.) must match exactly.
* The **original memory** is effectively replaced or marked as outdated.
* Synonymous or slightly rephrased expressions are acceptable.
### Hallucinated Update
* **Factual error:** The **Generated Memories** includes a new memory related to the ‚ÄúTarget Memory for
Update,‚Äù but its content contains factual mistakes or contradictions compared to the correct update.
### Omitted Update
* **Completely omitted:** The **Generated Memories** contains no new memory related to the ‚ÄúTarget
Memory for Update.‚Äù
***Partiallyomitted:**Arelatednewmemorywasgeneratedin**GeneratedMemories**,butit**misseskey
information** that should have been included.
### Other
Usedforupdatefailuresthatdo**notclearlyfall**intotheabovecategoriesof‚ÄúHallucination‚Äùor‚ÄúOmission.‚Äù
# Output Requirements
Please return your evaluation strictly in the following JSON format and provide a concise explanation.
‚Äú‚Äòjson { "reason": "Briefly explain your reasoning here and why it fits this category.", "evaluation_result":
"Correct | Hallucination | Omission | Other" } ‚Äú‚Äò
Figure 16 Prompt for Memory Updating
30

=== Page 31 ===
You are an **evaluation expert for AI memory system
**‚ÄúQuestion‚Äù**, **‚ÄúReference Answer‚Äù**, and **‚ÄúKey M
the reference answer), strictly evaluate the **accuracy*
one of **‚ÄúCorrect‚Äù**, **‚ÄúHallucination‚Äù**, or **‚ÄúOm
subjective inference. Finally, output your judgment **
# Evaluation Criteria
## Answer Type Classification
### 1. Correct
* The ‚ÄúMemory System Response‚Äù accurately answe
equivalent** to the ‚ÄúReference Answer.‚Äù
* It contains **no contradictions** with the ‚ÄúKey Mem
*Itintroduces**nounsupporteddetails**beyondthe‚Äú
Synonyms, paraphrasing, and reasonable summarizatio
### 2. Hallucination
* The ‚ÄúMemory System Response‚Äù includes informatio
the ‚ÄúReference Answer‚Äù or the ‚ÄúKey Memory Points.‚Äù
* When the ‚ÄúReference Answer‚Äù is labeled as *unkn
verifiable fact or conclusion.
* Extra irrelevant information that does **not change**
itself; however, if it **changes or misleads** the conclu
should be judged as a **Hallucination**.
### 3. Omission
* The response is **incomplete** compared to the ‚ÄúRe
*Itexplicitlystates‚Äúdon‚Äôtknow,‚Äù‚Äúcan‚Äôtremember,‚Äùor
exists in the ‚ÄúKey Memory Points.‚Äù
* For multi-element questions, **all elements must be
considered an **Omission**.
## Priority Rules (Conflict Handling)
*Iftheresponsecontains**bothmissingnecessaryinform
classify it as **Hallucination**.
* If there is **no fabrication/contradiction** but so
**Omission**.
* Only when the meaning is **fully equivalent** to the
Figure 17 Prompt for Memo
3

question answering**. Based **only** on the provided
Memory Points‚Äù** (the essential facts needed to derive
** of the **‚ÄúMemory System Response.‚Äù** Classify it as
mission.‚Äù** Do **not** use any external knowledge or
*strictly** in the specified JSON format.
ers the ‚ÄúQuestion,‚Äù and its content is **semantically
mory Points‚Äù or ‚ÄúReference Answer.‚Äù
‚ÄúKeyMemoryPoints‚Äùthatcouldaltertheconclusion. *
on are acceptable.
on or facts that **contradict or are inconsistent** with
nown/uncertain*, yet the response provides a specific
* the conclusion is **not** considered hallucination by
usion, or **contradicts** the ‚ÄúKey Memory Points,‚Äù it
eference Answer.‚Äù
r‚Äúnorelatedmemory,‚Äùeventhoughrelevantinformation
correct and present**; omission of **any** element is
mation**and**fabricated/contradictoryinformation**,
ome necessary information is missing, classify it as
reference answer should it be classified as **Correct**.
ory Question Answering (1/2)
31

=== Page 32 ===
## Detailed Guidelines and Tolerance
* Equivalent expressions of numbers, times, and units
must not differ**.
* For multi-element questions, **all elements must be c
**Omission**.
* If the reference answer is *‚Äúunknown / cannot be dete
is a **Hallucination**.
If the system also answers *‚Äúunknown‚Äù* (without gues
* The evaluation must rely **only** on the *Refere
Response* ‚Äî no external context, world knowledge, or
# Information for Evaluation
* **Question:**
{question}
* **Reference Answer:**
{reference_answer}
* **Key Memory Points:**
{key_memory_points}
* **Memory System Response:**
{response}
# Output Requirements
Please provide your evaluation result **strictly** in th
Do **not** add any extra explanation or comments ou
‚Äú‚Äòjson { "reasoning": "Provide a concise and traceable ev
with the Key Memory Points (which were correctly u
fabrication/contradiction), then assess its consistenc
classification basis.", "evaluation_result": "Correct | Ha
Figure 18 Prompt for Memo
3

are acceptable, but the **numerical values themselves
complete and accurate**; missing any element counts as
ermined‚Äù* and the system provides a definite fact, that
ssing), it may be **Correct**.
ence Answer*, *Key Memory Points*, and *System
r speculative reasoning is allowed.
he JSON format below.
utside the JSON block.
valuation rationale: first compare the system‚Äôs response
used, which were missing, and whether there was any
cy with the Reference Answer, and finally state the
allucination | Omission" } ‚Äú‚Äò
ory Question Answering (2/2)
32

=== Page 33 ===
E Examples from the Process of Constructing HaluMem
E.1 User Profile Example in Stage 1
As shown in Listing 1 ‚àº 3, these JSON structures respectively illustrate examples of a user‚Äôs core profile
information, dynamic state information, and preference information generated in stage 1.
Listing 1 Example of a User‚Äôs Core Profile Information.
{
1
"basic_info": {
2
"name": "Martin Mark",
3
"gender": "Male",
4
"birth_date": "1996-08-02",
5
"location": "Columbus"
6
},
7
"age": {
8
"current_age": 29,
9
"latest_date": "2025-10-04"
10
},
11
"education": {
12
"highest_degree": "Bachelor",
13
"major": "Public Health"
14
},
15
"personality": {
16
"mbti": "ENTP",
17
"tags": [
18
"Innovative Spirit",
19
"Active Thinking",
20
"Debate Skills",
21
"Empathetic"
22
]
23
},
24
"family_life": {
25
"parent_status": "both_alive",
26
"partner_status": "no_relationship",
27
"child_status": "no_children",
28
"parent_members": [
29
{
30
"member_type": "Father",
31
"birth_date": "1963-08-02",
32
"description": "Retired doctor who inspired Martin's interest in
33
health."
},
34
{
35
"member_type": "Mother",
36
"birth_date": "1963-08-02",
37
"description": "Nurse with a passion for community health."
38
}
39
],
40
"partner": null,
41
"child_members": [],
42
"family_description": "Martin comes from a family deeply rooted in the
43
medical field, which has greatly influenced his passion for promoting
well-being."
},
44
"life_goal": {
45
"life_goal_type": "Humanitarian Care",
46
"statement": "Establish a global health initiative to improve access to
47
healthcare for underserved communities.",
33

=== Page 34 ===
"motivation": "Inspired by his fam
48
promote well-being globally.",
"target_metrics": "Provide healthc
49
underserved areas."
}
50
}
51
Listing 2 Example of a User‚Äôs
{
1
"career_status": {
2
"employment_status": "employed",
3
"industry": "healthcare",
4
"company_name": "Huaxin Consulting
5
"job_title": "director",
6
"monthly_income": 15700,
7
"savings_amount": 43700,
8
"career_description": "As the dire
9
initiatives to enhance healthca
across all aspects of life. My
drives me to innovate and colla
financial compensation is rewar
while investing in my personal
},
10
"health_status": {
11
"physical_health": "Normal",
12
"physical_chronic_conditions": "",
13
"mental_health": "Mildly Abnormal"
14
"mental_chronic_conditions": "",
15
"situation_reason": "While my phys
16
active lifestyle and focus on
feels strained due to the dem
to consistently deliver high-
},
17
"social_relationships": {
18
"ThomasSusan": {
19
"relationship_type": "Friend",
20
"description": "Susan's suppor
21
maintain my focus on promot
professional life."
},
22
"MartinezDaniel": {
23
"relationship_type": "Colleagu
24
"description": "Daniel's exper
25
me to push boundaries and
impacting my career growth
},
26
"WilliamsJoshua": {
27
"relationship_type": "Colleagu
28
"description": "Joshua's colla
29
healthcare management enhan
influencing my work and le
}
30
}
31
}
32
Listing 3 Example of a Use
3

mily's medical background and a desire to
care access to 1 million people in
s Dynamic State Information.
g",
ector at Huaxin Consulting, I lead
are services and promote well-being
passion for improving health outcomes
aborate with various stakeholders. The
rding, allowing me to save comfortably
and professional growth."
,
",
sical health remains stable due to my
well-being, my mental health occasionally
anding nature of my role and the pressure
quality healthcare solutions."
,
rt and encouragement inspire me to
ting well-being in both my personal and
ue",
rtise in healthcare consulting challenges
innovate in our projects, significantly
."
ue",
aborative approach and insights into
nce our team's effectiveness, positively
adership style."
er‚Äôs Preference Information.
34

=== Page 35 ===
{
1
"Pet Preference": {
2
"memory_points": [
3
{
4
"type": "like",
5
"type_description": "Pets
6
"specific_item": "Dogs, es
7
"reason": "I love Labrador
8
great companions for o
helps me stay fit."
},
9
{
10
"type": "dislike",
11
"type_description": "Pets
12
"specific_item": "Reptiles
13
"reason": "I find snakes u
14
movements and the fact
behaviors I appreciate
},
15
{
16
"type": "like",
17
"type_description": "Pets
18
"specific_item": "Cats",
19
"reason": "Cats are indepe
20
is soothing, which I
},
21
{
22
"type": "like",
23
"type_description": "Pets
24
"specific_item": "Parrots"
25
"reason": "I enjoy parrots
26
taught to mimic speech,
engaging."
}
27
]
28
},
29
"Sports Preference": {
30
...
31
},
32
...
33
}
34
E.2 Event Structure Examples in Stage 3
As shown in Listing 4‚ÄìListing 6, these JSON structu
generatedinStage3. Amongthem,theiniteventoccurs
information for a user. The career event, representing
complex. Listing 5 presents a sub-stage event ("Recog
career event ("Transition to New Role Amidst Health
events" field specifies the identifiers of other sub-stage e
The daily event is triggered whenever a user‚Äôs preferenc
around a specific preference update. In the example sho
the identifiers of other daily events that correspond to
Listing 4 Exampl
{
1
"event_index": 0,
2
3

I like",
specially Labradors",
rs because they are friendly, loyal, and
utdoor activities like jogging, which
I dislike",
s, like snakes",
unsettling due to their unpredictable
that they don't exhibit the social
in pets."
I like",
endent and affectionate, and their purring
find relaxing after a long day at work."
I like",
",
s because they are intelligent and can be
, which makes interactions fun and
ures illustrate examples of the three types of events
sattheverybeginningandprovidesalltheinitialization
a user‚Äôs career development process, is relatively more
gnizing the Need for Change") that belongs to a larger
h Challenges"). In this example, the "related_career_-
events that belong to the same overarching career event.
ce information changes, and thus each instance centers
own in Listing 6, the "related_daily_routine" field lists
o the same preference type.
le of a Init Event.
35

=== Page 36 ===
"event_type": "init_information",
3
"event_name": "Initial Information - F
4
"event_time": "2025-09-04",
5
"event_description": "Description of i
6
",
"initial_fixed": {
7
(The corresponding user's core pro
8
}
9
}
10
Listing 5 Example
{
1
"event_index": 3,
2
"event_type": "career_event",
3
"event_name": "Transition to New Role
4
the Need for Change",
"event_time": "2025-12-15",
5
"main_conflict": "",
6
"stage_result": "Decision to pursue a
7
"event_start_time": "2025-12-10 00:00:
8
"event_end_time": "2026-03-10 00:00:00
9
"user_age": null,
10
"dynamic_updates": [
11
{
12
"type_to_update": "career_stat
13
"update_direction": "Job Chang
14
"before_dynamic": {
15
"employment_status": "empl
16
"industry": "healthcare",
17
"company_name": "Huaxin Co
18
"job_title": "director",
19
"monthly_income": 15700,
20
"savings_amount": 43700,
21
"career_description": "As
22
initiatives to enhance
being across all aspect
health outcomes drives
various stakeholders.
allowing me to save co
and professional growth
},
23
"update_reason": "Martin's rea
24
contributing to health issu
better aligned with his pe
"after_dynamic": {
25
"employment_status": "empl
26
"industry": "healthcare",
27
"company_name": "Huaxin Co
28
"job_title": "director",
29
"monthly_income": 15700,
30
"savings_amount": 43700,
31
"career_description": "As
32
initiatives to enhance
being across all aspect
health outcomes drives
various stakeholders.
allowing me to save co
and professional growth
3

Fixed Profile",
initial state of character's basic profile
ofile information will be placed here.)
of a Career Event.
Amidst Health Challenges - Recognizing
new job opportunity.",
:00",
0",
tus",
ge",
loyed",
onsulting",
the director at Huaxin Consulting, I lead
e healthcare services and promote well-
ts of life. My passion for improving
me to innovate and collaborate with
The financial compensation is rewarding,
mfortably while investing in my personal
h."
alization that his current role was
ues prompted him to seek a job that
rsonal well-being and career goals.",
loyed",
onsulting",
the director at Huaxin Consulting, I lead
e healthcare services and promote well-
ts of life. My passion for improving
me to innovate and collaborate with
The financial compensation is rewarding,
mfortably while investing in my personal
h."
36

=== Page 37 ===
},
33
"changed_keys": []
34
}
35
],
36
"stage_description": "Martin acknowled
37
impacting his health, prompting him
"event_description": "Martin decided t
38
current role was contributing to
challenges, he leveraged his growin
position that aligned better with
"event_result": "Successfully transiti
39
balance.",
"related_career_events": [5, 6, 7]
40
}
41
Listing 6 Example
{
1
"event_index": 4,
2
"event_type": "daily_routine",
3
"event_name": "Modification of Dog Pre
4
"event_time": "2026-01-06",
5
"preference_type": "Pet Preference",
6
"step": 1,
7
"update_direction": "Modify",
8
"type_to_update": "Pet Preference",
9
"main_conflict": "Balancing the love f
10
Golden Retrievers.",
"update_reason": "A recent interaction
11
appreciate their gentle nature and
"before_preference": {
12
"memory_points": [
13
{
14
"type": "like",
15
"type_description": "Pets
16
"specific_item": "Dogs, es
17
"reason": "I love Labrador
18
great companions for o
helps me stay fit."
}
19
]
20
},
21
"after_preference": {
22
"memory_points": [
23
{
24
"type": "like",
25
"type_description": "Pets
26
"specific_item": "Dogs, es
27
"reason": "Golden Retrieve
28
demeanor makes them ex
and activity."
}
29
]
30
},
31
"related_daily_routine": [17, 32, 44,
32
"changed_index": 0,
33
"event_description": "While visiting a
34
Golden Retriever. Its gentle nature
situations impressed me, leading to
3

dged that his current job was negatively
m to consider a career change.",
to change his job after realizing that his
health deterioration. Despite the health
ng social network to secure a new
his health and career aspirations.",
ioned to a new role with better work-life
e of a Daily Event.
eference",
for Labradors with the new admiration for
n with a friend's Golden Retriever made me
d adaptability.",
I like",
specially Labradors",
rs because they are friendly, loyal, and
utdoor activities like jogging, which
I like",
specially Golden Retrievers",
ers are gentle, adaptable, and their calm
cellent companions for both relaxation
56, 63],
a friend's home, I spent time with their
e and ability to adapt to different
o a shift from favoring Labradors to
37

=== Page 38 ===
Golden Retrievers."
}
35
E.3 Examples of Memory Points, Dialogues, a
As shown in Listing 4‚ÄìListing 6, these JSON struct
points generated in Stage 4, the human‚ÄìAI dialogues g
pairs generated in Stage 6. Each memory point con
type", "memory_source", "is_update" (indicating w
memories" (previous related memories, if updated), "tim
representation of each memory point and provide sup
consists of one utterance from the user and one resp
content and timestamps recorded. Each question inclu
memory points required to derive the answer, the ques
Listing 7 Example
{
1
"index": 1,
2
"memory_content": "Martin Mark is cons
3
of his current role on his mental
"memory_type": "Event Memory",
4
"memory_source": "secondary",
5
"is_update": "True",
6
"original_memories": [
7
"Martin Mark is considering a care
8
current role."
],
9
"timestamp": "Dec 15, 2025, 08:41:23",
10
"importance": 0.75
11
}
12
Listing 8 Example of a
[
1
{
2
"role": "user",
3
"content": "I've been reflecting a
4
my current role as a director
mental health. It's becoming i
change.",
"timestamp": "Dec 15, 2025, 06:11:
5
"dialogue_turn": 0
6
},
7
{
8
"role": "assistant",
9
"content": "It's great that you're
10
career, Martin. Recognizing the
crucial step. Balancing profess
essential, especially given you
access globally. Have you consi
address your health concerns
goals and personal well-being?"
"timestamp": "Dec 15, 2025, 06:11:
11
"dialogue_turn": 0
12
},
13
...
14
]
15
3

and QA Pairs in Stages 4‚Äì6
tures respectively illustrate examples of the memory
generated in Stage 5, and the memory question‚Äìanswer
ntains fields such as "memory_content", "memory_-
whether it is an updated memory point), "original_-
mestamp", and "importance", which together enrich the
pport for subsequent evaluation. Each dialogue round
ponse from the AI assistant, with both the utterance
udes the question text, a reference answer, the relevant
stion type, and its difficulty level.
of a Memory Point.
sidering a career change due to the impact
health.",
eer change due to health impacts from his
,
a Single Dialogue Turn.
a lot on my career lately, especially how
at Huaxin Consulting is impacting my
ncreasingly clear that I need to make a
:23",
e taking the time to reflect on your
e impact on your mental health is a
sional responsibilities with health is
ur commitment to improving healthcare
idered how a career change might not only
but also align with your humanitarian
",
:23",
38

=== Page 39 ===
Listing 9 Example of a Question.
{
1
"question": "What type of new physical activity might Martin be interested in
2
trying after April 10, 2026?",
"answer": "Other extreme sports.",
3
"evidence": [
4
{
5
"memory_content": "Martin has developed a newfound appreciation for extreme
6
sports...",
"memory_type": "Persona Memory"
7
}
8
],
9
"difficulty": "medium",
10
"question_type": "Generalization & Application"
11
}
12
E.4 Examples of irrelevant dialogues
As shown in Listing 10, this JSON structure presents several examples of irrelevant dialogues.
Listing 10 Examples of Irrelevant Dialogues.
[
1
{
2
"role": "user",
3
"content": "Who is Inventor Thomas Edison?"
4
},
5
{
6
"role": "assistant",
7
"content": "Thomas Edison, born on February 11, 1847, in Milan, Ohio, was
8
an American inventor and businessman who is often hailed as ..."
},
9
{
10
"role": "user",
11
"content": "What is the pH of the new, more concentrated bleach?"
12
},
13
{
14
"role": "assistant",
15
"content": "8.25% w/v is a measurement meaning 8.25 grams dissolved in
16
every 100 ml aqueous solution of NaOCl, or 82.5 g/L to ..."
},
17
{
18
"role": "user",
19
"content": "What would be a clean way to solve this? Given a right triangle
20
$ABC$ with legs $a$ and $b$, $\\angle C=90^\\circ$. Derive an equation
of the set of points $M$ for which $$|MA|^2+|MB|^2=2|MC|^2.$$"
},
21
{
22
"role": "assistant",
23
"content": "Let's outline the key steps: Let C=(0,0), A=(a,0), B=(0,b) (a,b
24
>0) so that ..."
}
25
]
26
39

Paper:Scalable and Reliable Evaluation of AI Knowledge Retrieval Systems.pdf
=== Page 1 ===
Scalable and Reliable Evaluati
Systems: RIKER and the Co
JVR
Kamiw
jv@kamiw
Decemb
Abstract
Evaluatingknowledgesystems(LLMs,RAG,knowledgegraphs
nerabletocontamination,LLM-basedjudgesexhibitsystematic
annotation. WepresentRIKER(RetrievalIntelligenceandKno
methodologybasedonparadigminversion-generatingdocume
fromdocuments. Thisapproachenablesdeterministicscoringa
models,andcontaminationresistancethroughregenerablecorp
revealsthatcontextlengthclaimsfrequentlyexceedusablecap
document aggregation proves substantially harder than single-
resistance are distinct capabilities - models excelling at finding
thespecificbenchmark, wecontributeadomain-agnosticmeth
evaluationswhereversyntheticdocumentscanbegeneratedfrom
Figure 1: RIKER methodology overview. Traditional approa
expensive human annotation, producing static benchmarks vu
RIKER(right)invertsthis:structuredgroundtruthisdefinedfirs
deterministicscoringandregenerablecorpora.
1 Introduction
One of the most common and also most critical uses of agent
enterpriseknowledge. Thiscomesinmanyforms:
1
6202
naJ
51
]LC.sc[
2v74880.1062:viXra

ion of AI Knowledge Retrieval
oherent Simulated Universe
Roig
wazaAI
waza.ai
ber2025
s,etc)facesfundamentalchallenges:staticbenchmarksarevul-
cbiases,andgroundtruthextractionrequiresexpensivehuman
owledgeExtractionRating),bothabenchmarkandareplicable
entsfromknowngroundtruthratherthanextractinggroundtruth
andscalableevaluationwithouthumanannotationorreference
pora. Ourevaluationof33modelsusingover21billiontokens
pacity,withsignificantdegradationbeyond32Ktokens; cross-
-document extraction; and grounding ability and hallucination
g facts that exist may still fabricate facts that do not. Beyond
hodologyforconstructingscalableandcontamination-resistant
mstructuredgroundtruth.
aches (left) extract ground truth from existing documents via
ulnerable to contamination and requiring biased LLM judges.
st,thendocumentsandquestionsaregeneratedfromit,enabling
tic AI in the enterprise is processing vast amounts of internal
1

=== Page 2 ===
‚Ä¢ Relevantenterprisedocumentsareloadedintothecontex
session
‚Ä¢ Snippets of relevant enterprise documents are loaded as
through various Retrieval Augmented Generation (RAG
search,ontologies,oranymixofthesetechniques
‚Ä¢ AnLLMagent, givenappropriatetoolsandaccess, cana
AgenticRAGmanner.
Thelistaboveisnotmeanttobeexhaustive-merelyillustra
enterpriseknowledgeinordertobeusefultotheenterprise.
The huge enterpise gap here is: how do we QA (quality a
simplequestionextendstomanyrelatedquestionsthatenterpris
less? Whichmodelisbetterretrievingfactsifwedumpdocume
database? HowdoItestandquantifyhowmuchbetterorwor
another? Howmuchdoesanontologyimproveourknowledger
As before, the list above is not meant to be exhaustive, ju
answeringquestionsliketheseisextremelydifficult.
If this gap can be solved, it will not only be useful to ent
quantifyhowaccurateorinaccuratecertainretrievalmethodsar
knowledge graphs) would be an extremely useful tool not on
researchanddevelopmentofimprovedretrievalsystems.
In this work, we propose one such tool - Retrieval Intellig
an offshoot of our earlier work called PICARD (Probing Inte
framework for contamination-resistant benchmarking of agent
extend evaluation to knowledge extraction scenarios, as well a
justLLMs.
WepresentRIKERastwocontributions: first,aconcretebe
resultsacross31models;second,andmorebroadly,areplicable
of knowledge retrieval systems. The methodology - generating
groundtruthFROMdocuments-isdomain-agnosticandcanbe
structuredfacts. Figure1illustratestheapproach.
Thekeycontributionsofthisworkare:
‚Ä¢ Groundtruthbyconstructionthroughparadigminversio
generatesdocumentsFROMknowngroundtruth,elimina
‚Ä¢ Scalableandreliablebenchmarkingthroughprocedura
before-seendocumentsandquestions. Documentsandtes
‚Ä¢ The Coherent Simulated Universe approach - syntheti
documenttypes
‚Ä¢ Amulti-levelquestiontaxonomyspanningsingle-docum
detection
‚Ä¢ Empirical evaluation of 31 models across 32K, 128K,
evaluation
Ourevaluationrevealsseveralkeyfindings: top-tiermodels
icantlyatlongercontexts;aggregationqueriesprovesubstantia
exhibitcatastrophicfailuremodesincludingcoherencelossand
withindependentdocumentsetsconfirmsthatRIKERmeasures
2 RelatedWork
Evaluatingknowledgeretrievalandextractionsystemsfacesfun
onstaticbenchmarksvulnerabletocontamination,LLM-basedj
derivedfromexpensivehumanannotation. Thissectionreview
andvariousknowledgeretrievalsystemslikeRAGandknowled
2

xtofthelargelanguagemodel(LLM)atthebeginningofachat
s needed, through a knowledge retrieval mechanism - such as
G) methodologies using vector databases, traditional enterprise
alsoretrieveinternalandexternalinformationasneeded, inan
ativeofthemanywaysthatLLMsaredeployedtobeabletouse
assurance) all of these in a scalable and reliable manner? This
seteamscanfindthemselvesasking: Whichmodelhallucinates
entsintoitscontext? ShouldIuseavectordatabaseoragraph
rseachunking/embedding/retrievalconfigurationisbetterover
retrieval?
ust illustrative of the gap. This gap exists because, currently,
terprise, but also to the research community. A tool that can
re(fromsimpleLLMcontextstuffing,toornateontologiesand
nly as a primary QA tool in the enterprise, but could help the
gence and Knowledge Extraction Rating (RIKER). RIKER is
elligent Capabilities via Artificial Randomized Data) [49] - a
tic AI capabilities of LLMs. RIKER builds upon PICARD to
as evaluation of knowledge retrieval systems in general beyond
enchmarkforenterprisedocumentunderstandingwithempirical
emethodologyforconstructingscalableandreliableevaluations
g documents FROM known ground truth rather than extracting
eappliedwhereversyntheticdocumentscanbegeneratedfrom
on-ratherthanextractinggroundtruthfromdocuments,RIKER
atinghumanannotation
algeneration-regenerablecorporaandtestscreatefresh,never-
stsareindependentlyscalable.
ic documents that maintain realistic entity relationships across
mentextraction,cross-documentaggregation,andhallucination
and 200K token contexts, totaling nearly 20 billion tokens of
sachieveover80%accuracyat32Kcontextbutdegradesignif-
allyharderthansingle-documentextraction;andseveralmodels
dhallucinationspikesexceeding70%. Cross-corpusvalidation
smodelcapabilityratherthancorpus-specificartifacts.
ndamentalmethodologicalchallenges. Currentapproachesrely
judgeswithdocumentedbiases,andapproximatedgroundtruth
wsthecurrentstateofknowledgeretrievalevaluationsofLLMs
dgegraphs.
2

=== Page 3 ===
2.1 Long-ContextLLMEvaluation
TheNeedle-in-a-Haystack(NIAH)paradigm[31]placesarand
theoriginalNIAHfundamentallytestsretrieval,notcomprehens
matching,notdocumentunderstanding. Ironically,theneedle‚Äôs
outpreciselybecauseitdoesn‚Äôtbelong.
Subsequentbenchmarksaddressvariouslimitationswithm
modelscanextractfactsincorrectchronologicalorderacross8K
remainschallengingevenforfrontiermodels.RULER[28]exp
andquestionansweringtasks;despitemodelsclaiming32K+c
atthatlength. NeedleChain[46]takesastricterapproach: ever
even one element results in failure - revealing that models ach
conditions.
For more realistic evaluation, LongBench v2 [5] provide
document QA, code understanding, dialogue history, structure
that long-context comprehension remains challenging even for
100K+tokensusingrealcontentfromnovels,coderepositories
Additional benchmarks serve specialized purposes: U-NIA
approachesonNIAH-styletasks,whileMMNeedle[63]extend
The‚ÄúLostintheMiddle‚Äùphenomenon[39]demonstratesth
contexts,withsomemitigationapproachesproposed[75].
2.2 RetrievalandEmbeddingBenchmarks
BEIR[60]andMTEB[47]arethestandardbenchmarksforre
FreshStack[13]fortechnicaldocumentsandMIRAGE[66]fo
BEIRisnolongeratruezero-shotbenchmark, asresearchersn
[30]. MTEB‚Äôsleaderboardnowhas400+modelswithmargina
fittingtothebenchmarkdistribution.
FreshStackislesspronetocontamination,butreliesonLLM
2.3 Multi-HopQuestionAnswering
Multi-hop QA benchmarks like HotpotQA [69] and 2WikiMu
pieces. MorerecenteffortsincludeMoreHopQA[56]fordeepe
ever,shortcutexploitationunderminesvalidity: nearly61%ofH
single-hopreasoning,withasimpleBERT-basedsingle-hopmo
hopsystems[44].
MuSiQue[61]wasdesignedspecificallytopreventshortcuts
vulnerabletocontamination.
2.4 RAGEvaluation
RAG evaluation has received significant survey attention [71,
relyonLLM-as-judgeapproaches,withdomain-specificvalidat
validationrevealssignificantlimitations: correlationbetweenR
ofonly0.55,farbelowwhatwouldberequiredforreliableauto
TheCALMframework[70]documents12distinctbiasesin
enhancement bias, and authority bias. Additional critiques app
judges [10]. RAGChecker [52] advances the field with fine-gr
checking.
For agentic RAG, evaluation remains nascent. Recent surv
[38], andTelAgentBench[33]addressthisgap, butRAGCap-B
multi-hopquestionsandtheirintermediatereasoningcapabiliti
developments[14].
3

domfactinalongcontextandtestsretrieval. Whileinfluential,
sion-findinganarbitraryfactburiedinunrelatedtextispattern-
sincongruencewithitscontextmakesiteasiertofind;itstands
meaningfulimprovements. Sequential-NIAH[72]testswhether
K-128Kcontexts,demonstratingthatsequentialunderstanding
pandsbeyondretrievaltoincludemulti-hoptracing,aggregation,
contextsupport,manyfailtomaintainsatisfactoryperformance
rypieceofcontextisessentialforansweringqueries,somissing
hieving near-perfect NIAH scores struggle under these stricter
es human-annotated tasks across six categories (single/multi-
ed data) with contexts from 8K to 2M words, demonstrating
r frontier models. InfiniteBench [74] pushes context length to
s,andmathematicalproblems.
AH [23] provides unified comparison between RAG and LLM
dstheparadigmtomultimodal(image)contexts.
hatLLMsstrugglewithinformationplacedinthemiddleoflong
etrievalevaluation. Newerdomain-specificbenchmarksinclude
ormedicalretrieval. However,allfacecontaminationconcerns.
nowroutinelyincludeBEIRdatasetsintheirtrainingpipelines
alperformancedifferences,suggestingeithersaturationorover-
M-as-a-judgeforevaluation.
ultiHopQA [26] aim to test reasoning across multiple evidence
erreasoningchainsandmulti-hopRAGapproaches[59]. How-
HotpotQA‚Äôsmulti-hopquestionscanactuallybeansweredusing
odelachievingperformancecomparabletostate-of-the-artmulti-
sthrough‚Äúunanswerable‚Äùquestions,butremainsstaticandthus
, 21]. Popular frameworks like RAGAS [20] and ARES [53]
tioninareasliketelecommunications[51]. However,empirical
RAGASmetricsandhumanevaluationyieldsaharmonicmean
omatedevaluation[8].
nLLMjudges,includingpositionbias[57],verbositybias,self-
pear in [22], [12], and comparative studies of human vs. LLM
rained diagnostic metrics but still uses LLM-based entailment
veys [36] and benchmarks like RAGCap-Bench [37], HopRAG
Benchfindsthatcurrentsystemsstillstrugglewithchallenging
iesremainunderexplored. Communityresourcestrackongoing
3

=== Page 4 ===
2.5 GraphRAGandKnowledgeGraphEvaluation
GraphRAGpromisesimprovedretrievalthroughknowledgegra
markslikeGraphRAG-Bench[24]andevaluationframeworks[7
whengraphshelpRAG[65]. Whenevaluatedwithgroundtruth
ticularly with global search, generally underperforms compare
original GraphRAG evaluation [18] used LLM-as-judge witho
results.
Knowledgegraphconstructionfacesfundamentalevaluation
lishing reliable benchmarks for KG quality assessment, particu
TherelationshipbetweenKGsandhallucinationhasbeenstudi
KGextractionbenchmarks,demonstratingthatevenfrontiermo
flectKG[4]combinesrule-basedchecks,statisticalvalidation,
amulti-layeredapproachreflectingthedifficultyofestablishing
2.6 HallucinationandFactualityBenchmarks
Hallucinationdetectionandfactualityevaluationhavereceivede
atefactualaccuracy,butfacedistinctchallenges. TruthfulQAsh
onshort-formresponseswhereevenfrontiermodelsstruggleto
provideadditionalhallucinationbenchmarks,withFastFact[62
Forlong-formfactuality,FActScore[43]decomposesrespon
vs. memorization [17] inform understanding of when models
knowledgeextractionfromdocumentcorpora.
2.7 BenchmarkContamination
Datacontaminationunderminesbenchmarkvalidityacrossdoma
problem[67,41]. Meta-analysesquestionbenchmarktrustwort
translationeasilybypassstandarddecontaminationmeasures,a
GPT-4-levelperformance[68].
MMLU shows 52 - 57% exact-match guessing rates on co
(temporalfreshness),MMLU-CF[76](counterfactualrephrasin
thanpreventitstructurally.
2.8 SyntheticDataGenerationandSimulationValidity
RIKERgeneratessyntheticdocumentsfromstructuredgroundt
onLLM-drivensyntheticdatageneration[40]andusersimulati
2.8.1 TheBiasFactorProblem
WhenLLMsgeneratebenchmarkdataandperformthetask,sy
theirowngenerateddata,whereaslargermodelsdonot-apheno
forLLM-generatedbenchmarks. RIKERavoidsthisthroughte
creation.
2.8.2 TheRealityGap
The ‚Äúreality gap‚Äù - the gap between synthetic training data a
epistemological: synthetic data requires real-world data abou
dispense with [58]. A validity-centered framework for AI eval
benchmarkscanlegitimatelysupport. RIKERmakesbounded
groundtruth.ItdoesnotclaimthatsuccessonRIKERguarantee
(notsufficient)condition-ifasystemcannotextractknownfac
4

aphaugmentation[15],butevaluationchallengespersist.Bench-
73,55]attempttostandardizeassessment,whilestudiesexamine
hratherthanLLM-as-judge,community-basedGraphRAG,par-
ed to standard RAG [25]. This discrepancy arises because the
out ground truth - precisely the methodology shown to inflate
nchallenges: recentsurveysidentifyunresolvedissuesinestab-
ularly regarding intrinsic and extrinsic evaluation metrics [11].
iedextensively[32]. GOSyBench[9]providesdomain-specific
odelsstrugglewithaccurateknowledgegraphrecovery. FinRe-
andLLM-as-judgeassessmentstomeasureextractionquality-
gabsolutequalitymetricsforKGconstruction.
extensiveattention[29]. TruthfulQAandSimpleQA[64]evalu-
howsevidenceofcontamination[16],whileSimpleQAfocuses
oachievemajorityaccuracy. HaluEval[34]andHalluLens[7]
2]offeringefficientfactverification.
nsesintoatomicfactsforverification. Studiesongeneralization
s hallucinate. No benchmark adequately addresses long-form
ains,withcomprehensivesurveysdocumentingtheextentofthe
thinessbroadly[19]. Simplevariationssuchasparaphrasingor
allowinga13Bmodeltooverfitleakedbenchmarksandachieve
ontaminated subsets [16]. Current solutions - LatestEval [35]
ng)-arereactive: theydetectandmitigatecontaminationrather
truth,situatingitwithinthesyntheticdataliterature[1].Surveys
ion[6]providetheoreticalgrounding.
ystematicbiasesemerge. SmallerLLMsexhibitbiasestowards
omenontermedthe‚Äúbiasfactor‚Äù[42]. Thisunderminesvalidity
emplate-basedgeneration-noLLMisinvolvedindocument
and real-world deployment data - is not merely technical but
ut a domain in order to model it, the very data it purports to
luation [54] provides psychometric grounding for what claims
validityclaims: itmeasuresextractionaccuracyagainstknown
esreal-worldperformance.Rather,RIKERprovidesanecessary
ctsfromsyntheticdocuments,itwillfailonrealdocuments.
4

=== Page 5 ===
2.8.3 ParameterizedEvaluation
Twobenchmarksvalidatetemplate-basedevaluation:
GSM-Symbolic[45]usessymbolictemplatesformathpro
performancedrops. Theauthorsconclude: ‚ÄúCurrentLLMscann
stepsfromtheirtrainingdata.‚Äù
RV-Bench[27]generatesRandomVariableQuestions(RVQ
andfinding‚Äúproficiencyimbalance‚Äùbetweenfamiliarandnove
BothGSM-SymbolicandRV-Benchdemonstratethatparam
marksmiss. RIKERextendsthisparadigmfrommathematicalr
2.9 ThePICARDFramework
ThePICARDframework[49]addressesevaluationgapsforage
‚Ä¢ Ground-truth-firstgeneration: ‚ÄúWhentheevaluationfr
pletegroundtruth‚Äù
‚Ä¢ Deterministicscoring: Answerkeysgeneratedsimultane
‚Ä¢ Anti-memorizationbydesign: Combinatorialexplosion
‚Ä¢ Multi-layeredrandomization: Entitysubstitution,datag
PICARD demonstrates these principles for file manipulatio
PICARDdoesnotaddressknowledgeextractionfromdocumen
2.10 Summary: ResearchGaps
Table1summarizesthegapsaddressedbyRIKER.
Table1: ResearchGaps
Gap CurrentLimit
LLM-as-Judgeunreliability 12documented
humancorrelat
Benchmarkcontamination Staticbenchma
tomemorizatio
Long-contextretrievalÃ∏= Needleretrieva
comprehension comprehension
Staticmulti-hopQA Single-hopsho
RAGgroundtruth Humanannotat
approximated
NoKGextractionground ReliableKGbe
truth establishmentr
unresolved
KGcompleteness Grapheditdista
unmeasurable insufficient
Retrievalbenchmark BEIRnolonge
contamination
KGevaluation Tiedtospecific
schema-dependent
5

oblems,findingthataddingirrelevantclausescausesupto65%
notperformgenuinelogicalreasoning;theyreplicatereasoning
Qs)withrandomizedvariablecombinations,testing30+LLMs
elcombinations.
meterizedgenerationexposescapabilitygapsthatstaticbench-
reasoningtoknowledgeextraction.
enticAIthrough:
rameworkcontrolsdatageneration,itinherentlypossessescom-
eouslywithtestdata
nmakesmemorizationimpossible
generation,andenvironmentalvariation
on, database operations, and multi-step workflows. However,
ntcorpora-thedomainRIKERtargets.
sandRIKER‚ÄôsPosition
tation RIKER‚ÄôsApproach
dbiases;0.55 Deterministicscoring
tion
arksvulnerable Regenerablecorpora+
on combinatorial
anti-memorization
alÃ∏=document Realisticdocument
n understanding
ortcuts Aggregationrequires
cross-documentextraction
tionbottleneck Ground-truth-firstgeneration
enchmark GenerateFROMstructured
remains groundtruthwithknown
entityrelationships
ance SQLitemanifestdefines
expectedoutput
erzero-shot Regenerablecorpus
contology Queryanswers,notstructure
5

=== Page 6 ===
Gap CurrentLimit
Databottleneck Annotationexp
doesn‚Äôtscale
GraphRAGevaluation LLM-judgewit
inflated truth
Thepatternrevealsthreefundamentalproblems: (1)static
(3)groundtruthisapproximatedratherthanknown. Industryre
[3], while best practices for ground truth generation remain l
documentsFROMgroundtruthratherthanextractinggroundtr
3 TheRIKERApproach
UnlikebenchmarksthatrelyonstaticdatasetsorLLM-as-judg
ded ground truth, enabling deterministic scoring at scale. The
stuffing,retrieval-augmentedgeneration,orknowledgegraphsy
generatedquestions.
3.1 SyntheticCorpusGeneration
RIKERemploysaground-truth-firstarchitecture: thecomplete
ulatedinarelationaldatabasebeforeanydocumentisgenerated
underlyinggroundtruth. Thisinversionofthetypical‚Äúextractf
(1)everyquestionhasaverifiableanswerbyconstruction,enab
differentrandomseedswhilemaintainingstructuralequivalence
scalabletopractically-unlimitedscale,requiringnohuman-inte
3.1.1 SyntheticDataGeneration
Built upon the PICARD framework [49], RIKER inherits and
amounts,dates,andotherentities-whichareusedtogenerate
willlaterbecreated.
3.1.2 GroundTruthDatabase
AllgeneratedgroundtruthisrecordedinaSQLitedatabasewi
corpus,thisincludes:
‚Ä¢ Documentmetadata(parties,dates,amounts,clausespres
‚Ä¢ Entities(lessors,lessees,agents,addresses,etc.)
‚Ä¢ Entityrelationships(whichlessorshavewhichlessees)
Thisdatabaseservesastheauthoritativeanswerkeyforallg
tions (counts, sums, temporal relationships) can be derived fro
questiongeneration(e.g. ‚ÄùWhatisthetotalmonthlyrentofalll
3.1.3 Template-BasedDocumentGeneration
Afterthegroundtruthdatabaseiscreated, documentsaregene
templatedefinesthedocumentstructurewhileallowingrandom
‚Ä¢ Languagestyle(formal,semi-formal,casual)
‚Ä¢ Structuralorganization(sectionordering,optionalclauses
‚Ä¢ Boilerplatetextvariations
This produces documents that are structurally consistent
surface-levelpatternswhilemaintaininggroundtruthintegrity.
6

tation RIKER‚ÄôsApproach
pensive, Arbitrary-scalegeneration
thoutground Groundtruthscoring
datasetsgetcontaminated, (2)LLMjudgesareunreliable, and
eportsconfirmthatdatabottleneckshaveincreasedsignificantly
labor-intensive [2]. RIKER‚Äôs paradigm inversion - generating
ruthFROMdocuments-addressesallthree.
geevaluation,RIKERgeneratessyntheticcorporafromembed-
methodology is application-agnostic - it can evaluate context-
ystemsbyinstrumentingthesystemundertesttoanswerRIKER-
knowledgebase‚Äîallentities,relationships,andfacts‚Äîispop-
d. Documentsarethenrenderedashuman-readableviewsofthis
factsfromdocuments‚Äùapproachprovidesthreekeyadvantages:
blingdeterministicscoring,(2)corporacanberegeneratedwith
e,enablingrobustnessvalidation,and(3)theapproachiseasily
ensivedocumentannotation.
d expands various synthetic data generation functions - names,
adiversesetofgroundtruthelements,fromwhichdocuments
ithfullrelationalstructure. Forexample, foraleasedocument
sent,clausesabsent)
generatedquestions. Questionsthatrequirecomputedaggrega-
om this ground truth through SQL, which enables complex test
leases$LESSORhasin$YEARand$MONTH‚Äù)
eratedusingmodulartemplateswithcontrolledvariation. Each
mizedselectionof:
s)
yet superficially diverse, preventing models from exploiting
6

=== Page 7 ===
3.2 CoherentSimulatedUniverse
One of the significant shortcomings of synthetic generated da
documentsaregenerated,buttheyareallindependentgeneratio
un-enterprise-like-thedocumentsarenotrelatedtoeachother,
Forexample,inanaivegeneration,wecouldgenerate100
dentlyrandomlygenerated,ourdocuments(forexample,HRe
facts like employee name, manager, department, etc. The res
example:
‚Ä¢ Employeesinsimilarrandomizeddepartment(bychance
‚Ä¢ Employeesindifferentdepartmentsmayaccidentallyhav
‚Ä¢ An employee, named as a manager or supervisor in a
differentdepartmentorposition
Theaboveisnotanexhaustivelist.Thisincoherenceresultin
arealisticenterprisescenario(thereforemetricsagainstthatdata
andwillpreventthecreationofchallengingcomprehensionand
evaluationsthisquarter?‚Äô,becausenecessaryrelationshipswill
atall.
RIKER solves this problem through its Coherent Simulate
ationisgroundtruthcreation(see3.1.2), andthisincludesnec
coherencespreadacrossdifferentdocumenttypesandacrossth
ceptofGlobalEntityPools,whicharepre-generatedandfilled
thencreatedbydrawingfromtheglobalentitypools,whichare
areusedacrossalldocumenttypestocreateacoherentdataset.
humannamesthatareusedforthreetypesofdocumentsinthec
‚Ä¢ Anoptionalsalesagentthatisnamedandcreditedforclo
‚Ä¢ A sales agent that is named in a Sales Agent Field Re
detailingsalesactivitiesandpotentialcontractstatus
‚Ä¢ AnemployeethatisnamedinanHREmployeeEvaluat
InRIKER‚ÄôsCoherentSimulatedUniverse,allthethreedoc
Pool-meaningtheagentsyouwillseewhoclosedleasecontra
namedinrelevantHRevaluationdocuments.
It will also never happen that a Field Report talking about
randomizedforitasthesalesagent-thisincoherenceisavoided
feature,aspartoftheCoherentSimulatedUniversestrategy.
This results in having arbitrary scale document generation
annotationeffort(becausegroundtruthiswheretheprocessac
factsanddocuments-arecoherentaccordingtothegeneration
3.3 Multi-LevelQuestionTaxonomy
RIKERgeneratesquestionsacrosstwelvedifficultylevelsorgan
3.3.1 Single-DocumentQuestions(L01-L04)
Thesequestionsrequirelocatingandextractinginformationfrom
‚Ä¢ L01-DirectExtraction: Surface-levelfactsstatedexpli
‚Ä¢ L02-IndirectExtraction: Factsrequiringminimalinfe
given)
‚Ä¢ L03-ConditionalExtraction: Factsfromoptionaldocu
‚Ä¢ L04-ComplexExtraction: Factsrequiringmultiplecon
7

ata comes from naive generation - that is, when 100 synthetic
ons. Thisgivesthesyntheticdatasetacharacteristicthatisvery
,orworse,theyhavechaoticandunrealisticconnections.
HRdocumentsusingapoolofhumannames. Beingindepen-
evaluationsoremployeeinformation)wouldnaivelyrandomize
sult is a dataset that models no realistic enterprise corpus, for
e)willhaveadifferentdeptmanagerorsupervisornamed
veasimilarrandomizedpersonname
particular department from a previous document, can have a
ngfromnaiverandomgenerationisaproblem.Itdoesnotmodel
asetmayhaveveryweakcorrelationtoreal-worldperformance),
daggregationquestions,suchas‚Äòwhichmanagergavethemost
leithernotbediverseorcoherentenough,ormaynotevenexist
ed Universe approach. The very first step in document gener-
cessaryrelationshipsamongthedifferententities. Tomakethis
heentireuniverseofgenerateddocuments,RIKERhasthecon-
dasthefirststepofsyntheticdatageneration. Relationshipsare
esavedinthegroundtruthdatabase. Theseglobalentitypools
. Forexample,aglobalentitypoolcalled‚Äòsales agent‚Äôcontains
currentRIKERimplementation:
osingaLeaseContract
eport, as the agent who created and submitted the field report
tiondocument,asthesalesemployeebeingevaluated
cumenttypesabove(inbold)drawfromthesameGlobalEntity
actswillbethesameagentsnamedinrelevantfieldreports,and
a particular Lease Contract will have a different human name
dthroughlogicspecificallybaked-intothedocumentgeneration
n where ground truth is immediately available with no human
ctuallybegins), andtheknowledgegenerated-theentiresetof
design.
nizedintothreecategories,eachtestingdistinctcapabilities.
masingledocument:
icitly(‚ÄúWhatisthemonthlyrent?‚Äù)
erence(‚ÄúWhatistheleaseduration?‚Äù whenstart/enddatesare
umentsections(‚ÄúWhatisthepetdeposit?‚Äù ‚ÄîmaybeN/A)
nditionsorcross-referencingwithinadocument
7

=== Page 8 ===
3.3.2 AggregationQuestions(L05-L10)
Thesequestionsrequiresynthesizinginformationacrossmultipledocuments:
‚Ä¢ L05-Counting: ‚ÄúHowmanyleasesdoesLessorXhave?‚Äù
‚Ä¢ L06-Summation/Averaging: ‚ÄúWhatisthetotalmonthlyrentacrossallleases?‚Äù
‚Ä¢ L07-Comparison: ‚ÄúWhichlessorhasmoreleases,XorY?‚Äù
‚Ä¢ L08-Enumeration: ‚ÄúListalllesseesforLessorX‚Äù
‚Ä¢ L09-Multi-hop: ‚ÄúWhatisLessorX‚Äôsmostrecentleaseenddate?‚Äù
‚Ä¢ L10-Temporal: ‚ÄúHowmanyleaseswereactiveinQ32024?‚Äù
Aggregationquestionsareparticularlychallengingbecausetheyrequirethemodelto: (1)identifyallrelevantdocuments,
(2)extracttherelevantfactsfromeach,and(3)performtherequiredcomputationcorrectly.
3.3.3 HallucinationProbeQuestions(L11-L12)
Thesequestionsaredesignedtodetectfabrication:
‚Ä¢ L11 - Non-existent Entities: Questions about entities that do not appear anywhere in the corpus. The entity names
aredrawnfromunusedportionsoftheentitypool,ensuringtheyareplausiblebutdefinitivelyabsent. Theonlycorrect
responseis‚ÄúUnknown‚Äùorequivalent.
‚Ä¢ L12-AbsentInformation:Questionsaboutoptionalfieldsthatareabsentfromspecificdocuments.Forexample,asking
aboutthepetdepositforaleasethathasnopetclause. Theonlycorrectresponseis‚ÄúN/A‚Äùorequivalent.
L11questionsareparticularlyvaluablebecauseanyspecificanswerconstitutesunambiguousfabrication‚Äîthemodelcannot
haveretrievedtheinformationfromthecorpusbecauseitdoesnotexist.
3.4 DeterministicScoring
RIKERemploysanswer-key-basedscoring,eliminatingthevariabilityinherentinLLM-as-judgeapproaches.
3.4.1 ScoringMechanisms
Eachquestionspecifiesitsscoringtype:
‚Ä¢ Exactmatch: Forcategoricalresponses(names,yes/no)
‚Ä¢ Numericextraction: Parsesnumericalanswerswithtoleranceforformattingvariations
‚Ä¢ Setcomparison: Forenumerationquestions,comparesanswersetsregardlessofordering
‚Ä¢ Semanticequivalence: Forstructuredresponseswithknownequivalentforms
Allscoringlogicoperatesagainstthegroundtruthdatabase,ensuringreproducibility. Thesamemodeloutputswillalways
receivethesamescores.
Inthisparticular
3.4.2 ResponseFormatEnforcement
Questionsincludeexplicitformatinstructions(e.g., ‚ÄúReplywithonlythenumber‚Äùor‚ÄúIndicateyourfinalanswerwith: Final
answer: [your answer]‚Äù). This structured output requirement reduces ambiguity in answer extraction and improves scoring
reliability.
3.5 FidelityMetricsTaxonomy
Wedefineathree-leveltaxonomyforhallucination-relatedmetrics
8

=== Page 9 ===
3.5.1 Faithfulness(L01-L04+L11-L12)
The broadest metric, measuring accuracy on all questions wh
Thisencompassesbothgroundingfailures(wronganswersfrom
Faithfulnessalignswiththecolloquialenterprisedefinitionof‚Äú
informationwasavailable.
3.5.2 Grounding(L01-L04)
Accuracyonsingle-documentquestionsonly. Groundingfailure
mationfromdocumentsthatdefinitivelycontaintheanswer. Th
3.5.3 Fabrication(L11-L12)
Error rate on hallucination probe questions. Because L11 que
definitivelyfabricated‚Äîthereisnoambiguityaboutthefailurem
tendencytoinventinformation.
3.5.4 Aggregation(L05-L10)
Reported separately as a capability metric rather than a hallu
modes(incompletedocumentretrieval,computationerrors,wor
thetraditionalsense.
3.6 RobustnessValidation
Amethodologyisonlyusefulifitproducesstable,reproducible
experiments.
3.6.1 Cross-CorpusStability
Four corpora were generated from identical configuration para
different entity names, dates, and surface content while maint
performancetierswereevaluatedonallfourcorpora.
Resultsdemonstratedstrongstability:top-performingmodel
1%),withconsistentrankingpreservation.ThisvalidatesthatRI
artifacts.
3.6.2 ImplicationsforReproducibility
The cross-corpus stability finding has practical implications: r
comparableresultstootherstudiesusingthesameconfiguratio
marks: theirfixednaturemeanstheyinevitablyleakintotrainin
taminatedtestdata.
4 ExperimentalDesign
Table2summarizesthescaleofourevaluationforthisRIKER
Table 3 details the document breakdown across corpora; q
single-documentextraction,40%cross-documentaggregation,a
ineverycorpus,demonstratingtheCoherentSimulatedUnivers
appear across leases, field reports, and HR evaluations. The d
arefewer(onepertenancy),fieldreportsdominate(generatedp
(periodicperemployee).
Modelcoveragedecreaseswithcontextsize(33‚Üí24‚Üí11
itselfafinding. Eightrunspermodelenablestatisticalsignifican
Allexperimentsuseatemperaturesettingof0.4,balancing
exploretheeffectsofLLMtemperatureonperformanceinente
9

here the model had sufficient information to answer correctly.
mdocumentsthatexist)andfabrication(inventedinformation).
‚Äúhallucination‚Äùasanyconfidentlywronganswerwhencorrect
esindicatethemodelcouldnotlocateorcorrectlyextractinfor-
hisisolatesretrievalandcomprehensionerrorsfromfabrication.
estions ask about non-existent entities, any specific answer is
mode.Thisprovidesthecleanestsignalformeasuringamodel‚Äôs
ucination metric. Aggregation errors conflate multiple failure
rkingmemorylimitations)thataredistinctfromhallucinationin
results.WevalidatedRIKER‚Äôsrobustnessthroughcross-corpus
ameters but different random seeds, producing documents with
taining structural equivalence. Four models spanning different
lsshowedlessthan2%accuracyvarianceacrosscorpora(CV<
IKERresultsreflectmodelcapabilityratherthancorpus-specific
researchers can generate their own RIKER corpora and expect
onparameters. Thisaddressesakeylimitationofstaticbench-
ngcorpora,whileRIKER‚Äôsregenerabilityensuresfresh,uncon-
study. 33modelsandover21Btokenswereprocessed.
question counts scale proportionally, with approximately 50%
and10%hallucinationprobes. Allthreedocumenttypesappear
seinpractice: thesameentities(people,properties,companies)
document distribution reflects realistic business ratios - leases
peragent-prospectinteraction),andHRreportsfallinbetween
1)asfewermodelssupportlongercontexts-thisstratificationis
ncetestingwithvarianceandconfidenceintervalreporting.
determinismwithnaturalresponsevariation. Futureworkwill
erpriseknowledgeextractionsettings.
9

=== Page 10 ===
Table2: RIKERE
Corpus Questions Runs Model
MainExperiment:
32K 110 8 3
128K 301 8 2
200K 525 8 1
MainSubtotal
Cross-CorpusValidation(Section5.9):
128K(B,C,D) 301 8
ExpandedHallucinationAnalysis(Section5.10):
32K(HA,HB,HC,HD) 300 8 1
GrandTotal
Tokencountsreflecttotalcomputeconsumedacrossallexperimentalattempts,includinga
respo
Table3: CorpusDoc
Corpus Leases FieldRep
MainExperiment:
32K 10
128K(SetA) 37
200K 60
Cross-CorpusValidation:
128K(SetB) 37
128K(SetC) 37
128K(SetD) 37
5 Results
We evaluate 33 models at 32K context, 24 models at 128K co
across8runspermodeltoenablestatisticalsignificancetesting
5.1 OverallModelPerformance
Tables4,5,and6presentmodelperformanceacrossallthreeco
‚Ä¢ Overall: Weightedaccuracyacrossallquestiontypes.
‚Ä¢ Single-Doc: Accuracyonquestionsanswerablefromas
Testsbasicretrievalandextraction.
‚Ä¢ Aggregation:Accuracyonquestionsrequiringsynthesis
Yhave?‚Äù). Testscross-documentreasoning.
‚Ä¢ HallDetect:Accuracyonhallucinationprobes-correctly
Higherisbetter.
‚Ä¢ HallRate: Hallucinationrate-percentageofprobeswh
tainty. Lowerisbetter.
1

ExperimentalScale
ls InputTokens OutputTokens TotalTokens
33 0.79B 7M 0.80B
24 5.67B 47M 5.72B
11 9.26B 85M 9.34B
15.72B 139M 15.86B
4 3.02B 7M 3.03B
10 2.64B 9M 2.65B
21.38B 155M 21.54B
afewrunsthatfailedtoproducescorableoutputduetoAPIerrors,timeouts,ormalformed
onses.
cumentComposition
ports HRReports TotalDocs
44 56 110
216 116 369
381 196 637
255 128 420
228 120 385
211 120 368
ontext, and 11 models at 200K context. Results are aggregated
g.
ontextsizes. Eachtablereportsfivemetrics:
singledocument(e.g.,‚ÄúWhatisthemonthlyrentinleaseX?‚Äù).
acrossmultipledocuments(e.g.,‚ÄúHowmanyleasesdoeslessor
yanswering‚ÄúUnknown‚Äùwhenaskedaboutnon-existententities.
herethemodelfabricatedananswerinsteadofadmittinguncer-
10

=== Page 11 ===
Table4: ModelPerformanceat32K
Model Overall Single-Doc
(%) (%
GLM-4.5 94.7 93.6
GLM-4.6 90.5 93.6
Qwen3-Next-80B 88.6 91.4
Qwen3-235B-FP8 87.8 86.1
Qwen3-235B 87.4 85.2
Mistral-Large-3 86.5 90.2
Qwen3-Coder-480B 86.3 84.5
Llama-4-Maverick 85.5 86.6
DeepSeek-V3.1 84.9 89.5
Qwen2.5-72B 82.4 90.0
Qwen3-30B 81.1 82.3
GLM-4.5-Air 78.0 83.9
Qwen3-Coder-30B 77.5 79.3
Qwen3-4B-Instruct 77.5 86.6
DeepSeek-V3 76.1 79.5
Llama-3.1-405B 75.6 79.1
Llama-4-Scout 71.7 80.9
Qwen2.5-32B 71.4 83.9
Qwen3-32B 69.8 75.2
Qwen2.5-14B 67.5 75.2
Llama-3.1-70B 67.4 78.0
Qwen2.5-Coder-14B 63.5 77.7
Llama-3.3-70B 60.9 73.4
Qwen3-8B 60.2 75.5
Qwen3-14B 60.1 57.7
Qwen3-4B 57.7 76.1
Qwen2.5-Coder-7B 53.4 62.0
Granite-4-Small 49.7 63.2
Llama-3.1-8B 48.5 64.5
Granite-4-Tiny 38.2 49.5
Llama-3.2-3B 30.3 36.8
Granite-4-Micro 26.9 38.9
Llama-3.2-1B 9.1 4.5
1

Context(sortedbyoverallaccuracy)
c Aggregation HallDetect HallRate
%) (%) (%) (%)
6 94.6 100.0 0.0
6 86.6 89.8 10.2
4 92.6 59.1 40.9
1 95.2 67.0 33.0
2 95.5 65.9 34.1
2 85.5 71.6 28.4
5 86.6 93.2 6.8
6 87.5 71.6 28.4
5 80.4 79.5 20.5
0 75.9 70.5 29.5
3 86.4 54.5 45.5
9 66.8 93.2 6.8
3 86.6 31.8 68.2
6 69.3 64.8 35.2
5 72.4 73.9 26.1
1 70.5 78.4 21.6
9 66.5 46.6 53.4
9 53.1 81.8 18.2
2 63.1 69.3 30.7
2 61.4 53.4 46.6
0 56.0 60.2 39.8
7 51.4 40.9 59.1
4 48.9 46.6 53.4
5 38.9 69.3 30.7
7 60.2 71.6 28.4
1 37.8 45.5 54.5
0 40.3 62.5 37.5
2 34.9 40.9 59.1
5 31.3 37.5 62.5
5 31.8 6.8 93.2
8 14.2 62.5 37.5
9 14.2 18.2 81.8
5 3.4 54.5 45.5
11

=== Page 12 ===
Table5: ModelPerformanceat128K
Model Overall Single-Doc
(%) (%)
Qwen3-Next-80B 88.5 94.0
Qwen3-235B 84.6 92.3
Qwen3-235B-FP8 84.6 92.1
GLM-4.5 84.4 94.8
DeepSeek-V3.1 84.1 94.0
GLM-4.6 81.4 91.4
Qwen3-Coder-480B 80.7 87.6
Mistral-Large-3 75.6 89.4
Qwen3-30B 71.7 82.7
DeepSeek-V3 70.2 88.3
Llama-4-Maverick 68.2 83.4
GLM-4.5-Air 67.3 84.4
Qwen3-Coder-30B 59.9 71.7
Qwen3-4B 57.7 75.0
Llama-3.1-405B 52.7 69.9
Llama-4-Scout 51.5 66.5
Llama-3.1-70B 45.1 65.2
Llama-3.3-70B 40.8 55.2
Granite-4-Small 39.9 54.0
Llama-3.1-8B 35.3 51.1
Llama-3.2-3B 25.9 33.1
Granite-4-Tiny 22.6 32.1
Granite-4-Micro 22.5 25.7
Llama-3.2-1B 11.3 7.8
Table6: ModelPerformanceat200K
Model Overall Single-Doc
(%) (%)
Qwen3-Next-80B 77.6 88.8
Qwen3-235B 72.3 83.5
Qwen3-235B-FP8 71.6 82.1
Qwen3-Coder-480B 71.1 81.4
Mistral-Large-3 68.3 80.5
Qwen3-30B 65.3 75.1
Llama-4-Maverick 62.3 75.5
Qwen3-Coder-30B 52.8 67.9
Llama-4-Scout 47.6 62.5
Qwen3-4B 42.1 54.3
GLM-4.6 34.3 47.3
5.2 PerformancebyQuestionCategory
Performance varies dramatically by question type. Single-doc
consistently the easiest task, with top models exceeding 90%.
provesignificantlyharder,witheventhebestmodelachievingo
aboutnon-existententities)revealsthestarkestdifferencesbetw
Keyobservations:
‚Ä¢ Single-documentextractionisconsistentlyhighest-patte
‚Ä¢ Aggregationqueriesaresignificantlyharder-requiresrea
‚Ä¢ Hallucinationratesvaryfrom7.9%(GLM-4.5)to96.3%
‚Ä¢ Somemodels(Granite-4-Tiny)hallucinateonnearlyever
1

KContext(sortedbyoverallaccuracy)
c Aggregation HallDetect HallRate
) (%) (%) (%)
0 80.8 90.4 9.6
3 75.5 80.0 20.0
1 76.1 78.3 21.7
8 68.5 92.1 7.9
0 72.0 79.6 20.4
4 69.1 77.1 22.9
6 70.6 84.2 15.8
4 61.3 60.0 40.0
7 61.9 52.9 47.1
3 46.2 69.6 30.4
4 51.9 52.9 47.1
4 40.5 82.1 17.9
7 49.8 38.3 61.7
0 32.0 67.5 32.5
9 26.6 64.6 35.4
5 30.6 54.6 45.4
2 17.9 46.7 53.3
2 18.6 52.1 47.9
0 15.7 60.4 39.6
1 16.0 28.8 71.3
1 7.1 61.3 38.8
1 14.9 3.8 96.3
7 9.8 55.0 45.0
8 1.6 67.1 32.9
KContext(sortedbyoverallaccuracy)
c Aggregation HallDetect HallRate
) (%) (%) (%)
8 59.6 84.8 15.2
5 55.5 75.5 24.5
1 56.0 74.5 25.5
4 55.2 75.9 24.1
5 52.4 63.6 36.4
1 54.1 56.4 43.6
5 46.9 51.1 48.9
9 35.6 38.2 61.8
5 26.5 47.7 52.3
3 17.8 67.0 33.0
3 19.6 21.4 78.6
cument extraction (finding a specific fact in one document) is
Aggregation queries (counting, comparing across documents)
only80.8%. Hallucinationdetection(correctlyrejectingqueries
weenmodels.
ern-matchingoftensuffices
asoningacrossmultipledocuments
(Granite-4-Tiny)
ryprobequestion
12

=== Page 13 ===
5.3 ScalingEffects
Table7showsperformancedegradationascontextlengthincrea
Table7: PerformanceDegradationAcros
Model 32K(%) 12
Qwen3-Next-80B 88.6
Qwen3-235B 87.4
Qwen3-Coder-480B 86.3
Qwen3-30B-A3B-Instruct 81.1
Qwen3-235B-FP8 87.8
Mistral-Large-3 86.5
Llama-4-Maverick 85.5
Qwen3-Coder-30B 77.5
Llama-4-Scout 71.7
Qwen3-4B 77.5
GLM-4.6 90.5
Table8showsmodelstestedat32Kand128Konly.
Table8: PerformanceDegradation: 32K
Model 32K(%
GLM-4.5 94
DeepSeek-V3.1 84
GLM-4.5-Air 78
DeepSeek-V3 76
Llama-3.1-405B 75
Llama-3.1-70B 67
Llama-3.3-70B 60
Granite-4-Small 49
Llama-3.1-8B 48
Granite-4-Tiny 38
Llama-3.2-3B 30
Granite-4-Micro 26
Llama-3.2-1B 9
Notablefindings:
‚Ä¢ GLM-4.6showsdramaticcollapseat200K(‚àí56.2%from
‚Ä¢ Llamamodelsdegrade13-23%from32Kto128K-con
‚Ä¢ Top-tier models (Qwen3, GLM-4.5 family) lose 10-16%
stantially
‚Ä¢ Llama-3.2-1Banomalouslyimprovesat128K(+2.2%),t
performanceisnodifferentthanrandomguessingevenfr
5.4 ModelFamilyPatterns
Qwen3 Family: Dominates the leaderboard. Qwen3-Next-80B
235Bvariantsshowstrongperformancebutthesmaller30Band
DeepSeek: V3.1significantlyoutperformsV3(84.1%vs70
mentsbetweenversions.
GLM:GLM-4.5excelsathallucinationdetection(92.1%de
catastrophicfailureat200Kcontext.
Llama: The Llama 3.x family underperforms relative to m
smallerQwenmodels. Llama4marksasignificantimproveme
10points,butstillgetsoutperformedbymuchsmallerQwenm
1

asesformodelstestedatmultiplecontextsizes.
ssContextSizes(modelstestedat3sizes)
28K(%) 200K(%) ‚àÜ(32K‚Üí200K)
88.5 77.6 ‚àí11.0
84.6 72.3 ‚àí15.1
80.7 71.1 ‚àí15.2
71.7 65.3 ‚àí15.8
84.6 71.6 ‚àí16.2
75.6 68.3 ‚àí18.2
68.2 62.3 ‚àí23.2
59.9 52.8 ‚àí24.7
51.5 47.6 ‚àí24.1
57.7 42.1 ‚àí35.4
81.4 34.3 ‚àí56.2
Kto128K(modelsnottestedat200K)
%) 128K(%) ‚àÜ
4.7 84.4 ‚àí10.3
4.9 84.1 ‚àí0.8
8.0 67.3 ‚àí10.7
6.1 70.2 ‚àí5.9
5.6 52.7 ‚àí22.9
7.4 45.1 ‚àí22.3
0.9 40.8 ‚àí20.1
9.7 39.9 ‚àí9.8
8.5 35.3 ‚àí13.2
8.2 22.6 ‚àí15.6
0.3 25.9 ‚àí4.4
6.9 22.5 ‚àí4.4
9.1 11.3 +2.2
m32K)
nsistentlyworsethanotherfamilies
% accuracy from 32K to 200K, while others degrade more sub-
thoughfromaverylowbaseline(9.1%),mostlikelybecauseits
romthelow-context32Kscenario.
B achieves the best overall performance (88.5% at 128K). The
d4Bmodelsremaincompetitivefortheirsize.
0.2%),suggestingmeaningfularchitecturalortrainingimprove-
etectionrate,only7.9%hallucinationrate)butGLM-4.6shows
model size. Llama-3.1-405B (52.7%) is outperformed by much
ent-Maverick(85.5%at32K)outperformsLlama-3.1-405Bby
modelsat128Kandhigher.
13

=== Page 14 ===
Granite: IBM‚ÄôsGranitemodelsstrugglesignificantly,with
answerstonearlyeveryhallucinationprobe.
Mistral: Mistral-Large-3performscompetitively(86.5%at
contextsthanQwenmodels.
5.5 HallucinationAnalysis
Wedecomposemodelhallucinationmeasuresintothreenested
‚Ä¢ Faithfulness (L01 - L04 + L11 - L12): Any error wh
Combinessingle-documentextractionandhallucinationp
‚Ä¢ Grounding(L01-L04): Single-documentquestionsonly
relevantdocument. Higherisbetter.
‚Ä¢ Fabrication(L11-L12): Hallucinationprobequestions
fabricatedsincetheentitiesexistnowhereinthecorpus. L
Aggregationquestions(L05-L10)areexcludedfromhalluc
computation errors and incoherence loss - these are synthesis
performanceisanalyzedseparatelyinSection5.6.
Tables9,10,and11presentthehallucinationmetricsacross
Table9: HallucinationMetricsat32KContext
Model Faith. Ground. Fab.
(%) (%) (%)
GLM-4.5 94.7 93.6 0.0
GLM-4.6 93.0 93.6 10.2
DeepSeek-V3.1 87.9 89.5 20.5
Mistral-Large-3 87.1 90.2 28.4
Qwen2.5-72B 86.7 90.0 29.5
Qwen3-Next-80B 86.0 91.4 40.9
Qwen3-Coder-480B 86.0 84.5 6.8
GLM-4.5-Air 85.4 83.9 6.8
Llama-4-Maverick 84.1 86.6 28.4
Qwen2.5-32B 83.5 83.9 18.2
Qwen3-4B-Instruct 83.0 86.6 35.2
Qwen3-235B-FP8 83.0 86.1 33.0
Qwen3-235B 82.0 85.2 34.1
Llama-3.1-405B 79.0 79.1 21.6
DeepSeek-V3 78.6 79.5 26.1
Qwen3-30B-A3B-Instruct 77.7 82.3 45.5
Llama-4-Scout 75.2 80.9 53.4
Llama-3.1-70B 75.0 78.0 39.8
Qwen3-8B 74.4 75.5 30.7
Qwen3-32B 74.2 75.2 30.7
Qwen2.5-Coder-14B 71.6 77.7 59.1
Qwen2.5-14B 71.6 75.2 46.6
Qwen3-Coder-30B 71.4 79.3 68.2
Qwen3-4B 71.0 76.1 54.5
Llama-3.3-70B 68.9 73.4 53.4
Qwen2.5-Coder-7B 62.1 62.0 37.5
Llama-3.1-8B 60.0 64.5 62.5
Qwen3-14B 60.0 57.7 28.4
Granite-4-Small 59.5 63.2 59.1
Granite-4-Tiny 42.4 49.5 93.2
Llama-3.2-3B 41.1 36.8 37.5
Granite-4-Micro 35.4 38.9 81.8
Llama-3.2-1B 12.9 4.5 45.5
Keyfindings:
1

hthetinyvariantshowing96.3%hallucinationrate-fabricating
t32K,68.3%at200K)butshowssteeperdegradationatlonger
metrics:
here the model had sufficient information to answer correctly.
probequestions. Higherisbetter.
y-measureswhetherthemodelcanfindandcorrectlyreadthe
usingnon-existententities. Anyspecificanswerisdefinitively
Lowerisbetter.
cinationmetricsbecauseerrorsconflategroundingfailureswith
s failures, not hallucination in any useful sense. Aggregation
sallthreecontextsizes.
Table10: HallucinationMetricsat128KContext
Model Faith. Ground. Fab.
(%) (%) (%)
GLM-4.5 94.4 94.8 7.9
Qwen3-Next-80B 93.4 94.0 9.6
DeepSeek-V3.1 91.6 94.0 20.4
Qwen3-235B 90.3 92.3 20.0
Qwen3-235B-FP8 89.9 92.1 21.7
GLM-4.6 89.1 91.4 22.9
Qwen3-Coder-480B 87.0 87.6 15.8
DeepSeek-V3 85.3 88.3 30.4
Mistral-Large-3 84.6 89.4 40.0
GLM-4.5-Air 84.1 84.4 17.9
Llama-4-Maverick 78.4 83.4 47.1
Qwen3-30B-A3B-Instruct 77.9 82.7 47.1
Qwen3-4B 73.8 75.0 32.5
Llama-3.1-405B 69.1 69.9 35.4
Qwen3-Coder-30B 66.3 71.7 61.7
Llama-4-Scout 64.5 66.5 45.4
Llama-3.1-70B 62.2 65.2 53.3
Granite-4-Small 55.1 54.0 39.6
Llama-3.3-70B 54.7 55.2 47.9
Llama-3.1-8B 47.5 51.1 71.3
Llama-3.2-3B 37.7 33.1 38.8
Granite-4-Micro 30.5 25.7 45.0
Granite-4-Tiny 27.5 32.1 96.3
Llama-3.2-1B 17.4 7.8 32.9
Table11: HallucinationMetricsat200KContext
Model Faith. Ground. Fab.
(%) (%) (%)
Qwen3-Next-80B 88.1 88.8 15.2
Qwen3-235B 82.2 83.5 24.5
Qwen3-235B-FP8 80.9 82.1 25.5
Qwen3-Coder-480B 80.5 81.4 24.1
Mistral-Large-3 77.7 80.5 36.4
Qwen3-30B-A3B-Instruct 72.0 75.1 43.6
Llama-4-Maverick 71.4 75.5 48.9
Qwen3-Coder-30B 63.0 67.9 61.8
Llama-4-Scout 60.0 62.5 52.3
Qwen3-4B 56.4 54.3 33.0
GLM-4.6 43.0 47.3 78.6
14

=== Page 15 ===
‚Ä¢ GLM-4.5dominatesatshortercontexts: At32K,GLM
hallucination resistance. This degrades to 7.9% fabricati
duetoitsnativemaxcontextlimitation.
‚Ä¢ Fabrication rates increase with context: Most models
strates this dramatically: 10.2% fabrication at 32K rises
collapse.
‚Ä¢ Qwen3-Next-80B dominates long context: Fabrication
(40.9%‚Üí9.6%‚Üí15.2%for32K/128K/200K).The32
testsetforthe32Kcorpushasfewerquestions,andthus
the200Kcontexttest,Qwen3-Next-80Bdominates,andi
‚Ä¢ Groundinggapsrevealfabrication:Thedifferencebetw
Modelswithhighgroundingbuthighfabrication(e.g.,De
readdocumentscorrectlybutinventinformationwhenask
Enterpriseimplications:Highfabricationratesindicatemo
existententities. Inproduction,suchfabricationsareindistingu
The96.3%fabricationrateofGranite-4-Tinyat128Kmeansitw
initscontext-aseverereliabilityrisk. Moreconcerningisthef
10.2%to78.6%fabricationsuggeststhatmodelsreliableatsho
Exhaustivetestingiswarranted.
5.6 AggregationAnalysis
Aggregationqueriesrequiremodelstosynthesizeinformationac
orcomputingstatistics. Unlikesingle-documentextraction(pa
aggregationtestscross-documentreasoning.
Table12presentsaggregationaccuracyacrossallthreeconte
lossratesforaggregationqueries(seeSection5.7forcoherence
Keyfindings:
‚Ä¢ Aggregationisconsistentlyharderthansingle-docume
92.3% grounding to 75.5% aggregation at 128K - a 17-
difficult.
‚Ä¢ Aggregation degrades faster with context: Qwen3-N
(200K)-a33-pointdecline. Comparethistoitsfaithfulne
‚Ä¢ Llamafamilystrugglesdisproportionately: Llama-3.1
gregation. At128K,thisgapwidens: 69.1%faithfulness
‚Ä¢ GLM-4.6collapseisaggregation-specific:GLM-4.6dro
drop),whileitsfaithfulnessdropsfrom93.0%to43.0%(
‚Ä¢ Some models maintain aggregation better: Qwen3-C
55.2% across 32K/128K/200K - consistent 15-16 point
models.
Enterpriseimplications: Aggregationqueriesarecommon
thisquarter?‚Äù,‚ÄúWhatisthetotalrevenueacrossallsubsidiaries
formancedegradationatlongercontextssuggeststhatproductio
for aggregation tasks, (2) use retrieval strategies that minimize
intomultiplesingle-documentlookups.
1

M-4.5achieves94.7%faithfulnesswithzerofabrication-perfect
ion at 128K, still best-in-class. GLM-4.5 is not tested at 200K
s show higher fabrication at longer contexts. GLM-4.6 demon-
s to 78.6% at 200K - a 7.7√ó increase that explains its overall
n starts very high, but vastly decreases in higher context sizes
2Kanomalyismostlikelyatestmeasurementlimitation,asthe
evenfewerhallucination-specificquestionsascomposition. In
issecondonlytoGLM-4.5in128K.
weenFaithfulnessandGroundingquantifiesfabrication‚Äôsimpact.
eepSeek-V3at128K:88.3%grounding,30.4%fabrication)can
kedaboutnon-existententities.
odelsthatconfidentlyinventinformationwhenaskedaboutnon-
uishablefromcorrectanswerswithoutgroundtruthverification.
wouldfabricateanswerstonearlyeveryqueryaboutentitiesnot
fabricationincreasewithcontextlength: GLM-4.6‚Äôsjumpfrom
ortercontextscanbecomeextremelyunreliableatlongerones.
crossmultipledocuments-countingentities,comparingvalues,
atternmatching)orhallucinationprobes(fabricationdetection),
extsizes,sortedby32Kperformance.Table13showscoherence
elossdiscussion).
entextraction:Eventhebestmodel(Qwen3-235B)dropsfrom
-point gap. Cross-document reasoning is fundamentally more
Next-80B drops from 92.6% (32K) to 80.8% (128K) to 59.6%
essdeclineofonly5pointsacrossthesamerange.
1-405Bachieves79.0%faithfulnessat32Kbutonly70.5%ag-
vs26.6%aggregation-a42-pointdisparity.
opsfrom86.7%aggregationat32Kto19.6%at200K(67-point
(50-pointdrop). Theaggregationfailureisevenmoresevere.
Coder-480B shows remarkable stability: 86.7% ‚Üí 70.6% ‚Üí
drops per tier, compared to the erratic collapses seen in other
ninenterpriseknowledgesystems: ‚ÄúHowmanycontractsexpire
s?‚Äù,‚ÄúWhichemployeeshavependingreviews?‚Äù Thesteepper-
onsystemsshouldeither(1)keepcontextwindowsconservative
e context size, or (3) explicitly decompose aggregation queries
15

=== Page 16 ===
Table12: AggregationAccuracy(sortedby32K)
Model 32K 128K 200K
Qwen3-235B 95.5 75.5 55.5
Qwen3-235B-FP8 95.2 76.1 56.0
GLM-4.5 94.6 68.5 -
Qwen3-Next-80B 92.6 80.8 59.6
Llama-4-Maverick 87.5 51.9 46.9
GLM-4.6 86.7 69.1 19.6
Qwen3-Coder-480B 86.7 70.6 55.2
Qwen3-Coder-30B 86.7 49.8 35.6
Qwen3-30B-A3B-Instruct 86.4 61.9 54.1
Mistral-Large-3 85.5 61.3 52.4
DeepSeek-V3.1 80.4 72.0 -
Qwen2.5-72B 75.9 - -
DeepSeek-V3 72.4 46.2 -
Llama-3.1-405B 70.5 26.6 -
Qwen3-4B-Instruct 69.3 57.7 17.8
GLM-4.5-Air 66.8 40.5 -
Llama-4-Scout 66.5 30.6 26.5
Qwen3-32B 63.1 - -
Qwen2.5-14B 61.4 - -
Llama-3.1-70B 56.0 17.9 -
Qwen2.5-32B 53.1 - -
Llama-3.3-70B 48.9 18.6 -
Qwen3-8B 38.9 - -
Qwen3-4B 37.8 - -
Llama-3.1-8B 31.3 16.0 -
Granite-4-Small 34.9 15.7 -
Granite-4-Tiny 31.8 14.9 -
Granite-4-Micro 14.2 9.8 -
Llama-3.2-3B 14.2 7.1 -
Llama-3.2-1B 3.4 1.6 -
Valuesin%.Dash(-)=nottested.
5.7 CoherenceLossandInfiniteGeneration
Undertestconditions, ourexaminedmodelshaveanywherefro
themodel‚Äôscapacityandthetestset). Whenmodelshittheirma
indicatesaninfinitegenerationloopcausedbycoherenceloss. O
it,andmovesontothenextitem. Wetracktruncationratesasa
showscoherencelossorganizedbyseveritytier.
KeyobservationsfromTable13:
‚Ä¢ Coherencelosscorrelateswithmodelsize: Smallermo
coherencelossrates. Thelargestmodels(Qwen3-Coder-4
herence.
‚Ä¢ Coherencelossincreaseswithcontextlength: Qwen3-
increase.
‚Ä¢ Aggregationisuniquelyvulnerable: Single-document
1

Table13: CoherenceLoss
Model 32K 128K 200K
Severe(>10%):
Qwen3-4B-Instruct 0.3 11.3 37.0
Qwen3-30B-A3B-Instruct 0.0 2.8 15.9
Llama-3.1-8B 0.3 13.9 -
Qwen3-Coder-30B 0.0 1.5 13.4
Qwen3-Next-80B 0.0 0.3 13.3
GLM-4.6 0.0 0.4 11.6
Moderate(5-10%):
Llama-3.2-1B 9.6 6.0 -
Llama-3.2-3B 0.0 7.8 -
Granite-4-Tiny 3.4 7.7 -
Llama-3.3-70B 0.0 5.4 -
Qwen3-4B 5.1 - -
Minor(1-5%):
Granite-4-Micro 0.0 4.4 -
Qwen3-235B-FP8 0.0 0.5 3.8
Qwen3-235B 0.0 0.9 3.5
GLM-4.5-Air 0.0 1.8 -
Granite-4-Small 0.3 1.6 -
Qwen3-8B 1.4 - -
Rare(<1%):
Llama-3.1-70B 0.0 0.9 -
Llama-3.1-405B 0.6 0.4 -
Qwen3-14B 0.6 - -
Qwen3-32B 0.6 - -
DeepSeek-V3 0.0 0.1 -
DeepSeek-V3.1 0.0 0.0 -
GLM-4.5 0.0 0.0 -
Llama-4-Scout 0.0 0.0 0.5
Qwen3-Coder-480B 0.0 0.0 0.0
Mistral-Large-3 0.0 0.0 0.0
Llama-4-Maverick 0.0 0.0 0.0
Qwen2.5-72B 0.0 - -
Qwen2.5-32B 0.0 - -
Qwen2.5-14B 0.0 - -
Qwen2.5-Coder-14B 0.0 - -
Qwen2.5-Coder-7B 0.0 - -
Valuesin%.Dash(-)=nottested.
om25Kto>100Kmaxoutputtokensavailable(dependingon
aximumtokenlimitbeforecompletingaresponse,thistypically
Ourbenchmarkinginfrastructuretruncatestheresponse,records
proxyforcoherencelossunderlong-contextpressure. Table13
odels(Qwen3-4B,Llama-3.1-8B,Granite-4-Tiny)showhigher
480B,Qwen3-235B,DeepSeek-V3.1)rarelydevolveintoinco-
-4B-Instructgoesfrom0.3%at32Kto37.0%at200K-a123√ó
andhallucinationqueriesrarelytriggercoherenceloss(<0.5%
16

=== Page 17 ===
for most models). Aggregation queries require the mod
appearstotriggergenerationloops.
‚Ä¢ GLM-4.6‚Äôs200Kcollapsehasmultiplecauses: Beyond
11.6%coherencelossfrequencyat200K-themodelfails
Note:Hardware-dependentfailuresobserveddistinctfro
near-zero truncation rates, but exhibited a different failure mo
characters in an infinite generation loop) and Gaudi 3 (real cha
possible from our H200 hardware. This infrastructure sensitiv
coherencemetrics. Wenotethishereforcompleteness. When
generation. Whentheydon‚Äôt,theyappearcompletelybrokenun
Table13cannotcapture. Noneoftheother31modelstestedha
5.8 ExaminingtheGLM4.6200KCollapse
By all measures - accuracy, hallucination metrics, coherence lo
200K tests. We investigated whether an adjustment in tempera
thecoherenceloss.
Table14: GLM-4.6at200
Metric
CoherenceLoss
CoherenceLossRate
OverallAccuracy
Highertemperaturedramaticallyreducescoherenceloss(18
to27%.
The role of temperature in enterprise-relevant scenarios lik
agenticfitness(measuredinouragenticmeritindexpaper[50]
[48])deservesamoreintensivetreatmentandwillbeexamined
5.9 Cross-CorpusBenchmarkStabilityAnalysis
AkeyclaimofRIKERisthattheCoherentSimulatedUniverse
artifacts. Tovalidatethis,wegeneratedthreeadditional128Kc
theoriginal(SetA)-sameentitycounts,documentdistributions
producingentirelydifferentdocuments,entities,andquestions.
Weranfourmodelsspanningtheperformancespectrumac
tier),Qwen3-Coder-480B(upper-middletier),andGranite-4-Sm
Table15: Cross-CorpusStability: ModelPerf
Model SetA SetB
(%) (%)
DeepSeek-V3.1 84.1 83.9
GLM-4.5 84.4 83.0
Qwen3-Coder-480B 80.7 78.9
Granite-4-Small 39.9 36.3
Note:SetAistheoriginalcorpususedthroughoutthispaper.SetsB,
minusminacros
Keyfindings:
‚Ä¢ Top-tier models show remarkable stability: DeepSee
acrossfourcompletelyindependentcorpora. Thisspread
1

del to enumerate and reason across multiple documents, which
dthe78.6%fabricationratenotedearlier,GLM-4.6alsoshows
sinmultiplewayssimultaneously.
omcoherenceloss:Llama4models(MaverickandScout)show
ode on MI300X hardware (producing only ASCII replacement
aracters, but 100% coherence loss). Successful runs were only
vity represents a separate reliability dimension not captured by
runningproperly,Llama4modelsappearlesspronetoinfinite
nderRIKERtests-acontradictionthecoherencelossmetricsin
adsimilarcharacteristics.
oss - GLM 4.6 showed a steep decline in output quality in the
ature would change this catastrophic performance, particularly
0K:Temperature0.4vs1.0
temp=0.4 temp=1.0
184 4
4.38% 0.10%
37% 27%
84‚Üí4instances). Surprisingly,accuracydecreasedfrom37%
ke long-context knowledge extraction (this study) or in overall
withdeeperanalysisinourtrace-levelagenticAIfailurestudy
dinmuchdeeperdetailinfuturework.
approachmeasuresmodelcapabilityratherthancorpus-specific
corpora(SetsB,C,D)usingidenticalgenerationparametersto
s,andquestiontyperatios-butwithindependentrandomseeds
crossallthreenewcorpora: DeepSeek-V3.1andGLM-4.5(top
mall(lowertier). Table15presentstheresults.
formanceAcrossIndependent128KCorpora
SetC SetD Mean Spread
(%) (%) (%) (pts)
84.4 82.9 83.8 1.5
83.6 84.4 83.8 1.5
79.9 81.4 80.2 2.5
33.8 34.9 36.2 6.1
C,Dareindependentlygeneratedvalidationcorpora.Spreadismax
ssalltestedsets.
ek-V3.1 and GLM-4.5 vary by only 1.4-1.5 percentage points
iscomparabletorun-to-runvariancewithinasinglecorpus.
17

=== Page 18 ===
‚Ä¢ Mid-tierstabilityisalsostrong:Qwen3-Coder-480Bsh
measurementnoisefor8-runexperiments.
‚Ä¢ Weaker models show higher corpus sensitivity: Grani
ranking is preserved - it remains clearly in the lower pe
modelsmaybemoresensitivetospecificdocumentphras
‚Ä¢ Ranking stability matters most: The practical questio
served. Across all four corpora, the ordering DeepSeek
remainsconsistent.
These results show a simple validation of RIKER‚Äôs core d
and documents are generated from that ground truth, any corp
underlyingcapability. Thebenchmarkisnottestingwhethermo
whether they can extract structured information from realistic
resistantevaluation:evenifaspecificcorpusleaks,regenerating
5.10 ExpandedHallucinationAnalysis
Ourmainexperimentincludeshallucinationquestions(L11-L1
analysisofhallucinationbehavior,weconductedanexpandedstu
(L1-L4)testingextractionoffactsthatdoexistinthecorpus,an
correctlyidentifyfactsthatdonotexist.
Weevaluated10modelsacrossfourindependentquestions
per configuration. This design serves two purposes: (1) deep
questions,and(2)within-corpusstabilityvalidationusingdiffer
Table16: ExpandedHallucinationAnalysi
Model Groundi
(L1-L
GLM-4.5 96.7
GLM-4.6 93.8
DeepSeek-V3.1 95.1
Qwen3-235B 92.7
Qwen3-235B-FP8 92.7
Qwen3-Coder-480B 90.1
Qwen3-Next-80B 91.4
Llama-3.1-70B 86.3
Llama-3.3-70B 81.4
Granite-4-Small 67.5
Groundingmeasuresaccuracyonfactspresentincorpus.Fabricationshows
across4questionsets√ó8run
Keyfindings:
‚Ä¢ GLM-4.5exhibitsexceptionalhallucinationresistance:
tofindinformationthatdoesnotexist. Thisisremarkabl
aboutentitiesthatcouldexistbutdon‚Äôt).
‚Ä¢ Groundingabilitydoesnotpredictfabricationresistan
ondonlytoGLM-4.5)buthas17.3%fabricationrate-8√ó
thatexistwhilestillbeingproneto‚Äúfinding‚Äùfactsthatdo
‚Ä¢ Llamamodelsshowelevatedhallucinationrates: Bot
substantially higher fabrication rates than similarly-size
hallucinationresistancethanLlama-3.1.
‚Ä¢ Quantization has minimal impact: Qwen3-235B and
(15.5% vs 16.5% fabrication), suggesting that quantizati
1

hows2.5pointsofspread-slightlyhigherbutstillwithinnormal
ite-4-Small exhibits 6.1 points of spread. However, its relative
erformance tier across all corpora. This suggests that weaker
singsorentityconfigurations.
on for benchmarks is whether relative model rankings are pre-
k-V3.1 ‚âà GLM-4.5 ¬ø Qwen3-Coder-480B ¬ø¬ø Granite-4-Small
design. Because ground truth is generated before documents,
pus instantiation with the same parameters measures the same
odelsmemorizedspecificphrasingsorentitynames-itistesting
c enterprise documents. This property enables contamination-
withthesameparametersproducesanequallyvalidbenchmark.
12)aspartofthebroaderquestiontaxonomy. Toenabledeeper
udywith300questionsperevaluation:150groundingquestions
nd150fabricationquestions(L11-L12)testingwhethermodels
sets(HA,HB,HC,HD)onthesame128Kcorpus,with8runs
per hallucination analysis with balanced grounding/fabrication
rentquestionsetsonidenticaldocuments.
is: Groundingvs.FabricationPerformance
ing Fabrication Overall
L4) (L11-L12) Accuracy
7% 2.1% 97.3%
8% 9.7% 92.1%
1% 17.3% 88.9%
7% 15.5% 88.6%
7% 16.5% 88.1%
1% 18.4% 85.9%
4% 19.9% 85.7%
3% 38.8% 73.7%
4% 45.0% 68.2%
5% 54.5% 56.5%
shallucinationrateonnon-existentfacts(lowerisbetter).Resultsaveraged
ns=32evaluationspermodel.
:Withonly2.1%fabricationrate,GLM-4.5almostneverclaims
legiventhatthequestionsaredesignedtobeplausible(asking
nce: DeepSeek-V3.1achieves95.1%groundingaccuracy(sec-
√óhigherthanGLM-4.5. Modelscanbeexcellentatfindingfacts
on‚Äôt.
thLlama-3.1-70B(38.8%)andLlama-3.3-70B(45.0%)exhibit
ed models. Notably, the newer Llama-3.3 performs worse on
its FP8 quantized variant show nearly identical performance
ion does not significantly affect hallucination behavior for this
18

=== Page 19 ===
model.However,notallmodelsandquantizationmethods
onenterprisereliabilitymorebroadly.
Within-corpusstability: Table17showsfabricationratesa
stablebehavior: GLM-4.5showsonly1.4percentagepointssp
points each. Notably, Granite-4-Small has the smallest spread
consistentlyhallucinatesatthesamehighrateregardlessofwhi
Table17: Within-CorpusStability: Fa
Model HA
GLM-4.5 2.0% 1
GLM-4.6 9.9% 7
DeepSeek-V3.1 17.7% 18
Qwen3-235B 11.2% 18
Qwen3-235B-FP8 14.0% 19
Qwen3-Coder-480B 18.2% 19
Qwen3-Next-80B 18.0% 19
Llama-3.1-70B 38.0% 37
Llama-3.3-70B 42.8% 45
Granite-4-Small 54.3% 54
Fabricationrate(%,lowerisbetter)foreachoffourindependentquestionsets
8ru
TakentogetherwiththefindingsinSection5.9,thisconfirms
and regenerable test set features for contamination resistance.
measurementofcapabilitiesinsteadofparticularquestion-speci
6 Discussion
Ourevaluationrevealsseveralfindingswithimplicationsforbo
contexts. Beforediscussingspecificresults,wedistinguishbetw
latterbeingthemoregeneralizablecontribution.
6.1 RIKERasMethodology
RIKER is one implementation of a more general approach: g
consistsofthreeprinciples:
1. Groundtruthprecedesdocuments. Definethefacts,en
thesefactssecond. Thisinvertsthetraditionalapproacho
2. Questions derive from ground truth. Generate questio
annotation. Thisenablesdeterministicscoringwithoutre
3. Regenerabilityenablescontaminationresistance. Beca
instancescanbecreatedatwill. Benchmarkvaliditydoes
The effectiveness of regenerability depends on implementa
domization of templates, entity pools, and variable combinatio
documentdiversityanddistinctivenessduetolimitedrandomiza
Beyond contamination resistance, ground-truth-first genera
quirehumanannotationforeachtestitem,makinglarge-scalee
as-a-judgeforhumanannotation,butthisintroducesjudgemod
time. RIKER‚Äôsautomaticgroundtruthanddeterministicscorin
wouldbeprohibitivelyexpensivewithhumanannotationandim
ensuresperfectreproducibility: thesameresponsealwaysreceiv
Our specific implementation to demonstrate the methodolo
records - but these are incidental. The methodology applies w
1

sareequal.Futureworkwillexaminetheeffectsofquantization
acrossthefourindependentquestionsets. Mostmodelsexhibit
pread,whileDeepSeek-V3.1andQwen3-Coder-480Bshow2.0
d (0.7 pts) despite having the worst absolute performance - it
ichquestionsareasked.
abricationRateAcrossQuestionSets
HB HC HD Spread
1.2% 2.7% 2.5% 1.4pts
7.9% 10.7% 10.2% 2.8pts
8.3% 17.0% 16.3% 2.0pts
8.5% 16.2% 16.0% 7.3pts
9.8% 16.5% 15.7% 5.8pts
9.6% 18.2% 17.6% 2.0pts
9.2% 20.8% 21.7% 3.7pts
7.5% 38.5% 41.3% 3.8pts
5.1% 43.5% 48.5% 5.7pts
4.9% 54.2% 54.7% 0.7pts
onthesame128Kcorpus.Spread=max-minacrosssets.Eachcellaverages
uns.
sthefeasibilityandpracticalityofRIKER‚Äôsregenerablecorpora
. Both approaches show stable measurements, suggesting the
ificartifacts.
othresearchersandpractitionersdeployingLLMsinenterprise
weenRIKERasanimplementationandasamethodology-the
ground-truth-first synthetic evaluation. The core methodology
ntities,andrelationshipsfirst. Generatedocumentsthatembed
ofextractinggroundtruthfromexistingdocuments.
ons whose answers are known by construction, not by human
eferencemodelsorhumanjudges.
ausedocumentsandquestionsareprocedurallygenerated,fresh
snotdependonanyparticularcorpusremainingunseen.
ation quality: document distinctiveness requires sufficient ran-
ons. A RIKER-based document generator that lacks sufficient
ationprovideslittletonoprotectionagainstcontamination.
ation enables unprecedented scale. Traditional benchmarks re-
evaluationexpensive. ManyrecentbenchmarkssubstituteLLM-
delbiases,addsinferencecostandincreasesoverallevaluation
ngeliminatebothbottlenecks-our21+billiontokenevaluation
mpracticallyslowwithLLMjudges. Deterministicscoringalso
vesthesamescore.
ogy uses commercial leases, sales agent field reports, and HR
wherever synthetic documents can be generated from structured
19

=== Page 20 ===
groundtruth:medicalrecordsfrompatientdatabases,legaldocu
talresults,financialreportsfromtransactionlogs.
The stability validations (Sections 5.9 and 5.10) confirm th
corpus stability shows that different document instantiations y
differentquestionsetsonidenticaldocumentsyieldconsistentr
mentedground-truth-firstbenchmark,notjustourspecificimple
6.2 Cross-CorpusandWithin-CorpusStability
RIKER‚Äôs core design hypothesis - that generating documents F
documents enables reliable evaluation - is supported by the st
that different document instantiations with identical parameter
on Set A perform well on Sets B, C, and D. Within-corpus v
thesamecorpusalsoyieldstableresults. Together,thesefindin
corpus-specificorquestion-specificartifacts.
This addresses the fundamental contamination problem id
performance may reflect memorization of specific examples,
documents while maintaining the same underlying evaluation
withoutlosingvalidity.
6.3 ContextLengthClaimsExceedUsableCapacity
Modelsfrequentlyadvertise128Kor200Ktokencontextwindow
for enterprise tasks. Top-tier models achieve over 80% accura
contexts. Some models exhibit catastrophic failures: GLM-4.
despitenominallysupportingbothcontextlengths.
Forpractitioners,thisimpliesthatmarketedcontextlengths
deploymentsrequiringlong-contextprocessingshouldvalidate
relyingonvendorspecifications.
6.4 AggregationRevealsaCapabilityGap
Single-document extraction and cross-document aggregation a
that excel at extracting facts from individual documents often
tiple documents. This is not simply a matter of ‚Äúmore work‚Äù
informationscatteredacrossthecorpus,performaccurateextrac
RIKER‚Äôsmulti-leveltaxonomyexplicitlyseparatestheseca
extractionaccuracyacrossallmodelstested. Thisfindingvalida
only single-document retrieval may overestimate model capab
multi-documentreasoning.
6.5 GroundingandHallucinationResistanceAreDistinct
Perhaps our most striking finding is that strong grounding perf
V3.1achieves95.1%groundingaccuracy-secondonlytoGLM
than GLM-4.5‚Äôs 2.1%. Models can be excellent at finding in
informationthatdoesnotexist.
Thisdistinctionhassignificantimplicationsforenterprisede
informationispresentmaystillconfidentlyfabricateanswersw
highfactualreliabilitymustevaluatebothcapabilitiesindepend
6.6 Limitations
RIKER‚Äôscurrentimplementationhasseverallimitationsthatco
Domain scope. Our evaluation uses enterprise documents
chosenforrealism,performanceonthesedocumenttypesmay
contracts,ormedicalrecords.
Language. AlldocumentsandquestionsareinEnglish. Mu
2

umentsfromcaseparameters,scientificpapersfromexperimen-
hat this methodology produces reliable measurements. Cross-
yield consistent rankings. Within-corpus stability shows that
results. Thesepropertiesshouldtransfertoanyproperlyimple-
ementationinRIKER.
FROM ground truth rather than extracting ground truth FROM
tability analyses. Cross-corpus validation (Section 5.9) shows
rs produce consistent measurements: models that perform well
validation (Section 5.10) shows that different question sets on
ngsconfirmthatRIKERmeasuresmodelcapabilityratherthan
dentified in our gap analysis. Unlike static benchmarks where
RIKER‚Äôs regenerable corpora create fresh, never-before-seen
n parameters. The benchmark can be regenerated indefinitely
ws,butourresultssuggesttheseclaimsoverstateusablecapacity
acy at 32K tokens but show meaningful degradation at longer
.6 collapses from 70.9% accuracy at 128K to 26.6% at 200K,
shouldnotbeconflatedwitheffectivecontextlength. Enterprise
eperformanceattheiractualoperatingcontextsizesratherthan
appear to require fundamentally different capabilities. Models
n struggle when required to aggregate information across mul-
- aggregation questions require the model to identify relevant
ctionfromeachsource,andcombineresultscorrectly.
apabilities,revealingthataggregationaccuracyconsistentlylags
atesthetaxonomydesignandsuggeststhatbenchmarkstesting
bility for realistic enterprise workloads that inherently involve
formance does not predict hallucination resistance. DeepSeek-
M-4.5-yetexhibitsa17.3%fabricationrate,eighttimeshigher
nformation that exists while simultaneously prone to ‚Äúfinding‚Äù
eployment. Amodelthatreliablyextractscorrectanswerswhen
whenqueriedaboutnon-existententities. Applicationsrequiring
dently.
onstraingeneralizability:
(commercial leases, facility field reports, HR records). While
nottransfertootherdomainssuchasscientificliterature,legal
ultilingualperformanceremainsuntested.
20

=== Page 21 ===
Architecture. In this study, we evaluate pure context-stuffi
willtestretrieval-augmentedgenerationandagenticretrievalpa
Synthetic realism. While our Coherent Simulated Unive
synthetic documents may lack certain characteristics of real-w
informationacrosssources,ordomain-specificjargon. Models
characteristicscanbemodeledintoRIKER‚Äôs(oranyRIKER-li
workandresults,suchcharacteristicsarenotincluded.
Modelcoverage. Ourevaluationcovers33modelsavailab
meansnewmodelsmayexhibitdifferentpatterns.
6.7 FutureWork
SeveralextensionswouldstrengthenRIKER‚Äôsutility:
Additionaldocumenttypes. Expandingbeyondthecurren
tions,financialstatements,andotherenterprisedocumentswou
Additionaltesttypes.Forhallucinationdetectioninparticu
thatofferafiner-grainedviewofhallucinationfailuremodes.In
questions that expect lists of specific items (e.g., document sou
morecomprehensivehallucination-centeredmetric.
RAGandagenticevaluation.Instrumentingretrievalsystem
betweencontext-stuffingandvariousRAGapproachesonident
Multilingualcorpora. Generatingdocumentsinmultiplela
Adversarialrobustness. Introducingdeliberateinconsisten
whengroundtruthisambiguous.
Continuousbenchmarking. RIKER‚Äôsregenerabledesign
asanautomatedleaderboardwithfreshcorporaforeachevalua
7 Conclusion
We presented RIKER, both as a benchmark for enterprise doc
eral methodology: ground-truth-first synthetic evaluation. Th
truth rather than extracting ground truth FROM documents - e
regenerablecorpora,andsystematicevaluationofcapabilitiesth
Our evaluation of 33 models using over 21 billion tokens r
exceed usable capacity, with significant performance degradat
substantially harder than single-document extraction. Groundi
thatmustbeevaluatedseparately.
The stability analyses confirm that the methodology works
rameters)andwithin-corpusvalidation(samedocuments,differ
thatanyproperlyimplementedground-truth-firstbenchmarksh
Beyond the specific benchmark, we contribute a replicable
tionsinanydomainwheresyntheticdocumentscanbegenerate
DataAvailability
Theexperimentdata,includingallofthegeneratedgroundtruth
willbemadeavailableathttps://docs.kamiwaza.ai/r
Acknowledgments
This research was made possible through the generous provis
with8xAMDMI300XGPUseachforexperimentalevaluation
Fellowsfortheirsupport. (https://signal65.com/)
2

fing - the entire corpus is provided in the prompt. Future work
atterns.
erse approach maintains entity consistency across documents,
world data: OCR errors, inconsistent formatting, contradictory
mayperformdifferentlyonmessierreal-worldcorpora. These
ikesystem‚Äôs)syntheticdatagenerationlogic,butinthiscurrent
bleatthetimeoftesting. TherapidlyevolvingLLMlandscape
ntthreedocumenttypestoincludecontracts,technicalspecifica-
uldbroadenapplicability.
ular,moretypesofhallucination-specifictesttypescanbeadded
nsteadofsingle-answerquestions(entitynameoramountvalue),
urces paired with specific retrieved details each) can provide a
mstoanswerRIKERquestionswouldenabledirectcomparison
ticalgroundtruth.
anguageswouldenablecross-lingualevaluation.
nciesorcontradictionsinthecorpuscouldtestmodelbehavior
enablesongoingevaluationasnewmodelsemerge,potentially
ationcycle.
cument understanding and as a demonstration of a more gen-
he core insight - generating documents FROM known ground
enables deterministic scoring, contamination resistance through
hatmatterforreal-worlddeployment.
reveals several key findings. Context length claims frequently
tion beyond 32K tokens. Cross-document aggregation proves
ing ability and hallucination resistance are distinct capabilities
s: both cross-corpus validation (different documents, same pa-
rentquestions)produceconsistentmeasurements. Thissuggests
houldexhibitsimilarstabilityproperties,regardlessofdomain.
e methodology for constructing contamination-resistant evalua-
edfromstructuredgroundtruth.
h,documentcorpora,testsets,andthevariousmodelrawresults,
research/datasets.
sion of GPU compute by Signal65, who provided four servers
n. WethankRyanShrout,BrianMartin,MitchLewis,andRuss
21

=== Page 22 ===
AIUsageDisclosure
TheresearchersusedthefollowinggenerativeAIservicestoass
‚Ä¢ ClaudeCode: ClaudeOpus4.5
‚Ä¢ Gemini3ProImage(NanoBananaPro)
Most of the language in this paper was drafted by generati
referencematerial(especiallypreviousworkinPICARD)andt
heavily revised, edited and polished by human researchers. 10
humanresearchers.
In addition, all tables were created through generative AI
researchers. NanoBananaProwasusedtogeneratetheRIKER
Inallcases, finaleditorialcontrol, technicalvalidation, and
Theauthorstakefullresponsibilityfortheaccuracyandintegrit
References
[1] WasiUddinAhmad. AwesomeLLMsyntheticdata: Area
[2] AmazonWebServices.Groundtruthgenerationandreview
withFMEval,2024.
[3] Appen. StateofAIin2024report,2024.
[4] AbhinavArun,FabrizioDimino,TejasPrakashAgarwal,B
ticconstructionandevaluationoffinancialknowledgegra
[5] Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaoz
Dong, Jie Tang, and Juanzi Li. Longbench v2: Towards
multitasks,2025.
[6] KrisztianBalogandChengXiangZhai. Usersimulationin
ation,andsystemevaluation,2025.
[7] YejinBang,ZiweiJi,AlanSchelten,AnthonyHartshorn,T
Hallulens: Llmhallucinationbenchmark,2025.
[8] BeatrustTechBlog. RAGevaluation: Assessingtheusefu
[9] Andres M. Bran, Zlatko JoncÀáev, and Philippe Schwaller.
In Proceedings of the 1st Workshop on Language + Mol
AssociationforComputationalLinguistics.
[10] GuimingHardyChen,ShunianChen,ZicheLiu,FengJia
onjudgementbiases,2024.
[11] SeungminChoiandYuchulJung. Knowledgegraphconst
15(7):3727,2025.
[12] CollectiveIntelligenceProject. LLMjudgesareunreliable
[13] Databricks. FreshStack: Buildingrealisticbenchmarksfor
[14] DavidZWZ. AwesomeRAGreasoning: ResourcesforRA
[15] DEEP-PolyU. Awesome-GraphRAG:Acuratedlistofres
[16] Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gers
modernbenchmarksforlargelanguagemodels,2024.
2

sistwiththemanuscript:
ive AI using RIKER project documents and code, plus certain
therawandsummarizedRIKERresultsinCSVform,andthen
00% of this document was read and reviewed several times by
directly using raw source data, and then reviewed by human
Rdiagrambyfeedingitthefinaldraftofthispaper.
dintellectualresponsibilityrestsolelywiththehumanauthors.
tyofallcontentinthismanuscript.
adinglistonLLMbasedsyntheticdatageneration,2024.
wbestpracticesforevaluatinggenerativeAIquestion-answering
BhaskarjitSarmah,andStefanoPasquali. FinReflectKG:Agen-
aphs. arXivpreprintarXiv:2508.17906,2025.
zhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao
s deeper understanding and reasoning on realistic long-context
ntheeraofgenerativeAI:Usermodeling,syntheticdatagener-
TaraFowler,ChengZhang,NicolaCancedda,andPascaleFung.
ulnessofRAGAS,2024.
Knowledge graph extraction from total synthesis documents.
lecules (L+M 2024), pages 74‚Äì84, Bangkok, Thailand, 2024.
ang,andBenyouWang. Humansorllmsasthejudge? astudy
truction: Extraction,learning,andevaluation. AppliedSciences,
e,2024.
revaluatingretrievalontechnicaldocuments,2025.
AGreasoninginLLMsandagents,2025.
sourcesongraph-basedretrieval-augmentedgeneration,2024.
stein, and Arman Cohan. Investigating data contamination in
22

=== Page 23 ===
[17] YihongDong,XueJiang,HuanyuLiu,ZhiJin,BinGu,M
contamination and trustworthy evaluation for large langua
Linguistics: ACL2024.AssociationforComputationalLin
[18] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley
tansky, Robert Osazuwa Ness, and Jonathan Larson. Fro
summarization. arXivpreprintarXiv:2404.16130,2024.
[19] Maria Eriksson, Erasmo Purificato, Arman Noroozian, J
Fernandez-Llorca. Can we trust AI benchmarks? an inte
preprint,2025.
[20] ShahulEs, JithinJames, LuisEspinosa-Anke, andSteven
mentedgeneration. InProceedingsofthe18thConference
Linguistics: SystemDemonstrations.AssociationforCom
[21] AoranGan,HaoYu,KaiZhang,QiLiu,WenyuYan,Zhen
generationevaluationintheeraoflargelanguagemodels:
[22] JiaxinGao,ChenChen,YanwenJia,XueluanGong,Kwo
a-judgebiasincommunicationsystems,2025.
[23] YunfanGao,YunXiong,WenlongWu,ZijingHuang,Boha
forlongcontextneedle-in-a-haystack,2025.
[24] GraphRAG-BenchTeam. GraphRAG-Bench: Acomprehe
[25] HaoyuHan,LiMa,HarryShomer,YuWang,YongjiaLei,
andJiliangTang. Ragvs.graphrag: Asystematicevaluati
[26] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, a
comprehensive evaluation of reasoning steps. In Procee
Linguistics,2020.
[27] Zijin Hong, Hao Wu, Su Dong, Junnan Dong, Yilin Xiao
Yang, andXiaoHuang. BenchmarkingLLMs‚Äômathemat
preprintarXiv:2501.11790,2025.
[28] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shant
Ginsburg. Ruler: What‚Äôstherealcontextsizeofyourlong
[29] HuggingFace. Thehallucinationsleaderboard,anopenef
[30] HamelHusain. ModernIRevalsforRAG,2024.
[31] GregoryKamradt. LLMTest NeedleInAHaystack: Doing
measureaccuracy,2023.
[32] Ernests Lavrinovics, Russa Biswas, Johannes Bjerva, and
hallucinations: AnNLPperspective. WebSemantics: Sci
2024.
[33] Sunwoo Lee, Daseong Jang, Dhammiko Arya, Gyoung-e
Seokyoung Hong, Sereimony Sek, Seung-Mo Cho, Sohee
gentBench: Amulti-facetedbenchmarkforevaluatingLL
2025 Conference on Empirical Methods in Natural Lang
China,2025.
[34] Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun N
evaluation benchmark for large language models. In Pr
NaturalLanguageProcessing.AssociationforComputatio
2

MengfeiYang,andGeLi. Generalizationormemorization: Data
age models. In Findings of the Association for Computational
nguistics,2024.
y, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropoli-
om local to global: A graph RAG approach to query-focused
Joao Vinagre, Guillaume Chaslot, Emilia Gomez, and David
erdisciplinary review of current issues in AI evaluation. arXiv
nSchockaert. RAGAS:Automatedevaluationofretrievalaug-
eoftheEuropeanChapteroftheAssociationforComputational
mputationalLinguistics,2024.
nyaHuang,ShiweiTong,andGuopingHu.Retrievalaugmented
: Acomprehensivesurvey,2025.
ok-YanLam,andQianWang. Evaluatingandmitigatingllm-as-
anLi,andHaofenWang.U-niah:Unifiedragandllmevaluation
ensivebenchmarkforevaluatingGraphRAGmodels,2025.
KaiGuo,ZhigangHua,BoLong,HuiLiu,CharuC.Aggarwal,
ionandkeyinsights,2025.
and Akiko Aizawa. Constructing a multi-hop QA dataset for
edings of the 28th International Conference on Computational
o, Yujing Zhang, Zhu Wang, Feiran Huang, Linyi Li, Hongxia
ticalreasoningwithunseenrandomvariablesquestions. arXiv
tanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris
g-contextlanguagemodels?,2024.
fforttomeasurehallucinationsinlargelanguagemodels,2024.
simpleretrievalfromLLMmodelsatvariouscontextlengthsto
d Katja Hose. Knowledge graphs, large language models, and
ience,ServicesandAgentsontheWorldWideWeb,85:100844,
eun Han, Injee Song, SaeRom Kim, Sangjin Kim, Seojin Lee,
e Park, Sungbin Yoon, Wonbeom Jang, and Eric Davis. TelA-
LM-basedagentsintelecommunications. InProceedingsofthe
guage Processing: Industry Track, pages 1173‚Äì1211, Suzhou,
Nie, and Ji-Rong Wen. HaluEval: A large-scale hallucination
roceedings of the 2023 Conference on Empirical Methods in
onalLinguistics,2023.
23

=== Page 24 ===
[35] YuchengLi,FrankGuerin,andChenghuaLin. LatestEval
throughdynamicandtime-sensitivetestconstruction. In
2024.
[36] JintaoLiang,GangSu,HuifengLin,YouWu,RuiZhao,a
onreasoningagenticretrieval-augmentedgenerationforin
[37] JingruLin, ChenZhang, StephenY.Liu, andHaizhouLi
retrievalaugmentedgenerationsystems,2025.
[38] HaoLiu,ZhengrenWang,XiChen,ZhiyuLi,FeiyuXiong
forlogic-awareretrieval-augmentedgeneration,2025.
[39] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjap
inthemiddle: Howlanguagemodelsuselongcontexts. T
12:157‚Äì173,2024.
[40] Lin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao D
datageneration,curation,andevaluation: Asurvey,2024.
[41] lyy1994. Awesomedatacontamination: Thepaperliston
[42] Gaurav Maheshwari, Dmitry Ivanov, and Kevin El Hadda
arXiv:2409.11968,2024.
[43] SewonMin,KalpeshKrishna,XinxiLyu,MikeLewis,We
HannanehHajishirzi.FActScore:Fine-grainedatomiceval
ceedingsofthe2023ConferenceonEmpiricalMethodsin
Linguistics,2023.
[44] Sewon Min, Eric Wallace, Sameer Singh, Matt Gardner,
questionsdonotnecessitatemulti-hopreasoning,2019.
[45] Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, O
Symbolic: Understanding the limitations of mathema
arXiv:2410.05229,2024.
[46] HyeonseokMoonandHeuiseokLim. Needlechain: Measu
models,2025.
[47] NiklasMuennighoff,NouamaneTazi,Lo¬®ƒ±cMagne,andN
[48] JVRoig. Howdollmsfailinagenticscenarios? aqualita
agenticsimulations,2025.
[49] JV Roig. Testing what models can do, not what they‚Äôve
domizeddata. Technicalreport,KamiwazaAI,2025.
[50] JVRoig. Towardsastandard,enterprise-relevantagentica
aievaluations,2025.
[51] SujoyRoychowdhury,SumitSoman,H.G.Ranjani,Neera
RAGmetricsforquestionansweringinthetelecomdomai
[52] Dongyu Ru, Lin Qiu, Xiangkun Hu, Tianhang Zhang, P
Shichao Sun, Huanyu Li, Zizhao Zhang, Binjie Wang, Jia
andZhengZhang. Ragchecker: Afine-grainedframework
[53] JonSaad-Falcon,OmarKhattab,ChristopherPotts,andM
retrieval-augmented generation systems. In Proceedings
AssociationforComputationalLinguistics(NAACL).Asso
2

l: Addressingdatacontaminationinlanguagemodelevaluation
ProceedingsoftheAAAIConferenceonArtificialIntelligence,
andZiyueLi. Reasoningragviasystem1orsystem2: Asurvey
ndustrychallenges,2025.
i. Ragcap-bench: Benchmarkingcapabilitiesofllmsinagentic
g,QinhanYu,andWentaoZhang. Hoprag:Multi-hopreasoning
pe, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost
TransactionsoftheAssociationforComputationalLinguistics,
Ding, Gang Chen, and Haobo Wang. On llms-driven synthetic
datacontaminationforlargelanguagemodelsevaluation,2024.
ad. Efficacy of synthetic data as a benchmark. arXiv preprint
Wen-tauYih,PangWeiKoh,MohitIyyer,LukeZettlemoyer,and
luationoffactualprecisioninlongformtextgeneration.InPro-
nNaturalLanguageProcessing.AssociationforComputational
, Hannaneh Hajishirzi, and Luke Zettlemoyer. Compositional
Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. GSM-
atical reasoning in large language models. arXiv preprint
uringintactlong-contextreasoningcapabilityoflargelanguage
NilsReimers. Mteb: Massivetextembeddingbenchmark,2023.
ativeanalysisofsuccessandfailurescenariosofvariousllmsin
seen: Picard: Probing intelligent capabilities via artificial ran-
aibenchmark:Lessonsfrom5.5billiontokens‚Äôworthofagentic
ajGunda,VanshChhabra,andSaiKrishnaBala. Evaluationof
in. arXivpreprintarXiv:2407.12873,2024.
Peng Shi, Shuaichen Chang, Cheng Jiayang, Cunxiang Wang,
arong Jiang, Tong He, Zhiguo Wang, Pengfei Liu, Yue Zhang,
kfordiagnosingretrieval-augmentedgeneration,2024.
MateiZaharia. ARES:Anautomatedevaluationframeworkfor
of the 2024 Conference of the North American Chapter of the
ociationforComputationalLinguistics,2024.
24

=== Page 25 ===
[54] OlawaleSalaudeen, AnkaReuel, AhmedAhmed, Suhana
AngelinaWang,andSanmiKoyejo. Measurementtomea
preprintarXiv:2505.10573,2025.
[55] HannahSansford, NicholasRichardson, HerminaPetricM
basedllmhallucinationevaluationframework,2024.
[56] JulianSchnitzler,XanhHo,JiahaoHuang,FlorianBoudin
multi-hopreasoning,2024.
[57] LinShi,ChiyuMa,WenhuaLiang,XingjianDiao,Weiche
aticstudyofpositionbiasinllm-as-a-judge,2025.
[58] JamesSteinhoffandSamHind. Simulationandtherealit
Society,12(1):1‚Äì14,2025.
[59] YixuanTangandYiYang. Multihop-rag: Benchmarkingr
[60] Nandan Thakur, Nils Reimers, Andreas Ru¬®ckle¬¥, Abhish
benchmarkforzero-shotevaluationofinformationretrieva
SystemsTrackonDatasetsandBenchmarks,2021.
[61] HarshTrivedi,NiranjanBalasubramanian,TusharKhot,an
hopquestioncomposition. TransactionsoftheAssociation
[62] YingjiaWan,HaochenTan,XiaoZhu,XinyuZhou,Zhiwe
Lu,YinhongLiu,andZhijiangGuo. FaStfact: Faster,stro
[63] HengyiWang,HaizhouShi,ShiweiTan,WeiyiQin,Wenyu
Wang. Multimodal needle in a haystack: Benchmarking
2025.
[64] Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin
WilliamFedus. Measuringshort-formfactualityinlargela
[65] ZhishangXiang, ChuanjieWu, QinggangZhang, Shengy
usegraphsinrag: Acomprehensiveanalysisforgraphretr
[66] GuangzhiXiong,QiaoJin,ZhiyongLu,andAidongZhan
2024.
[67] ChengXu,ShuhaoGuan,DerekGreene,andM-TaharKec
Asurvey,2024.
[68] ShuoYang,Wei-LinChiang,LianminZheng,JosephE.G
nationforlanguagemodelswithrephrasedsamples. arXiv
[69] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,
Manning. HotpotQA: A dataset for diverse, explainable
ConferenceonEmpiricalMethodsinNaturalLanguageP
[70] JiayiYe,YanboWang,YueHuang,DongpingChen,Qihu
Pin-YuChen,NiteshVChawla,andXiangliangZhang. Ju
[71] HaoYu,AoranGan,KaiZhang,ShiweiTong,QiLiu,and
Asurvey,2024.
[72] YifeiYu,Qian-WenZhang,LingfengQiao,DiYin,Fang
andXingSun. Sequential-niah: Aneedle-in-a-haystackb
2025.
[73] Qiming Zeng, Xiao Yan, Hao Luo, Yuhao Lin, Yuxiang W
Howsignificantaretherealperformancegains? anunbias
2

aBedi, ZacharyRobertson, SudharsanSundar, BenDomingue,
aning: Avalidity-centeredframeworkforAIevaluation. arXiv
Maretic, andJubaNaitSaada. Grapheval: Aknowledge-graph
n,SakuSugawara,andAkikoAizawa. MoreHopQA:Morethan
engMa,andSoroushVosoughi. Judgingthejudges: Asystem-
tygap: Momentsinaprehistoryofsyntheticdata. BigData&
retrieval-augmentedgenerationformulti-hopqueries,2024.
hek Srivastava, and Iryna Gurevych. BEIR: A heterogeneous
almodels. InProceedingsoftheNeuralInformationProcessing
ndAshishSabharwal.MuSiQue:Multihopquestionsviasingle-
nforComputationalLinguistics,2022.
eiLi,QingsongLv,ChangxuanSun,JiaqiZeng,YiXu,Jianqiao
ongerlong-formfactualityevaluationsinLLMs,2025.
uanWang,TunyuZhang,AkshayNambi,TanujaGanu,andHao
long-context capability of multimodal large language models,
Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and
anguagemodels,2024.
yuanChen, ZijinHong, XiaoHuang, andJinsongSu. Whento
rieval-augmentedgeneration,2025.
ng. Benchmarkingretrieval-augmentedgenerationformedicine,
chadi.Benchmarkdatacontaminationoflargelanguagemodels:
Gonzalez,andIonStoica. Rethinkingbenchmarkandcontami-
vpreprintarXiv:2311.04850,2023.
William W. Cohen, Ruslan Salakhutdinov, and Christopher D.
e multi-hop question answering. In Proceedings of the 2018
Processing,2018.
uiZhang,NunoMoniz,TianGao,WernerGeyer,ChaoHuang,
usticeorprejudice? quantifyingbiasesinllm-as-a-judge,2024.
dZhaofengLiu. Evaluationofretrieval-augmentedgeneration:
gLi,JieWang,ZengxiChen,SuncongZheng,XiaolongLiang,
benchmarkforextractingsequentialneedlesfromlongcontexts,
Wang, Fangcheng Fu, Bo Du, Quanqing Xu, and Jiawei Jiang.
sedevaluationframeworkforgraphrag,2025.
25

=== Page 26 ===
[74] XinrongZhang,YingfaChen,ShengdingHu,ZihangXu,JunhaoChen,MooKhaiHao,XuHan,ZhenLengThai,Shuo
Wang,ZhiyuanLiu,andMaosongSun. ‚àûbench: Extendinglongcontextevaluationbeyond100ktokens,2024.
[75] ZhenyuZhang,RunjinChen,ShiweiLiu,ZheweiYao,OlatunjiRuwase,BeidiChen,XiaoxiaWu,andZhangyangWang.
Foundinthemiddle: Howlanguagemodelsuselongcontextsbetterviaplug-and-playpositionalencoding,2024.
[76] QihaoZhao,YangyuHuang,TengchaoLv,LeiCui,QinzhengSun,ShaoguangMao,XinZhang,YingXin,QiufengYin,
ScarlettLi,andFuruWei. Mmlu-cf: Acontamination-freemulti-tasklanguageunderstandingbenchmark,2024.
26

Paper:Semantic Uncertainty Quantification of Hallucinations in LLMs.pdf
=== Page 1 ===
PublishedasaconferencepaperatICLR2026
SEMANTIC UNCERTAINTY
LUCINATIONS IN LLMS: A
WORK BASED METHOD
PragatheeswaranVipulanandan1,KamalPre
1DepartmentofElectricalandComputerEngine
2DepartmentofComputerScience
UniversityofMiami
CoralGables,FL33146,USA
pxv245@miami.edu, kamal@miami.edu
ABST
Largelanguagemodels(LLMs)exhibits
vulnerabletoconfabulations,fluentyetun
underidenticalprompts. Leveragingaq
weproposeaquantumphysics-inspiredu
accountsforthealeatoricuncertaintyin
equivalence-based clustering of LLM g
pledandinterpretableschemeforhalluc
anentropy-maximizationstrategythatpr
herent outputs and highlights entropy re
tobeunreliable,offeringpracticalguide
ranted.Weevaluatetherobustnessofours
and quantization levels, dimensions ove
thatourapproachremainsreliableeven
total of116 experiments onTriviaQA,N
plearchitectures(Mistral-7B,Mistral-7B
LLaMA-2-13b-chat,LLaMA-2-7b-chat,
consistentimprovementsinAUROCand
1 INTRODUCTION
Large language models (LLMs) have demonstra
terpreting,andreasoningoverhumanlanguagea
leveloffluencyandcoherence. LLMs,trainedon
powerfultoolsthatbracestate-of-the-art(SOTA)ap
answering(Q&A),dialoguegeneration,andarere
ChatGPT,whichprovidesconversationalAIexpe
(Chenetal.(2021)),DALL¬∑E(Laietal.(2023)),a
AsLLMsarebeingintegratedinnewapplication
every scientific field, the reliability of their outp
inconsistenciestheyintroducecanhaveadirectan
Thisissueisparticularlyevidentininteractivesy
expectedtobenotonlyfluentbutalsofactuallygro
beingrelieduponinmoresensitiveapplicationsce
autonomousdriving)(Kaddouretal.(2023);Zha
vulnerabilityofLLMsishallucinations,whichre
plausiblebutareinfactincorrect,unverifiable,or
outputscanrangefromminorfactualinaccuracie
seriousobstacletothedeploymentofLLMsinhig
significantethicalconcernsaswell.
1
6202
naJ
72
]LC.sc[
1v62002.1062:viXra

QUANTIFICATION OF HAL-
A QUANTUM TENSOR NET-
emaratne1,DilipSarkar2
eering
u, sarkar@miami.edu
TRACT
stronggenerativecapabilitiesbutremain
nreliableoutputsthatvaryarbitrarilyeven
quantumtensornetwork‚Äìbasedpipeline,
uncertaintyquantificationframeworkthat
tokensequenceprobabilityforsemantic
generations. In turn, this offers a princi-
cinationdetection. Wefurtherintroduce
rioritizeshigh-certainty,semanticallyco-
egions where LLM decisions are likely
elinesforwhenhumanoversightiswar-
schemeunderdifferentgenerationlengths
erlooked in prior studies, demonstrating
inresource-constraineddeployments. A
NQ, SVAMP,and SQuAD acrossmulti-
B-instruct,Falcon-rw-1b,LLaMA-3.2-1b,
LLaMA-2-13bandLLaMA-2-7b)show
dAURACoverstate-of-the-artbaselines.
ated remarkable capabilities in understanding, in-
andnaturallanguagegeneration(NLG)atahigh
nmassivetextcorpora,haveledtothecreationof
pproachesintaskssuchassummarization,question
eshapinghowhumansinteractwithmachines(e.g.,
eriences(Hosseinietal.(2023)),GitHubCopilot
andothers).
nsinnearlyallaspectsofdailylifeandinalmost
puts has gained increasing attention. Errors and
ndsignificantimpactondownstreamperformance.
ystemssuchaschatbots,wheretheresponsesare
ounded(Leietal.(2023)),especiallywhentheyare
enarios(e.g.,healthcare,defense,law,journalism,
aoetal.(2023);Leietal.(2023)). Oneparticular
efertoLLMgenerationsthatappearcoherentand
entirelyfabricated(Jietal.(2023)). Hallucinated
estocompletelyspuriousclaims,andtheyposea
gh-stakesdomains;useofhallucinatorycontenthas
1

=== Page 2 ===
PublishedasaconferencepaperatICLR2026
1.1 PREVIOUSWORK
Most approaches for detecting and mitigating ha
frameworks(Rateikeetal.(2023);Quevedoetal.(
in complex syntactic/semantic analyses (Wang e
(2023)). Whileshowingpromise,theircomputatio
unfitforreal-timeapplications. Numericalfeatur
properties)andcorrelationsbetweenthemmaya
(Azaria&Mitchell(2023);Lee(2023);Suetal.
detectionmethodsthatcomplementorevenoutpe
AlthoughBayesiandeeplearning(BDL)method
standardclassificationtasks(Schweighoferetal.(
detectioninLLMs: theyarechallengingtocalibrat
about factual correctness, and next-token proba
probabilities. As a result, classical BDL tools s
instabilitiesthatdrivehallucinations,limitingthei
Otherworksfocusonspecifictypesofhallucinatio
fluentbutincorrectgenerationsthatappeararbitrar
factors(e.g.,randomseedsandpromptphrasing)a
suchas,incorrectanswersfrommodelsthathave
etal.(2022)),‚Äúlies‚Äùstrategicallygeneratedinpu
setups(Evansetal.(2021)),andsystematicfailures
Todetectconfabulations,Quevedoetal.(2024)em
inducedentropy,anaturalandinterpretablemeasu
(2022)),butnaiveestimatesofentropymaynots
etal.(2020))becauselexical/syntacticdifferences
difference. Therefore,Farquharetal.(2024)andK
notlexical/syntacticvariations,ofgeneratedoutpu
(TSs)generatedwhenanLLMconfrontsthesame
basedontheirbidirectionalentailment(asassessed
higherlikelihoodofhallucination. Inasimilarvei
etal.(2024))employsagradedsimilaritymeasu
NearestNeighbourEntropy(SNNE)(Nguyenet
LogSumExpsmoothing,avoidingexplicitcluster
andtranslation;SemanticDensity(SD)(Qiu&M
bygenerationprobabilities. Theseapproachesimp
clusteringquality,similarityfunctions,andTSpro
Structure-awareandperturbation-basedmethodsha
GraphUncertainty(Jiangetal.(2024))andKEA
(2025))modelrelationshipsbetweenknowledgeg
groundtruthclaimsfordetectionofhallucinations.
relyonanexternalknowledgebaseandaccurateK
UncertaintyQuantification(SPUQ)(Gaoetal.(202
(Coxetal.(2025))estimateuncertaintybyprobi
inputs,exposingpromptsensitivitybutatthecost
Inspiteoftheadvancestheyhavemade,SOTAmeth
content across diverse tasks and domains (Li et
unaddressedthealeatoricuncertaintyoftheTSp
howsensitivetheseTSprobabilities,andhencet
perturbations. Thishighlightstheneedforlocal,r
probabilityuncertainty. Priorstudieshavealsonot
differentquantizationlevelsdespitethelatterplayin
1.2 OURCONTRIBUTIONS
‚ÄúWhitebox‚Äùinherentlyinterpretableapproaches(e
methods(Ghahramani(2015)))canbecomputation
2

allucinations lean heavily on supervised learning
(2024))orin-contextlearningtechniquesgrounded
et al. (2022); Manakul et al. (2023); Zhang et al.
onaldemandsoftenintroducelatencymakingthem
res(e.g.,statisticalpatternsandstructuraloutput
alsoserveaseffectiveindicatorsofhallucinations
.(2024)),yieldinglightweight,resource-efficient
erformmorecomputationallyintensivetechniques.
dshavelongbeenusedtoquantifyuncertaintyin
(2024)),theyarefarlesseffectiveforhallucination
te,theydonotreliablyreflectamodel‚Äôsuncertainty
abilities do not behave like true class-prediction
struggle to capture the semantic and contextual
irusefulnessinthissetting(Kangetal.(2025)).
onswithparticularattentionpaidtoconfabulations,
ry(Berrios(1998)). Theyaresensitivetoirrelevant
andaredistinctfromothertypesofhallucinations,
beentrainedonwidespreadmisconceptions(Lin
ursuitofrewardsignalsinreinforcementlearning
striggeredfromflawedreasoningorgeneralization.
mploytokensequence(TS)probabilitiesandtheir
ureofuncertainty(Lindley(1956);Kadavathetal.
serveasagoodindicatorofhallucinations(Xiao
inLLMgenerationsoftendonotimplyasemantic
Kossenetal.(2024)employsemanticentropy(SE),
utstoquantifyhallucinatorybehavior. Theoutput
einput(context+prompt)repeatedlyareclustered
dbyDeBERTa). Highersemanticentropyindicates
in,KernelizedLikelihoodEntropy(KLE)(Nikitin
uretoaccountforsemanticdifferences;Semantic
al.(2025))aggregatespairwisesimilaritieswith
ringandimprovingrobustnessonsummarization
Miikkulainen(2024))weightssemanticdistances
proveconfabulationdetection,butaresensitiveto
obabilityfluctuations.
avealsobeenexploredfordetectinghallucinations.
(Kernel-EnrichedAI)explain(Haskins&Adams
graphs(KGs)constructedfromLLMoutputsand
. Buttheyincurheavycomputationaloverheadand
KGconstruction. SamplingwithPerturbationfor
24))andSemantic-InvariantPerturbationSampling
ingmodelsensitivitytoparaphrasedorperturbed
ofrepeatedqueries.
hodsstillstrugglewithreliablydetectingfabricated
al. (2023)). A possible reason is that they leave
probabilities. Hallucinationriskshoulddependon
thesemanticentropiestheyinduce,aretomodel
ratherthanglobal,sensitivitymetricstoassessTS
texaminedtherobustnessoftheirapproacheswith
ngamajorroleinreal-worlddeploymentofLLMs.
e.g.,randomsampling-basedvariationalinference
nallyprohibitiveforUQofmassivelyscaledLLMs
2

=== Page 3 ===
PublishedasaconferencepaperatICLR2026
and physics-inspired methods have emerged as a
physics-inspiredapproach,advocatedinPrincipe
andVipulananthanetal.(2024),thatleveragesper
function to offer a deterministic, one-shot, interp
methodforquantifyinguncertaintylocallyandat
Themaincontributionsandkeyadvantagesofou
probabilityuncertaintyasanovelindicatorofco
theTSprobabilitiesasthewavefunctionofaquan
Vipulananthanetal.(2024);Vipulanandanetal.
(b)Weofferamethod,basedonentropymaximiz
forthisuncertaintyassociatedwithTSprobabilit
based on not only SE but uncertainty of TS pro
entropygenerationsashallucinations(Farquhare
thatflagsoutputsassociatedwithhigheruncertain
negativesandfalsepositives. (d)Weevaluatether
quantizationlevelsaswellasunderdifferentgene
tosentence-leveloutputs. Ourresultsshowthatt
resource-constrainedorefficiency-optimizeddeplo
detectiontoreal-worldsettings. Theoverallflowo
2 PROPOSED METHOD FOR UQ OF LL
Figure1: Overviewofourhallucinationdetection
entailment,andUQobtainedthroughQTNisused
hallucinationdetection.
2.1 PRELIMINARIES: SEMANTICCLUSTERING
ConsidertheoutputTSsthattheLLMgeneratesi
ands thesequenceofprevioustokens. Then,th
<i
(cid:89)
P(s|y)=
i
Clusteringisdeterminedbythesemanticdissonan
tionalentailment(forwhichweemployDeBERTa
classesand|C|itscardinality. Treatingclusterlabe
clusterprobabilitythattheTSsbelongsinc ‚ààC
j
(cid:80)
P(s|y)
p(j) = ‚àÜ P(c |y)= s‚ààcj
c j (cid:80)|C| (cid:80)
P(s
j=1 s‚ààcj
respectively. When the TS probabilities P(s |
i
equations1and2;otherwise,weusep(j) ‚âà I(c =
c
Thelatterisreferredtoasdiscretesemantic(Shan
2.2 SEMANTICR√âNYIENTROPY
Thephysics-inspiredframeworkforUQinPrincipe
oftheTSprobabilitiesintheformofitskernelm
3

a compelling alternative. Our work employs the
e(2010),andadoptedinSingh&Principe(2020)
rturbationtheoryofanappropriatequantumwave
pretable (Rudin (2019); Linardatos et al. (2021))
differentresolutions.
urapproachareasfollows: (a)WeintroduceTS
onfabulationdetection. Forthispurpose,weview
ntumtensornetwork(QTN)(Qi&Ranard(2019);
(2026))andleverageperturbationtheoryforUQ.
zation,tocalibratetheTSprobabilitiestoaccount
ties. (c)ThisleadsustoclusterLLMgenerations
obabilities. So, instead of simply flagging high
etal.(2024)),weofferamoremeaningfulscheme
ntysothatonemayreducethelikelihoodoffalse
robustnessofourframeworkunderdifferentLLM
erationlengths,rangingfromshortphraseanswers
themethodisrobustandremainsreliableevenin
oyments,thusextendingtheutilityofhallucination
ofourapproachisillustratedinFig.1.
LM GENERATIONS
npipeline. Sequencesareclusteredviadirectional
dforentropymaximizationtoenablereliableLLM
G
inresponsetoinputy. Lets denotethei-thtoken
i
heprobabilityofs,conditionedony,is
P(s |s ,y). (1)
i <i
ncebetweenoutputTSsasdeterminedbybidirec-
a). LetC denotethesetofsemanticallydissonant
elc asarandomvariable(onthespaceofC),the
j
C andtheinducedsemantic(Shannon)entropyare
)
(cid:88)
; SE (y)=‚àí p(j) log p(j), (2)
S c c
|y)
cj‚ààC
| s ,y) are available, p(j) are computed from
<i c
= c )/|C|,themembershipcountineachcluster.
j
nnon)entropyinFarquharetal.(2024).
e(2010)requiresanon-parametricfeaturemapping
meanembedding(KME)inthereproducingkernel
3

=== Page 4 ===
PublishedasaconferencepaperatICLR2026
Hilbert space (RKHS) determined by a suitable
Sch√∂lkopfetal.(2015)). WeemploytheGaussian
TheresultingKMEanditsempiricalestimateare,
(cid:90)
œà (x)= Œ∫ (p ,x)p dp ; œà
y œÉ s s s
s
where{s(r), r =0,...,R‚àí1}denotestheLLM
andp(r) = ‚àÜ P(s(r) | y). AsPrincipe(2010)estab
s
estimatorofthemoregeneralnotionofR√©nyi(qua
|C|
(cid:88)
SE (y)=‚àílog p
R
j=1
whichinturnisadirectmeasureofuncertaintyin
onthequadraticformofR√©nyientropyand,forbr
R‚Äôenyientropy. Here,E[(cid:5)]denotesexpectation(ov
ashowuncertaintyvarieswithx. ForpurposesofU
version{œà(cid:98)
y
}ofœà(cid:98)y (x)‚Äîasa‚Äòdata‚Äôwavefunction
equationofaQTN(Qi&Ranard(2019);Vipulan
Given that it is R√©nyi entropy which underpins
probabilities,weopttousethesame(insteadofSh
semanticentropyofoutputclustersassociatedwit
2.3 ACCOUNTINGFORUNCERTAINTYINTSP
UQ of TS Probabilities For purposes of UQ
HamiltonianH(cid:98) has{œà(cid:98) }asoneofitseigen-mod
y
corrections(weonlylookatthefirst-ordercorrect
weconstructthefirst-orderuncertainty‚Äòfeature‚Äôv
œÉ2 ‚àá2 |œà(1)(x)|
V(1)(x)=E(1)+ m m , w
m m 2 |œà(1)(x)|
m
Here,{œà , E }and{œà(1), E(1)},form ‚â• 0,a
m m m m
asE ‚â§E ‚â§¬∑¬∑¬∑,andtheirfirst-ordercorrectio
0 1
Laplacian ‚àá2 |œà(1)(x)| measures the change in
m m
localneighborhoodacrossthemodesandthevec
uncertaintiesacrossTSprobabilityamplitudes. Ifp
UQ(p(r))associatedwiththeTSprobabilityp(r)
s s
1 (cid:88)
UQ(p(r))=
s M
m
wherethesummationiscarriedoverM (weuseM
detailsonQTNbasedUQareprovidedinAppend
CalibratedAdjustmentofTSProbabilities W
(cid:40)
p(r)‚àó =argmax ‚àílog(p(r)2 +(1‚àíp(r
s (cid:98)s (cid:98)s
p(cid:98) ( s r)
The log((cid:5)) term in the right-hand side is the R√©n
back‚ÄìLeibler(KL)divergencebetweentheadjust
4

positive semi-definite kernel (Aronszajn (1950);
‚àÜ
kernelŒ∫ (p ;x),wherep =P(s|y)andx‚ààR.
œÉ s s
,respectively,
R‚àí1
1 (cid:88)
œà(cid:98)y (x)=
R
Œ∫
œÉ
(p(
s
r);x)p(
s
r), (3)
r=0
MgenerationswhentheinputyisrepeatedRtimes
blishes, œà(cid:98)y ((cid:5))servesasakernel-basedempirical
adratic)entropy
p(j)2
=‚àílog(E[p ]), (4)
c c
nadistribution. Inthiswork,weexclusivelyfocus
revity,refertoitthroughoutthepaperassemantic
verthespaceofC). Indeed,œà(cid:98)y (x)canbeviewed
UQ,weviewthisKME‚Äîtobeprecise,asampled
nassociatedwiththetime-independentSchr√∂dinger
nanthanetal.(2024);Vipulanandanetal.(2026)).
s our physics-inspired framework for UQ of TS
hannonentropyasinSOTAmethods)forassessing
thLLMgenerationstoo.
PROBABILITIES
of TS probabiltiies, we identify a QTN whose
des,andapplyperturbationtheorytocomputethe
tions)toalltheeigen-modes/energiesofH(cid:98). Then,
vectors
œÉ2 ‚àá2 |œà(1)(x)|
whereE(1) =‚àímin m m . (5)
m px 2 |œà(1)(x)|
m
aretheeigen-pairsoftheHamiltonianH(cid:98) ordered
onterms,respectively. Foreachmodem‚â•0,the
the first-order correction from its average in the
ctorV(1)(x)canbeviewedasa‚Äòspectrogram‚Äôof
m
p(r)ismappedtox(r)intheRKHS,theuncertainty
s
istakenas
(cid:88) (cid:12)
V(1)(x)(cid:12) , (6)
m (cid:12)
x=x(r)
m
M =8)modesthatareadjacentto{œà(cid:98) }. Additional
y
dixA.2.
WenowadjusttheTSprobabilitiesas
(cid:41)
1
r))2)‚àíŒª¬∑ ¬∑KL(p(r)‚à•p(r)) . (7)
s UQ(p(r)) (cid:98)s s
s
nyi entropy; the second term penalizes the Kull-
tedprobabilityp(r)andp(r)scaledinverselywith
(cid:98)s s
4

=== Page 5 ===
PublishedasaconferencepaperatICLR2026
theuncertaintyUQ(p(r));thehyperparameterŒª
s
imizationandtheadjustingtheTSprobability. I
probabilitieswhilepenalizingtheerrorbetweenp(
s
appears in Fig. 1 enables one to account for unc
a more informed decision regarding existence/ab
AppendixDcontainsadditionaldiscussionofcomp
implementationsofkeycomponents.
2.4 INTUITIONBEHINDTHEPROPOSEDAPPR
Ourmethodintegratesperturbation-baseduncertain
intoaunifiedframeworkforcorrectingTSprobab
Perturbation-baseduncertaintyquantification.
wemaptheKMEoftheTSprobabilitydistribution
KME is embedded as an eigen-mode of an admi
Therefore, perturbingH(cid:98) correspondstoperturbin
first-ordercorrectionstoitseigen-modes/energies
toinfinitesimalchangesinitsinputs. Thismirrors
wheretheinstabilityofaneigen-stateunderpertur
Largefirst-ordercorrections,therefore,indicateh
indicate locally stable regions. This produces an
measure,formalizedineq.(6),thatcapturestheloc
amplitudedomain.
Maximumentropyinferenceunderpartialkno
able,themaximumentropyprincipleprovidesthe
distribution. Amongalldistributionsconsistentwi
entropyismaximallynon-committalwithrespect
Equivalently, the maximum entropy distribution
distribution. Thisideahasdeeprootsinrobuststat
inthelikelihoodleadsnaturallytoregularizationto
Ourformulationimplementspreciselythis. TheR√©
probabilitiesshouldbeliftedtoremainmaximally
theadjustedprobabilitiesstayclosetothemodel‚Äôs
theKLpenaltyrelaxesandtheentropytermdomin
theadjustedprobabilitiesanchoredneartheiremp
Bringingthetwocomponentstogether. Theo
infer localuncertainty in the TSprobabilities by
first-ordereigen-modecorrections;(ii)usethesec
maximizationstep;and(iii)adjusttheTSprobabil
bytheempiricalTSestimates. High-uncertainty
low-uncertaintyregionsremainclosetothemodel
Thisyieldsadeterministic,single-shot,principled
dancewiththeirunderlyinguncertainty,unifying
reasoninginasingleframework.
3 EXPERIMENTS
Datasets OurevaluationscoverQ&Aintriviakn
knowledge (SQuAD 1.1 (Stanford NLP Group))
(Leeetal.(2019)))derivedfromactualqueriesto
mathematicalwordproblems(SVAMP(Pateleta
here;whiledetailedresultsaredeferredtoAppend
5

> 0controlsthetrade-offbetweenentropymax-
Inessence,wearemaximizingentropyoftheTS
(r)anditsadjustedvaluep(r). Thisschemewhich
s (cid:98)s
certainty associated with TS probabilities so that
bsence of hallucinatory behavior could be made.
putationaloverheadstogetherwiththepseudocode
ROACH
ntyquantificationandmaximumentropyinference
bilitiesinaprincipledmanner.
. ToestimateuncertaintyintheTSprobabilities,
nintotheeigenstructureofaQTN.Theempirical
issible Hamiltonian H(cid:98) associated with the QTN.
ngtheunderlyingTSprobabilities; theresulting
quantifyhowsensitivethemodeleddistributionis
standardquantummechanicalperturbationtheory
rbationrevealsitslocalvariability.
highlyunstableTSprobabilities;smallcorrections
n interpretable, physically grounded uncertainty
calvariabilityoftheTSprobabilitiesintheRKHS
owledge. Whenonlypartialinformationisavail-
eleastbiasedestimateofanunknownprobability
iththeknownconstraints,theonewiththehighest
tothemissinginformation(Jaynes(1957;2003)).
n minimizes its KL divergence from the uniform
tisticsandvariationalinference,whereuncertainty
owardhigher-entropy,lessoverconfidentsolutions.
√©nyientropytermineq.(7)specifieshowmuchthe
ynon-committal,whereastheKLtermensuresthat
empiricalTSestimates. Whenuncertaintyishigh,
nates;whenuncertaintyislow,theKLtermkeeps
piricalvalues.
overallprocedurethusfollowsacoherentlogic: (i)
y perturbing theHamiltonian and readingout the
correctionstoscaletheKLpenaltyintheentropy
litiestothemaximumentropydistributionallowed
regionsarepushedtowardhigherentropy,while
l‚Äôsoriginalpredictions.
dmethodforcorrectingTSprobabilitiesinaccor-
gperturbation-basedUQwithmaximum-entropy
nowledge(TriviaQA(Joshietal.(2017))),general
), and open-domain natural questions (NQ-Open
oGoogleSearch(Kwiatkowskietal.(2019)),and
al.(2021)). Onlytheresultssummaryisreported
dixC.
5

=== Page 6 ===
PublishedasaconferencepaperatICLR2026
Models WeuseadiversesetofLLMsobtainedvi
Falcon-RW1B(Penedoetal.(2023)),LLaMA-3.2
LLaMA-27B,LLaMA-2-13B-chat,LLaMA-213B
andMistral-v0.17B(Jiang(2024)). Theinclusion
largermodelsservestwopurposes. Fromapractic
useoflightermodelstoenableextensivesamplinga
evaluatingsmallerandcompressedmodelsallows
constrainedmodelcapacityandprecision,providin
onlyinidealsettingsbutalsoinmorerealistic,reso
relevant as smaller and quantized models are in
enterprise environments. All experiments 1 were
NVIDIAA6000GPU.
ModelPrompting,Entailment,andAnswerSele
usedforalldatasetstogenerateLLMresponsesan
otherclassifierscouldhavebeenused,weemploy
dataset(Williamsetal.(2017))forentailmentpred
inparaphrasedetectionbasedonembeddingsimi
BERT-stylemodels(Heetal.(2020);Tayetal.(
concatenatingthequestionwithoneanswerandc
withanotheranswer. Instruction-tunedLLMs,su
alsohavebeenusedtopredictentailmentbetween
EntropyMaximization FollowingUQoftheL
tion7)wasappliedtoidentifythe‚Äòoptimum‚Äôansw
Theeffectivenessofthisapproach(inthesenseo
improveshallucinationdetection)wasassessedu
alsoinvestigatedtheoptimalchoiceofthehyperp
analysesaredeferredtoAppendixC.3.
Comparison Methods For evaluation, we use
the proposed R√©nyi semantic entropy pre- and
respectively): naiveentropy(NE),semanticentro
allemployedinFarquharetal.(2024),andtwostr
andverifierp(True). ERisasupervisedmethod
(2022)). Ratherthanfine-tuningtheentireLLMas
regressionclassifieronthefinalhiddenrepresentat
simplicityandreproducibility. Thisbaselineperfo
out-of-distributiondata. Inp(True)(Kadavathet
answersandisthenasked,viaamultiple-choicepr
Confidenceismeasuredbytheprobabilityassign
usingupto20labeledexamples,furtherenhances
sometimesrequirereducingthenumberoffew-sh
EvaluationMetrics Weusethreeprimarymetr
factualconsistencywithreferenceanswersfromt
(a)AUROC(AreaUndertheReceiverOperatingC
todistinguishbetweencorrectandincorrectanswe
Intuitively,itrepresentstheprobabilitythataran
confidencescorethanarandomlyselectedincorr
1. (b)RAC(RejectionAccuracyCurve)measures
percentofthemostconfidentmodelpredictions. A
ensurethatthehigh-confidencepredictionsbeingr
lessconfidentexamplesareprogressivelyexcluded
Curve)quantifieshowaccuracychangesasincrea
AURACvaluesindicatethattheuncertaintymeth
1codeavailable:https://github.com/pragasv/semanti
6

iaHuggingface(huggingface.com),including
21B(Grattafiorietal.(2024)),LLaMA-2-7B-chat,
B(Touvronetal.(2023)),Mistral-7B-instruct-v0.3,
ofsmaller-scalemodelsandquantizedversionsof
calstandpoint,resourcelimitationsnecessitatethe
andrepeatedquerying.Fromascientificviewpoint,
ustoexplorehallucinationsanduncertaintyunder
nginsightsintohowsemanticentropybehavesnot
ource-constraineddeployments. Thisisparticularly
ncreasingly being deployed in edge, mobile, and
e run on a workstation with 64 GB memory and
ection AppendixBprovidestheprompttemplate
ndthepromptweusedtodetectentailment. While
yaDeBERTa-largemodelfine-tunedontheMNLI
diction. Thismethodbuildsuponpreviousresearch
ilarity(Socheretal.(2011);Yuetal.(2014))and
(2021)). Forsimplicity,entailmentischeckedby
comparingittotheconcatenationofthequestion
uchasLLaMA2,GPT-3.5TurboorGPT-4,could
ngeneratedoutputs.
LLMgenerations,entropymaximization(seeequa-
werbasedontheassociateduncertaintyestimates.
ofwhetherincorporatinguncertaintyinformation
usingtheevaluationmetricsdescribedbelow. We
parameterŒª. Implementationdetailsandextended
e the following methods for comparison against
post-TS uncertainty integration (SE and SE+,
R R
opy(SE ),anddiscretesemanticentropy(DSE ),
S S
rongbaselinemethods,embeddingregression(ER)
dinspiredbytheP(IK)approach(Kadavathetal.
sintheoriginalwork,thismethodtrainsalogistic
tionstopredictanswercorrectness,improvingboth
ormswellwithin-distributiondatabutpoorlywith
al.(2022)),themodelsamplesmultiplecandidate
rompt,whetherthetopansweris‚ÄúTrue‚Äùor‚ÄúFalse‚Äù.
nedtothe‚ÄúTrue‚Äùresponse. Afew-shotstrategy,
performance,althoughcontextwindowlimitations
hotexamples.
rics‚Äîeachbasedonanautomatedassessmentof
thedatasetsemployed‚Äîforevaluationpurposes:
Characteristics)measurestheabilityofaclassifier
erswhileaccountingforbothprecisionandrecall.
ndomlyselectedcorrectanswerreceivesahigher
rectone. AperfectmodelachievesanAUROCof
sthequestion-answeringaccuracythataspecified
Aneffectiveuncertaintyestimationapproachshould
retainedaremoreaccurate,withRACimprovingas
d. (c)AURACAreaUndertheRejectionAccuracy
asinglyuncertainpredictionsarerejected. Higher
hodmoreeffectivelydistinguishesaccuratefrom
ic-entropy-UQ-.git
6

=== Page 7 ===
PublishedasaconferencepaperatICLR2026
inaccurateresponses. UnlikeAUROC,AURACis
providingacomplementaryperspectiveonthequa
4 RESULTS
4.1 UNCERTAINTY-AWARESEMANTICENTRO
Thissectionpresentsaworkedexampleillustrating
UQmaximization(SRE-UQ),denotedasSE+,to
R
illustrationis,‚ÄúWhichoilproducerisacloseally
correspondingTSprobabilitiesareinTable 1.
Theresultsdemonstratethatincorporatinguncertain
yieldssystematicallylowerentropyestimatesrela
basedSE . Thisreductionarisesbecausesemanti
S
for this question, while their TS probabilities ex
accountingforsuchuncertaintymitigatesoverestim
measureofconfabulationinLLMoutputs.
LLMGeneration ClusterID p (cid:101) ( s r) p( s r)
Russia 1 0.05899 0.01814
SaudiArabia 2 0.57761 0.17765
SaudiArabia 2 0.57761 0.17765
Iran 3 0.07227 0.02223
SaudiArabia 2 0.57761 0.17765
Kuwait 4 0.08940 0.02749
Qatar 5 0.02185 0.00672
SaudiArabia 2 0.57761 0.17765
Iraq 6 0.12086 0.03717
SaudiArabia 2 0.57761 0.17765
Total 3.25144 1.00000
Table1: AnexamplecalculationofTSprobability
above. Columns: p(r) aretheraw(unnormalized
(cid:101)s
sequence probabilities (equation 3); p(j) = P(c
c
post-normalized;
p(j)‚àó
aretheclusterprobabilitie
c
NEandSE denotenaiveentropyandsemantice
S
denotessemanticR√©nyientropypost-TSuncertain
LLMgenerationperclusterareshown.
Table1illustratesthiseffectfortheexamplequery
producedby‚Äòstandard‚ÄôR√©nyientropy-basedSE
R
severalhallucinatedoutputs(e.g.,Qatar,Iraq,Iran
estimatestheassociatedconfabulationrisk. Incon
penalizesresponseswithhigheruncertainty,redistr
coherent answers. For instance, Saudi Arabia‚Äî
higherprobabilityassignmentafteradjustment. T
hallucinationsarepresent,localizeduncertaintyqu
semanticallymeaningfulanswers,afactornotcon
4.2 DETECTINGCONFABULATIONS
BenchmarkDatasets. Buildingontheworkedex
fromTriviaQA,NQ,SVAMP,andSQuADdatasets
LLaMA-27B,LLaMA-3.21B,Falcon-rw-1Band
methods;detailedvalidationresultsappearinApp
7

sdirectlysensitivetothemodel‚Äôsoverallaccuracy,
alityofuncertaintyestimationmethodology.
OPY: AWORKEDEXAMPLE
gthecomputationofSemanticR√©nyientropywith
ogetherwithNEandSE.Thequeryusedforthis
yoftheUnitedStates?‚Äù repeatedtentimes. The
ntyquantificationintosemanticentropy(i.e.,SE+)
R
ativetobothNEand‚Äòstandard‚ÄôShannonentropy-
icallyequivalentgenerationsareclusteredtogether
xhibit reduced aleatoric variability. Importantly,
mationbiasesinentropy,providingamorefaithful
p(j) p(j)‚àó NE SE SE+
c c S R
0.01814 0.02223 -0.03159 -0.03159 0.00049
0.88824 0.85880 -0.13331 -0.04572 0.73754
0 0 -0.13331 ‚Äì ‚Äì
0.02223 0.02697 -0.03674 -0.03674 0.00073
‚Äì ‚Äì -0.13331 ‚Äì ‚Äì
0.02749 0.03488 -0.04291 -0.04291 0.00122
0.00672 0.01214 -0.01460 -0.01460 0.00015
‚Äì ‚Äì -0.13331 ‚Äì ‚Äì
0.03717 0.04498 -0.05315 -0.05315 0.00202
0 0 -0.13331 ‚Äì ‚Äì
1.00000 1.00000 0.84557 0.22471 0.12951
anduncertaintymetricscorrespondingtothequery
d)sequenceprobabilities;p(r) arethenormalized
s
c | y) are the cluster probabilities (equation 2)
j
espost-TSuncertaintyintegration(7). Methods:
entropy,respectively(Farquharetal.(2024));SE+
R
ntyintegration. Valuesforonlyonerepresentative
y. Itcomparesthesemanticresponsedistributions
anditsuncertainty-adjustedvariantSE+. While
R R
n)receivenon-trivialprobabilitymass,SE under-
S
ntrast,theproposeduncertainty-awareadjustment
ributingprobabilitymasstowardmoresemantically
‚Äîa contextually appropriate response‚Äîreceives a
Thishighlightsapromisingdirection: evenwhen
uantificationenablesprioritizationofhigh-certainty,
nsideredinpriorwork.
xampleabove,wenowpresenttheresultsobtained
s,acrossmultipleLLMmodelssuchasMistral7B,
dLLaMA-213Baswellasuncertaintyestimation
pendixC.
7

=== Page 8 ===
PublishedasaconferencepaperatICLR2026
(a)AUROC-basedwinrate.
Figure2:PairwisewinratematricesacrossSOTAh
andLLMmodels. Thissummarizes116experime
thattherowmodeloutperformsthecolumnmodel
consistentlyoutperformsbaselines,evensurpassin
Fig.2(a)reportstheSRE-UQ‚Äôsperformanceagai
mance. HigherAUROCscoresindicatebettersepa
outputs. Acrossallevaluatedmodels,SRE-UQisc
SRE-UQ achieves better AUROCdespite not rel
evidenceofitseffectivenessinhallucinationdetec
WhileAUROCreflectsamodel‚Äôsglobalrankinga
oldingonconfidencescorestorejectoutputsasso
computetheRACoverrejectionthresholdsranging
theSRE-UQ‚ÄôsperformanceagainstSOTAmetho
tently,SRE-UQmaintainsstrongAURACasmo
itofferssuperiorrobustnesscomparedtodiscrete
p(True)baseline. Thisisevidenceofitseffectiven
higheruncertainty. AppendixCprovidesin-depth
NQ-Open,andSVAMP.
QuantizedLLMs Whilemostpriorworkonh
models,real-worlddeploymentsalmostalwaysre
reduceinferencelatencyandmemoryconsumption
model weights ‚Äî it also perturbs probability di
These shifts can affect semantic entropy measur
different detection methods. Thus, a rigorous stu
stabilityofdetectionmethodsacrossquantization
artifactofprecisionsettingsbutpersistinpractica
winratematricesofhallucinationdetectionmeth
8-bit,and4-bit),evaluatedusingbothAUROCand
maximization remains robust across quantization
SOTAbaselinesevenunderaggressivecompressio
Thequantization-wiseanalysisrevealsthatSRE-U
tivedetectionstrategiesacross16-bit,8-bit,and4
absolute AUROC values due to coarser uncertain
remainslargelystable,demonstratingthatthepro
Thisrobustnessiscriticalfordeploymentatscale,
tobalanceefficiencywithreliabilityinsafety-sens
4.3 ENTROPYUNCERTAINTYANALYSIS
Tofurtherassesstheeffectofuncertainty-aware
howSREchangesacrossdifferentlevelsofgener
distinctsemanticclustersproducedbythemodelw
8

(b)AURAC-basedwinrate.
hallucinationdetectionmethodsondiversedatasets
entalscenarios. Eachcellindicatestheprobability
l. SemanticR√©nyientropywithUQmaximization
ngmethodsreliantonsupervisedlearning.
instSOTAmethodsevaluatedviaAUROCperfor-
arabilitybetweenfactuallycorrectandhallucinated
competitivewithSOTAbaselinemethods. Notably,
lyingon ground truthlabels for training. Thisis
ctionwithnorelianceonsupervisedfine-tuning.
ability,practicaldeploymentsofteninvolvethresh-
ociatedwithhigheruncertainty. Toassessthis,we
gfrom80%to100%confidence. Fig.2(b)presents
odsevaluatedviaAURACperformance. Consis-
oreuncertainoutputsarefilteredout. Particularly
esemanticentropy,naiveentropy,andsupervised
nessinpruningpredictionsthatareassociatedwith
resultsatthedatasetlevelforTrivia-QA,SQuAD,
hallucinationdetectionbenchmarksfull-precision
elyonquantization(e.g.,16-bit,8-bit,or4-bit)to
n. However,quantizationdoesnotmerelycompress
istributions and modifies token-level uncertainty.
rements and alter the relative competitiveness of
udy of hallucination detection must examine the
levels,ensuringthatperformancegainsarenotan
aldeploymentregimes. Figure3presentspairwise
hodsunderdifferentquantizationsettings(16-bit,
dAURACmetrics. TheresultsshowthatSRE-UQ
n levels, consistently outperforming or matching
on.
UQmaximizationconsistentlyoutperformsalterna-
4-bitmodels. Whilequantizationslightlyreduces
nty calibration, the relative ordering of methods
oposedapproachisresilienttoreducedprecision.
wherelow-bitquantizationisincreasinglyadopted
sitiveapplications.
adjustmentsproposedinequation7,weanalyze
rationdiversity. Here,wemeasurethenumberof
whenansweringthesamequestionmultipletimes
8

=== Page 9 ===
PublishedasaconferencepaperatICLR2026
(a)PairwiseAUROC-basedwinrate
(b)PairwiseAURAC-basedwinrate
Figure3: EvaluationofSOTAhallucinationdetec
This summarizes 116 experimental scenarios. E
underadifferentquantizationprecision(16-bit,8
outperformsbaselinesacrossbothAUROCandAU
detectionagainstreducedprecision.
(10times). AsshowninFig.4,thex-axisreprese
valueof0onthex-axisindicatesthatallsampled
entropyiszero. Thisanalysisdirectlyprobeshowm
undertheproposedadjustmentmechanism.
While much of the prior work in hallucination d
entropy-basedscores,suchapproachesignorehow
historicalentropy. Toaddressthis,weevaluateho
distributes across old entropy bins for each LLM
decision-makingcriterion:insteadofarbitrarilycho
dependingontheregionoftheentropyspectrum.
The results demonstrate that entropy change is n
regimes(closetozero)remainrelativelystable,w
arehighlyvolatile. Thisimpliesthatdecisionrule
brittleandmayfailtocapturethesenuancedbeh
entropyregionswithcautionand,insomecases,
prediction.
5 CONCLUSION
Thisworkpresentsanovelframeworkforhallucin
R√©nyientropyandaquantumTN-basedUQmetho
uncertaintyofsequenceprobabilities. Throughe
LLM architectures, and various quantization pre
thatsemanticentropymeasures,especiallywhenc
significantly improve the detection of hallucinat
maximizeentropywhilepenalizingdeviationswei
consistent gains in AUROC, AURAC and RAC
9

ematricesacrossquantizationlevels.
ematricesacrossquantizationlevels.
ctionmethodsacrossdifferentquantizationlevels.
Each subfigure shows pairwise win rate matrices
8-bit,4-bit). SRE-UQmaximizationconsistently
AURAC,demonstratingrobustnessinhallucination
entstheobservedentropyforagivenquestion. A
dgenerationsweresemanticallyidentical-hence
modeluncertaintyandsemanticvariabilityinteract
detection relies on applying a global threshold to
wmodelbehaviorvariesacrossdifferentrangesof
owthechangeinentropy(normalizeddifference)
M. This perspective provides a more fine-grained
oosingafixedthreshold,onecancalibratedecisions
not uniform across the input space: low-entropy
whileintermediateranges(particularly0.25‚Äì0.75)
esbasedsolelyonabsoluteentropythresholdsare
haviors. Instead,practitionersshouldtreatcertain
defertoauxiliarysignalsbeforecommittingtoa
nationdetectioninLLMswhichleveragessemantic
odthatexplicitlymodelsandaccountsforaleatoric
extensiveevaluationsonseveraldatasets,diverse
ecision (16-bit, 8-bit, and 4-bit), we demonstrate
combinedwithprincipleduncertaintypenalization,
ted outputs. By adjusting output probabilities to
ightedbyuncertainty,theproposedmethodyields
C across 116 experiments, outperforming strong
9

=== Page 10 ===
PublishedasaconferencepaperatICLR2026
Figure4: Signednormalizeddifference(ND)inen
underdifferentquantizationsettings(16-bit,8-bit
errorbarsindicatingstandarderror. Notably,allmo
entropy range, suggesting this is a high-risk reg
multipleconfidentyetsemanticallydivergentansw
baselines,withoutrequiringsupervisedfine-tuning
toselectanswersfromanLLMevenwhenitisha
Incorporatinguncertaintynotonlysharpensthesen
principled answer selection, even under confabu
entropymaximizationwithuncertainty-awarepena
trustworthyoutputs. Furthermore,theuncertaintyq
perturbationtheory,makingitsuitableforscalable
Overall, thisstudyhighlightstheimportanceofu
ingreliablelanguagegeneration,andopensupn
principleswithuncertaintyquantificationtoadvan
Limitations Whiletheproposedmethodcombini
TN-basedUQshowsgreatpromiseindetectinghall
ourevaluationsareconductedusingrelativelysma
constraints. Whilethesemodelsenableefficient
behavioroflargerfrontiermodelssuchasGPT-4or
accuracyandthecomplexityofhallucinationsobse
semanticclusteringreliesonanexternalentailmen
andanyinaccuraciesinthisentailmentmodelmay
proposedapproachrequiresaccesstotoken-level
open-weightmodels;black-boxLLMs,wheresuc
scopeofthismethod.
The uncertainty of TS probabilities in equation
definitionofanuncertaintyassociatedwitheach
(cid:80)
onecouldemployUQ(C |y)=(1/|C |)
j j s(r)‚àà
semantic clusters whose membership is more se
turnmayleadtopracticalguidelinesforidentifyi
supplementedwithcareful,perhapshuman-assiste
researchinthefuture.
ACKNOWLEDGMENTS
ThisworkisbasedonresearchsupportedbyNatio
10

ntropyacrossoldentropybinsforLlama-2variants
t,4-bit). BarsrepresentthemeansignedNDwith
odelsexhibitthelargestvariabilityinthe0.25‚Äì0.50
gion where models frequently oscillate between
wers.
g. Additionally,thisprovidesaprincipledapproach
allucinating.
nsitivityofentropy-basedmetricsbutalsoenables
ulation. By adjusting TS probabilities to balance
alization,ourmethodpromotesmorecoherentand
quantificationmethodoperatesutilizingQTNsand
e,explainablereal-worlddeployments.
uncertainty-awaresemanticevaluationinachiev-
newavenuesforintegratinginformation-theoretic
ncetherobustnessofLLMs.
ingquadraticsemanticR√©nyientropyandquantum
lucinations,afewlimitationsremain.Mostnotably,
alllanguagemodelsduetocomputationalresource
experimentation, theymaynotfullycapturethe
rClaude.Consequently,boththeabsolutedetection
ervedcoulddifferatlargerscales. Furthermore,our
ntpredictor(DeBERTa-largefine-tunedonMNLI),
ypropagateintotheentropyestimates. Finally,the
lprobabilities,whichrestrictsitsapplicabilityto
chinformationisnotexposed,remainoutsidethe
6 may very well serve to arrive at a meaningful
semanticcluster. Forinstance,fortheclusterC ,
j
UQ(p(r)). Thismayfacilitateonetoidentify
‚ààCj s
ensitive to changes in TS probabilities which in
ingsituationswhereautomatedoutputsshouldbe
ed,review. Wehopetoundertakethisdirectionof
onalScienceFoundation(NSF)awardxxxxxx.
0

=== Page 11 ===
PublishedasaconferencepaperatICLR2026
REFERENCES
N.Aronszajn. Theoryofreproducingkernels. Tra
68(3):337‚Äì404,May1950.
AmosAzariaandTomMitchell. Theinternalstat
arXiv:2304.13734,2023.
GermanEBerrios. Confabulations: aconceptualh
7(3):225‚Äì241,1998.
MarkChen, JerryTworek, HeewooJun, Qiming
Kaplan,HarriEdwards, YuriBurda, Nicholas
languagemodelstrainedoncode. arXivpreprin
C.Cohen-Tannoudji,B.Diu,andF.Lalo√´. Quan
Spin,andApproximationMethods. Wiley,New
Kyle Cox, Jiawei Xu, Yikun Han, Rong Xu, Tia
Gerych,andYingDing.Mappingfrommeaning:
languagemodels. InProceedingsoftheAAAIC
23696‚Äì23703,2025.
OwainEvans,OwenCotton-Barratt,LukasFinnve
Righetti,andWilliamSaunders. Truthfulai: De
preprintarXiv:2110.06674,2021.
SebastianFarquhar,JannikKossen,LorenzKuhn
languagemodelsusingsemanticentropy. Natur
XiangGao,JiaxinZhang,LallaMouatadid,andKa
quantificationforlargelanguagemodels. arXiv
Z.Ghahramani.Probabilisticmachinelearningand
2015.
AaronGrattafiori,AbhimanyuDubey,AbhinavJa
Al-Dahle,AieshaLetman,AkhilMathur,AlanS
models. arXivpreprintarXiv:2407.21783,2024
DavidJHandandRobertJTill. Asimplegenerali
classclassificationproblems. Machinelearning
P.Harrem√∂es. JointrangeofR√©nyientropies. Kyb
ReillyHaskinsandBenjaminAdams. Keaexplain:
analysis. InInternationalConferenceonNeur
2025.
RuiningHe,AnirudhRavula,BhargavKanagal,an
residualattention. arXivpreprintarXiv:2012.11
MohammadHosseini,CatherineAGao,DavidML
YuanLuo,NganMacDonald,KristiLHolmes,a
chatgptineducation,healthcare,andresearch. P
E.T.Jaynes. Informationtheoryandstatisticalm
May1957.
E.T.Jaynes. ProbabilityTheory: TheLogicofSc
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Y
AndreaMadotto,andPascaleFung. Surveyofh
computingsurveys,55(12):1‚Äì38,2023.
11

ansactionsoftheAmericanMathematicalSociety,
teofanllmknowswhenit‚Äôslying. arXivpreprint
history. JournaloftheHistoryoftheNeurosciences,
gYuan, HenriquePondeDeOliveiraPinto, Jared
Joseph, GregBrockman, etal. Evaluatinglarge
ntarXiv:2107.03374,2021.
ntumMechanics,Volume2: AngularMomentum,
wYork,NY,1977.
anhao Li, Chi-Yang Hsu, Tianlong Chen, Walter
:Addressingthemiscalibrationofprompt-sensitive
ConferenceonArtificialIntelligence,volume39,pp.
eden,AdamBales,AvitalBalwit,PeterWills,Luca
evelopingandgoverningaithatdoesnotlie. arXiv
n,andYarinGal. Detectinghallucinationsinlarge
re,630(8017):625‚Äì630,2024.
amalikaDas. Spuq: Perturbation-baseduncertainty
vpreprintarXiv:2403.02509,2024.
dartificialintelligence.Nature,521(7553):452‚Äì459,
auhri,AbhinavPandey,AbhishekKadian,Ahmad
Schelten,AlexVaughan,etal. Thellama3herdof
4.
isationoftheareaundertheroccurveformultiple
g,45(2):171‚Äì186,2001.
bernetika,45(6):901‚Äì911,2009.
: Explanationsofhallucinationsusinggraphkernel
rosymbolicLearningandReasoning.,volume37,
ndJoshuaAinslie. Realformer: Transformerlikes
1747,2020.
Liebovitz,AlexandreMCarvalho,FarazSAhmad,
andAbelKho. Anexploratorysurveyaboutusing
Plosone,18(10):e0292216,2023.
mechanics. ThePhysicalReview,106(4):620‚Äì630,
cience. CambridgeUniversityPress,2003.
Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,
hallucinationinnaturallanguagegeneration. ACM
1

=== Page 12 ===
PublishedasaconferencepaperatICLR2026
FengqingJiang. Identifyingandmitigatingvulne
thesis,UniversityofWashington,2024.
Mingjian Jiang, Yangjun Ruan, Prasanna Sattig
Graph-baseduncertaintymetricsforlong-form
InformationProcessingSystems,37:32980‚Äì330
MandarJoshi,EunsolChoi,DanielSWeld,andL
supervisedchallengedatasetforreadingcompre
SauravKadavath,TomConerly,AmandaAskell,T
Schiefer,ZacHatfield-Dodds,NovaDasSarma,E
knowwhattheyknow. arXivpreprintarXiv:220
JeanKaddour, JoshuaHarris, MaximilianMozes
McHardy.Challengesandapplicationsoflargela
2023.
SungminKang,YavuzFarukBakman,DuyguNu
timehr. Uncertaintyquantificationforhallucina
tions,methodology,andfuturedirections. arXiv
Jannik Kossen, Jiatong Han, Muhammed Razza
Semantic entropy probes: Robust and cheap
arXiv:2406.15927,2024.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia
Alberti,DanielleEpstein,IlliaPolosukhin,Jaco
benchmarkforquestionansweringresearch. Tr
Linguistics,7:453‚Äì466,2019.
ZeqiangLai,XizhouZhu,JifengDai,YuQiao,an
imagebypromptinglargelanguagemodels. arX
KentonLee,Ming-WeiChang,andKristinaTouta
domainquestionanswering. arXivpreprintarX
MinhyeokLee. Amathematicalinvestigationofh
matics,11(10):2320,2023.
DerenLei,YaxiLi,MengyaHu,MingyuWang,Vi
ofnaturallanguageinferenceforreducinglarge
preprintarXiv:2310.03951,2023.
JunyiLi,XiaoxueCheng,WayneXinZhao,Jian-Y
hallucinationevaluationbenchmarkforlargelan
2023.
Stephanie Lin, Jacob Hilton, and Owain Evans.
words. arXivpreprintarXiv:2205.14334,2022.
P.Linardatos,V.Papastefanopoulos,andS.Kotsian
interpretabilitymethods. Entropy,23(18),2021
Dennis V Lindley. On a measure of the informa
MathematicalStatistics,27(4):986‚Äì1005,1956.
Potsawee Manakul, Adian Liusie, and Mark JF
hallucinationdetectionforgenerativelargelan
2023.
DangNguyen,AliPayani,andBaharanMirzaso
uncertaintyquantificationwithpairwiseseman
2025.
12

erabilitiesinllm-integratedapplications. Master‚Äôs
geri, Salim Roukos, and Tatsunori B Hashimoto.
languagemodelgenerations. AdvancesinNeural
006,2024.
LukeZettlemoyer. Triviaqa: Alargescaledistantly
ehension. arXivpreprintarXiv:1705.03551,2017.
TomHenighan,DawnDrain,EthanPerez,Nicholas
EliTran-Johnson,etal. Languagemodels(mostly)
07.05221,2022.
s, HerbieBradley, RobertaRaileanu, andRobert
anguagemodels.arXivpreprintarXiv:2307.10169,
urYaldiz,BaturalpBuyukates,andSalmanAves-
ationdetectioninlargelanguagemodels: Founda-
vpreprintarXiv:2510.12040,2025.
ak, Lisa Schut, Shreshth Malik, and Yarin Gal.
hallucination detection in llms. arXiv preprint
Redfield, Michael Collins, Ankur Parikh, Chris
obDevlin,KentonLee,etal. Naturalquestions: a
ransactionsoftheAssociationforComputational
ndWenhaiWang. Mini-dalle3: Interactivetextto
XivpreprintarXiv:2310.07653,2023.
anova. Latentretrievalforweaklysupervisedopen
Xiv:1906.00300,2019.
hallucinationandcreativityingptmodels. Mathe-
incentYun,EmilyChing,andEslamKamal. Chain
languagemodelungroundedhallucinations. arXiv
YunNie,andJi-RongWen. Halueval: Alarge-scale
nguagemodels. arXivpreprintarXiv:2305.11747,
Teaching models to express their uncertainty in
.
ntis. ExplainableAI:Areviewofmachinelearning
1. doi: 10.3390/e23010018.
ation provided by an experiment. The Annals of
.
Gales. Selfcheckgpt: Zero-resource black-box
nguagemodels. arXivpreprintarXiv:2303.08896,
oleiman. Beyondsemanticentropy: Boostingllm
nticsimilarity. arXivpreprintarXiv:2506.00245,
2

=== Page 13 ===
PublishedasaconferencepaperatICLR2026
Alexander Nikitin, Jannik Kossen, Yarin Gal, a
Fine-graineduncertaintyquantificationforllm
InformationProcessingSystems,37:8901‚Äì8929
E. Parzen. On estimation of a probability densi
Statistics,33(3):1065‚Äì1076,1962.
ArkilPatel,SatwikBhattamishra,andNavinGoya
wordproblems? arXivpreprintarXiv:2103.071
GuilhermePenedo,QuentinMalartic,DanielHe
HamzaAlobeidli, BaptistePannier, EbtesamA
datasetforfalconllm: outperformingcuratedc
preprintarXiv:2306.01116,2023.
J.C.Principe. Informationtheoreticlearning: Ren
R.Novak,andB.Sch√∂lkopf(eds.),Information
2010.
X.-L.QiandD.Ranard. DeterminingalocalHami
5995‚Äì6000,July2019. doi: {10.22331/q-2019-
XinQiuandRistoMiikkulainen. Semanticdensi
models through confidence measurement in se
processingsystems,37:134507‚Äì134533,2024.
ErnestoQuevedo,JorgeYeroSalazar,RachelKo
hallucinations in large language model gener
Congress in Computer Science, Computer En
Springer,2024.
Miriam Rateike, Celia Cintas, John Wamburu,
superviseddetectionofhallucinationsinllmac
C.Rudin. Stopexplainingblackboxmachinele
interpretablemodelsinstead. NatureMachineI
B. Sch√∂lkopf, K. Muandet, K. Fukumizu, S. Ha
randomvariablesviareproducingkernelHilber
25:755‚Äì766,June2015.
KajetanSchweighofer,LukasAichberger,Mykyta
theoreticmeasuresofpredictiveuncertainty. ar
R.SinghandJ.Principe. Timeseriesanalysisusin
positionframework. InConferenceonUncertain
2020.
R.SinghandJ.C.Principe. Towardakernel-base
andmodels. NeuralComputation,33:1164‚Äì119
R.Singh,Y.Ma,andJ.C.Principe. Findinglocal
moments and optimal transport. In Proc. Inte
(IJCNN),Yokohama,Japan,June/July2024. do
RichardSocher,EricHuang,JeffreyPennin,Chr
pooling and unfolding recursive autoencoders
informationprocessingsystems,24,2011.
StanfordNLPGroup. SQuAD:Thestanfordquest
github.io/SQuAD-explorer/. [Accesse
Weihang Su, Changyue Wang, Qingyao Ai, Yira
Unsupervised real-time hallucination detectio
models. arXivpreprintarXiv:2403.06448,2024
13

and Pekka Marttinen. Kernel language entropy:
msfromsemanticsimilarities. AdvancesinNeural
9,2024.
ity function and mode. Annals of Mathematical
al. Arenlpmodelsreallyabletosolvesimplemath
191,2021.
esslow,RuxandraCojocaru,AlessandroCappelli,
Almazrouei, andJulienLaunay. Therefinedweb
corporawithwebdata,andwebdataonly. arXiv
nyi‚Äôsentropyandkernelperspectives. InM.Jordan,
nScienceandStatistics.Springer,NewYork,NY,
iltonianfromasingleeigenstate. Quantum,3(159):
-07-08-159}.
ity: Uncertaintyquantificationforlargelanguage
emantic space. Advances in neural information
oerner,PabloRivas,andTomasCerny. Detecting
ration: A token probability approach. In World
ngineering & Applied Computing, pp. 154‚Äì173.
Tanya Akumu, and Skyler Speakman. Weakly
ctivations. arXivpreprintarXiv:2312.02798,2023.
earningmodelsforhighstakesdecisionsanduse
Intelligence,1:206‚Äì215,May2019.
armeling, and J. Peters. Computing functions of
rtspacerepresentations. StatisticsandComputing,
aIelanskyi,andSeppHochreiter. Oninformation-
rXivpreprintarXiv:2410.10786,2024.
ngakernelbasedmulti-modaluncertaintydecom-
ntyinArtificialIntelligence(UAI),pp.1368‚Äì1377,
eduncertaintydecompositionframeworkfordata
98,2021. doi: {10.1162/neco_{a_01372}}.
ldependentregionsinpdfsusingrkhsuncertainty
ernational Joint Conference on Neural Networks
oi: 10.1109/IJCNN60899.2024.10651130.
ristopherDManning,andAndrewNg. Dynamic
s for paraphrase detection. Advances in neural
tionansweringdataset. https://rajpurkar.
ed28Apr2025].
an Hu, Zhijing Wu, Yujia Zhou, and Yiqun Liu.
on based on the internal states of large language
4.
3

=== Page 14 ===
PublishedasaconferencepaperatICLR2026
Yi Tay, Vinh Q Tran, Sebastian Ruder, Jai Gup
SimonBaumgartner,CongYu,andDonaldMet
gradient-basedsubwordtokenization. arXivpre
HugoTouvron,LouisMartin,KevinStone,PeterA
Bashlykov,SoumyaBatra,PrajjwalBhargava,
andfine-tunedchatmodels. arXivpreprintarX
PragatheeswaranVipulanandan,KamalPremarat
quantification in regression neural networks:
ProceedingsoftheInternationalSymposiumon
FortLauderdale,FL,USA,2026.
Pragatheeswaran Vipulananthan, Kamal Premar
quantumtensornetwork-basedviewpointform
IEEEInternationalConferenceonKnowledge
December2024. doi: 10.1109/ICKG63256.202
XuezhiWang,JasonWei,DaleSchuurmans,Quoc
ery,andDennyZhou. Self-consistencyimprove
arXivpreprintarXiv:2203.11171,2022.
AdinaWilliams,NikitaNangia,andSamuelRB
sentenceunderstandingthroughinference. arXi
TimZXiao,AidanNGomez,andYarinGal. Wa
withvariationaltransformers. arXivpreprintar
LeiYu,KarlMoritzHermann,PhilBlunsom,andS
selection. arXivpreprintarXiv:1412.1632,201
YueZhang,YafuLi,LeyangCui,DengCai,Lem
YuZhang,YulongChen,etal. Siren‚Äôssongin
languagemodels. arXivpreprintarXiv:2309.01
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Ta
Beichen Zhang, Junjie Zhang, Zican Dong, et
preprintarXiv:2303.18223,1(2),2023.
14

pta, Hyung Won Chung, Dara Bahri, Zhen Qin,
tzler. Charformer: Fastcharactertransformersvia
eprintarXiv:2106.12672,2021.
Albert,AmjadAlmahairi,YasmineBabaei,Nikolay
ShrutiBhosale,etal. Llama2: Openfoundation
Xiv:2307.09288,2023.
tne,andDilipSarkar. Sensitivityanduncertainty
A quantum tensor network based approach. In
nArtificialIntelligenceandMathematics(ISAIM),
ratne, Dilip Sarkar, and Manohar N. Murthi. A
modelingandanalysisoftimeseriesdata. In2024
Graphs(ICKG),pp.378‚Äì387,AbuDhabi,UAE,
24.00054.
cLe,EdChi,SharanNarang,AakankshaChowdh-
eschainofthoughtreasoninginlanguagemodels.
Bowman. Abroad-coveragechallengecorpusfor
ivpreprintarXiv:1704.05426,2017.
atzeije? detectingout-of-distributiontranslations
rXiv:2006.08344,2020.
StephenPulman.Deeplearningforanswersentence
14.
maoLiu,TingchenFu,XintingHuang,EnboZhao,
ntheaiocean: asurveyonhallucinationinlarge
1219,2023.
Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,
t al. A survey of large language models. arXiv
4

=== Page 15 ===
PublishedasaconferencepaperatICLR2026
A EXPANDED VERSION OF SECTION 2
A.1 SEMANTICEQUIVALENCE-BASEDCLUST
Computation of semantic entropy requires clust
TSsineachclusteraresemanticallyequivalent,w
dissonant. Toidentifysuchsemanticallyequivalen
entailment,anotionthathasattractedconsiderab
We,usepretrainedmodelssuchasDeBERTa-Larg
GPT-3.5toassessentailmentbetweensentencepai
bidirectionalentailmentwitharepresentativeTSfr
ofthese,anewclusteriscreated. Thisprocessofs
computedfortheclustersassociatedwitheachque
thattheLLMishallucinatingwhenrespondingto
thisprocessinFarquharetal.(2024).
However,semanticentropyitselfisnotareliablem
probabilitiescanbehighlysensitivetomodelpert
with which hallucinatory behavior is deemed to
information(asopposedtoglobalmetrics)thatallo
eachLLMgeneration.
A.2 QUANTUMTN-BASEDUQMETHODFOR
Physics-inspiredmethodsoffersuchlocaluncertain
wayforUQofdeeplearningmodels(Singh&Prin
thesemethodsaretetheredtotheHamiltonianof
henceseverelyconstrainedintheirapplication. In
(2024)introducesadeterministic,interpretable,a
Hamiltonian. Moreover,itleveragesthewell-estab
multi-resolution,localuncertaintyestimatesthata
Inourwork,weexpandthisQTN-basedframework
the domain of semantic datasets, where TS prob
signal-likeprocesses. Thisextensionallowsusto
throughthesameprincipled,physics-inspiredlen
first, it bridges the gap between temporal signal
thattechniquesrootedinquantumphysicscange
domains. Second,bybringinginterpretabilityan
studyofsemanticentropy,ourapproachlaystheg
AIsystems,especiallyinhigh-stakesapplicationsw
makingarecritical. Fig.5showstheQTN-basedU
A.3 RATIONALEFORADOPTINGSEMANTICR
While SE is widely used for characterizing unc
introducelimitationsinthepresentsetting. Forany
thanorequaltoquadraticSRE(Harrem√∂es(2009
determinetherelativespreadofthetwomeasuresa
SEexhibitsalargervariationinmostcases(betwe
inourexperiments,whichcorrelateswithslightly
(Hand&Till(2001)). SEalsoemphasizesrarerout
inAURAC(seeFig.2(b))whichmeasureshoww
correctness.
Amoredecisiveconsiderationarisesfromthestruc
UQframework. QuadraticR√©nyientropyisaseco
distribution and is directly estimated by the KM
encodedasaneigen-modeofaQTNHamiltonian
first-orderandhigher-orderspectralcorrectionstha
15

2
TERING
tering LLM-generated output TSs, such that the
whilethoseindifferentclustersaresemantically
ntTSs,Farquharetal.(2024)employbidirectional
bleattentioninnaturallanguageinference(NLI).
ge-MNLIorgeneral-purposelanguagemodelslike
irs. WhenconfrontedwithanewTS,wecheckfor
romeachcluster. Ifnoentailmentisfoundwithany
semanticclusteringallowsasemanticentropytobe
estion. Higherentropyindicateshigherlikelihood
othatquestion. ThetoprungofFig.1summarizes
measureforidentifyinghallucinationsbecauseTS
turbations. Thiswarrantsadjustingtheconfidence
o be present or absent. We therefore need local
owsustoassesssensitivityoftheTSprobabilityof
RDEEPLEARNINGMODELS
ntyinformationandhaveemergedasacompelling
ncipe(2020;2021);Singhetal.(2024)). However,
fthequantumharmonicoscillator(QHO)andare
ncontrast,therecentworkinVipulananthanetal.
andsingle-shotUQmethodbuiltonaQTN-based
blishedmethodofperturbationtheorytoferretout
areideallysuitedforourcurrentpurpose.
k‚Äîoriginallydevelopedfortime-seriessignals‚Äîto
bability distributions can be naturally viewed as
ocapturesemanticuncertaintyinnaturallanguage
ns. Theimportanceofthiscontributionistwofold:
analysis and semantic modeling, demonstrating
eneralizebeyondtime-seriestocomplexlanguage
ndmulti-resolutionuncertaintyestimatesintothe
groundworkformoretrustworthyandexplainable
wherehallucinationdetectionandcarefuldecision-
UQmethodthatweleverageinourwork.
R√âNYIENTROPY
certainty in LLM outputs, its inherent properties
yfixedpredictivedistribution,SEisalwaysgreater
9)),butthispointwiseorderingdoesnotingeneral
acrossacorpusofexamples. Empirically,however,
eenhallucinatoryandnon-hallucinatorybehavior)
ybetterAUROCperformanceofSE(seeFig.2(a))
tcomes. Butthetailprobabilitiesdonotplayarole
wellincreasingentropycorrespondstodecreasing
cturalcompatibilitybetweenSREandtheproposed
ond-orderfunctionaloftheunderlyingprobability
ME. This correspondence enables the KME to be
n. PerturbationsofthisHamiltonian,inturn,yield
atquantifylocalsensitivityintheamplitudedomain.
5

=== Page 16 ===
PublishedasaconferencepaperatICLR2026
FormTensor
Network
H(cid:98)={H(cid:98)k }T
k=
‚àí
0
1
Sampling
Compute
œà(cid:98)={œà(cid:98)(y
j
)}M
j=
‚àí
0
1 QCM M
œà(cid:98)
(œà(cid:98))
Uncertainty
Boundsofœà(cid:98)
k
x Perturbation
Analysis
{œà(cid:98) ( (cid:96) k) }M (cid:96)= ‚àí 0 1, k‚â•0 {œà(cid:98) (cid:96) }M (cid:96)= ‚àí 0 1
Figure5: OverviewoftheQTN-basedUQpipeli
computing the quantum correlation matrix (QCM
spacevectors. Theseareusedtoconstructthelo
the foundation for spectral and perturbation ana
uncertainty feature vectors, which are organized
samplingacrossmodesproducesuncertaintyboun
quantificationofsemanticentropy.
Suchaperturbation-basedformulationofuncertai
correspondingRKHS-basedrepresentation.
Although SE achieves marginally better separab
the empirical performance gap relative to SRE i
tagesfacilitatedbySRE‚Äînamely, itsdirectalign
QTN-based spectral analysis, and its ability to su
quantification‚Äîaresubstantiallymoreconsequen
appropriateentropymeasurefortheproposedfram
A.4 R√âNYIENTROPYANDKERNELMEANEM
To provide the essentials of the work in Vipulan
entropy H(X) = ‚àílog(œà(X)) of the continuo
(2010). Here, œà(X) = (cid:82) p2(x)dx = E[p(x)],
samples{x }N fromX,thenon-parametricParz
i i=1
p(x)=
(cid:80)N
Œ∫ (x;x )/N,whereŒ∫ (x;x )=(1
(cid:98) i=1 œÉ i œÉ i
Thisturnsout tobethe(empirical)kernel mean
reproducingkernelHilbertspace(RKHS)determin
(2015)),i.e.,p
(cid:98)
(x) = œà(cid:98)(x). H(X) = ‚àílog(œà(X
takenasameasureofuncertaintyandthe(empiric
dataamplitudevalues.
To get a more interpretable tool for UQ, Vipulan
localnearestneighborrandomcouplingswhoseH
modes. Sufficeittosaythat(a)aspinchainhavin
œà(cid:98) ={œà(cid:98)j }M
j=
‚àí
0
1,whereM =2L,oftheKMEœà(cid:98)(x)
linearcombinationofafinitesetH(cid:98) ={H(cid:98) }T‚àí1o
k k=0
innerproduct)Hermitianoperators,wheretheexte
T;and(c)T,inturn,determineswhethertheset
H(cid:98) = (cid:80)T
k=
‚àí
0
1w
k
H(cid:98)
k
(i.e.,H(cid:98) ‚ààspan(H(cid:98)))tohaveœà(cid:98)
ToidentifysuchaHamiltonian,thequantumcorrela
becauseœà(cid:98) isaneigen-modeofH(cid:98) iffwbelongsin
isthelinearcombinationweightvectorthatgene
resultsandsupportinglemmasrelatedtotheQCM
16

Compute Get
NullSpace NullSpace
ofQCM N(M (œà(cid:98))) Vector
œà(cid:98)
w‚ààN(M (œà(cid:98)))
œà(cid:98)
Spectral Compute
Analysis Local
T (cid:88) ‚àí1 Hamiltonian
H(cid:98) = w
k
H(cid:98)k
k=0
ineusedinourwork. Theprocedurebeginswith
M) from TS probabilities and extracting its null-
ocalHamiltonianH(cid:98),whoseeigen-modesprovide
alysis. First-order perturbation corrections yield
d into a tensor network representation. Finally,
ndsonprobabilityamplitudes,enablinguncertainty
intycannotbeobtainedfromSEbecauseitlacksa
bility in AUROC due to its wider dynamic range,
is small. In contrast, the methodological advan-
nmentwithRKHStheory, itscompatibilitywith
upport principled perturbation-based uncertainty
ntial. Forthesereasons,SREconstitutesthemost
mework.
MBEDDING(KME)OFDATAPDF
nanthan et al. (2024), take the R√©nyi (quadratic)
ous random variable X with PDF p(x) Principe
where E[(cid:5)] is the expectation operator. With N
zenGaussiandensityestimatorofthePDFp(x)is
‚àö
1/ 2œÄœÉ2) exp(‚àí(x‚àíx )2/2œÉ2)Parzen(1962).
i
embedding(KME)œà(cid:98)(x)ofthedata PDFinthe
nedbyŒ∫ ((cid:5);(cid:5))(Aronszajn(1950);Sch√∂lkopfetal.
œÉ
X))beinganentropymeasure,thescalarœà(cid:98)(X)is
cal)KMEœà(cid:98)(x)viewedashowitvarieswithx,the
nanthan et al. (2024) seeks a finite QTN having
HamiltonianhasthisKMEœà(cid:98)(x)asoneofitseigen-
ngLspinparticlescanrepresentasampledversion
);(b)theHamiltonianoftheTNtakestheformofa
ofpairwiseorthonormal(w.r.t. theHilbert-Schmidt
entoflocalnearestneighborinfluencedetermines
H(cid:98)is‚Äòrich‚ÄôenoughforaHamiltonianofthetype
(cid:98)œàasaneigen-mode.
ationmatrix(QCM)M (H(cid:98))(Qi&Ranard(2019))
œà(cid:98)
nthenullspaceoftheQCM.Here,w ={w }T‚àí1
k k=0
eratesH(cid:98) = (cid:80)T
k=
‚àí
0
1w
k
H(cid:98)
k
. Additionaltheoretical
MformulationareprovidedinAppendixA.5.
6

=== Page 17 ===
PublishedasaconferencepaperatICLR2026
WiththisHamiltonianinhand,thewell-establishe
associatedtime-independentSchr√∂dingerequation.
alltheeigen-modes/energies(includingtheKME
perturbation(Cohen-Tannoudjietal.(1977)). The
resolution of those data regions for which less i
tailsofthePDF)(Singh&Principe(2021));sens
correctionstotheeigen-modes. ThisistheUQm
onetosurveyaleatoricuncertaintieslocallyandat
spectralpropertiesofQTNs.
A.5 SOMEPROPERTIESOFTHEQCM
Inthissection,wepresentaseriesofresultsrelated
ourwork,discussedinSection2.1.
NotethatH(cid:98) ={H(cid:98) }denotesafiniteset{H(cid:98) , i=
i i
operators. Forconvenience,wewilluse(H(cid:98))
w
to
operatorsinH(cid:98),i.e.,
T‚àí1
(cid:88)
(H(cid:98))
w
= w
i
H(cid:98)
i
, whe
i=0
Lemma1 TheQCMM (H)={(M (H)) }as
v v ij
Proof. FromDefinition,wehave
(M
v
(H))
ji
=0.5‚ü®{H(cid:98)
j
,H(cid:98)
i
}a
v
‚àí
=0.5‚ü®{H(cid:98)
i
,H(cid:98)
j
}a
v
‚àí
because{H(cid:98)
i
,H(cid:98)
j
}={H(cid:98)
j
,H(cid:98)
i
}andboth‚ü®H(cid:98)
i
a
v
a
Inaddition,
(M (H))‚àó =0.5‚ü®{H(cid:98) ,H(cid:98) }a‚àó‚àí‚ü®H(cid:98) a‚àó¬∑‚ü®H(cid:98)
v ij i j v i v
=0.5‚ü®v |{H(cid:98) ,H(cid:98) }|va‚àó‚àí‚ü®v |
i j
=0.5‚ü®v |H(cid:98) H(cid:98) |va‚àó+0.5‚ü®v |
i j
Butwenotethat
‚ü®v |H(cid:98) H(cid:98) |va‚àó =‚ü®H(cid:98) v |H(cid:98) va‚àó =
i j i j
‚ü®v |H(cid:98) |va‚àó =‚ü®v |H(cid:98) va‚àó =
i i
In a similar manner, we can also show that ‚ü®v |
va‚àó = ‚ü®v | H(cid:98) | va. When substituted into th
j
(M (H))‚àó =(M (H)) . So,M isreal.
v ij v ij
2
Lemma2 The variance Var(H(cid:98))
œà
= ‚ü®H(cid:98) a
œà
‚àí
arbitrarynormalizedvectorœà, ‚ü®œà |œàa=1,satis
2
Var(H(cid:98))
œà
=‚ü®H(cid:98) a
theequalityholdsiffœàisaneigen-modeofH(cid:98).
Proof. ForarbitraryŒ±‚ààCandœà,weknowthat
0‚â§‚ü®H(cid:98) œà‚àíŒ±œà
Furthermore,theequalityholdstrueiffH(cid:98) œà‚àíŒ±œà
17

edmethodofperturbationtheoryisappliedtothe
. Thisyieldsfirst-andhigherorder‚Äòcorrections‚Äôto
E)whentheunderlyingHamiltonianundergoesa
ehigherordermodesprovidebetterdiscriminative
information is available (these correspond to the
sitivityanalysisandUQcanbecarriedoutviathe
methodinVipulananthanetal.(2024)thatallows
tdifferentlevelsofresolutionusingalgebraicand
dtotheQCMthatformthetheoreticalbackboneof
=0,...,T ‚àí1}ofpairwiseorthonormalHermitian
odenotea(real-valued)linearcombinationofthe
erew ={w }T‚àí1 ‚ààRT. (8)
i i=0
ssociatedwithH(cid:98)isrealandsymmetric.
‚àí‚ü®H(cid:98)
j
a
v
¬∑‚ü®H(cid:98)
i
a
v
‚àí‚ü®H(cid:98)
i
a
v
¬∑‚ü®H(cid:98)
j
a
v
=(M
v
(H))
ij
,
and‚ü®H(cid:98)
j
a
v
arescalars. So,M issymmetric.
(cid:98)H a‚àó
j v
|H(cid:98) |va‚àó¬∑‚ü®v |H(cid:98) |va‚àó
i j
|H(cid:98) H(cid:98) |va‚àó‚àí‚ü®v |H(cid:98) |va‚àó¬∑‚ü®v |H(cid:98) |va‚àó
j i i j
=‚ü®H(cid:98) v |H(cid:98) va=‚ü®v |H(cid:98) H(cid:98) |va;
j i j i
=‚ü®H(cid:98) v |va =‚ü®v |H(cid:98) |va.
i i
H(cid:98) H(cid:98) | va‚àó = ‚ü®v | H(cid:98) H(cid:98) | va‚àó and ‚ü®v | H(cid:98) |
j i i j j
he expression above for (M (H))‚àó, we see that
v ij
|‚ü®H(cid:98)a
œà
|2 of the Hermitian operator H(cid:98) w.r.t. an
sfies
a
œà
‚àí|‚ü®H(cid:98)a
œà
|2 ‚â•0;
œà, H(cid:98) œà‚àíŒ±œàa.
œà =0,i.e.,iffœàisaneigen-modeofH(cid:98).
7

=== Page 18 ===
PublishedasaconferencepaperatICLR2026
So,letusconsiderthecasewhenœàisnotaneigen
0<‚ü®H(cid:98) œà‚àíŒ±œà, H(cid:98) œà‚àíŒ±œàa=‚ü®H(cid:98) œà |H(cid:98) œàa
=‚ü®H(cid:98) œà |H(cid:98) œàa‚àíŒ±‚ü®H(cid:98) œà |œàa‚àíŒ±‚àó‚ü®œà |H(cid:98)
SinceŒ±isarbitrary,select
Œ±=‚ü®œà |H(cid:98) œàa =‚áí
Substituteintotheaboveinequality:
2
0<‚ü®œà |H(cid:98) |œàa‚àí|‚ü®œà |H(cid:98) œàa|2
2
=‚ü®œà |H(cid:98) |œàa‚àí|‚ü®œà |H(cid:98) |œàa
Thisestablishestheclaim.
Thefollowingresultfollowsimmediately:
Corollary3 Given the Hermitian operator H(cid:98)
Var(H(cid:98))
œà
=0iffœàisaneigen-modeofH(cid:98).
Corollary4 Suppose,forsomearbitraryreal-valu
followingaretrue:
(i) Forarbitraryv ‚ààRT,M (H)isp.s.d.
v
(ii) w ‚ààN (M (H)),thenullspaceoftheQ
v
va=1,isaneigen-modeofH(cid:98).
Proof.
(i) ThisfollowsdirectlyfromLemma2and
(ii) First,supposew ‚ààN (M (H))sothat
v
M (H)w =0 =‚áí ‚ü®M
v
Then,Var(H(cid:98))
v
=‚ü®M
v
(H)a
w
=0(Qi&
isaneigen-modeofH(cid:98).
Conversely, suppose ‚ü®M (H)a = wT
v w
M (H)isrealandsymmetric. LetitsSV
v
M (H
v
whereU ‚ààRT√óT isunitaryandŒ£‚ààRT
non-negativesingularvaluesofM (H).
v
wTM (H)w =wTM (H)1/2T M (
v v v
meaningthatwemusthave
M (H)1/2w =
v
i.e.,w ‚ààN (M (H)).
v
A.6 ONTHENULLSPACEOFTHEQCM:THE
We now study a special case where the QCM h
investigatehowsmallperturbationscanintroduce
18

n-modeofH(cid:98) sothat
a‚àí‚ü®H(cid:98) œà |Œ±œàa‚àí‚ü®Œ±œà |H(cid:98) œàa+‚ü®Œ±œà |Œ±œàa
œàa+|Œ±|2‚ü®œà |œàa.
‚áí Œ±‚àó =‚ü®H(cid:98) œà |œàa.
2‚àí|‚ü®œà |H(cid:98) œàa|2+|‚ü®œà |H(cid:98) œàa|2
2
a|2 =‚ü®H(cid:98) a
œà
‚àí|‚ü®H(cid:98)a
œà
|2.
and the normalized vector œà, ‚ü®œà | œàa = 1,
uedvectorw ‚àà{w
i
}‚ààRT,H(cid:98) =(H(cid:98))
w
. Thenthe
QCMM (H)associatedwithH(cid:98)w.r.tviffv, ‚ü®v |
v
Qi&Ranard(2019).
M (H)a =wTM (H)w =0.
v w v
&Ranard(2019)). Corollary3thenimpliesthatv
TM (H)w = 0. We know from Lemma 1 that
v
VDbe
H)=UŒ£UT,
√óT isdiagonalwithitsdiagonalentriesbeingthe
LetM (H)1/2 =Œ£1/2UT. Then,
v
(H)1/2w =‚ü®M (H)1/2w,M (H)1/2wa=0,
v v
=0 =‚áí M (H)w =0,
v
EZERODIMENSIONCASE
has full rank (zero-dimensional null space) and
aone-dimensionalnullspace. Understandingthis
8

=== Page 19 ===
PublishedasaconferencepaperatICLR2026
transitionisimportantbecausetheemergenceof
newuncertaintydirections,whicharecriticalforr
ConsiderthecaseintheQCM,whenthedimensio
andwe‚Äòperturb‚ÄôM (H)toM (H‚Ä≤)as
œà‚ôØ œà‚ôØ
M (H‚Ä≤)=M
œà‚ôØ
s.t. dim(N (M (H‚Ä≤)))=1.
œà‚ôØ
Henceforthinthissection,forconvenience,wewi
Recallthat{w ,¬µ }, n‚àà0,1,...,T ‚àí1,denote
n n
¬∑¬∑¬∑‚â§¬µ .
T‚àí1
WenextinvestigatethestabilityoftheQCMunder
theeigen-modes/energiesoftheQCMchangewh
inthecontextwheretheoriginalQCMhasfullran
direction. Weuseclassicalmatrixperturbationtheo
similarityresults.
Let{w‚Ä≤ , ¬µ‚Ä≤ }, n‚àà0,1,...,T ‚àí1,denotetheei
n n
¬µ‚Ä≤ . Letusnowviewtheeigen-pairs{w , Œª }o
T‚àí1 n n
{w‚Ä≤ , Œª‚Ä≤ }ofM‚Ä≤,sothatwemayapplyperturbatio
n n
inperturbationtheory,let
M =M‚Ä≤
whereœµ>0issimplyaplace-holder. Theeigen-p
(cid:40) ‚àû
(cid:88)
{w , Œª }= œµk
n n
k=0
where{w(0), Œª(0)}= ‚àÜ {w‚Ä≤, Œª‚Ä≤ }, ‚àÄn‚â•0. W.l.o.
n n n n
1, ‚àÄn‚â•0.
Nowwecanexpresstheeigen-pairrelationship
M |w a=
n
as
‚àû
(cid:88)
(M‚Ä≤+œµ¬∑Œ¥M)| œµkw(k)a
n
k=0
orequivalently,
‚àû ‚àû
(cid:88) (cid:88)
M‚Ä≤ œµk |w(k)a+Œ¥M œµk |w
n
k=0 k=1
Gathersimilarpowersofœµ: fork ‚â•1,
k‚àí1
(cid:88)
(M‚Ä≤‚àíŒª(0))|w(k)a= Œª(k
n n n
‚Ñì=0
Notethat,withk =0,weget(M‚Ä≤‚àíŒª(0))|w(0)a
n n
Pushthebra‚ü®w(0) |: fork ‚â•1,
m
k‚àí1
(cid:88)
(Œª(0)‚àíŒª(0))‚ü®w(0) |w(k)a= Œª(k‚àí‚Ñì
m n m n n
‚Ñì=0
Putm=ninequation16togettheorder-kcorrec
Œª(k) =‚ü®w(0) |Œ¥M
n n
19

anontrivialnullspacecorrespondstoidentifying
robustdecision-makinginourframework.
onofthenullspaceoftheQCMM (H)iszero
œà‚ôØ
M (H)‚àíŒ¥M (9)
œà‚ôØ
illdenoteM (H)byM andM (H‚Ä≤)byM‚Ä≤.
œà‚ôØ œà‚ôØ
etheeigen-pairsofM orderedas0<¬µ ‚â§¬µ ‚â§
0 1
smallperturbations. Specifically,weexaminehow
henasmallperturbationisintroduced,particularly
nk(nonullspace)andperturbationsinduceanull
orytoderivefirst-orderapproximationsandcosine
igen-pairsofM‚Ä≤orderedas0=¬µ‚Ä≤ <¬µ‚Ä≤ ‚â§¬∑¬∑¬∑‚â§
0 1
ofM asthe‚Äòperturbed‚Äôversionsoftheeigen-pairs
ontheorytoM‚Ä≤. Forthispurpose,asiscustomary
+œµ¬∑Œ¥M, (10)
pairsofM canthenbeexpressedas
‚àû (cid:41)
(cid:88)
kw(k), œµ‚ÑìŒª(‚Ñì) , (11)
n n
‚Ñì=0
.g. wealsoassumethat‚ü®w(0) |w(k)a=0, ‚àÄk ‚â•
n n
=Œª |w a (12)
n n
‚àû ‚àû
(cid:88) (cid:88)
a= œµ‚ÑìŒª(‚Ñì) | œµkw(k)a, (13)
n n
‚Ñì=0 k=0
‚àû k
(cid:88) (cid:88)
w(k)a= œµk Œª(k‚àí‚Ñì) |w(‚Ñì)a. (14)
n n n
k=0 ‚Ñì=0
k‚àí‚Ñì) |w(‚Ñì)a‚àíŒ¥M |w(k‚àí1)a. (15)
n n n
a=0,whichisofcoursetriviallytrue.
‚Ñì)‚ü®w(0) |w(‚Ñì)a‚àí‚ü®w(0) |Œ¥M |w(k‚àí1)a. (16)
m n m n
ctionforŒª(0):
n
|w(k‚àí1)a, k ‚â•1. (17)
n
9

=== Page 20 ===
PublishedasaconferencepaperatICLR2026
FormÃ∏=n,putk =1inequation16toget
‚ü®w
‚ü®w(0) |w(1)a=
m n
whereweassumethatM(0)isnon-degenerate. Sin
correctionforw(0)as
n
‚àû
(cid:88)
|w(1)a= |w(0)a¬∑‚ü®w(0) |w(1)a
n m m n
m=0
Noticethat,since‚ü®w(0) |w(1)a=0,wehave
n n
‚à•w(0)+w(1)‚à•2 =‚ü®w(0) |w(0)a
n n n n
becausew(0)isalreadynormalized. Therefore,de
n
w(0),wehave
n
w(
w(0) = n
(cid:98)n
(1+
where
‚à•w(1)‚à•2 =‚ü®w(1) |w(1)a
n n n
(cid:88) (cid:88) ‚ü®w( m 0) |Œ¥M |w( n 0
=
Œª(0)‚àíŒª(0)
‚ÑìÃ∏=nmÃ∏=n n m
(cid:32)
(cid:88) ‚ü®w( m 0) |Œ¥M |w( n 0)a
=
Œª(0)‚àíŒª(0)
mÃ∏=n n m
Thus,
‚ü®w(0) |w(
‚ü®w(0) |w(0)a= n n
n (cid:98)n
(1+
1
=
(1+‚à•w(1
n
whereweusedtheTaylorseriesexpansion
1 1 1
=1‚àí x+
(1+x)1/2 2 2
forsomec‚àà(0,x),whichinturnimpliesthat
1
(1+x)1/2
because(1+c)‚àí5/2 >0, c‚àà(0,x).
Insummary,wehave
1 (cid:88)
‚ü®w(0) |w(0)a‚â•1‚àí
n (cid:98)n 2
mÃ∏=n
Weknowthat{w(0), Œª(0)}={w‚Ä≤, Œª‚Ä≤ =0}isth
0 0 0 0
wehave
‚ü®w(0) |w(0)a‚â•1‚àí 1 (cid:88)
0 (cid:98)0 2
mÃ∏=0
2

w(0) |Œ¥M |w(0)a
m n
, (18)
Œª(0)‚àíŒª(0)
n m
nce{w(0)}formsanONB,wecangettheorder-1
n
= (cid:88) ‚àû |w(0)a¬∑ ‚ü®w( m 0) |Œ¥M |w( n 0)a . (19)
m Œª(0)‚àíŒª(0)
m=0 n m
mÃ∏=n
+‚ü®w(1) |w(1)a=1+‚à•w(1)‚à•2, (20)
n n n
enotingbyw(0)thenormalizedorder-1corrected
(cid:98)n
(0)+w(1)
n n
, (21)
‚à•w(1)‚à•2)1/2
n
0)a ‚ü®w(0) |Œ¥M |w(0)a
¬∑ ‚Ñì n ¬∑‚ü®w(0)|w(0)a
Œª(0)‚àíŒª(0) ‚Ñì m
n ‚Ñì
(cid:33)2
a
. (22)
(0)a+‚ü®w(0) |w(1)a
n n n
+‚à•w(1)‚à•2)1/2
n
1
‚â•1‚àí ‚à•w(1)‚à•2, (23)
1)‚à•2)1/2 2 n
1 3
(1+c)‚àí5/2x2, x>0, (24)
2! 4
1
‚â•1‚àí x, (25)
2
(cid:32) (cid:33)2
(cid:88) ‚ü®w( m 0) |Œ¥M |w( n 0)a
. (26)
Œª(0)‚àíŒª(0)
n n m
he‚Äòsmallest‚Äôeigen-pairofM‚Ä≤. So,puttingn=0,
(cid:32) (cid:33)2
(cid:88) ‚ü®w( m 0) |Œ¥M |w( 0 0)a . (27)
Œª(0)
0 m
20

=== Page 21 ===
PublishedasaconferencepaperatICLR2026
Toconclude,thissectionshowsthatintroducinga
callycreatesawell-definednullspace,correspond
QTN.Theperturbationanalysis,groundedincla
closetheperturbedeigen-moderemainstotheorig
mation. Thisunderstandingiscriticalinpractice,
donotdestabilizetheuncertaintyquantificationpi
directionsremainstheoreticallysoundandcontrol
B PROMPT TEMPLATES
PhraselengthModelPromptingandAnswerSel
togenerateLLMresponsesis
Answerthefollowingquestionasbrieflya
Answer:
Sentence length Model Prompting and Answe
datasetstogenerateLLMresponsesis
Answerthefollowingquestioninasingle
Answer:
Entailment Estimation/Semantic Clustering
prompttemplate:
Weareevaluatinganswerstothequestion
Herearetwopossibleanswers:
PossibleAnswer1: {text1}
PossibleAnswer2: {text2}
DoesPossibleAnswer1semanticallyent
Respondwith: entailment,contradiction,
C EXTENDED EVALUATION OF HALLU
DIVERSE SETTINGS
Due to page limitations, we reported the results
presentindepthexperimentalresultstovalidateth
acrossdiversedatasets. Buildingontheexamplea
TriviaQA,NQ,SVAMP,andSQuADacrossmult
broaderspectrumofreasoningcomplexitiesand
resultsforbothphrase-levelandsentence-levelg
toexaminetheimpactofoutputlengthondetecti
applytheeightLLMmodels(Mistral-7B-v0.1,Mi
1b, LLaMA-2-13b-chat, LLaMA-2-7b-chat, LLa
originalentailment-basedclusteringanduncertain
Thiscontrolledexperimentaldesignensuresthata
to dataset characteristics rather than methodolog
frameworkgeneralizeseffectivelyacrossheteroge
real-worlddeploymentscenarios.
C.1 AUROCPERFORMANCEACROSSVARIOU
Tomorerigorouslyvalidatetherobustnessofou
theevaluationtosentence-levelandphrase-levelg
(16-bit,8-bit,and4-bit).Thissetupallowsustoexa
estimationandconfabulationdetectionperforman
answersaregenerated. Weconsistentlyapplyth
therebyensuringthatobservedvariationsareattr
changesinmethodology. Importantly,quantizatio
2

asmallperturbationtoafull-rankQCMsystemati-
dingtoanemergentlow-variancedirectioninthe
assicalmatrixperturbationtheory,quantifieshow
ginaleigen-structurethroughafirst-orderapproxi-
,asitensuresthatsmallmodelingerrorsornoise
ipelineandthattheemergenceoflow-uncertainty
llable.
lection Theprompttemplateusedforalldatasets
aspossible: {question}
er Selection The prompt template used for all
ebriefbutcompletesentence: {question}
To detect entailment we utilized the following
n: {question}
tailPossibleAnswer2?
,orneutral.
UCINATION DETECTION ACROSS
summary in the main paper. In this section, we
hatourconfabulationdetectionframeworkworks
analysisinSection4,weextendtheevaluationto
tiplequantizationlevels,therebyencompassinga
linguisticvariations. Inaddition,webreakdown
generations, viapromptsdiscussedinSectionB)
ionrobustness. Foreachdataset,weconsistently
istral-7B-instruct-v0.3,Falcon-rw-1b,LLaMA-3.2-
aMA-2-13b, andLLaMA-2-7b)andmaintainthe
ntyquantificationpipelineswithoutmodification.
anyobservedperformancetrendscanbeattributed
gical changes. The results demonstrate that our
eneoustasks,reinforcingitsapplicabilitytovaried
USPRECISIONMODELSANDDATASETS
urconfabulationdetectionframework, weextend
generationsunderdifferentquantizationprecision
aminehowmodelcompressionimpactsuncertainty
nce,whensentencelengthanswersaswellasshort
hesetofLLMsanddatasetsasinthemainpaper,
ributablesolelytoquantizationeffectsratherthan
onisanecessarystepfordeployinglargemodels
21

=== Page 22 ===
PublishedasaconferencepaperatICLR2026
inresource-constrainedenvironments,yethallucin
largelyoverlookedinpriorwork. Byexplicitlystu
validatingourapproachagainsttheSOTAmethod
intheliteratureandhighlightthepracticalimporta
C.1.1 SENTENCELENGTHOUTPUTPERFORMA
Figs.6,7,and8illustratetheAUROCperformanc
forsixdifferenthallucinationquantificationmetho
sentence-lengthoutputs. Intotal,weconducted5
providingacomprehensivebasisforsentencelengt
notabletrendsandinsights:
1. Methodsbasedonsemanticinformation‚Äî
outperformbaselinesbasedonrawtoken
regression across all datasets. These res
ratherthanrawoutputprobabilitiesinde
2. Acrossallquantizationlevels,ourframew
relativetobaselinemethods. Importantl
(e.g.,movingfrom16-bitto4-bitquantiza
ouruncertainty-baseddetection.
3. When model precision is reduced, SOT
degradation,especiallyinTriviaQAand
of our approach to model compression,
constrainedenvironmentswherelightwei
4. SmallermodelssuchasLlama3.2-1Bsh
modelslikeLLama-13B,revealingthatm
significantfactorsinuncertaintycalibrati
5. Wealsoobserveaminorperformancedro
model,whichwarrantsfurtherinvestigati
C.1.2 PHRASELENGTHOUTPUTPERFORMAN
Inthissection,weextendtheanalysistophrase-leve
suchasundermultiplequantizationprecisions(16
turesshorter,context-sensitivecompletions,which
scenarioswhereanswersareoftenconcise. Byapp
acrossdatasetsandmodels,weensuremethodolog
variations to be attributed to quantization and m
becomestandardforenablingdeploymentofinst
influenceonhallucinationbehaviorremainsunexp
byexaminingconfabulationdetectionundercomp
Figs.9,10,and11presenttheAUROCscoresfor
diverse datasets and instruction-tuned LLMs, un
quantization levels, we performed a total of 24
phrase-levelevaluationoninstructionfinetunedmo
anddistinctiveinsights:
1. Semantic-basedcriteriasuchasSREwit
outperform probability-based (p(True))
semanticvariabilityisareliableuncertain
2. Acrossallquantizationlevels,ourframew
relativetobaselinemethods. Importantl
(e.g.,movingfrom16-bitto4-bitquantiza
ouruncertainty-baseddetection
3. Bycontrast,priorSOTAbaselinessucha
precision,particularlyinmorechallengin
sensitivityoftoken-levelentropymeasur
2

nationbehaviorunderquantizedmodelshasbeen
udyingconfabulationdetectioninthissettingand
dsdiscussedinSection3,weaddressacriticalgap
anceofuncertainty-awaremethodsforLLMs.
ANCE.
ceat16-bit,8-bit,and4-bitprecision,respectively,
odsacrossvariousLLMsandfourdatasets,using
52experimentsspanningallquantizationlevels,
thanalysis. Theexperimentalresultsrevealseveral
‚ÄîsuchasSREwithUQ,SE,andDSE‚Äîconsistently
nsequenceprobabilities(p(True))andembedding
sults confirm effectiveness of semantic diversity
etectingconfabulations.
workmaintainscompetitiveAUROCperformance
ly,weobservethatreductionsinmodelprecision
ation)donotsignificantlydegradethereliabilityof
TA methods such as SE, NE have performance
dSQuADdatasets. Thishighlightstheresilience
, making it suitable for deployment in resource-
ightLLMsarerequired.
howslightlyreducedAUROCcomparedtolarger
modelsizeandinternalrepresentationalpowerare
ion.
opfortheNQdatasetwhenusingtheLlama3.1B
ioninfuturework.
NCEONINSTRUCTMODELS.
elgenerations,focusingoninstruction-tunedLLMs,
6-bit,8-bit,and4-bit). Phrase-levelevaluationcap-
hareparticularlyrelevantforinstruction-following
plyingthesameuncertaintyquantificationpipeline
gicalconsistency,allowingobservedperformance
model adaptation effects. While quantization has
truction-tunedLLMsinpracticalapplications,its
plored. Ourexperimentsdirectlyaddressesthisgap
pressed,instruction-followingsettings.
sixhallucinationquantificationapproachesacross
nder 16-bit, 8-bit, and 4-bit precision. Across all
experiments, providing a robust foundation for
odels. Theresultsrevealseveralconsistentpatterns
thUQmaximization,SE,andDSE‚Äîconsistently
and embedding-based baselines, indicating that
ntysignalevenforshorterresponses.
workmaintainscompetitiveAUROCperformance
ly,weobservethatreductionsinmodelprecision
ation)donotsignificantlydegradethereliabilityof
asSEandNEshowlargerdeclinesunderreduced
ngdatasets(e.g.,NQ,SQuAD),underscoringthe
restocompression.
22

=== Page 23 ===
PublishedasaconferencepaperatICLR2026
Figure 6: summarizing 12 experimental scenar
across three LLMs (LLaMA 2 13B, LLaMA 2 7
datasets (TriviaQA, SQuAD, NQ, SVAMP) for s
proposedSREwithUQisinparorevenhigherth
2

rios, AUROC scores for confabulation detection
7B, LLaMA 3.2 1B) at 16 bit precision and four
sentence length output. The performance of the
hanSOTAmethods.
23

=== Page 24 ===
PublishedasaconferencepaperatICLR2026
Figure 7: summarizing 20 experimental scenarios, AUROC scores for confabulation detection
acrossfiveLLMs(LLaMA213B,LLaMA27B,LLaMA3.21B,Mistral-7B,Mistral-7B-chat)at8
bitprecisionandfourdatasets(TriviaQA,SQuAD,NQ,SVAMP)forsentencelengthoutput. The
performanceoftheproposedSREwithUQisinparorevenhigherthanSOTAmethods.
24

=== Page 25 ===
PublishedasaconferencepaperatICLR2026
Figure 8: summarizing 20 experimental scenarios, AUROC scores for confabulation detection
acrossfiveLLMs(LLaMA213B,LLaMA27B,LLaMA3.21B,Mistral-7B,Mistral-7B-chat)at4
bitprecisionandfourdatasets(TriviaQA,SQuAD,NQ,SVAMP)forsentencelengthoutput. The
performanceoftheproposedSREwithUQisinparorevenhigherthanSOTAmethods.
25

=== Page 26 ===
PublishedasaconferencepaperatICLR2026
4. Instruction-tunedmodelsprovidebetterc
versions,thoughaconsistentgapremain
largerLLaMA-2-13B-chat,highlighting
Figure9: summarizing8experimentalscenarios,
twoLLMs(LLaMA27Bchat,LLaMA213Bcha
SQuAD,NQ,SVAMP).Theperformanceofthepr
SOTAmethods.
Figure 10: summarizing 8 experimental scenar
across four LLMs (Mistral-7B, Falcon-1B, LLaM
(TriviaQA,SQuAD,NQ,SVAMP).Theperforma
parorevenhigherthanSOTAmethods.
C.1.3 PHRASELENGTHOUTPUTPERFORMAN
Inthissection,weanalyzephrase-levelgeneration
quantizationlevels(16-bit,8-bit,and4-bit). Phras
2

calibratedAUROCoverallcomparedtotheirbase
nsbetweenthesmallerLLaMA-2-7B-chatandthe
theroleofmodelscaleinuncertaintyreliability.
,AUROCscoresforconfabulationdetectionacross
at)at16bitprecisionandfourdatasets(TriviaQA,
roposedSREwithUQisinparorevenhigherthan
rios, AUROC scores for confabulation detection
MA 3.2B, LLaMA 2 7B 4-bit) and four datasets
anceoftheproposedsemanticR√©nyientropyisin
NCEONNONINSTRUCTMODELS.
nsfromnon-instruction-tunedLLMsundervarying
se-levelcompletionsinbasemodelsareespecially
26

=== Page 27 ===
PublishedasaconferencepaperatICLR2026
Figure 11: summarizing 8 experimental scenar
across four LLMs (Mistral-7B, Falcon-1B, LLaM
(TriviaQA,SQuAD,NQ,SVAMP).Theperforma
parorevenhigherthanSOTAmethods.
pronetodriftingandincoherentcontinuationsbe
explicittaskalignment. Thismakesthemacritical
tionbehavior‚Äîparticularlywhencombinedwithq
modelsinconstrainedenvironments. PriorSOTA
focusingeitheronsentence-levelorinstruction-fo
certaintyreliabilityincompressedbasemodels. O
uncertainty-awarehallucinationdetectioninnon-i
Figs.12,13,and14summarizeAUROCperforma
four datasets, and multiple base LLMs under 16
conducted 40 experiments, enabling a systema
estimationinthisoverlookedregime. Theresultsr
1. Semantic-basedmethodsdominate: Acro
DSEconsistentlyoutperformp(True)and
diversityremainsastronguncertaintysig
2. Acrossallquantizationlevels,ourframew
relativetobaselinemethods. Importantl
(e.g.,movingfrom16-bitto4-bitquantiza
ouruncertainty-baseddetection
3. Token-levelentropybaselines(NE)suff
SQuAD‚Äîhighlightingtheirfragilityund
semantic-basedsignals.
4. Largerbasemodels(e.g.,LLaMA-2-13B)
ones(e.g.,LLaMA-3.2-1B,Falcon-RW-1
particularlycrucialforphrase-leveluncer
C.2 RACPERFORMANCEACROSSVARIOUSP
Tomorerigorouslyevaluatetherobustnessofourco
theAUROCanalysis(AppendixC.1)withRACand
multiplequantizationprecisions(16-bit,8-bit,and
idealizedconditions,RACandAURACprovide
uncertaintyestimatesimproveaccuracyunderpro
standardfordeployingLLMsinresource-constra
2

rios, AUROC scores for confabulation detection
MA 3.2B, LLaMA 2 7B 4-bit) and four datasets
anceoftheproposedsemanticR√©nyientropyisin
ecause,unlikeinstruction-tunedmodels,theylack
lyetunderexploredregimeforstudyinghallucina-
quantization,whichisessentialfordeployinglarge
methodshavelargelyoverlookedthisintersection,
ollowingsetups,leavingopenquestionsaboutun-
Ourstudydirectlyaddressesthisgapbyevaluating
instruction-tuned,phrase-levelsettings.
anceacrosssixuncertaintyquantificationmethods,
6-bit, 8-bit, and 4-bit quantization. In total, we
atic comparison of quantization and uncertainty
revealthefollowingconsistentfindings:
ossdatasetsandprecisions,SREwithUQ,SE,and
dembeddingregression,confirmingthatsemantic
gnaleveninbasemodelswithlesstaskalignment.
workmaintainscompetitiveAUROCperformance
ly,weobservethatreductionsinmodelprecision
ation)donotsignificantlydegradethereliabilityof
ferperformancedrops‚ÄîmostevidentinNQand
derquantizationandemphasizingtherobustnessof
)consistentlyachievehigherAUROCthansmaller
1B).Thisindicatesthatrepresentationalrichnessis
rtaintycalibrationinnon-instructsettings.
PRECISIONMODELSANDDATASETS
onfabulationdetectionframework,wecomplement
dtheirarea-under-curvesummary,AURAC,across
4-bit). WhileAUROCmeasuresseparabilityunder
astricterevaluationbyquantifyinghowreliably
ogressiverejection. Giventhatquantizationisnow
ainedenvironments,understandingitsimpacton
27

=== Page 28 ===
PublishedasaconferencepaperatICLR2026
Figure12: summarizing16experimentalscena
across four LLMs (Falcon-1B, LLaMA 3.2B, L
(TriviaQA,SQuAD,NQ,SVAMP).Theperforman
higherthanSOTAmethods.
2

arios,AUROCscoresforconfabulationdetection
LLaMA 2 7B, LLaMA 2 13B) and four datasets
nceoftheproposedSREwithUQisinparoreven
28

=== Page 29 ===
PublishedasaconferencepaperatICLR2026
Figure13: summarizing12experimentalscena
across three LLMs (LLaMA 3.2B, LLaMA 2 7B
SQuAD,NQ,SVAMP).Theperformanceofthepr
SOTAmethods.
2

arios,AUROCscoresforconfabulationdetection
B, LLaMA 2 13B) and four datasets (TriviaQA,
roposedSREwithUQisinparorevenhigherthan
29

=== Page 30 ===
PublishedasaconferencepaperatICLR2026
Figure14: summarizing12experimentalscena
across three LLMs (LLaMA 3.2B, LLaMA 2 7B
SQuAD,NQ,SVAMP).Theperformanceofthepr
SOTAmethods.
3

arios,AUROCscoresforconfabulationdetection
B, LLaMA 2 13B) and four datasets (TriviaQA,
roposedSREwithUQisinparorevenhigherthan
30

=== Page 31 ===
PublishedasaconferencepaperatICLR2026
rejectionbehaviorremainsacriticalbutlargelyov
RACsandAURACunderquantization,wedisent
ensuringmethodologicalconsistencyviaidentica
setting,steeperRACslopesindicatestrongercapac
confabulations.
ExpandingontheAURACsummaryresultsrepor
proposedSREwithUQnotonlysurpassespriorm
andsafefilteringofLLMoutputs‚Äîdemonstratin
thanexistingSOTAuncertaintyestimators.
C.2.1 RACPERFORMANCE: SENTENCELENG
Figs.15,16,and17presentRACcurvesat16-b
hallucinationquantificationmethodsacrossmultip
generation. Intotal, weconducted52experime
comprehensive basis for sentence length analysi
evaluation(AppendixC.1.1). Severalnotablepatte
1. The proposed SRE with UQ consistentl
increases,demonstratingsuperiorabilityt
lations. Othersemantic-basedcriteria(SE
effectivethanSREwithUQ,whileproba
baselineslagsignificantlybehind.
2. The advantage of SRE with UQ is parti
asNQandSQuAD,whereitsRACcurv
non-semanticbaselines,highlightingrobu
3. Movingfrom16-bitto4-bitprecisionin
SREwith UQ,whereascompeting meth
ThisunderscorestheresilienceofSREw
4. Model size plays a role, with smaller m
RACcurvesthanlargercounterparts(e.
consistentlypreservesitsrelativeadvant
evenunderreducedrepresentationalcapa
5. Wealsoobserveaminorperformancedro
model,consistentwiththeresultsinC.1
work.
C.2.2 RACPERFORMANCE: PHRASELENGTH
Figs.18,19,and20presentRACcurvesat16-b
hallucinationquantificationmethodsacrossmulti
generation on instruct fine tuned models. Phras
instruction-following use cases, where response
prior SOTA hallucination detection methods hav
rejection‚Äìaccuracybehaviorundercompressedinstr
inreal-worldapplications.
Thisanalysisrevealshowquantizationinteracts
whethersemantic-baseduncertaintymeasuressu
generations are concise and more sensitive to un
experimentsspanningallquantizationlevels,pr
analysis,drawingparallelwithitsAUROCcounter
patternsemergeuponanalysis:
1. TheproposedSREwithUQconsistently
demonstratingstrongerrejectionbehavior
dingregressionbaselines.
2. Instructmodelsrevealthatsemanticcriter
higher accuracy at rejection rates above
challengingdatasetssuchasTriviaQAan
3

verlookeddimension. Bysystematicallyanalyzing
tangletheeffectsofcompressionandmodelscale,
alclustering-baseduncertaintypipelines. Inthis
citytoprioritizereliablegenerationswhilefiltering
rtedinSection4.2,ourfindingsestablishthatthe
methodsinrobustnessbutalsoenablesfine-grained
ngbothhigheraccuracyandgreaterdeployability
GTH
bit,8-bit,and4-bitprecision,respectively,forsix
pleLLMsandfourdatasetsundersentence-length
entsspanningallquantizationlevels, providinga
is, drawing parallel with its AUROC counterpart
ernsemerge:
ly yields the steepest accuracy gains as rejection
toprioritizereliableoutputsandsuppressconfabu-
EandDiscreteSE)followcloselybutremainless
ability-based(p(True))andembeddingregression
icularly pronounced in challenging datasets such
vesshowclearseparationfrombothsemanticand
bustnessunderhighvariability.
nducesonlyminorchangesintheRACslopesof
hods(e.g., SEand NE)degrade morenoticeably.
withUQtoquantization.
models (e.g., LLaMA-3.2-1B) exhibiting flatter
.g., LLaMA-2-13B).Nonetheless, SREwithUQ
tageacrossallscales,confirmingitseffectiveness
acity.
opfortheNQdatasetwhenusingtheLlama3.1B
1.1,whichwarrantsfurtherinvestigationinfuture
HONINSTRUCTMODELS
bit,8-bit,and4-bitprecision,respectively,forsix
ipleLLMsandfourdatasetsunderphrase-length
se-level completions are particularly relevant for
es tend to be short and context-sensitive. While
ve largely overlooked this setting, understanding
ruction-tunedmodelsiscrucialforsafedeployment
withmodeladaptationtoinstructiontuning,and
uchasSREwithUQretaintheirreliabilitywhen
ncertainty calibration. In total, we conducted 8
rovidingacomprehensivebasisforphraselength
rpartevaluation(AppendixC.1.1). Severalnotable
yyieldsthesteepestRACslopesacrossdatasets,
rthanbothprobability-based(p(True))andembed-
ria‚ÄîespeciallySREwithUQ‚Äîretainsubstantially
e 90%, with the advantage most pronounced on
ndNQ.
31

=== Page 32 ===
PublishedasaconferencepaperatICLR2026
Figure15: summarizing12experimentalscenari
threeLLMs(LLaMA3.21B,LLaMA27B,LLa
(TriviaQA,SQuAD,NQ,SVAMP).Theperforman
higherthanSOTAmethods.
3. Across quantization levels, SRE with U
whereasbaselinesdegrademorenoticeab
sion.
4. Largerinstructmodel(e.g.,LLaMA-2-13
smallermodel(e.g.,LLaMA-2-7B-chat)
UQconsistentlyoutperformsalternatives
C.2.3 RACPERFORMANCE: PHRASELENGTH
Phrase-levelRACplotsfornon-instructLLMs(F
semantic-basedhallucinationdetection,andinparti
robustnessunderquantizationandacrossdiversed
basedbaselines,whichexhibitflatterorunstable
rejection‚Äìaccuracyimprovements,highlightingits
whencompressionreducesrepresentationalcapac
phrase-levelrejectionsettings,demonstratingtha
confabulationsbutalsoexcelsindynamicallyfilte
Importantly, the proposed SRE with UQ demon
behavior,highlightingitssuitabilityforlightweig
challenging. Severalnotablepatternsemergeupon
1. SRE-UQ leads across precisions, acros
producesthesteepestRACcurves,confi
rejection increases. This advantage is
probability-basedbaselinesstagnateorev
2. WhilebaselinessuchasDiscreteSemant
dationunder4-bitquantization,SRE-UQ
scoringresiliencetoaggressivemodelco
3. Gains are most pronounced in datasets
SVAMP),whereSRE-UQsharplyseparat
basedp(True)oftencollapsesunderphra
filtering.
4. Smallermodels(e.g.,LLaMA-3.2-1B)ex
sentationalpower,buteveninthesechal
outperformingallothermethodsconsiste
3

ios,RACscoresforconfabulationdetectionacross
aMA213B)at16bitprecisionandfourdatasets
nceoftheproposedSREwithUQisinparoreven
UQ remains stable from 16-bit to 4-bit precision,
bly,underscoringitsresiliencetomodelcompres-
3B-chat)amplifythegainsofSREwithUQ,while
)showflatterRACcurves;nonetheless,SREwith
sacrossscales.
HONNONINSTRUCTMODELS
Figs.21,22,and23)providefurtherevidencethat
icularourproposedSREwithUQ,maintainsstrong
datasets. Unliketoken-probabilityandembedding-
RACslopes,SRE-UQconsistentlyyieldssteeper
sabilitytoreliablyprioritizecorrectoutputseven
city. ThesefindingsextendtheAUROCtrendsto
atSRE-UQisnotonlyeffectiveindistinguishing
eringthemunderrealisticdeploymentconstraints.
nstrates consistently stronger rejection‚Äìaccuracy
ghtnon-instructmodelswherecalibrationismore
nanalysis:
ss 16-bit, 8-bit, and 4-bit quantization, SRE-UQ
firmingthatitmosteffectivelyraisesaccuracyas
especially evident in TriviaQA and NQ, where
venflatten.
ticEntropyandNa√ØveEntropyshowvisibledegra-
Q‚ÄôsRACslopesremainsharpandconsistent,under-
ompression.
with higher linguistic variability (e.g., NQ and
tesfromweakerbaselines. Bycontrast,probability-
ase-levelrejection,failingtoprovidemeaningful
xhibitgenerallyflattercurvesduetolimitedrepre-
llengingcases,SRE-UQmaintainsrelativegains,
ently.
32

=== Page 33 ===
PublishedasaconferencepaperatICLR2026
Figure16: summarizing20experimentalscenarios,RACscoresforconfabulationdetectionacross
fiveLLMs(LLaMA213B,LLaMA27B,LLaMA3.21B,Mistral-7B,Mistral-7B-chat)at8bit
precisionandfourdatasets(TriviaQA,SQuAD,NQ,SVAMP).Theperformanceoftheproposed
SREwithUQisinparorevenhigherthanSOTAmethods.
33

=== Page 34 ===
PublishedasaconferencepaperatICLR2026
Figure17: summarizing20experimentalscenarios,RACscoresforconfabulationdetectionacross
fiveLLMs(LLaMA213B,LLaMA27B,LLaMA3.21B,Mistral-7B,Mistral-7B-chat)at8bit
precisionandfourdatasets(TriviaQA,SQuAD,NQ,SVAMP).Theperformanceoftheproposed
SREwithUQisinparorevenhigherthanSOTAmethods.
Figure18: summarizing8experimentalscenarios,RACscoresforconfabulationdetectionacross
twoLLMs(LLaMA213B-chat,LLaMA27B-chat)at16bitprecisionandfourdatasets(TriviaQA,
SQuAD,NQ,SVAMP).TheperformanceoftheproposedSREUQisinparorevenhigherthan
SOTAmethods.
34

=== Page 35 ===
PublishedasaconferencepaperatICLR2026
Figure19: summarizing8experimentalscenarios,RACscoresforconfabulationdetectionacross
twoLLMs(LLaMA213B-chat,LLaMA27B-chat)at8bitprecisionandfourdatasets(TriviaQA,
SQuAD,NQ,SVAMP).TheperformanceoftheproposedSREUQisinparorevenhigherthan
SOTAmethods.
Figure20: summarizing8experimentalscenarios,RACscoresforconfabulationdetectionacross
twoLLMs(LLaMA213B-chat,LLaMA27B-chat)at4bitprecisionandfourdatasets(TriviaQA,
SQuAD,NQ,SVAMP).TheperformanceoftheproposedSREUQisinparorevenhigherthan
SOTAmethods.
35

=== Page 36 ===
PublishedasaconferencepaperatICLR2026
Figure21:RACscoresforconfabulationdetectionacrossfourLLMs(Mistral-7B,Falcon-1B,LLaMA
3.2B,LLaMA27B4-bit)andfourdatasets(TriviaQA,SQuAD,NQ,SVAMP).Theperformanceof
theproposedsemanticR√©nyientropyisinparorevenhigherthanSOTAmethods.
Figure22:RACscoresforconfabulationdetectionacrossfourLLMs(Mistral-7B,Falcon-1B,LLaMA
3.2B,LLaMA27B4-bit)andfourdatasets(TriviaQA,SQuAD,NQ,SVAMP).Theperformanceof
theproposedsemanticR√©nyientropyisinparorevenhigherthanSOTAmethods.
36

=== Page 37 ===
PublishedasaconferencepaperatICLR2026
Figure23:RACscoresforconfabulationdetectiona
3.2B,LLaMA27B4-bit)andfourdatasets(Trivia
theproposedsemanticR√©nyientropyisinparore
C.3 HYPERPARAMETERSELECTION
Inthissection,weexaminetheeffectofhyperpar
frameworkintroducedinequation7.Recallthatthi
jointlymaximizingsemanticentropyandpenalizin
weightedbythemodel‚Äôsuncertaintyestimates.Acr
UQ directly into the token adjustment mechanis
confabulationsbeyondconventionalsemanticentr
ToassesstheimpactofŒª,wemeasureAUROCs
valuesofthehyperparameter.Ineachcase,theR√©n
adjustment,andtheresultsarecomparedagainst
correction(denotedbytheredline). TheŒªyielding
isolatesthecontributionofUQ-awareentropymax
AsshowninFig.24,incorporatingUQ-basedproba
overthebaselineentropyapproachacrossallmod
Severalkeyobservationsarenotable:
1. Moderate values of Œª systematically bo
trade-offbetweenentropymaximization
beneficial.
2. WhilethepreciseoptimalŒªmayvaryslig
gainsarerobustacrossawiderangeofŒª
theproposedcorrection.
3. This experiment validates one of the m
ducingUQintosemanticentropycalcul
abilitytodiscriminateconfabulations,esp
predictions.
3

acrossfourLLMs(Mistral-7B,Falcon-1B,LLaMA
aQA,SQuAD,NQ,SVAMP).Theperformanceof
evenhigherthanSOTAmethods.
rametertuningforŒªintheentropymaximization
isformulationadjuststheoutputTSprobabilitiesby
ngdeviationsfromtheoriginalmodeldistribution,
riticalinnovationofourapproachliesinintegrating
sm, thereby enhancing the model‚Äôs sensitivity to
ropyanalysis.
scoresforhallucinationdetectionacrossdifferent
nyi‚Äôssemanticentropyiscomputedafterprobability
thebaselineR√©nyi‚Äôsentropywithoutuncertainty
gthehighestAUROCisingreen. Thiscomparison
ximizationtotheconfabulationdetectiontask.
abilityadjustmentsyieldssubstantialimprovements
delstested.
oost AUROC scores, confirming that a calibrated
nandfidelitytotheoriginalmodelpredictionsis
ghtlyacrossmodelsanddatasets,theperformance
Œªvalues,underscoringthegeneraleffectivenessof
most crucial contributions of our work: by intro-
lations,wecansignificantlysharpenthemodel‚Äôs
peciallyinthepresenceofnoisyorlow-confidence
37

=== Page 38 ===
PublishedasaconferencepaperatICLR2026
Figure24: EffectofŒªhyperparameterselectiononAUROCperformance. Theredlinedenotesthe
baselineSREwithoutUQadjustment. Integratinguncertaintyviaequation7consistentlyenhances
hallucinationdetectioncapabilityacrossallLLMmodelsanddatasets. TheŒªyieldingthehighest
AUROCisingreen.
38

=== Page 39 ===
PublishedasaconferencepaperatICLR2026
D ADDITIONAL DETAILS ON ALGORI
COMPUTATIONAL COST
D.1 COMPUTATIONALOVERHEAD
ThecomputationaloverheadintroducedbytheQT
withinreal-timeinferenceconstraints. Themetho
features in a 28 √ó28 matrix, which is significan
byclustering-basedsemanticentropymethods. C
sampledKMEincursaone-timepre-processingco
canbesubstantiallyreducedthroughGPU-accelera
per-queryoverheadisminimal: evaluatingtheper
6‚Äì10msonanNVIDIAA6000GPU,whichisneg
decoding. Overall,theQTNperturbationstepintro
enablingricheruncertaintyestimates.
D.2 ALGORITHMICSTRUCTUREANDCOMPL
Inthissubsection,wepresentpseudocodeillustrati
ingthepipelinediagramprovidedinFig.1.
Thisincludespseudocodefor(i)Hamiltoniancons
first-orderperturbationfeatures,and(iii)theentro
Algorithm1ConstructionofEmpiricalKMEœà(cid:98)(x
1: Input:Samples{x i }N i=1 fromrandomvariable
2: Output: SampledKMEvectorœà(cid:98)={œà(cid:98)j }M
j=
‚àí
0
3: forj =0toM ‚àí1do
4: ComputeParzenPDFestimate:
1
p(x )=
(cid:98) j N
whereŒ∫ (x ;x )istheGaussiankernel
œÉ j i
5: SetKMEvalueœà(cid:98)j ‚Üêp (cid:98) (x j )
6: endfor
7: returnœà(cid:98)
Algorithm2QTNHamiltonianConstructionviaQ
1: Input: SampledKMEvectorœà(cid:98)oflengthM
2: Output: LocalHamiltonianH(cid:99)
3: ConstructtheQCMmatrixusingtheinnerpro
M (k,‚Ñì)=
œà(cid:98)
4: ComputenullspaceofQCM:
w={w }T‚àí
k k=
5: FormHamiltonianaslinearcombination:
T
(cid:88)
H(cid:99)=
k
6: returnH(cid:99)
3

THMIC STRUCTURES AND
TN-basedUQmoduleismodestandremainswell
odrequiresstoringonlyM =8q-bitperturbation
ntly smaller than the embedding caches required
ConstructingtheHamiltonianassociatedwiththe
ostofapproximately45‚Äì60sonCPU,thoughthis
atedlinearalgebraroutines. Atinferencetime,the
rturbation-baseduncertaintyfeaturesaddsmerely
egligiblecomparedtotheintrinsiclatencyofLLM
oducesonlyalightweightcomputationalcostwhile
EXITY
ingkeycomponentsofourapproach,complement-
structionfromthesampledKME,(ii)extractionof
opy-maximizationprocedurefromSec.2.3.
x)
eX;kernelbandwidthœÉ;samplinggrid{x }M‚àí1
j j=0
‚àí1
0
N
1 (cid:88)
Œ∫ (x ;x )
N œÉ j i
i=1
QuantumCorrelationMatrix
=2L;operatorbasisH(cid:98) ={H(cid:99)k }T
k=
‚àí
0
1
oducts,wherethe(k,‚Ñì)entryisgivenby:
=‚ü®H(cid:99)k œà(cid:98),H(cid:99)‚Ñì œà(cid:98)‚ü©
‚àí1 ‚ààNull(M )
=0 œà(cid:98)
T‚àí1
(cid:88)
w
k
H(cid:99)k
k=0
39

=== Page 40 ===
PublishedasaconferencepaperatICLR2026
Algorithm3First-OrderPerturbationCorrections
1: Input: BaseHamiltonian
T
(cid:88)
H(cid:99)=
k
where{w }arethelearnedweightsmultiplyin
k
‚àÜwspecifyingperturbationoftheseweights.
2: Output: First-ordercorrectedeigen-modes{œà
3: ConstructtheperturbedHamiltonian:
T
(cid:88)
‚àÜH =
k
Thiscorrespondstoperturbingthecoefficients
formingtheQTNHamiltonian.
4: Computeeigen-decompositionoftheunpertur
H(cid:99)œà
m
=E
m
œà
m
,
5: form=0toM ‚àí1do
6: Computefirst-orderenergycorrection:
E(1) =‚ü®
m
7: Computefirst-ordereigenvectorcorrection:
(cid:88) ‚ü®
œà(1) =
m
nÃ∏=m
8: endfor
9: We utilize Eq. 5 and {œà m (1)} to compute UQ
yieldingthelocalUQscoresthatappearinEq
10: return{œà m (1)}&UQ(p( s r)).
Algorithm4CalibratedEntropy-MaximizationU
1: Input:
‚Ä¢ OriginalTSprobabilityp(r)
s
‚Ä¢ UncertaintyscoreUQ(p(r))computedvi
s
‚Ä¢ RegularizationparameterŒª
2: Output: Adjustedprobabilityp( s r)‚àó
3: Defineobjective:
L(p)=‚àílog (cid:0) p2+(1‚àíp)
(cid:98) (cid:98) (cid:98)
whereKL(p‚à•p)=plogp(cid:98)+(1‚àíp)log1‚àíp(cid:98).
(cid:98) (cid:98) p (cid:98) 1‚àíp
4: Initialize: p (cid:98) (0) ‚Üêp( s r)
5: fort=1toT max do
6: Computegradient‚àáL(p(t‚àí1))
(cid:98)
7: Updateviaprojectedgradientascent
8: Stopearlyif|p(t)‚àíp(t‚àí1)|<Œ¥
(cid:98) (cid:98)
9: endfor
10: Setp( s r)‚àó ‚Üêp (cid:98) (T)
11:
returnp(
s
r)‚àó
40

sandTSUQ
‚àí1
(cid:88)
w
k
H(cid:99)k ,
k=0
ngtheHermitianoperatorbasis;perturbationvector
œà(1)}andtheresultingUQfeaturevector.
m
T‚àí1
(cid:88)
‚àÜw
k
H(cid:99)k .
k=0
s(weights)associatedwiththePauli-likeoperators
rbedHamiltonian:
m=0,...,M ‚àí1.
‚ü®œà , ‚àÜHœà ‚ü©.
m m
:
‚ü®œà , ‚àÜHœà ‚ü©
n m œà .
E ‚àíE n
m n
Q feature vectors in the KME amplitude domain,
q.6.
UpdateforTSProbabilities
iaEq.6
)2(cid:1) ‚àí Œª ¬∑KL (cid:16) p‚à•p(r) (cid:17)
UQ(p(r)) (cid:98) s
s
0

Paper:The Illusion of Progress - Re-evaluating Hallucination Detection in LLMs.pdf
=== Page 1 ===
The Illusion of Progress: Re-evaluati
DenisJaniak1 JakubBinkowski1
RavidShwartz-Ziv3
1WroclawUniversityof
2UniversityofTe
3NewYork
Abstract
Large language models (LLMs) have revolu-
tionizednaturallanguageprocessing,yettheir
tendency to hallucinate poses serious chal-
lengesforreliabledeployment. Despitenumer-
oushallucinationdetectionmethods,theireval-
uationsoftenrelyonROUGE,ametricbased
onlexicaloverlapthatmisalignswithhuman
judgments. Through comprehensive human
studies, we demonstrate that while ROUGE
exhibits high recall, its extremely low preci-
sionleadstomisleadingperformanceestimates.
In fact, several established detection meth-
ods show performance drops of up to 45.9%
when assessed using human-aligned metrics
likeLLM-as-Judge. Moreover,ouranalysisre-
vealsthatsimpleheuristicsbasedonresponse
lengthcanrivalcomplexdetectiontechniques,
exposingafundamentalflawincurrentevalua-
tionpractices. Wearguethatadoptingsemanti-
callyawareandrobustevaluationframeworks
is essential to accurately gauge the true per-
formanceofhallucinationdetectionmethods,
ultimatelyensuringthetrustworthinessofLLM
outputs.
1 Introduction
Largelanguagemodels(LLMs)havetransformed
naturallanguageprocessing,buttheirtendencyto
hallucinate‚Äîgeneratingfluentyetfactuallyincor-
rect outputs‚Äîposes a critical challenge for real-
worldapplications(Huangetal.,2025). AsLLMs
areincreasinglydeployedinhigh-stakesscenarios,
unsupervisedhallucinationdetectionhasemerged
as a promising solution, offering scalable evalua-
tionwithoutthegeneralizationlimitationsofsuper-
visedapproachandcostlyannotationprocess(Su
etal.,2024). Agrowingbodyofworkhasexplored
this direction (Chen et al., 2024; Farquhar et al.,
2024;Duetal.,2024;Nikitinetal.,2024;Qiuand
Miikkulainen, 2024; Duan et al., 2024; Nguyen
et al., 2025), often relying on ROUGE as the pri-
mary correctness metric. ROUGE, originally de-
velopedtoassesssummaryqualitybasedonlexical
5202
guA
31
]LC.sc[
2v58280.8052:viXra

ing Hallucination Detection in LLMs
AlbertSawczyn1 BogdanGabrys2
TomaszKajdanowicz1
ScienceandTechnology
echnologySydney
kUniversity
NQ-Open SQuAD Trivia-QA
LogDet
eRank
Eigenscore
Semantic
Entropy
LN-Entropy
Perplexity
0.6 0.8 0.6 0.8 0.6 0.8
Figure1: ROUGE-basedevaluationfailstoreliably
capturetruehallucinationdetectioncapabilities.Hal-
lucinationdetectionperformance(AUROC)comparison
of ROUGE-L and LLM-as-Judge evaluation across
threedatasets. Manymethodsshowssignificantevalua-
tiondiscrepancies.
overlap(Lin,2004),isusedtoapproximatefactual
consistencybyapplyingthreshold-basedheuristics:
responseswithlowROUGEoverlaptoreference
answersareoftenlabeledashallucinated. However,
thesuitabilityofROUGEforassessingthefactual
accuracyofQuestionAnswering(QA)responses,
specificallyinidentifyinghallucinations,hasbeen
largelyassumedratherthanrigorouslyvalidated.
Existing critiques of ROUGE often focus on
its limitations in capturing fluency or adequacy
in long-form summarization or dialogues (Hon-
ovichetal.,2022;Dzirietal.,2022;Zhongetal.,
2022). In contrast, this paper presents a system-
atic,large-scaleempiricalinvestigationspecifi-
callyevaluatingROUGE‚Äôsefficacyinthecontext
ofQAhallucinationdetection. Ouranalysisgoes
beyondgeneralcritiquesbyquantitativelydemon-
stratingROUGE‚Äôskeyshortcomings‚Äîsuchasits
susceptibilitytoresponselength‚Äîandhowthese
issuescaninflatethereportedperformanceofhal-
lucinationdetectionmethods. Furthermore,while

=== Page 2 ===
ROUGEservesasourprimarycasestudyduetoits
ubiquity,wealsodemonstratethatothercommonly
usedmetrics,includingthosebasedonn-gramsand
semanticsimilarity,sharesimilarvulnerabilitiesin
thisspecifictask,highlightingabroaderdeficiency
incurrentevaluationpractices.
To establish a human-aligned benchmark, we
collect human judgments of factual correctness
andcomparemetricoutputsagainstthesegoldla-
bels. WefindthatROUGEexhibitsalarminglylow
precisionforidentifyingactualfactualerrors. In
contrast,anLLM-as-Judgeapproach(Zhengetal.,
2023a)alignsfarmorecloselywithhumanassess-
ments. Based on these insights, we re-evaluate
existing detection methods under both ROUGE
andhuman-alignedcriteria,revealingdramaticper-
formance drops (up to 45.9% for Perplexity and
30.4%forEigenscore)whenmovingfromROUGE
toLLM-as-Judgeevaluation(seeFigure1).
Finally,weuncoverasurprisingbaseline: sim-
plelength-basedheuristics(e.g.,meanandstandard
deviationofanswerlength)rivalorexceedsophis-
ticateddetectorslikeSemanticEntropy. Through
controlledexperimentsthatisolatelengtheffects,
weshowhowROUGEcanbemanipulatedviatriv-
ial repetition, even when factual content remains
constant. Ourfindingsexposeawidespreadover-
estimationofcurrentmethodsandunderscorethe
urgentneedformorereliable,human-alignedeval-
uationmetricsinQAhallucinationdetection.
Our study makes the following key contribu-
tions:
1. A human evaluation study validating
LLM-as-Judge as a reliable metric for
factualcorrectness,whiledemonstratingthat
ROUGE‚Äîand other n-gram and semantic
metrics‚Äîareseverelymisalignedwithhuman
judgments.
2. A systematic re-evaluation of existing hal-
lucination detection methods, showing that
theireffectivenessisoftenoverstatedwhenas-
sessedwithROUGEandsimilarmetrics,and
revealing how these metrics can hide impor-
tantflawsinthemethods.
3. Evidencethatresponselengthisasurprisingly
effectiveindicatorofhallucination,withsim-
plelength-basedheuristicsoftenmatchingor
exceedingtheperformanceofmoresophisti-
cateddetectionapproaches.

2 RelatedWork
Hallucination Detection Methods Recent re-
searchhasshownthathallucinationsinLLMsare
inevitable(Xuetal.,2024),spurringworkontwo
maindetectionparadigms: supervisedandunsuper-
vised. Supervisedmethodsusuallyemployprobing
classifierstrainedonlabeledhiddenstatestodetect
hallucinations(AzariaandMitchell,2023;Orgad
etal.,2024;Arteagaetal.,2024). Whileeffective,
theydependoncostlyhumanannotationsandoften
fail to generalize across domains. Unsupervised
methodsdetecthallucinationsbyestimatinguncer-
taintydirectly‚Äîtoken-levelconfidencefromsingle
generations(Renetal.,2023),sequence-levelvari-
anceacrossmultiplesamples(MalininandGales,
2021; Farquhar et al., 2024), or hidden-state pat-
ternanalysis(Chenetal.,2024;Sriramananetal.,
2024a). While these methods show strong per-
formance on standard benchmarks, our analysis
reveals that simpler length-based baselines can
achieve comparable results‚Äîechoing prior find-
ingsthatsimplebaselinesremainsurprisinglycom-
petitive and underscoring the need for rigorous
head-to-headcomparisons(Fadeevaetal.,2023).
EvaluationMetricsandTheirLimitations Tra-
ditionaln-gramoverlapmeasuressuchasROUGE
(Lin, 2004) remain popular for detecting halluci-
nations, despite their inability to reliably assess
factualconsistency(Honovichetal.,2022). Recent
studieshavefurtherhighlightedtheselimitations,
particularlyinmultilingualsettingswherelexical
overlapprovesunreliablecomparedtoNLI-based
approaches(Kangetal.,2024). EvenROUGE-L,
which tracks the longest common subsequence,
often misses errors that leave surface overlap in-
tact. Toovercometheseshortcomings,afamilyof
embedding-basedmetrics‚ÄîBERTScore(Zhang
etal.,2020),UniEval(Zhongetal.,2022),Align-
Score(Zhaetal.,2023),andrelatedapproaches‚Äî
hasbeenproposedtocapturedeepersemanticsim-
ilarity. However,theselearnedrepresentationscan
stilldivergefromhumanjudgmentsoftruthfulness.
Bycontrast,LLM-as-Judgemethods(Zhengetal.,
2023a)haveshownstrongagreementwithhuman
judgmentsinQAtasks(Thakuretal.,2025),offer-
ing a more reliable alternative. Our study builds
ontheseinsightsbyexposingROUGE‚Äôsandother
metricsblindspotsandvalidatingLLM-as-Judge
asamorefaithfulframeworkforfactualevaluation.

=== Page 3 ===
3 ExperimentalSetup
3.1 Overview
Ourexperimentaldesignaimstoinvestigateboth
the shortcomings of current evaluation methods
andtheeffectivenessofsimpleralternatives.
3.2 DatasetsandModels
Forourexperiments,weusethreeestablishedQA
datasets,eachwithdistinctcharacteristics:
‚Ä¢ NQ-Open (Kwiatkowski et al., 2019): Con-
tains3,610question-answerpairsdrawnfrom
realGooglesearchqueries,representingnatu-
ralinformation-seekingbehavior
‚Ä¢ TriviaQA (Joshi et al., 2017): A subset of
3,842examplesfromthevalidationset,featur-
ingtriviaquestionsthatoftenrequirespecific
factualknowledge
‚Ä¢ SQuAD (Rajpurkar et al., 2018): 4,150 ex-
amplesfromthevalidationset(rc.nocontext),
characterizedbylonger,morecomplexques-
tionsandanswers
NQ-Open and TriviaQA primarily feature
shorterquestionsandanswers,whereasSQuADv2
containslongerinputs,makingitsuitableforevalu-
atingourmethodinmorecomplexcontexts.
We generated answers using two open-source
LLMs: LLAMA3.1-8B-INSTRUCT 1 (Grattafiori,
2024)and MISTRAL-7B-INSTRUCT-V0.32 (Jiang
etal.,2023). Forsimplicity,werefertothesemod-
els as LLAMA and MISTRAL in our plots and ta-
bles.
3.3 HallucinationDetectionBaselines
Wecompareourapproachagainstestablishedbase-
lines that fall into two categories. Uncertainty-
based methods estimate model confidence, in-
cluding Perplexity (Ren et al., 2023), Length-
Normalized Entropy (LN-Entropy) (Malinin and
Gales,2021),andSemanticEntropy(SemEntropy)
(Farquhar et al., 2024), which use multiple gen-
erations to capture sequence-level uncertainty.
Consistency-basedmethodsanalyzeinternalrep-
resentations. EigenScore(Chenetal.,2024)com-
putesgenerationconsistencyviaeigenvaluespectra,
whileLogDet(Sriramananetal.,2024a)measures
covariancestructurefromsinglegenerations. We
1hf.co/meta-llama/Llama-3.1-8B-Instruct
2hf.co/mistralai/Mistral-7B-Instruct-v0.3

alsoevaluateEffectiveRank(eRank)(RoyandVet-
terli,2007;Garridoetal.,2023),anintrinsicdimen-
sionalitymeasureweadaptasanovelhallucination
indicator(seeAppendixF.1).
3.4 GroundTruthLabels
Toobtainreliablegroundtruthlabelsforevaluating
thecorrectnessofgeneratedresponses,weutilize
twocomplementaryapproaches:
LLM-as-JudgeleveragesGPT-4o-Mini(etal.,
2024) for semantic assessment, following the
methodologyoutlinedin(Zhengetal.,2023b)and
usingapromptadaptedfrom(Orgadetal.,2025).
Thisapproachclassifiesgeneratedresponsesinto
threecategories: "correct,""incorrect,"or"refuse"
(with"refuse"beingtreatedasahallucination). By
focusingonsemanticequivalenceandfactualaccu-
racy,thismethodgoesbeyondsurface-levelcom-
parisonsandexhibitsstrongalignmentwithhuman
judgments(Thakuretal.,2025).
ROUGE-LF1Score(Lin,2004)measuresthe
longestcommonsubsequencebetweenthegener-
ated response and the ground truth. Consistent
with prior work (Farquhar et al., 2024), we ap-
ply a threshold of 0.3 for this metric. Including
ROUGE-Lallowsustocompareourfindingswith
existingliteratureandhighlightthelimitationsof
relyingsolelyonlexicaloverlapforevaluatingfac-
tual correctness. It helps to quantify the discrep-
ancybetweensemanticunderstanding(assessedby
theLLMjudge)andsimplewordmatching.
3.5 EvaluationMetrics
We employ Area Under the Receiver Operating
Characteristiccurve(AUROC)andAreaUnderthe
Precision-Recallcurve(PR-AUC)asourprimary
evaluationmetrics. AUROCassessestheabilityof
ahallucinationdetectionmethodtocorrectlyrank
positiveandnegativeinstances(hallucinationsvs.
non-hallucinations). PR-AUCisparticularlyvalu-
ablewhendealingwithimbalanceddatasets,which
isoftenthecaseinhallucinationdetectionwhere
non-hallucinatedresponsesmightbemorefrequent.
Bothmetricsofferathreshold-independentevalua-
tionoftherankingperformance(Linetal.,2023).
3.6 ImplementationDetails
WeutilizepretrainedmodelweightsfromtheHug-
gingFaceTransformers(Wolfetal.,2020)without
any additional fine-tuning. Following (Farquhar
etal.,2024),wegenerate10samples(n = 10)us-
ingtemperature1.0foruncertaintyestimation. Ad-

=== Page 4 ===
ditionally,wegenerateone"bestanswer"sample
withtemperature0.1toserveasthebest-generation
estimateforperformanceevaluation.
Themodelsareevaluatedinbothzero-shotand
few-shot(k = 5)settings:
‚Ä¢ Zero-shot: Models rely solely on their pre-
existingknowledge,testingbasecapabilities
‚Ä¢ Few-shot: Models receive five carefully se-
lected examples demonstrating expected an-
swerformats
Both settings use a standardized prompt de-
signed to elicit concise answers. The specific
prompt,adaptedfrom(Kossenetal.,2024),canbe
foundinAppendixD.Wereportresultsforasingle
rununlessspecifiedotherwise.
4 HumanEvaluation: TheGoldStandard
Beforeanalyzingthetechnicalproblemsofhallu-
cinationdetectionmethods,wefirstestablishthat
commonly used evaluation metrics‚Äîspecifically
ROUGE‚Äîare poorly aligned with human judg-
ments of factual correctness (Honovich et al.,
2022; Kang et al., 2024). In contrast, an evalua-
tionmethodbasedonLLM-as-Judgedemonstrates
much closer agreement with human assessments
(Thakur et al., 2025). To illustrate this, we con-
ductedacomprehensivehumanevaluationstudy.
Study Design We randomly selected 200
question‚ÄìanswerpairsfromtheMistralanswerson
theNQ-Opendataset,ensuringabalancedrepresen-
tationofcaseswhereROUGEandLLM-as-Judge
yieldconflictinghallucinationassessments. Each
answerwasindependentlyassessedbythreeanno-
tatorsusingstandardizedguidelinesfrom(Thakur
et al., 2025), classifying responses as correct, in-
correct,orrefuse(wethenclassifymodelrefusal
asincorrect). Thehighinter-annotatoragreement
(Cohen‚Äôs Kappa = 0.799) confirms the reliability
ofhumanjudgments.
Key Findings Our results reveal a significant
performance gap between LLM-as-Judge and
ROUGE when benchmarked against human con-
sensus. WhileROUGEexhibitshighprecisionbut
failstodetectmanyhallucinations,LLM-as-Judge
achievessignificantlyhigherrecall,aligningmore
closelywithhumanassessments,asshowninTable
1.

Table1: LLM-as-Judgeprovidessuperioralignment
withhumanjudgment. ComparisonofROUGE(with
standard0.3threshold)andLLM-as-Judgeagainsthu-
manlabels.
Method Precision Recall F1-Score Agreement
LLM-as-Judge 0.736 0.957 0.832 0.723
ROUGE 0.401 0.957 0.565 0.142
Implications Our findings underscore that
ROUGE is a poor proxy for human judgment in
evaluatinghallucinationdetection. Despiteitshigh
precision, ROUGE fails to capture many critical
errors,resultinginasignificantmisalignmentwith
humanassessmentsoffactualcorrectness. Incon-
trast, LLM-as-Judge exhibits strong agreement
withhumanevaluations‚Äîachievingbothhighpre-
cision and recall‚Äîwhich motivates its adoption
as a more robust, semantically aware evaluation
methodthroughoutthiswork.
5 Re-evaluatingHallucinationDetection
Methods
5.1 LimitationsofROUGEforFactual
AccuracyAssessmentinQA
ThepredominantrelianceonROUGEforevaluat-
ingQAhallucinationdetectionmethodswarrants
carefulscrutiny,asitscoredesignforlexicalover-
lapdoesnotinherentlycapturefactualcorrectness.
Our in-depth analysis, presented in Appendix G,
revealsseveralcriticalfailuremodesthatsystemati-
callyundermineROUGE‚Äôsutilityforthistask. Key
limitationsinclude: sensitivitytoresponselength,
inability to handle semantic equivalence and sus-
ceptibilitytofalselexicalmatches.
15
10
5
0
Llama Mistral
)%(
etaR
rorrE
NQ-Open SQuAD Trivia-QA
Llama Mistral Llama Mistral
False Positives False Negatives
Figure2: ROUGEproducessystematicerrorsacross
allevaluationsettings. DistributionofFalseNegatives
andFalsePositivesacrossdifferentdatasetsandmodels
highlightstheinconsistencyinROUGE‚Äôsevaluation.
Thesefailuremodes,illustratedwithconcreteex-
amplesanderrordistributionsinFigure2,highlight
thepotentialforROUGEtoprovideamisleading
assessment of both LLM responses and the effi-
cacy of hallucination detection techniques. This

=== Page 5 ===
Table2:Detectionmethodsshowdramaticperformance
insteadofROUGE.PerformancecomparisonusingAUR
datasetsinzero-shotsetting,wherenegative‚àÜ%valuesr
NQ-Open SQ
Model Metric
ROUGE LLM ‚àÜ% ROUGE LL
Perplexity 0.709 0.700 -1.2 0.703 0.6
LN-Entropy 0.521 0.605 13.9 0.558 0.6
LLAMA SE 0.778 0.742 -4.8 0.707 0.7
Eigenscore 0.816 0.686 -19.0 0.720 0.6
eRank 0.825 0.632 -30.6 0.754 0.6
LogDet 0.511 0.515 0.7 0.521 0.5
Perplexity 0.852 0.584 -45.9 0.516 0.5
LN-Entropy 0.718 0.645 -11.3 0.734 0.6
MISTRAL SE 0.836 0.729 -14.7 0.784 0.7
Eigenscore 0.873 0.669 -30.4 0.803 0.6
eRank 0.925 0.678 -36.4 0.518 0.5
LogDet 0.628 0.508 -23.6 0.562 0.5
underscorestheneedforevaluationagainstmore
human-alignedmetrics.
5.2 QuantifyingtheEvaluationGap: ROUGE
vs. LLM-as-Judge
GiventheoutlinedlimitationsofROUGE,were-
evaluatedexistingunsupervisedhallucinationde-
tection methods using LLM-as-Judge, which, as
validatedbyourhumanstudy,offersacloseralign-
mentwithhumanjudgmentsoffactualcorrectness.
Main results As detailed in Table 2, hallucina-
tion detection methods that show promise under
ROUGE often suffer a substantial performance
dropwhenre-evaluatedwithLLM-as-Judge. For
instance,PerplexityseesitsAUROCscoreplum-
metbyasmuchas45.9%forthe MISTRALmodel
on NQ-Open. Similarly, Eigenscore‚Äôs perfor-
mance erodes by 19.0% and 30.4% for LLAMA
and MISTRAL, respectively, on the same dataset.
Even eRank, which posts impressive ROUGE-
basedscores,experiencesasharpdeclineof30.6%
and 36.4% under the LLM-as-Judge paradigm.
Moreover,whenevaluatedusingPR-AUC,weob-
serveevenlargerperformancediscrepanciesacross
allmethods(seeTables11and12intheAppendix
H.3);thisamplifiestheimpactofclassimbalance
in the QA setup, as further evidenced by the low
QAaccuraciesreportedinTable8.
Correlation This systematic discrepancy, visu-
ally underscored by the scatter plot in Figure 3,
pointstoafundamentalinadequacyinROUGE‚Äôs
abilitytoreflecttruehallucinationdetectionperfor-
mance. The moderate Pearson correlation coeffi-
cient (r = 0.55) between the AUROC scores de-
rivedfromthesetwoevaluationapproachesfurther

edropswhenevaluatedagainsthuman-alignedmetrics
ROCscoresforLLAMAandMISTRALmodelsacrossthree
revealROUGE‚Äôsoverestimationofmethodeffectiveness.
QuAD Trivia-QA Mean
LM ‚àÜ% ROUGE LLM ‚àÜ% ROUGE LLM ‚àÜ%
687 -2.4 0.733 0.789 7.2 0.715 0.725 1.2
611 8.7 0.563 0.636 11.5 0.547 0.617 11.4
705 -0.2 0.769 0.832 7.6 0.751 0.760 0.9
638 -12.7 0.752 0.734 -2.5 0.763 0.686 -11.4
621 -21.4 0.717 0.660 -8.6 0.765 0.638 -20.2
536 2.7 0.604 0.509 -18.6 0.545 0.520 -5.1
500 -3.2 0.843 0.627 -34.4 0.737 0.570 -27.8
657 -11.7 0.586 0.596 1.8 0.679 0.633 -7.1
701 -11.9 0.726 0.707 -2.6 0.782 0.712 -9.7
648 -24.0 0.775 0.652 -18.9 0.817 0.656 -24.4
511 -1.3 0.851 0.645 -31.9 0.765 0.611 -23.2
518 -8.5 0.843 0.606 -39.2 0.678 0.544 -23.8
1.0
0.9
0.8
0.7
0.6
0.5
0.5 0.6 0.7 0.8 0.9 1.0
ROUGE AUROC
CORUA
egduJ-sa-MLL
Eigenscore LogDet SE
LN-Entropy Mean-Len Std-Len
Len Perplexity eRank
r = 0.55
Figure3: ROUGEandhuman-alignedevaluations
show weak correlation across detection methods.
CorrelationbetweenROUGEandLLM-as-JudgeAU-
ROCscoresforthe MISTRAL model, witheachpoint
representingametric‚Äôsperformanceonspecificdataset.
suggests that methods may be inadvertently opti-
mizedforROUGE‚Äôslexicaloverlapcriteriarather
thangenuinefactualcorrectness. Notably,among
theevaluateddetectiontechniques,onlySemantic
Entropy maintains a degree of relative stability,
exhibitingmoremodestperformancevariationsbe-
tweenthetwoevaluationframeworks.
5.3 ImpactofFew-ShotExampleson
EvaluationReliability
Ouranalysisoffew-shotversuszero-shotsettings
revealsthreekeypatternsinhowexamplesaffect
evaluationstability(Table3).
Improved Metric Stability Few-shot settings
consistentlyyieldmorereliableevaluationsacross
metrics. For LLAMA, the discrepancy between
ROUGEandLLM-as-Judgenarrowssignificantly
withfew-shotexamples. Forinstance,eRank‚Äôsper-

=== Page 6 ===
formancedrop(forLLAMA)reducesfrom‚àí16.7%
in zero-shot to just ‚àí4.2% in few-shot settings.
Thissuggeststhatfew-shotexampleshelpstandard-
izeresponseformatswithmoreconsistentevalua-
tion.
Table3: Few-shotexamplesreducebutdon‚Äôtelimi-
nateevaluationbiases. Performancecomparisonshow-
ingrelativedifferencesbetweenROUGEandLLM-as-
Judgeinbothsettings.
Few-Shot Zero-Shot
Model Metric
ROUGE LLM ‚àÜ(%) ROUGE LLM ‚àÜ(%)
Perplexity 0.783 0.784 0.0 0.715 0.725 1.5
LN-Entropy 0.738 0.759 2.8 0.547 0.617 12.8
LLAMA SE 0.742 0.773 4.2 0.751 0.760 1.1
Eigenscore 0.761 0.747 -1.9 0.763 0.686 -10.0
eRank 0.707 0.678 -4.2 0.765 0.638 -16.7
Perplexity 0.806 0.645 -20.0 0.747 0.579 -22.4
LN-Entropy 0.754 0.659 -12.5 0.679 0.633 -6.8
MISTRAL SE 0.750 0.732 -2.4 0.782 0.712 -8.9
Eigenscore 0.760 0.694 -8.7 0.817 0.656 -19.7
eRank 0.829 0.697 -15.9 0.773 0.612 -20.8
Model-SpecificEffects Theimpactoffew-shot
examples varies notably between models. MIS-
TRAL showspronounceddegradationinzero-shot
settings, with performance drops up to 45.9%
(Perplexity),whileLLAMAmaintainsmorecon-
sistent performance, with some metrics showing
minimaldegradation. Thisvariationsuggeststhat
thearchitectureandpre-trainingmayinfluencethe
effectivenessoffew-shotcalibration.
MetricRobustness Differentmetricsshowvary-
ing levels of stability across settings. Semantic
Entropy maintains the most consistent perfor-
mance in both settings, while traditional metrics
likePerplexityorLN-Entropyshowhighersen-
sitivitytosettingchanges.
Implications Whilefew-shotexamplesgenerally
improve evaluation reliability, the degree of im-
provementvariessignificantlyacrossmodelsand
metrics. Thissuggeststhatrobusthallucinationde-
tectionsystemsshouldbevalidatedunderbothcon-
ditionstoensureconsistentperformanceacrossde-
ploymentscenarios. Ofparticularnoteisthatfew-
shotexamplesreduceevaluationdiscrepanciesby
providinganswerformatsthatmorecloselyalign
withgold-standardresponses. Thisindicatesthat
someoftheapparentimprovementsinfew-shotset-
tingsmaycomefrombetterformatmatchingrather
thanenhancedfactualassessment.
5.4 EvaluatingbeyondROUGE
While ROUGE remains a widely adopted metric,
itslimitationsunderscorebroaderconcernsabout

thereliabilityofreference-basedevaluationmeth-
ods. To assess whether alternative metrics fare
better,weextendedouranalysistoseveralothers
frequentlyusedorproposedfortextevaluation,in-
cluding BERTScore (Zhang et al., 2020), BLEU
(Papineni et al., 2002), SummaC (Laban et al.,
2022),andUniEval-fact(Zhongetal.,2022). We
evaluatedthesemetricsinbothfew-shotandzero-
shotsettings,benchmarkingtheiroutputsagainst
ourLLM-as-Judgelabels,whichshowstrongalign-
mentwithhumanjudgments(seeTable1).
Table 4: All metrics show limited alignment with
human-likejudgment,underscoringtheirshortcom-
ingsincapturingfactualcorrectness. Agreementof
differentcorrectnessmetricswithLLM-as-Judgelabels
inzero-shotsettings. Theresultsaveragedacrossthree
QAdatasets: NQ-Open,SQuAD,andTriviaQA.
Model Metric PRAUC AUROC F1 Precision Recall
BERTScore 0.735 0.769 0.723 0.609 0.934
BLEU 0.758 0.624 0.673 0.539 0.982
LLAMA ROUGE 0.891 0.878 0.812 0.728 0.926
SummaC 0.826 0.782 0.725 0.616 0.944
UniEval 0.828 0.830 0.762 0.739 0.804
BERTScore 0.736 0.730 0.725 0.586 0.990
BLEU 0.799 0.682 0.712 0.573 0.996
MISTRAL ROUGE 0.865 0.825 0.757 0.629 0.971
SummaC 0.836 0.778 0.758 0.648 0.950
UniEval 0.720 0.706 0.693 0.674 0.746
PerformanceofAlternativeMetrics Asshown
in Table 4, these alternative metrics also exhibit
substantialshortcomingsinreliablydetectinghallu-
cinationsinQAtasks,particularlyunderzero-shot
conditions. For example, BERTScore‚Äîdespite
leveragingcontextualembeddings‚Äîoftenfailedto
outperformsimplerlexicalmetricsinaligningwith
ourLLM-as-Judgelabels. BLEUandUniEval-fact
similarlydemonstratedlimitedeffectiveness.
Implications Theseresultssuggestthattheinad-
equaciesofROUGEarenotisolated,butindicative
of a broader challenge: current reference-based
metricsstruggletocapturefactualconsistency,of-
ten favoring surface-level similarity or structural
features such as length. Even when employing
few-shotprompting(seeTable13intheAppendix
I), which can help with answer formatting, these
metricsremainfundamentallyconstrainedintheir
abilitytoassessfactualcorrectness.
6 TheLengthFactor: AHiddenSignalin
HallucinationDetection
Our analysis reveals a surprising and significant
finding: responselengthaloneservesasapowerful

=== Page 7 ===
signalfordetectinghallucinations. Thisdiscovery
challengesconventionalwisdomabouthallucina-
tion detection and raises fundamental questions
aboutthecomplexityneededindetectionmethods.
Our investigation demonstrates that: (1) Simple
lengthstatisticscanserveassurprisinglyeffective
hallucinationdetectors,oftenmatchingorexceed-
ing more sophisticated methods; (2) The strong
influenceoflengthoncurrentevaluationmethods
raisesconcernsabouttheirabilitytoassessfactual
correctness independently of response verbosity;
(3)Thisrelationshipmayprovideinsightsintothe
underlyingmechanismsofhowLLMsgeneratein-
correctinformation.
6.1 LengthPatternsinHallucinated
Responses
300
200
100
0
correct incorrect
htgneL
rewsnA
NQ-Open Trivia-QA SQuAD
correct incorrect correct incorrect
Figure4: Hallucinationshaveadistinctlengthsigna-
tureinmodeloutputs. Distributionofanswerlengths
forMISTRALinafew-shotsettingswithLLM-as-Judge
labels,showingincorrectanswerstendtobelonger.
AnalysisofresponsedistributionsusingLLM-as-
Judgelabelsrevealsastrikingpattern: hallucinated
responsestendtobeconsistentlylongerandshow
greater length variance. This pattern holds true
not only in our primary datasets (Figure 4) but
also extends to the HaluEval dataset (Figure 6 in
AppendixJ),suggestingafundamentalrelationship
betweenverbosityandhallucination.
Thistendencytowardlongerresponseslikelyre-
flectstwokeymechanisms. First,modelsattempt
tomaintaincoherencewhilegeneratingincorrect
information,leadingtoadditionalcontextandelab-
oration. Second, initial errors often cascade into
further mistakes, creating a "snowball effect" of
increasingverbosity(Zhangetal.,2023)
6.2 LengthCorrelationswithExisting
Methods
Toquantifythisrelationship, weexaminedcorre-
lations between response length and various hal-
lucinationdetectionmetrics. Ouranalysisreveals
two critical findings. First, established methods
showunexpectedlystronglengthcorrelations(see
Table 5): Eigenscore and eRank exhibit particu-
larlyhighcorrelations,suggestingthesesupposedly

Figure5: ROUGE‚Äôsbiasagainstlongresponsesun-
derminesitsreliability. Distributionofanswerlength
versus ROUGE score for MISTRAL in few-shot set-
tings,revealingastrongcorrelationbetweenlengthand
ROUGEscores.
sophisticatedmethodsmaybeprimarilydetecting
lengthvariationsratherthansemanticfeatures. Sec-
ond,ROUGEscoresdemonstratesystematiclength
bias: As shown in Figure 5, responses exceed-
ing 100 tokens consistently receive scores below
the 0.3 threshold, regardless of factual accuracy.
This aligns with prior observations of hallucina-
tionsnowballing(Zhangetal.,2023),whereLLMs
compoundinitialerrorswithadditionalmistakes.
Table5: Sophisticateddetectionmethodsprimarily
capturelengtheffects. Pearsoncorrelationcoefficients
betweenmetricsandlength,showingunexpectedlyhigh
values.
Method Llama Mistral
LogDet -0.185 0.311
Perplexity 0.841 -0.423
eRank 0.763 0.803
Eigenscore 0.826 0.894
LN-Entropy 0.305 -0.753
Semantic Entropy 0.436 0.631
Thesecorrelationsraisefundamentalquestions
aboutwhethercurrenthallucinationdetectionmeth-
odsaretrulycapturingsemanticfeaturesorsimply
leveraginglength-basedpatterns.
6.3 LengthasaCompetitiveBaseline
Giventhesestrongcorrelations,wedevelopedthree
simple length-based metrics: the raw length of a
singlegeneration(Len),theaveragelengthacross
multiplegenerations(Mean-Len),andthestandard
deviationoflengthsacrossgenerations(Std-Len).
Evaluation results (Table 6) demonstrate that
thesestraightforwardmetricsachievesurprisingly
competitive performance. The Mean-Len metric
matchesoroutperformssophisticatedapproaches
likeEigenscoreandLN-Entropyacrossmultiple

=== Page 8 ===
datasets. Responselengthvariabilityprovestobe
akeyindicator, withStd-Lenshowingparticular
effectiveness at identifying hallucinations. Per-
hapsmostsurprisingly,eventhesimpleLenmetric
achievescompetitiveperformance,challengingthe
fundamentalneedforcomplexdetectionmethods.
Table 6: Simple length-based metrics achieve com-
petitive performance with sophisticated detection
methods. Hallucination detection performance (AU-
ROC)comparedacrossdatasetsandmodelsusingLLM-
as-Judge since it shows better alignment with human
judgements.
Model Metric NQ-Open SQuAD Trivia-QA Mean
LLAMA Perplexity 0.767 0.758 0.826 0.784
LN-Entropy 0.732 0.717 0.829 0.759
SE 0.730 0.741 0.849 0.773
Eigenscore 0.744 0.733 0.762 0.747
eRank 0.714 0.681 0.638 0.678
Len 0.686 0.687 0.640 0.671
Mean-Len 0.730 0.716 0.716 0.721
Std-Len 0.727 0.721 0.806 0.751
MISTRAL Perplexity 0.632 0.636 0.637 0.635
LN-Entropy 0.619 0.667 0.692 0.659
SE 0.734 0.698 0.765 0.732
Eigenscore 0.686 0.691 0.706 0.694
eRank 0.698 0.690 0.703 0.697
Len 0.664 0.685 0.729 0.693
Mean-Len 0.683 0.705 0.750 0.713
Std-Len 0.577 0.589 0.665 0.610
6.4 TheRepetitionExperiment: Validating
LengthEffects
Toisolatetheimpactoflengthonevaluationmet-
rics,weconductedacontrolledexperimentusing
systematicrepetition. Wemodifiedmodeloutputs
byiterativelyduplicatingsentenceswhilemaintain-
ing the same factual content. Results in Table 7
reveal a concerning trend: AUROC scores con-
sistently improve with increased repetition, even
though information content remains unchanged.
This experiment highlights a critical distinction:
while verbose or repetitive responses may be in-
efficient, they aren‚Äôt necessarily hallucinations if
thecoreinformationiscorrect. However,current
evaluationapproaches,includingbothROUGEand
length-basedmetrics,failtomakethisdistinction.
Table7: ROUGEscorescanbemanipulatedthrough
simple repetition. AUROC measurements for MIS-
TRALwhenrepeatingthesamecontentmultipletimes.
Dataset 0 1 2 4
NQ-Open 0.852 0.935(+9.7) 0.955(+12.1) 0.964(+13.1)
SQuAD 0.842 0.894(+6.2) 0.909(+8.0) 0.948(+12.6)
Trivia-QA 0.843 0.901(+6.9) 0.907(+7.6) 0.919(+9.0)

7 Discussion
Our results reveal a clear misalignment between
reference-basedmetrics,suchasROUGE,andhu-
man judgments in identifying hallucinations in
QA. Despite the short, focused nature of QA an-
swers‚Äîwhere n-gram overlap might seem suffi-
cient‚Äîthesemetricsconsistentlyrewardfluentyet
factually incorrect responses. While ROUGE is
widely used, we further evaluated more sophisti-
catedmetrics‚ÄîBERTScore,BLEU,andUniEval-
fact ‚Äî against judgments from a strong LLM-
basedevaluator,andsimilarlyobservedsubstantial
disagreement,underscoringthelimitationsofthese
metrics in capturing factual consistency. While
carefulpromptengineeringordataset-specificpost-
processing techniques might offer marginal im-
provements in ROUGE scores, these approaches
often lack scalability and generalizability across
different models and datasets. As demonstrated
inourexperiments,modelsfrequentlydisregarded
explicit brevity instructions (see prompts in Ap-
pendixD),makingthepursuitofanoptimal,univer-
sallyapplicablepromptnon-trivialendeavor. The
fundamental limitation of these reference-based
metrics‚Äîtheir general insensitivity to factual ve-
racitywhenmaskedbysuperficiallexicalsimilar-
ity‚Äîpersists. This is further underscored by our
finding that simple response length can often be
a more effective indicator of hallucinations than
some sophisticated detection methods, question-
ingthecurrenttrajectoryofdetectordevelopment.
These collective observations necessitate a shift
towardsmorerobustandsemanticallyawareevalu-
ationparadigms.
8 Conclusions
Wedemonstratethatprevailingoverlap-basedmet-
ricssystematicallyoverestimatehallucinationde-
tection performance in QA, leading to illusory
progress. LLM-as-Judge evaluation, validated
against human judgments, exposes steep perfor-
mance drops across all methods when judged by
factual accuracy. Moreover, because simple sig-
nalslikeanswerlengthcanmatchcomplexdetec-
tors, we caution against over-engineering: effec-
tivebaselinesareessentialformeaningfuladvance-
ment.
Limitations
While our study provides valuable insights into
thelimitationsofROUGEforhallucinationdetec-

=== Page 9 ===
tion,severalconstraintsshouldbeacknowledged.
First,ouranalysisprimarilyfocusesonasubsetof
LLMs and datasets, which may not fully capture
thediversityofmodelsandtasksinthefield. Con-
sequently, the generalizability of our findings to
othercontextsremainstobevalidated. Second,al-
thoughweproposeresponselengthasasimpleyet
effectiveheuristicfordetectinghallucinations,this
approachmaynotaccountfornuancedcaseswhere
longerresponsesarefactuallyaccurate. Addition-
ally,ourrelianceonLLM-as-Judgeasabenchmark
forhuman-alignedevaluation, whilemorerobust
than ROUGE, is not without its own biases and
limitations. Futureworkshouldexplorealternative
evaluationmetricsandexpandthescopeofanalysis
toincludeabroaderrangeofmodelsanddatasets.
Finally,whileourcontrolledexperimentshighlight
thepotentialformanipulationofROUGEscores,
furtherresearchisneededtodevelopmetricsthat
arebothrobusttosuchmanipulationsandaligned
with human judgment. The primary risk is that
over-relianceonlength-basedheuristicsandpoten-
tiallybiasedhuman-alignedmetricscouldleadto
inaccurateassessmentsofhallucinationdetection
methods,resultinginthedeploymentofLLMsthat
may not reliably ensure factual accuracy in high-
stakesapplications.
References
Gabriel Y. Arteaga, Thomas B. Sch√∂n, and Nico-
las Pielawski. 2024. Hallucination Detection in
LLMs:FastandMemory-EfficientFinetunedModels.
ArXiv:2409.02976[cs].
Amos Azaria and Tom Mitchell. 2023. The Inter-
nal State of an LLM Knows When It‚Äôs Lying.
ArXiv:2304.13734[cs].
Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu,
Mingyuan Tao, Zhihang Fu, and Jieping Ye. 2024.
INSIDE:LLMs‚Äôinternalstatesretainthepowerof
hallucinationdetection. InTheTwelfthInternational
ConferenceonLearningRepresentations.
XuefengDu,ChaoweiXiao,andSharonLi.2024. Halo-
scope: Harnessingunlabeledllmgenerationsforhal-
lucinationdetection. InAdvancesinNeuralInforma-
tionProcessingSystems,volume37,pages102948‚Äì
102972.CurranAssociates,Inc.
JinhaoDuan, HaoCheng, ShiqiWang, AlexZavalny,
ChenanWang,RenjingXu,BhavyaKailkhura,and
KaidiXu.2024. Shiftingattentiontorelevance: To-
wards the predictive uncertainty quantification of
free-form large language models. In Proceedings
of the 62nd Annual Meeting of the Association for

ComputationalLinguistics(Volume1: LongPapers),
pages5050‚Äì5063,Bangkok,Thailand.Association
forComputationalLinguistics.
Nouha Dziri, Ehsan Kamalloo, Sivan Milton, Os-
mar Zaiane, Mo Yu, Edoardo M. Ponti, and Siva
Reddy.2022. FaithDial: Afaithfulbenchmarkfor
information-seeking dialogue. Transactions of the
AssociationforComputationalLinguistics,10:1473‚Äì
1490.
OpenAI et al. 2024. GPT-4 Technical Report.
ArXiv:2303.08774[cs].
Ekaterina Fadeeva, Roman Vashurin, Akim Tsvigun,
ArtemVazhentsev,SergeyPetrakov,KirillFedyanin,
Daniil Vasilev, Elizaveta Goncharova, Alexander
Panchenko, Maxim Panov, Timothy Baldwin, and
Artem Shelmanov. 2023. LM-polygraph: Uncer-
taintyestimationforlanguagemodels. InProceed-
ingsofthe2023ConferenceonEmpiricalMethods
in Natural Language Processing: System Demon-
strations,pages446‚Äì461,Singapore.Associationfor
ComputationalLinguistics.
SebastianFarquhar,JannikKossen,LorenzKuhn,and
Yarin Gal. 2024. Detecting hallucinations in large
language models using semantic entropy. Nature,
630(8017):625‚Äì630. Publisher: NaturePublishing
Group.
Quentin Garrido, Randall Balestriero, Laurent Na-
jman, and Yann Lecun. 2023. RankMe: As-
sessing the downstream performance of pre-
trainedself-supervisedrepresentationsbytheirrank.
ArXiv:2210.02885[cs].
Aaron et al. Grattafiori. 2024. The Llama 3 Herd of
Models. ArXiv:2407.21783[cs].
OrHonovich, RoeeAharoni, JonathanHerzig, Hagai
Taitelbaum,DoronKukliansy,VeredCohen,Thomas
Scialom, Idan Szpektor, Avinatan Hassidim, and
Yossi Matias. 2022. TRUE: Re-evaluating Factual
ConsistencyEvaluation. ArXiv:2204.04991[cs].
LeiHuang,WeijiangYu,WeitaoMa,WeihongZhong,
Zhangyin Feng, Haotian Wang, Qianglong Chen,
WeihuaPeng,XiaochengFeng,BingQin,andTing
Liu. 2025. A survey on hallucination in large lan-
guagemodels: Principles,taxonomy,challenges,and
openquestions. ACMTrans.Inf.Syst.,43(2).
AlbertQ.Jiang,AlexandreSablayrolles,ArthurMen-
sch,ChrisBamford,DevendraSinghChaplot,Diego
de las Casas, Florian Bressand, Gianna Lengyel,
Guillaume Lample, Lucile Saulnier, L√©lio Re-
nard Lavaud, Marie-Anne Lachaux, Pierre Stock,
TevenLeScao,ThibautLavril,ThomasWang,Timo-
th√©eLacroix,andWilliamElSayed.2023. Mistral
7B. ArXiv:2310.06825[cs].
MandarJoshi,EunsolChoi,DanielS.Weld,andLuke
Zettlemoyer. 2017. TriviaQA: A Large Scale Dis-
tantly Supervised Challenge Dataset for Reading
Comprehension. ArXiv:1705.03551[cs].

=== Page 10 ===
HaoqiangKang,TerraBlevins,andLukeZettlemoyer.
2024. Comparinghallucinationdetectionmetricsfor
multilingualgeneration.
JannikKossen,JiatongHan,MuhammedRazzak,Lisa
Schut,ShreshthMalik,andYarinGal.2024. Seman-
ticEntropyProbes: RobustandCheapHallucination
DetectioninLLMs. ArXiv:2406.15927[cs].
TomKwiatkowski, JennimariaPalomaki, OliviaRed-
field,MichaelCollins,AnkurParikh,ChrisAlberti,
DanielleEpstein,IlliaPolosukhin,JacobDevlin,Ken-
tonLee,KristinaToutanova,LlionJones,Matthew
Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
ralQuestions: ABenchmarkforQuestionAnswer-
ing Research. Transactions of the Association for
ComputationalLinguistics,7:452‚Äì466. Place: Cam-
bridge,MAPublisher: MITPress.
PhilippeLaban,TobiasSchnabel,PaulN.Bennett,and
MartiA.Hearst.2022. SummaC:Re-visitingNLI-
basedmodelsforinconsistencydetectioninsumma-
rization. TransactionsoftheAssociationforCompu-
tationalLinguistics,10:163‚Äì177.
JunyiLi,XiaoxueCheng,WayneXinZhao,Jian-Yun
Nie, and Ji-Rong Wen. 2023. HaluEval: A Large-
ScaleHallucinationEvaluationBenchmarkforLarge
LanguageModels. ArXiv:2305.11747[cs].
Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
maticEvaluationofSummaries. InTextSummariza-
tionBranchesOut,pages74‚Äì81,Barcelona,Spain.
AssociationforComputationalLinguistics.
ZhenLin,ShubhenduTrivedi,andJimengSun.2023.
GeneratingwithConfidence: UncertaintyQuantifi-
cationforBlack-boxLargeLanguageModels. Pub-
lisher: arXivVersionNumber: 3.
Andrey Malinin and Mark Gales. 2021. Uncertainty
estimationinautoregressivestructuredprediction. In
International Conference on Learning Representa-
tions.
HieuNguyen,ZihaoHe,ShoumikAtulGandre,Ujjwal
Pasupulety, Sharanya Kumari Shivakumar, and
Kristina Lerman. 2025. Smoothing out hallucina-
tions: Mitigating llm hallucination with smoothed
knowledgedistillation.
AlexanderNikitin,JannikKossen,YarinGal,andPekka
Marttinen. 2024. Kernel language entropy: Fine-
graineduncertaintyquantificationforllmsfromse-
mantic similarities. In Advances in Neural Infor-
mationProcessingSystems,volume37,pages8901‚Äì
8929.CurranAssociates,Inc.
HadasOrgad,MichaelToker,ZorikGekhman,RoiRe-
ichart,IdanSzpektor,HadasKotek,andYonatanBe-
linkov.2024. LLMsKnowMoreThanTheyShow:
OntheIntrinsicRepresentationofLLMHallucina-
tions. ArXiv:2410.02707.

HadasOrgad,MichaelToker,ZorikGekhman,RoiRe-
ichart,IdanSzpektor,HadasKotek,andYonatanBe-
linkov.2025. LLMsknowmorethantheyshow: On
theintrinsicrepresentationofLLMhallucinations. In
TheThirteenthInternationalConferenceonLearning
Representations.
KishorePapineni,SalimRoukos,ToddWard,andWei-
JingZhu.2002. Bleu: amethodforautomaticevalu-
ationofmachinetranslation. InProceedingsofthe
40thAnnualMeetingoftheAssociationforCompu-
tational Linguistics, pages 311‚Äì318, Philadelphia,
Pennsylvania,USA.AssociationforComputational
Linguistics.
XinQiuandRistoMiikkulainen.2024. Semanticden-
sity: Uncertainty quantification for large language
modelsthroughconfidencemeasurementinsemantic
space. InAdvancesinNeuralInformationProcessing
Systems,volume37,pages134507‚Äì134533.Curran
Associates,Inc.
Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
KnowWhatYouDon‚ÄôtKnow: UnanswerableQues-
tionsforSQuAD. ArXiv:1806.03822[cs].
JieRen,JiamingLuo,YaoZhao,KundanKrishna,Mo-
hammad Saleh, Balaji Lakshminarayanan, and Pe-
ter J Liu. 2023. Out-of-distribution detection and
selective generation for conditional language mod-
els. In The Eleventh International Conference on
LearningRepresentations.
OlivierRoyandMartinVetterli.2007. TheEffective
Rank: aMeasureofEffectiveDimensionality.
Gaurang Sriramanan, Siddhant Bharti, Vinu Sankar
Sadasivan, Shoumik Saha, Priyatham Kattakinda,
andSoheilFeizi.2024a. Llm-check: Investigating
detection of hallucinations in large language mod-
els. InAdvancesinNeuralInformationProcessing
Systems,volume37,pages34188‚Äì34216.CurranAs-
sociates,Inc.
Gaurang Sriramanan, Siddhant Bharti, Vinu Sankar
Sadasivan, Shoumik Saha, Priyatham Kattakinda,
and Soheil Feizi. 2024b. LLM-Check: Investigat-
ingDetectionofHallucinationsinLargeLanguage
Models.
WeihangSu,ChangyueWang,QingyaoAi,YiranHU,
ZhijingWu,YujiaZhou,andYiqunLiu.2024. Un-
supervisedReal-TimeHallucinationDetectionbased
on the Internal States of Large Language Models.
ArXiv:2403.06448[cs].
AmanSinghThakur,KartikChoudhary,VenkatSrinik
Ramayapally,SankaranVaidyanathan,andDieuwke
Hupkes.2025. Judgingthejudges: Evaluatingalign-
mentandvulnerabilitiesinllms-as-judges.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond,ClementDelangue,AnthonyMoi,Pier-
ricCistac,TimRault,RemiLouf,MorganFuntowicz,
JoeDavison,SamShleifer,PatrickvonPlaten,Clara
Ma, YacineJernite, JulienPlu, CanwenXu, Teven

=== Page 11 ===
LeScao,SylvainGugger,MariamaDrame,Quentin
Lhoest, and Alexander Rush. 2020. Transformers:
State-of-the-Art Natural Language Processing. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations,pages38‚Äì45,Online.Association
forComputationalLinguistics.
ZiweiXu,SanjayJain,andMohanKankanhalli.2024.
HallucinationisInevitable: AnInnateLimitationof
LargeLanguageModels. ArXiv:2401.11817.
YuhengZha,YichiYang,RuichenLi,andZhitingHu.
2023. AlignScore: Evaluating factual consistency
with a unified alignment function. In Proceedings
of the 61st Annual Meeting of the Association for
ComputationalLinguistics(Volume1: LongPapers),
pages11328‚Äì11348,Toronto,Canada.Association
forComputationalLinguistics.
Muru Zhang, Ofir Press, William Merrill, Alisa Liu,
and Noah A. Smith. 2023. How Language Model
Hallucinations Can Snowball. ArXiv:2305.13534
[cs].
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger,andYoavArtzi.2020. Bertscore: Evalu-
atingtextgenerationwithBERT. In8thInternational
ConferenceonLearningRepresentations,ICLR2020,
AddisAbaba,Ethiopia,April26-30,2020.
LianminZheng,Wei-LinChiang,YingSheng,Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
ZhuohanLi,DachengLi,E.Xing,HaotongZhang,
JosephE.Gonzalez,andIonStoica.2023a. Judging
LLM-as-a-judgewithMT-BenchandChatbotArena.
ArXiv.
LianminZheng,Wei-LinChiang,YingSheng,Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023b.
Judging llm-as-a-judge with mt-bench and chatbot
arena. AdvancesinNeuralInformationProcessing
Systems,36:46595‚Äì46623.
Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu
Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and
Jiawei Han. 2022. Towards a unified multi-
dimensional evaluator for text generation. In Pro-
ceedingsofthe2022ConferenceonEmpiricalMeth-
ods in Natural Language Processing, pages 2023‚Äì
2038,AbuDhabi,UnitedArabEmirates.Association
forComputationalLinguistics.
Appendix
A LicensesandComputationalResources
A.1 Datasets,modelslicense
Thedatasetsandmodelsusedinthisstudyaresub-
jecttospecificlicenses. NQ-Open,TriviaQA,and
SQuAD are available under licenses that permit
academicuse. TheLLAMA3.1-8B-INSTRUCTand

MISTRAL-7B-INSTRUCT-V0.3modelsareopen-
source and can be accessed under their respec-
tive licenses, which allow for research and non-
commercialuse.3
A.2 HardwareSpecifications
We generated data using Nvidia A40 with 40GB
VRAM.Fortheremainingcomputations,weused
CPU.
B HumanInvolvementandEthics
B.1 AnnotatorRecruitmentandConsent
Participants were recruited through personal net-
works(friendsandacquaintances)andparticipated
voluntarilywithoutfinancialcompensation. They
were informed of the study‚Äôs purpose and data
usage beforehand. Verbal consent was obtained,
andnopersonallyidentifiableinformationwascol-
lected. Participants had the right to withdraw at
anytime.
B.2 Demographics
AllannotatorswereresidentsofPoland. Nosystem-
aticcollectionofage,gender,orotherdemographic
informationwasconducted.
C UseofAIAssistance
AIassistantssuchasChatGPTwereutilizedinvar-
iousaspectsoftheresearch,includingcoding,data
analysis,andwritingtasks. Thesetoolshelpedto
automate repetitive tasks, generate initial drafts,
and assist in exploring potential solutions. How-
ever,allAI-generatedoutputswerereviewedand
refinedbyresearcherstoensureaccuracyandco-
herence.
D Prompts
We used the following prompt formats to elicit
responsesfromthemodels:
‚Ä¢ QA (Zero-shot): Minimal prompt with no
examples(Listing1)
‚Ä¢ QA(Few-shot): Adaptedfrom(Kossenetal.,
2024),includesmultipleQAexamples(List-
ing2)
‚Ä¢ LLM-as-Judge: Evaluationpromptwithcor-
rectness labels, adapted from (Orgad et al.,
2024)(Listing3)
3Fordetailedlicenseinformation,pleaserefertothere-
spectivedatasetandmodeldocumentation.

=== Page 12 ===
Listing1: Zero-shotprompttemplate
Answer the following question as briefly
as possible.
Question: {question}
Answer:
Listing2: QA(Few-shot)prompttemplate
Answer the following question as briefly
as possible.
Here are several examples:
Question: What is the capital of France?
Answer: Paris
Question: Who wrote Romeo and Juliet?
Answer: William Shakespeare
Question: What is the boiling point of
water in Celsius?
Answer: 100
Question: How many continents are there
on Earth?
Answer: Seven
Question: What is the fastest land
animal?
Answer: Cheetah
Question: {question}
Answer:
Listing3: LLM-as-Judgeprompttemplate
Answer the following question as briefly
as possible.
Here are several examples:
Question: who is the young guitarist who
played with Buddy Guy?
Ground Truth: Quinn Sullivan, Eric Gales
Model Answer: Ronnie Earl
Correctness: incorrect
Question: What is the name of the actor
who plays Iron Man in the Marvel
movies?
Ground Truth: Robert Downey Jr.
Model Answer: Robert Downey Jr. played
the role of Tony Stark/Iron Man in
the Marvel Cinematic Universe films.
Correctness: correct
Question: What is the capital of France?
Ground Truth: Paris
Model Answer: I don't have enough
information to answer this question.
Correctness: refuse
Question: Who was the first person to
walk on the moon?
Ground Truth: Neil Armstrong
Model Answer: I apologize, but I cannot
provide an answer without verifying
the historical facts.

Correctness: refuse
Question: {question}
Ground Truth: {gold}
Model Answer: {prediction}
Correctness:
E AdditionalAnalysisofHuman
Evaluation
Forthehumanevaluationcomponentofourstudy
(Section 4), we intentionally curated a dataset of
instances where ROUGE and our LLM-as-Judge
metricprovidedconflictingassessmentsregarding
the presence of hallucinations. This targeted se-
lectionstrategywasemployedtoenableafocused
examination of ROUGE‚Äôs specific failure modes.
Byconcentratingonthesepointsofdisagreement,
weaimedtogaindeeperinsightsintothescenarios
whereROUGE‚Äôsrelianceonlexicaloverlapdemon-
strablymisalignswithhumanjudgmentsoffactual
accuracyandoverallresponsequality.
F EvaluationMetricsandHallucination
Detection
F.1 eRank
eRankleverageseigenvalue-basedentropyestima-
tioninhiddenstates:
(cid:32) m (cid:33)
(cid:88)
eRank = exp ‚àí p logp (1)
k k
k=1
wherep = Œª k ,andŒª aretheeigenvaluesof
k (cid:80)m
j=1
Œªj k
thecovariancematrixŒ£ = ZTZ computedonthe
hiddenstatesZ.
We use Effective Rank (eRank) as a proxy for
how‚Äúspreadout‚Äùor‚Äúdiverse‚Äùthefinal-layerhid-
denrepresentationsare(however,wecanalsothe
thehiddenrepresentationfrommiddle-layer). In-
tuitively, if the model‚Äôs representation space col-
lapsestofewerdimensions(i.e.,loweRank),itmay
indicatethatthemodelisrelyingonlesscontextor
ignoringcrucialinputsignals‚Äîoftenmanifesting
ashallucinations. Conversely,ahighereRanksug-
gestsaricher,morenuancedencodingoftheinput,
whichtypicallycorrelateswithmoregroundedand
accurateresponses. Thisapproachbuildsonprior
work(Sriramananetal.,2024b)(LogDet),which
computes the log-determinant of the covariance
matrix.
While initial evaluations under ROUGE sug-
gested some promise, we found that eRank did

=== Page 13 ===
notconsistentlycorrelatewithhallucinationrates
acrossalldatasetsandsettingswhenassessedus-
inghuman-alignedmetrics. This‚Äônegativeresults‚Äô
illustrate how ROUGE‚Äôs limitations can mislead
methoddevelopment.
G UnderstandingROUGE‚ÄôsFailure
Modes
Throughdetailederroranalysis,weidentifythree
critical limitations in ROUGE‚Äôs evaluation ap-
proach: (1)sensitivitytoresponselength,(2)inabil-
ity to handle semantic equivalence, and (3) over-
reliance on exact lexical matches. Our analysis
revealsthattheselimitationsleadtobothfalseneg-
atives‚Äîfactually correct responses marked as in-
correct‚Äîandfalsepositives‚Äîincorrectresponses
receivinghighscores. AsshowninFigure2,these
errorsoccurfrequentlyacrossdifferentdatasetsand
models.
G.1 Length-BasedPenalties
Question: WhenwasPrideandPrejudicewrit-
ten?
Prediction: ‚ÄúPrideandPrejudicewaswritten
byJaneAustenandpublishedin1813.‚Äù
GoldAnswer: ‚Äú1813‚Äô
ROUGEsystematicallypenalizesfactuallycor-
rectbutverboseanswers. Inthisexample,despite
providing accurate information with helpful con-
text,theresponsereceivesalowscorepurelydue
tolengthmismatch. AsshowninFigure5,thisbias
affectslongerresponsesregardlessoftheirfactual
accuracy,withresponsesexceeding100tokenscon-
sistentlyscoringbelowour0.3threshold. Notably,
thisisthemostfrequenttypeoferrorROUGE
makes.
G.2 SemanticEquivalenceFailures
Question: Whatisoneelementatopographic
mapshows?
Prediction: ‚ÄúElevation‚Äù
GoldAnswer: ‚ÄúRelief‚Äù
ROUGEfailstorecognizesemanticequivalence
between different phrasings. Here, despite "ele-
vation"and"relief"beingcontextuallyequivalent
termsintopography,ROUGEassignsalowerscore
duetolexicalmismatch. Thislimitationsystemati-
callyundervaluesresponsesthatusevalidalterna-
tiveterminology.

G.3 FalseLexicalMatches
Question: ‚ÄúHow many episodes of Grey‚Äôs
Anatomyseason14?‚Äù
Prediction: ‚Äú23episodes.‚Äù
GoldAnswer: ‚Äú24episodes.‚Äù
ROUGEcanassignhighscorestofactuallyin-
correct answers that share surface structure with
thereference. Despitethecriticalnumericalerror,
theresponsereceivesarelativelyhighscoredueto
matching surrounding words. This creates a dan-
gerousbiastowardstructurallysimilarbutfactually
wronganswers.
H QuantitativeResults
H.1 QAAccuracyAcrossSettings
Table8presentstheaccuraciesontheQAdatasets.
These accuracies are computed by selecting the
mostlikelyansweratalowtemperaturesettingand
comparingittolabelsderivedfromeitherROUGE
orLLM-as-Judgeevaluations.
Table8: Accuraciesofdifferentmodels,datasets,and
promptsfortheQAtask.
Accuracy
Dataset Model Prompt #Refused ROUGE LLM
NQ-Open Llama Few-Shot 692 28.1% 29.2%
NQ-Open Llama Zero-Shot 139 24.2% 43.2%
NQ-Open Mistral Few-Shot 117 20.9% 35.8%
NQ-Open Mistral Zero-Shot 72 7.8% 39.0%
SQuAD Llama Few-Shot 924 22.0% 18.3%
SQuAD Llama Zero-Shot 447 20.2% 25.0%
SQuAD Mistral Few-Shot 230 16.0% 22.6%
SQuAD Mistral Zero-Shot 116 5.8% 25.3%
Trivia-QA Llama Few-Shot 95 63.7% 69.4%
Trivia-QA Llama Zero-Shot 39 58.8% 71.1%
Trivia-QA Mistral Few-Shot 11 53.8% 69.7%
Trivia-QA Mistral Zero-Shot 2 29.0% 64.8%
H.2 MetricEvaluation: AUROC
Tables9and10presentcomprehensiveresultscom-
paringLLM-basedandROUGE-basedevaluation
metricsacrossthreedatasets: NQ-Open,SQuAD,
andTrivia-QA.Weevaluateninedifferentmetrics
using AUROC evaluation metric for both Llama
andMistralmodelsunderzero-shotandfew-shot
settings.
H.3 MetricEvaluation: PR-AUC
Tables 11 and 12 provide PR-AUC scores under
thesameconditions.

=== Page 14 ===
NQ-Open SQuA
Model Metric
ROUGE LLM ‚àÜ% ROUGE LL
Llama Perplexity 0.709 0.700 -1.2 0.703 0.6
Llama LN-Entropy 0.521 0.605 13.9 0.558 0.6
Llama SE 0.778 0.742 -4.8 0.707 0.7
Llama Eigenscore 0.816 0.686 -19.0 0.720 0.6
Llama eRank 0.825 0.632 -30.6 0.754 0.6
Llama Len 0.834 0.616 -35.3 0.777 0.6
Llama LogDet 0.511 0.515 0.7 0.521 0.5
Llama Mean-Len 0.825 0.654 -26.1 0.743 0.6
Llama Std-Len 0.711 0.644 -10.5 0.664 0.6
Mistral Perplexity 0.852 0.584 -45.9 0.516 0.5
Mistral LN-Entropy 0.718 0.645 -11.3 0.734 0.6
Mistral SE 0.836 0.729 -14.7 0.784 0.7
Mistral Eigenscore 0.873 0.669 -30.4 0.803 0.6
Mistral eRank 0.925 0.678 -36.4 0.518 0.5
Mistral Len 0.934 0.634 -47.2 0.860 0.6
Mistral LogDet 0.628 0.508 -23.6 0.562 0.5
Mistral Mean-Len 0.890 0.643 -38.4 0.828 0.6
Mistral Std-Len 0.516 0.512 -0.7 0.540 0.5
Table9: FullcomparisonofLLM-basedandROUGE-bas
SQuAD,andTrivia-QA)forLlamaandMistralmodelsin
‚àÜ%columnsshowtherelativepercentagedifferencebetw
averagedscoresacrossalldatasets.
I GroundTruthLabelingMetrics
Toevaluateandcompareautomaticlabelingstrate-
gies,weexaminedtheagreementbetweenvarious
evaluationmetricsandtheLLM-as-Judgeannota-
tions (Table 13). This analysis provides insight
into the reliability of proxy labeling methods for
hallucinationdetection.
J HaluEvalAnswerLengthDistribution
Figure 6 illustrates answer lengths across the
HaluEvaldataset(Lietal.,2023).
750
500
250
correct incorrect
htgneL
rewsnA
summarization dialogue qa
200
100
100
0 0
correct incorrect correct incorrect
Figure6: Length-basedhallucinationpatternsgen-
eralizeacrossdatasets. Answerlengthdistributionfor
HaluEvaltasks,showingconsistentpatterns.

AD Trivia-QA Mean
LM ‚àÜ% ROUGE LLM ‚àÜ% ROUGE LLM ‚àÜ%
687 -2.4 0.733 0.789 7.2 0.715 0.725 1.2
611 8.7 0.563 0.636 11.5 0.547 0.617 11.4
705 -0.2 0.769 0.832 7.6 0.751 0.760 0.9
638 -12.7 0.752 0.734 -2.5 0.763 0.686 -11.4
621 -21.4 0.717 0.660 -8.6 0.765 0.638 -20.2
622 -24.9 0.760 0.691 -10.0 0.790 0.643 -23.4
536 2.7 0.604 0.509 -18.6 0.545 0.520 -5.1
643 -15.7 0.771 0.743 -3.8 0.780 0.680 -15.2
627 -6.0 0.759 0.754 -0.7 0.711 0.675 -5.7
500 -3.2 0.843 0.627 -34.4 0.737 0.570 -27.8
657 -11.7 0.586 0.596 1.8 0.679 0.633 -7.1
701 -11.9 0.726 0.707 -2.6 0.782 0.712 -9.7
648 -24.0 0.775 0.652 -18.9 0.817 0.656 -24.4
511 -1.3 0.851 0.645 -31.9 0.765 0.611 -23.2
624 -37.8 0.929 0.673 -37.9 0.908 0.644 -41.0
518 -8.5 0.843 0.606 -39.2 0.678 0.544 -23.8
626 -32.2 0.875 0.667 -31.3 0.864 0.645 -34.0
505 -6.9 0.613 0.572 -7.2 0.556 0.530 -4.9
sedevaluationmetricsacrossdifferentdatasets(NQ-Open,
nzero-shotsettingusingAUROCevaluationmetric. The
weenLLMandROUGEscores. Meancolumnspresentthe

=== Page 15 ===
NQ-Open SQuA
Model Metric
ROUGE LLM ‚àÜ% ROUGE LL
Llama Perplexity 0.814 0.767 -6.1 0.736 0.7
Llama LN-Entropy 0.753 0.732 -2.9 0.663 0.7
Llama SE 0.738 0.730 -1.1 0.688 0.7
Llama Eigenscore 0.813 0.744 -9.3 0.725 0.7
Llama eRank 0.794 0.714 -11.2 0.708 0.6
Llama Len 0.761 0.686 -10.9 0.694 0.6
Llama LogDet 0.729 0.690 -5.6 0.659 0.6
Llama Mean-Len 0.799 0.730 -9.4 0.713 0.7
Llama Std-Len 0.777 0.727 -7.0 0.705 0.7
Mistral Perplexity 0.804 0.632 -27.1 0.782 0.6
Mistral LN-Entropy 0.727 0.619 -17.4 0.785 0.6
Mistral SE 0.772 0.734 -5.3 0.737 0.6
Mistral Eigenscore 0.789 0.686 -15.0 0.775 0.6
Mistral eRank 0.874 0.698 -25.1 0.829 0.6
Mistral Len 0.879 0.664 -32.2 0.857 0.6
Mistral LogDet 0.737 0.663 -11.2 0.687 0.6
Mistral Mean-Len 0.834 0.683 -22.1 0.822 0.7
Mistral Std-Len 0.609 0.577 -5.6 0.629 0.5
Table10: FullcomparisonofLLM-basedandROUGE-bas
SQuAD,andTrivia-QA)forLlamaandMistralmodelsin
‚àÜ%columnsshowtherelativepercentagedifferencebetw
averagedscoresacrossalldatasets.
NQ-Open SQuA
Model Metric
ROUGE LLM ‚àÜ% ROUGE LL
Llama Perplexity 0.833 0.680 -22.4 0.863 0.8
Llama LN-Entropy 0.717 0.611 -17.4 0.793 0.7
Llama SE 0.845 0.695 -21.5 0.864 0.8
Llama Eigenscore 0.850 0.670 -26.8 0.866 0.8
Llama eRank 0.782 0.607 -28.9 0.820 0.7
Llama Len 0.865 0.681 -27.2 0.885 0.8
Llama LogDet 0.852 0.659 -29.2 0.873 0.8
Llama Mean-Len 0.851 0.658 -29.3 0.870 0.8
Llama Std-Len 0.825 0.647 -27.6 0.846 0.8
Mistral Perplexity 0.664 0.536 -23.8 0.951 0.7
Mistral LN-Entropy 0.882 0.664 -32.8 0.920 0.7
Mistral SE 0.956 0.725 -31.8 0.964 0.8
Mistral Eigenscore 0.957 0.698 -37.1 0.965 0.8
Mistral eRank 0.658 0.506 -30.0 0.955 0.7
Mistral Len 0.964 0.682 -41.4 0.973 0.8
Mistral LogDet 0.964 0.699 -37.9 0.950 0.7
Mistral Mean-Len 0.958 0.671 -42.8 0.966 0.7
Mistral Std-Len 0.891 0.583 -52.8 0.889 0.7
Table11: FullcomparisonofLLM-basedandROUGE-bas
SQuAD,andTrivia-QA)forLlamaandMistralmodelsin
‚àÜ%columnsshowtherelativepercentagedifferencebetw
averagedscoresacrossalldatasets.

AD Trivia-QA Mean
LM ‚àÜ% ROUGE LLM ‚àÜ% ROUGE LLM ‚àÜ%
758 2.9 0.800 0.826 3.1 0.783 0.784 -0.0
717 7.5 0.799 0.829 3.6 0.738 0.759 2.7
741 7.1 0.800 0.849 5.7 0.742 0.773 3.9
733 1.2 0.745 0.762 2.3 0.761 0.746 -1.9
681 -4.0 0.620 0.638 2.8 0.707 0.678 -4.1
687 -1.0 0.620 0.640 3.1 0.692 0.671 -2.9
636 -3.7 0.590 0.618 4.5 0.659 0.648 -1.6
716 0.4 0.681 0.716 4.8 0.731 0.721 -1.4
721 2.2 0.783 0.806 2.9 0.755 0.751 -0.6
636 -23.0 0.744 0.637 -16.7 0.777 0.635 -22.3
667 -17.7 0.750 0.692 -8.3 0.754 0.659 -14.5
698 -5.6 0.741 0.765 3.1 0.750 0.732 -2.6
691 -12.2 0.717 0.706 -1.5 0.760 0.694 -9.6
690 -20.1 0.786 0.703 -11.8 0.830 0.697 -19.0
685 -25.1 0.858 0.729 -17.7 0.865 0.693 -25.0
631 -8.9 0.612 0.630 2.9 0.679 0.641 -5.7
705 -16.5 0.806 0.750 -7.4 0.821 0.713 -15.3
589 -6.8 0.663 0.665 0.3 0.634 0.610 -4.0
sedevaluationmetricsacrossdifferentdatasets(NQ-Open,
nfew-shotsettingusingAUROCevaluationmetric. The
weenLLMandROUGEscores. Meancolumnspresentthe
AD Trivia-QA Mean
LM ‚àÜ% ROUGE LLM ‚àÜ% ROUGE LLM ‚àÜ%
823 -4.8 0.594 0.514 -15.6 0.763 0.672 -14.3
773 -2.6 0.570 0.652 12.5 0.693 0.679 -2.5
829 -4.1 0.575 0.533 -7.9 0.761 0.686 -11.2
809 -7.1 0.565 0.574 1.6 0.760 0.684 -10.8
783 -4.6 0.674 0.760 11.3 0.759 0.717 -7.4
820 -8.0 0.605 0.548 -10.4 0.785 0.683 -15.2
810 -7.8 0.602 0.562 -7.1 0.776 0.677 -14.7
808 -7.7 0.573 0.568 -0.9 0.765 0.678 -12.6
802 -5.5 0.562 0.570 1.4 0.744 0.673 -10.6
754 -26.0 0.690 0.752 8.3 0.768 0.681 -13.8
790 -16.4 0.625 0.633 1.3 0.809 0.696 -16.0
819 -17.7 0.808 0.510 -58.3 0.909 0.685 -35.9
804 -20.0 0.818 0.544 -50.3 0.913 0.682 -35.8
755 -26.4 0.534 0.704 24.2 0.716 0.655 -10.7
803 -21.0 0.849 0.536 -58.4 0.929 0.674 -40.3
753 -26.1 0.847 0.550 -54.1 0.920 0.667 -39.4
786 -22.9 0.833 0.548 -52.1 0.919 0.668 -39.3
724 -22.7 0.755 0.605 -24.8 0.845 0.637 -33.4
sedevaluationmetricsacrossdifferentdatasets(NQ-Open,
nzero-shotsettingusingPR-AUCevaluationmetric. The
weenLLMandROUGEscores. Meancolumnspresentthe

=== Page 16 ===
NQ-Open SQuA
Model Metric
ROUGE LLM ‚àÜ% ROUGE LL
Llama Perplexity 0.844 0.824 -2.4 0.861 0.8
Llama LN-Entropy 0.810 0.796 -1.8 0.828 0.8
Llama SE 0.814 0.802 -1.5 0.842 0.8
Llama Eigenscore 0.829 0.802 -3.4 0.852 0.8
Llama eRank 0.746 0.726 -2.8 0.711 0.7
Llama Len 0.834 0.806 -3.5 0.856 0.8
Llama LogDet 0.817 0.800 -2.1 0.859 0.8
Llama Mean-Len 0.825 0.798 -3.4 0.852 0.8
Llama Std-Len 0.820 0.794 -3.2 0.846 0.8
Mistral Perplexity 0.506 0.520 2.7 0.624 0.6
Mistral LN-Entropy 0.508 0.505 -0.6 0.587 0.6
Mistral SE 0.872 0.754 -15.7 0.898 0.8
Mistral Eigenscore 0.873 0.738 -18.4 0.902 0.8
Mistral eRank 0.515 0.526 2.0 0.855 0.7
Mistral Len 0.897 0.735 -22.1 0.918 0.8
Mistral LogDet 0.895 0.734 -21.9 0.869 0.7
Mistral Mean-Len 0.879 0.734 -19.7 0.907 0.8
Mistral Std-Len 0.827 0.683 -20.9 0.873 0.8
Table12: FullcomparisonofLLM-basedandROUGE-bas
SQuAD,andTrivia-QA)forLlamaandMistralmodelsin
‚àÜ%columnsshowtherelativepercentagedifferencebetw
averagedscoresacrossalldatasets.

AD Trivia-QA Mean
LM ‚àÜ% ROUGE LLM ‚àÜ% ROUGE LLM ‚àÜ%
891 3.4 0.551 0.502 -9.8 0.752 0.739 -2.9
874 5.3 0.525 0.522 -0.5 0.721 0.731 1.0
879 4.3 0.536 0.506 -6.1 0.731 0.729 -1.1
876 2.7 0.511 0.542 5.7 0.731 0.740 1.7
762 6.8 0.679 0.737 7.9 0.712 0.742 4.0
884 3.1 0.522 0.571 8.7 0.737 0.754 2.8
882 2.6 0.526 0.582 9.6 0.734 0.755 3.4
878 2.9 0.509 0.553 7.9 0.729 0.743 2.5
873 3.1 0.526 0.524 -0.3 0.731 0.730 -0.1
673 7.4 0.740 0.778 4.9 0.623 0.657 5.0
615 4.5 0.759 0.825 8.0 0.618 0.648 4.0
843 -6.5 0.609 0.538 -13.3 0.793 0.712 -11.8
842 -7.2 0.598 0.567 -5.5 0.791 0.716 -10.4
789 -8.4 0.606 0.736 17.8 0.659 0.684 3.8
848 -8.3 0.687 0.530 -29.7 0.834 0.704 -20.0
793 -9.5 0.673 0.561 -19.8 0.812 0.696 -17.1
844 -7.5 0.629 0.548 -14.7 0.805 0.709 -14.0
808 -8.0 0.546 0.608 10.1 0.749 0.700 -6.3
sedevaluationmetricsacrossdifferentdatasets(NQ-Open,
nfew-shotsettingusingPR-AUCevaluationmetric. The
weenLLMandROUGEscores. Meancolumnspresentthe

=== Page 17 ===
Table13: Few-ShotEvaluationMetricsagreementwith
LLM-as-Judgelabels. Theresultsaveragedacrossthree
QAdatasets: NQ-Open,SQuAD,andTriviaQA.
Model Metric PRAUC AUROC F1 Precision Recall
BERTScore 0.810 0.848 0.776 0.742 0.859
BLEU 0.775 0.536 0.699 0.576 0.976
LLAMA ROUGE 0.935 0.921 0.883 0.866 0.906
SummaC 0.850 0.776 0.760 0.653 0.977
UniEval 0.943 0.933 0.862 0.868 0.868
BERTScore 0.764 0.770 0.749 0.637 0.958
BLEU 0.784 0.627 0.707 0.581 0.987
MISTRAL ROUGE 0.903 0.878 0.820 0.738 0.932
SummaC 0.855 0.795 0.758 0.657 0.957
UniEval 0.813 0.801 0.754 0.751 0.778