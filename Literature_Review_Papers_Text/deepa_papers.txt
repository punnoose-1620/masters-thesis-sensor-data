=== Page 1 ===
2024 International Conference on Artificia
Enhancing Large Mode
Answering through Re
Jingwen Zeng1a, Rongrong Zheng1b, Chenhui Wan
1 State Grid Information & Telecom
2 State Grid Sgitg Digital Technolog
asgitb3099@163.com,bchristinazrr@126.com,c264025558@
*Corresponding author Email: z
Abstract Document question answering(DocQA) requires
models to provide comprehensive answers, given a series of
document content and questions. In recent years, large neural
models have been widely applied in fields such as natural
language processing and computer vision, yielding significant
results. However, these applications usually face a challenge that
the large language model often produce fake information, owing
to model illusions. To tackle this problem, the recent proposed
document question answering has proven to be effective. Current
methods primarily rely on vector for passage retrieval, but the
effectiveness is often limited, which restrict the model’s
performance. To address this issue effectively, we propose a
dual-path retrieval along with precise ranking framework to
enhance existing knowledge retrieval. To better evaluate this task,
we also constructed a manually annotated test set for validation.
Experimental results demonstrate the significant advantages of
our model.
Keywords: document question answering, large language
modal
I. INTRODUCTION
Document question answering(DocQA) requires models to
provide comprehensive answers based on a series of
document content and questions. Currently, the emergence of
numerous large-scale language models, such as ChatGPT [4],
Baichuan [7], qwen [6], Vicuan [9], and Llama [8] attracts
wild attention. They possess advantages in terms of data,
parameters, and architecture, exhibiting powerful language
comprehension and generation capabilities, leading to
outstanding performance in natural language processing tasks.
Therefore, they are widely applied in downstream natural
language processing and information extraction tasks.
However, these models may encounter data distributions
during training that differ from those in actual application
scenarios, making it impossible to access real-time, non-
public, or offline data, resulting in a lack of relevant
knowledge and the generation of false information in practical
applications, known as model illusions. Hence, it is risky to
directly apply such large-scale language models for
information retrieval and queries.
To address this issue, some methods [12,13,14,15,16]
adopt external knowledge to alleviate the illusions of large
models. By incorporating an external knowledge base, models
can access additional background knowledge and information,
which helps enhance the model's generalization ability and
robustness. These external knowledge resources can
encompass various types of data, including structured data,
979-8-3503-9221-0/24/$31.00 ©2024 IEEE 24
DOI 10.1109/AIPS64124.2024.00057
75000.4202.42146SPIA/9011.01
:IOD
|
EEEI
4202©
00.13$/42/0-1229-3053-8-979
|
)SPIA(
smetsyS
rewoP
dna
ecnegilletnI
laicifitrA
no
ecnerefnoC
lanoitanretnI
4202
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

al Intelligence and Power Systems (AIPS)
el Document Question
etrieval Augmentation
ng1c, Wenting Xue1d, Xiaoyang Yu1e, Tao Zhang 2*
mmunication Branch, Beijing, China
gy(Beijing) Co., Ltd, Beijing, China
@qq.com,dxuewt442@163.com,eyu1224754419@163.com,
zhangtao01@sgitg.sgcc.com.cn
text data, graph data, and more. By integrating this external
knowledge, models can better understand and interpret data,
enrich their semantic expression capabilities, and improve
their ability to generalize to unknown data. External
knowledge can also mitigate data sparsity issues and assist
models in learning domain-specific information. The
introduction of an external knowledge base can enhance the
model's cognitive abilities, improve its performance across
various tasks, effectively mitigating the hallucination issues
present in large models.
In order to accurately generate answers to questions, the
core module of introducing external knowledge relies on the
model's ability to find the most relevant parts from a massive
corpus of knowledge. Currently, there are two mainstream
methods for implementing the document retrieval step: 1) the
use of vector retrieval, such as models like M3E [2],
Text2Vec [1], OpenAI-Ada [4], etc. These methods, trained
on large-scale sentence pair datasets, can cover the majority of
retrieval scenarios and are highly efficient, making them
wildly used in industrial applications. 2) The use of matching
models, such as BERT [10] and BGE [5]. Although these
models often achieve promising results, they are less efficient
in large-scale text retrieval and reasoning, thus limiting their
application in document question answering.
Current DocQA methods often employ vector retrieval to
retrieve document fragments. Although this method can
handle some specific usage scenarios, it has two drawbacks
that limit the capabilities of document question answering: 1)
Since the vector retrieval model is based on retrieving the
entire sentence, it performs poorly when dealing with sparse
documents (i.e., documents containing a large amount of
scattered keyword information); 2) Compared to the deep
interactive capabilities of matching models, vector models
often shows low accuracy, making it easier to retrieve a large
number of irrelevant documents during the retrieval process,
thus affecting the generated results. In this paper, we propose
a dual-path retrieval composite recall architecture, to
overcome the aforementioned limitations. To address the
shortcomings of vector models in dealing with sparse
information retrieval, we introduce lexical matching to
enhance the retrieval of sparse information. Through this dual-
path retrieval approach, effective information can be confined
to a very small range. This step is also known as coarse-
ranking process. To further enhance the model's effectiveness,
we also design a matching model for precision ranking
retrieval, thereby further filtering out irrelevant information.
Since the dual-path retrieval has removed most irrelevant
49
ed on January 30,2026 at 08:22:21 UTC from IEEE Xplore. Restrictions apply.

=== Page 2 ===
information, the candidate texts are greatly reduced, ensuring
the efficiency of the precision ranking retrieval. Furthermore,
as there is currently no publicly available Chinese test set for
document question answering, we constructed a document
question answering test set to evaluate the effectiveness of
different models, which has been annotated by professionals.
Experimental results demonstrate that our proposed approach
surpasses previously proposed models, validating the
effectiveness of this method.
II. RELATEDWORK
DocQA [12,13,14,15,16] has become an important
paradigm in open-domain question answering, where the
Figure 1. The framework of our method, comprising three main steps: cr
A. Creating Knowledge Base
The creation of the knowledge base involves three steps:
knowledge loading, knowledge segmentation, and knowledge
vectorization. Generally, new data other than the training
dataset is referred to as external data. The data loading phase
includes operations such as loading data from documents, and
processing the data into a unified format based on its
characteristics. In the text segmentation phase, two main
factors are considered: 1) the token limitations of the
embedding model, and 2) the impact of semantic integrity on
overall retrieval effectiveness. Segmenting text into fixed
lengths (e.g., 256/512 tokens) results in a loss of significant
semantic information. Therefore, we segment the text based
on sentences to preserve the complete semantic meaning of
each sentence. Common segmentation symbols include
periods, exclamation points, question marks, line breaks, etc.
Subsequently, context concatenation is performed within 512
words to reduce computing burden.
Vectorization is the process of converting text data into
vectors, which directly influences the subsequent retrieval
effectiveness. It transform data into vector representations and
store them in a vector database by creating a knowledge base.
The M3E [2] is the most commonly used embedding models,
generally meet most requirements. Considering that this is a
25
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

retriever model first obtain relevant passages, which are then
processed by the reader model to generate answers. Reader
models are typically classified as either extractive or
generative models, with the former pinpointing the answer
span in the provided context and the latter generating answers
token by token. However, current DocQA [11] mainly relies
on vector-based passage retrieval, which limits the model's
capabilities.
III. METHOD
Our framework is illustrated in Figure 1, comprising three
main steps: creating knowledge base, relevant passage retrieve,
and LLM generation.
reating knowledge base, relevant passage retrieve, and LLM generation.
common scenarios, we do not further finetune embedding
models. Generally, the process of indexing the vectorized data
and loading it into the database can be summarized as the data
entry process. In this case, we utilize the FAISS as vector
knowledge base, consider its outstanding performance.
B. Relevant Passage Retrieve
The purpose of this step is to retrieve the most relevant
content snippets from the knowledge base for a given query.
Specifically, the retrieval module we designed consists of two
main modules: dual-path coarse retrieval and precise matching
retrieval. For a given query, we first utilize the effective M3E
[2] model and the bm25 [17] lexical retrieval model to
retrieve several candidate text blocks most relevant to the
current query from the vector database. This dual-path fusion
search approach provides superior retrieval results by
combining two complementary search algorithms—it
considers both semantic similarity between the query and
stored documents and keyword matching, thus catering to
retrieval needs at different levels of information granularity.
Although some candidate retrieval results are obtained
through the dual-path algorithm, they may still contain
irrelevant information. Due to the differing evaluation criteria
of vector retrieval and keyword retrieval, precise sorting
cannot be achieved through them. To address this issue, we
50
ed on January 30,2026 at 08:22:21 UTC from IEEE Xplore. Restrictions apply.

=== Page 3 ===
re-rank the coarse-ranking passages using a matching model
[3] to filter out irrelevant content, ensuring that subsequent
large-scale models can access higher-quality knowledge to
enhance generation quality. It is important to note that since a
significant amount of irrelevant content has already been
filtered out through dual-path coarse ranking, applying the
matching model for precise sorting does not significantly
increase computational overhead, thereby maintaining the
efficiency of the model's operation.
C. LLM Generation
This is the final step of the whole system—generating
answers based on all the retrieved contexts and the given user
query. Here, we concatenate all the acquired contexts with the
query statement and provide them to the LLM. By
incorporating the retrieved relevant data, we enhance the
capabilities of the language model. This step effectively
combines prompt engineering techniques with LLM,
empowering prompts to enable large language models to
generate accurate answers to user queries.
IV. EXPERIMENT
A. Dataset and Evaluation
We selected 50 real documents from the financial domain,
covering the current performance and expectations across
various industries. We invited three senior analysts from the
financial sector to construct 150 common queries in the
financial domain based on their expertise, with each question
being associated with a paragraph from the corresponding text
document. Subsequently, these questions were annotated with
standard answers by the analysts. We provide data analysis in
Table 1, where the dataset contains 150 questions and 50
documents. The longest question is 67 words. In our
experiments, we split the documents into 6800 snippets, each
containing less than 1000 words.
In the evaluation process, we used matching accuracy and
generation accuracy to assess the model's quality. Matching
accuracy can be automatically calculated, considering a
retrieved text snippet as correct as long as it fully contains the
knowledge segment; otherwise, it is considered incorrect if it
does not contain or only partially contains the segment.
Generation accuracy, on the other hand, was manually
annotated by the three analysts based on their expertise.
Table 1. data analysis.
dataset statistics
question 150
document 50
passage snippet 6800
Max length of question 67
Max length of snippet 1000
B. Experiment Setting
In the table 2, we compared different models. ChatGPT
represents the large model directly providing answers,
DocQA(V) represents using a vector model to retrieve one
knowledge snippets to enhance the capabilities of ChatGPT,
which is currently the most common approach. Additionally,
25
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

DocQA(V+S) introduces lexical retrieval and semantic
retrieval, each retrieving one snippets to enhance the model's
capabilities. For DocQA (V+S+R), we also introduced a
precision ranking model using BGE-base model. DocQA
(V+S+R+) indicates further domain adaptation of the BGE
model using document data. Specifically, we segmented the
articles into numerous fragments, utilizing ChatGPT to
generate a question for each fragment, and then fine-tuned the
BGE model using the questions and knowledge snippets.
Table 2. Information on video and audio files that can accompany a
manuscript submission.
Model Generation ACC Recall ACC
ChatGPT 10.67% None
DocQA(V) 53.33% 56%
DocQA(V+S) 62% 64.67%
DocQA(V+S+R) 73.33% 76.67%
DocQA(V+S+R+) 80.67% 84%
C. Main Result
In our experimental results shown in Table 2, it can be
observed that without introducing any external knowledge,
ChatGPT performs the worst due to generating a significant
amount of illusions and false knowledge. With the
introduction of vector retrieval, i.e., DocQA(V), the
generation accuracy increased from 10.67% to 53.33%,
highlighting the importance of incorporating external
knowledge. Furthermore, in cases of correct retrieval,
ChatGPT is likely to be correct, confirming the critical role of
the retrieval stage. When both semantic and lexical retrieval
are employed, i.e., DocQA(V+S), the model's performance
further improves the performance to 62% and 64.67 in terms
of recall and generation accuracy respectively, demonstrating
the importance of dual-path retrieval in retrieving information
in situations of sparse knowledge. Additionally, the
introduction of a precision ranking model, i.e., DocQA
(V+S+R), further enhances generation accuracy to 73.33%,
showcasing the effectiveness of our approach. Finally,
DocQA (V+S+R+) further finetunes the BGE model, which
increase the accuracy to 80.67%, as it boost the domain
adaptability. Through these experiments and comparative
studies, the results indicate a significant improvement of our
method on this dataset. This helps better understand the issue
of knowledge illusions in large models, enhancing the model's
robustness and performance. Finally, we obverse that
D. Case Study
To visually demonstrate the superiority of our model, we
provide a case study in Table 3, which compares our model
with mainstream vector retrieval methods. In this example, the
question is "What impact does the El Niño phenomenon have
on agricultural products," involving terms such as "El Niño"
and "agriculture." We can observe that traditional retrieval
models easily locate incorrect segments due to the presence of
these terms in erroneous snippets. However, by employing a
composite retrieval approach, the retrieval capability
significantly improves, enabling the correct retrieval of
knowledge snippets and consequently providing the accurate
answer.
51
ed on January 30,2026 at 08:22:21 UTC from IEEE Xplore. Restrictions apply.

=== Page 4 ===
Table 3. Information on video and audio files
Model Recall passage
Recall model According to the latest report released by the World M
Organization (WMO) on May 3, this year's possibilit
the El Niño climate phenomenon is increasing, which
temperatures and extreme weather. The WMO predic
May to July 2023, the likelihood of transitioning from
to an El Niño event is 60%, rising to 80% from July t
Niño event will lead to global climate patterns becom
in frequent disastrous weather events. Typically, the E
causes drought conditions in regions such as Southea
while increasing rainfall in the central equatorial Paci
to the west coast of South America. For China, El Ni
cause warm winters, heavy rain and flooding in the so
conditions in the north, and cold summers in the nort
that extreme weather events such as droughts and flo
food production patterns, bringing more uncertainty t
supplies. Agriculture, Forestry, Animal Husbandry, a
Tracking Report | May 23, 2023. Please read the disc
after the text. Nino 3.4 Index (°C) Data source: Natio
Center of the United States (including forecasts), CIT
Research Department Note: It is generally considered
3.4 Index exceeds 0.5°C and persists for more than 6
classified as an El Niño event; if it exceeds 1.5°C and
than 9 months, it can be classified as a strong El Niño
ours The uncertainty surrounding El Niño is increasing, an
situation in the sugar market may continue. Accordin
Meteorological Organization (WMO) forecast, this ye
El Niño climate phenomenon is increasing, which ma
temperatures and extreme weather globally. It is belie
weather events such as droughts and floods may disru
production order, bringing more uncertainty to intern
According to the forecast data from the global trade c
in May 2023, it is indicated that for the 2023/24 fisca
a gap between supply and demand for sugar globally,
situation is expected to continue. The El Niño event m
weather in Asia this year compared to normal years, a
reduced sugar production in Brazil and India. Domes
expected to be tight, and in the short term, domestic s
continue to rise. It is recommended that domestic sug
such as COFCO Sugar, with a business model encom
overseas," "processing + trading," and "futures + spo
industry chain layout, are well positioned in the sugar
Ground-truth El Niño uncertainty is increasing, and the tight sugar
persist. According to the World Meteorological Orga
likelihood of El Niño climate phenomenon occurring
increasing, potentially bringing high temperatures and
globally. We believe that extreme weather events suc
floods may disrupt the global food production order,
uncertainty in international food supply. Data from th
consulting firm Czapp in May 2023 indicates that the
between supply and demand for sugar in the 2023/24
supply situation is expected to continue. The El Niño
weather in Asia this year compared to normal years a
reduced sugarcane production in Brazil and India. Do
supply is expected to be tight, and domestic sugar pri
rise in the short term. We recommend that domestic s
COFCO Sugar should adopt a business model encom
overseas," "processing + trading," "futures + spot trad
complete industry chain layout.
25
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

that can accompany a manuscript submission.
Answer
Meteorological Based on the information available, the El Niño
ty of the occurrence of phenomenon may have the following impacts on
h may bring about high agricultural products:
cts in the report that in It may lead to unstable climate patterns globally,
m a neutral ENSO state triggering frequent disastrous weather events,
to September. The El including extreme weather phenomena such as
ming unstable, resulting droughts and floods.
El Niño phenomenon It may cause drought conditions in regions such as
ast Asia and Australia, Southeast Asia and Australia, while increasing
cific region extending rainfall in the central equatorial Pacific region
iño events are likely to extending to the west coast of South America.
outh, hot and dry In China, it may lead to abnormal climate
heast. It is believed phenomena such as warm winters, heavy rain and
oods may disrupt global flooding in the south, hot and dry conditions in the
to international food north, and cold summers in the northeast.
and Fishery Industry
claimer and statement
onal Climate Prediction
TIC Securities
d that when the Nino
6 months, it can be
d persists for more
o event.
nd the tight supply The El Niño phenomenon may have the following
ng to the World impacts on agricultural products:
year's likelihood of the It could lead to global high temperatures and
ay bring about high extreme weather, potentially triggering extreme
eved that extreme climate events like droughts and floods, disrupting
upt the global food the global food production order, and increasing
national food supplies. uncertainty in international food supply.
consulting firm Czapp The Asian region may become drier, which could
al year, there still exists result in reduced sugarcane production in
y, and the tight supply countries like Brazil and India.
may lead to drier There is still a gap between supply and demand for
and it may result in sugar globally, and the tight supply situation is
stic sugar supply is expected to continue, potentially affecting the
sugar prices may sugar industry and prices.
gar industry giants
mpassing "domestic +
ot trading," and a full
ar industry.
supply situation may Based on the information mentioned in the
anization (WMO), the document, the El Niño phenomenon may lead to a
g this year is tendency for global climate patterns to become
nd extreme weather unstable, potentially bringing high temperatures
ch as droughts and and extreme weather. Typically, the El Niño
leading to more phenomenon leads to drought conditions in
he global trade Southeast Asia, Australia, and increased rainfall in
ere is still a gap the central equatorial Pacific to the west coast of
4 period, and the tight South America. For China, the El Niño event is
o event may cause drier likely to result in a mild winter, increased risk of
and could result in heavy rainfall and flooding in the south, elevated
omestic white sugar risk of high temperatures and drought in the north,
ices may continue to and the potential for a cooler summer in the
sugar industry leader northeast. In the forecast for the Yangtze River
mpassing "domestic + Basin, the overall precipitation is expected to be
ding," and establish a below average this year due to the influence of the
El Niño phenomenon, resulting in a greater
prevalence of drought over flooding. Additionally,
the El Niño event may disrupt the global food
production order, leading to increased uncertainty
in international food supply. Therefore, it can be
anticipated that the El Niño phenomenon will have
a certain impact on agricultural production and
supply, including the potential for drought,
flooding, and extreme weather. However, the
specific extent and scope of these impacts still
require further observation and assessment.
52
ed on January 30,2026 at 08:22:21 UTC from IEEE Xplore. Restrictions apply.

=== Page 5 ===
V. CONCLUSION
This paper introduces a novel dual-path method to address
the issue of knowledge illusions in large models, achieving
superior results. To better evaluate this task, we also
constructed a manually annotated test set for validation. In the
future, we will further optimize retrieval and ranking models,
and explore applications in other domains to validate the
generalizability and scalability of this method.
ACKNOWLEDGEMENT
This work is sponsored by State Grid Information &
Telecommunication Branch self-managed technology project
(ID: 529939230008).
REFERENCES
[1] Xu Ming, Text2vec: A Tool for Text to Vector, 2022.
[2] Wang Yuxin, Sun Qingxuan, He sicheng，M3E: Moka Massive Mixed
Embedding Model, 2023.
[3] J ohnson J, Douze M, Jégou H. Billion-scale similarity search with GPUs
[J]. IEEE Transactions on Big Data, 2019, 7(3): 535-547.
[4] Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and
Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and
Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others,
Training language models to follow instructions with human feedback,
Advances in Neural Information Processing Systems, 2022
[5] Xiao S, Liu Z, Zhang P, et al. C-pack: Packaged resources to advance
general chinese embedding [J]. arXiv preprint arXiv:2309.07597, 2023.
[6] Qwen Group, Qwen Technical Report, arXiv preprint arXiv, 2023.
25
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

[7] baichuan2023baichuan2, Bichuan 2: Open Large-scale Language
Models, arXiv preprint, 2023.
[8] LLaMA: Open and Efficient Foundation Language Models.
[9] V icuna: An Open-Source Chatbot Impressing GPT-4 with 90%*
ChatGPT Quality.
[10] D evlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep
bidirectional transformers for language understanding[J]. arXiv preprint
arXiv:1810.04805, 2018.
[11] Soong D, Sridhar S, Si H, et al. Improving accuracy of gpt-3/4 results
on biomedical data using a retrieval-augmented language model[J].
arXiv preprint arXiv:2305.17116, 2023.
[12] Es S, James J, Espinosa-Anke L, et al. Ragas: Automated evaluation of
retrieval augmented generation[J]. arXiv preprint arXiv:2309.15217,
2023.Dense passage retrieval for opendomain question answering.
[13] Guu K, Lee K, Tung Z, et al. Retrieval augmented language model pre-
training[C]//International conference on machine learning. PMLR, 2020:
3929-3938.
[14] Jiang Z, Araki J, Ding H, et al. How can we know when language
models know? on the calibration of language models for question
answering [J]. Transactions of the Association for Computational
Linguistics, 2021, 9: 962-977.
[15] Izacard G, Grave E. Leveraging passage retrieval with generative
models for open domain question answering [J]. arXiv preprint
arXiv:2007.01282, 2020.
[16] Lewis P, Perez E, Piktus A, et al. Retrieval-augmented generation for
knowledge-intensive nlp tasks[J]. Advances in Neural Information
Processing Systems, 2020, 33: 9459-9474.
[17] R obertson S, Zaragoza H, Taylor M. Simple BM25 extension to
multiple weighted fields[C]//Proceedings of the thirteenth ACM
international conference on Information and knowledge management.
2004: 42-49.
53
ed on January 30,2026 at 08:22:21 UTC from IEEE Xplore. Restrictions apply.

Paper:A_distributed_search_engine_based_on_a_re-ranking_algorithm_model.pdf
=== Page 1 ===
The 10th International Conference on
Computer Science & Education (ICCSE 2015)
July 22-24, 2015. Fitzwilliam College, Cambridge University, UK
A Distributed Search Engi
Algorithm
Jingyong Wan, Beizhan Wang*, W
Software School of
Xiamen
jywan@outlook.com, wangbz@xmu.edu.cn, guowei910516@1
Abstract—With the rapid increase of websites and the explosive
growth of Internet information, the centralized search engine will
face great challenge in mass data processing and mass data
storage. However, the distributed search engine can solve the
problem effectively. In this paper, we describe the design and
implementation of a distributed search engine that is based on
Apache Nutch, Solr and Hadoop. Considering users click logs, we
propose a re-ranking algorithm based on Lucene scoring. Our
experimental results show that our approaches significantly
satisfy users’ massive data searching demand while maintaining
high reliability and scalability.
Index Terms—Distributed Search Engine, Hadoop, Re-ranking
Algorithm.
I. INTRODUCTION
With the rapid increase of websites and the arrival of Big
Data, the centralized search engine will face great challenge in
the mass websites crawling, indexing and the real-time
searching. Search engine should have the distributed
processing capabilities to enhance the system's ability of
processing information. Distributed search engine can make up
for the shortcomings of centralized search engine.
A distributed search engine can be divided into three
modules: distributed crawling module, distributed indexing
module and distributed retrieval module [1]. In this paper, we
design a distributed search engine system, which is based on
three open-source software: Nutch[2,3], Hadoop[4,5] and
Solr[6]. We use the MapReduce parallel computing framework
and distributed file system HDFS to implement our distributed
search engine. In particular, we use MapReduce to crawl the
web pages and store all the pages in the HDFS. Then the pages
are submitted to Solr server to create index. In order to improve
system reliability and performance, we set up three Solr search
engine servers and use ZooKeeper[7] to manage them, which is
a service for coordinating processes of distributed applications.
ZooKeeper distributes the processing job of mass pages to
multiple servers to create indexes in indexing phase. After that,
every Solr server stores a part of the whole index files. It
avoids the storage problem of only one machine and enhances
the indexing efficiency because of distributed indexing. It is
also because the index files are located in many machines, we
need ZooKeeper to coordinate all search results in every Solr
server, then return the final result to users. Figure 1 shows the
system architecture diagram of the distributed search engine we
designed.
978-1-4799-6600-4/15/$31.00 ©2015 IEEE 64
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

K FrP5.4
ine Based on a Re-ranking
m Model
Wei Guo, Kang Chen, Jiajun Wang
Xiamen University
n, China
163.com, checkking@foxmail.com, wangjiajun422@gmail.com
Solr is a high performance search server built on Apache
Lucene, so its scoring method is Lucene scoring actually.
Lucene implements a variant of the TF-IDF scoring model [8].
This scoring strategy is determined only by the correlation of
document and query. It is a prior ranking strategy. Since the
users’ demand is different although they give the same query,
the search results based on this strategy cannot meet users’
demand in many times. Since users’ true intentions can often
be seen from their click behaviors, we propose a novel re-
ranking algorithm, which takes users’ feedback into account
according to users click logs, to make up for the inadequacy of
Lucene scoring.
This paper is organized as follows. In Section 2, we
describe the design of distributed search engine. Section 3
focuses on the re-ranking algorithm we proposed. Section 4 is
our experimental process and results. We end this paper with
some conclusions in section 5.
II. DISTRIBUTED SEARCH ENGINE DESIGN
The distributed search engine includes web pages crawling
and parsing, index creating and searching. Apache Nutch Wiki
[9] describes the crawling process in detail. In this section, we
Solr Solr Solr
Index Database Index Database Index Database
Searcher Indexer Searcher Indexer Searcher Indexer
Query
Search Engine
ZooKeeper
Interface
Re-ranking
Result
Hadoop
Mapper Mapper
Reducer Reducer
Nutch Mapper Mapper
Mapper Reducer Mapper Reducer
WWW
HDFS
Fig.1 The system architecture diagram of the distributed search engine
40
ed on January 30,2026 at 12:14:48 UTC from IEEE Xplore. Restrictions apply.

=== Page 2 ===
describe the implementation of the distributed indexing module
in detail.
The web pages contents, which are parsed in the previous
stages, are stored in HDFS. We encapsulate the parsed web
contents and related properties in NutchDocument class using
MapReduce job. Then we create indexes by Solr interface. The
MapReduce processes are as follows:
1) Map phase
Input: <url, Parse Contests>, “Parse Contents” represents the
parsed web contents.
Process: Filtering. Encapsulating in NutchWritable Class.
Output: <url, NutchWritable>.
2) Reduce Phase
Input: <url, [NutchWritable Set]>.
Process: Parsing out the url, title, content, host, segment,
boost, digest and tstamp fields. Encapsulating in
NutchDocument Class.
Output: <url, NutchDocument>.
The NutchDocument, outputted by reducer, is indexed by
Solr servers. We use ZooKeeper to manage multiple Solr
servers, when indexing request is received, ZooKeeper will
allocate the indexing requests to one of the multiple Solr
servers. The indexing jobs will be assigned to multiple Solr
servers and each server will store a part of the whole index files.
That can reduce the burden to create and store the index files
using only a single server.
III. RE-RANKING ALGORITHM
A. Re-ranking Algorithm Model
In our model, we define some notations for brevity. The
click represents whether the result is clicked or not. The q is
short for query which represents the user’s search query. The
pos is the position in the given result page. Our distributed
search engine system will calculate P(click|q,url) and
P(click| pos), which represent the click ratio of the url when
given the query and the click ratio in the position pos when
given result page respectively. Our model is based on statistical
probability theory. Only when the number of occurrences of a
query is more than the threshold we set did we use our re-
ranking algorithm. It makes our model more convincing.
Equation 1 is the scoring formula which consists of two
parts: ClickScore and LuceneScore , where (cid:79) is the
q,url q,url
weighting distribution parameters. The users’ click score is
denoted by ClickScore . The Lucene original score is
q,url
denoted by LuceneScore .
q,url
Score (cid:32)(cid:79)(cid:152)ClickScore (cid:14)(1(cid:16)(cid:79))(cid:152)LuceneScore (1)
q,url q,url q,url
In most times, the users click feedback implies their demand.
So we give ClickScore a weight higher
q,url
thanLuceneScore , in other words, (cid:79)(cid:143)(cid:11)0.5,1(cid:12). The
q,url
detailed calculation expressions are as follows:
ClickScore (cid:32)1/(cid:11)1(cid:14)exp((cid:16)(cid:68)(cid:152)P(click|q,url)/P(click|pos))(cid:12) (2)
q,url
P(click|q,url)(cid:32)Count(click|q,url)/Count(click|q) (3)
64
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

FrP5.4
P(click|pos(cid:32)x)(cid:32)Count(click|pos(cid:32)x)/(cid:166)N Count(click|pos(cid:32)i) (4)
i(cid:32)1
Where(cid:68)is a constant parameter which is given initially.
The Count function is to calculate the number of the users
clicks over a period of time such as one day.
Count(click|q,url)indicates the number of clicks in an url
when the query is given. Count(click|q) represents the
number of clicks of a query. And P(click| pos(cid:32)x) represents
the priori click ratio in the position x, which is equal to the
number of clicks in the position x divided by the total number
of clicks. This value is updated once a month, that is, we
calculate it every month.
When we calculateClickScore , we normalize it into (0,1)
q,url
interval. According to Eq. 1, our final score, which belongs to
(0, 1) interval, is consistent with the original Lucene score. The
above is our re-ranking algorithm model.
B. Re-ranking Algorithm MapReduce Process
The format of a record in users click log is as follows:
timestamp \t ip \t query \t click \t url \t position
We collect and save the click log into log files. Each row of the
log files is called a row record. Now we describe MapReduce
process of every step in detail.
Step 1: CalculatingCount(click|q).
1) Map Phase
Input: <Row Key, Row Record> from users click log files.
Process: Parsing out the query in each record.
Output: <query, 1>
2) Reduce Phase
Input: <query, [Click Count Set]>
Process: Summing the number of click with the same Key.
Filtering out the total click below the threshold we set.
Output: <query, Sum of the click query_cnt>
The outputs of this step are the results we need. We save
them to HDFS and use them in the later steps.
Step 2: CalculatingCount(click|q,url).
1) Map Phase
Input: <Row Key, Row Record> from users click log files.
Process: Parsing out the query \t url in each record.
Filtering out the query if it is not in the output of the first step.
Output: <query \t url, 1>
2) Reduce Phase
Input: < query \t url, [Click Count Set]>
Process: Summing the number of click with the same Key.
Output: < query \t url, Sum of the click url_cnt>
Step 3: CalculatingCount(click| pos(cid:32)x).
1) Map Phase
Input: <Row Key, Row Record> from users click log files.
Process: Parsing out the position in each record. Filtering
out the position if it is out of the range we set.
Output: <position, 1>
2) Reduce Phase
Input: < position, [Click Count Set]>
Process: Summing the number of click with the same Key.
Output: < position, Sum of the click position_cnt>
41
ed on January 30,2026 at 12:14:48 UTC from IEEE Xplore. Restrictions apply.

=== Page 3 ===
Theoretically, we should scan the whole users log files and
calculate the click times of all possible positions. In this case,
the computation is very heavy. Fortunately, under the guidance
of the recent-biased principle, we only need to care about a
period of recent time, for example, one or three months.
Meanwhile, less than 10% of search engine users click on
search engine results pages that appear after the third page. To
reduce computational complexity, we calculate the top 30
positions instead of all possible positions.
Step 4: CalculatingClickScore .
q,url
According to the Eq. 2, 3 and 4, we can calculate the click
score of each url with given query. We save it in a dictionary
for each query. The format is as follows:
query \t url1 clickscore1 \t url2 clickscore2 \t …\t urlN clickscoreN
Our search engine system stores these dictionaries into file
system in the form of HashMap. When an user search a query,
our system will determine whether it is in the HashMap. We
extract the click score of the query if it is in the HashMap, and
get the final score according to the Eq. 1. Then the results
pages of the search query are re-ranked by the final score.
IV. EXPERIMENTAL EVALUATION
In this section, we present the concrete implementation
process of our distributed search engine, including
experimental datasets and results. In our experiment, we use 11
ordinary Linux (CentOS) hosts to do our experiment.
A. Experimental Datasets
In our experiment, we crawl and parse data on the Software
School of Xiamen University (SSXMU) related web pages
using the distributed search engine we designed. A total of
41,529 pages about SSXMU are crawled and parsed in ten
hours. The size of these data are about 3.02G and we store
them in HDFS. We had used stand-alone mode to crawl and
parse the pages in ten hours as a comparison. The result is only
20,653 pages were crawled and parsed. It shows that our
distributed web crawling and parsing approach enhance the
efficiency greatly.
B. Crawling and Parsing Evaluation
Table I shows the performance of crawling and parsing web
pages with different data size of pages using two hosts. As
table I shows, the average speed of crawling and parsing is
77.87MB/h with two hosts. When we gradually increase the
number of cluster hosts, we calculate the average process speed
of with different number of cluster hosts using the same way as
the two hosts. As Fig. 2 shows that we draw a line chart with
TABLE I THE PERFORMANCE OF CRAWLING AND PARSING PAGES WITH 2
HADOOP CLUSTER NODES
Data size of Process Hours Process
pages(MB) (h) Speed(MB/h)
1 22.10 0.30 73.66
2 10.34 0.20 51.70
3 50.10 0.56 89.46
Sum 82.54 1.06 77.87
64
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

the average speed of different hosts.
We can see from Fig. 2 that the processing speed is not
linear growth while the number of hosts grows. It is because
the network bandwidth is limited, and there is a performance
bottleneck of the crawled sites.
C. Indexing Evaluation
We test indexing performance in multiple sets of data and
calculate the average indexing speed. Table II shows the
indexing speed with a single search engine server. We can see
from table II that the average indexing speed, with only one
search engine server, is 95.30MB/h. Similarity, we get the
average indexing speed with two and three search engine
servers respectively. Then we draw a line chart with the
average indexing speed with different number of hosts, as Fig.
3 shows. When the number of search engine servers increasing,
the average speed of search engine indexing increase, but the
increase speed gradually decreases.
D. Searching Evaluation
In order to test the performance of searching, we use
http_load [10], a Linux performance testing tool, to test the
throughput of the distributed search engine servers by
simulating the concurrent users. Figure 4 shows our test results.
Increasing the number of servers, the system throughput
decreases only in the case of fewer concurrent users. With the
increase of the number of concurrent users, the increment of
the system throughput is not obvious. This is mainly because
coordinating user query requests and merging multiple servers
search result need to spend extra time.
250
200
220.55
150 186.53
100
116.5
50
77.87
0
2 4 6 8
)h/BM(deepS
secorP
FrP5.4
Process Speed
Number of search engine cluster nodes
Fig.2. Average crawling and parsing speed of different hosts
TABLE II A SINGLE SEARCH ENGINE SERVER INDEX TEST RESULTS
Data size of Process Hours Process
pages(MB) (h) Speed(MB/h)
1 22.10 0.17 128.23
2 10.34 0.09 116.44
3 50.10 0.52 96.79
4 98.53 1.12 87.97
Sum 82.54 1.90 95.30
42
ed on January 30,2026 at 12:14:48 UTC from IEEE Xplore. Restrictions apply.

=== Page 4 ===
250
210.66
200 170.22
150
95.3
100
50
0
1 2 3
)h/BM(deepS
gnixednI
Indexing Speed
Number of search engine cluster nodes
Fig.3. Indexing speed for different search engine cluster nodes
E. Re-ranking Algorithm Evaluation
In order to verify the reliability of our re-ranking algorithm
model based on users click logs, we create a test web page with
high Lucene score about the query "(cid:17823)(cid:1318)(cid:4502)(cid:19602)"(Software
School) and put it into the web crawler database. Without using
our re-ranking algorithm model, the search results top five are
shown in Fig. 5. As we can see, the first position is our test
page. We repeat the search query many times in the test page,
so it ranks first because of high Lucene score. It reveals the
drawbacks of the Lucene score strategy.
To simulate the users click behavior, we ask many students
to search the query "Software School" and click the result they
need. We collect the users’ click log and save them into HDFS.
Finally, we collect 30,000 click logs in total. A fragment of the
users click logs is shown in Fig. 6.
Based on the click logs, we use the re-ranking algorithm
model described above to re-rank the search results, and the
final re-ranked results are shown in Fig. 7.
12000
10000
8000
6000
4000
2000
0
1 10 30 50 100
1Server 6786.031235.32622.11 104.36 36.24
2 Servers9852.331456.21701.63 130.24 42.65
3 Servers11215.81526.18823.17 142.05 54.75
)s/sqer(tuphguorhT
FrP5.4
Fig.5. Top five of the search results without using re-ranking algorithm model
Fig.6. Users click log
Fig.7. Top five of the search results using re-ranking algorithm model
We see that the original rank first result drop to the second
position. With the accumulation of users click logs, the results
we predict based on our model tend to be better. It proves that
Number of concurrent users
our re-ranking algorithm model have solved the problem that
1Server 2 Servers 3 Servers Lucene built-in scoring algorithms do not consider users’
feedback. Our re-ranking algorithm takes the users’ feedback
into account and gives the better result.
Fig.4 Throughput of different number of concurrent users
643
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloaded on January 30,2026 at 12:14:48 UTC from IEEE Xplore. Restrictions apply.

=== Page 5 ===
V. CONCLUSIONS
In this paper, we designed and implemented a distributed
search engine by using Nutch, Solr and Hadoop. We mainly
described the MapReduce procedure of our system and
proposed a new re-ranking algorithm model. The experimental
results showed that our distributed search engine had good
performances in crawling, parsing, indexing and searching, it
solved the problem of massive data processing to some extent.
The re-ranking algorithm makes the results pages more reliable
and reasonable, because it takes the users’ feedback into
consideration.
REFERENCES
[1] Chen N, Xiangyang C. Investigation of Distributed Search
Engine Based on Hadoop[J]. TELKOMNIKA Indonesian
Journal of Electrical Engineering, 2014, 12(9): 6954-6957.
64
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

FrP5.4
[2] Apache Nutch, http://nutch.apache.org/
[3] Khare R, Cutting D, Sitaker K, et al. Nutch: A flexible and
scalable open-source web search engine[J]. Oregon State
University, 2004, 32.
[4] Apache Hadoop, http://hadoop.apache.org/.
[5] Ding J, Yang B. A new model of search engine based on cloud
computing[J]. International Journal of Digital Content
Technology and its Applications, 2011, 5(6): 236-243.
[6] Apache Lucene, http://lucene.apache.org/solr/.
[7] Apache ZooKeeper, http://zookeeper.apache.org/.
[8] http://lucene.apache.org/core/5_0_0/core/org/apache/lucene/sear
ch/similarities/TFIDFSimilarity.html.
[9] http://wiki.apache.org/nutch/Nutch2Crawling.
[10] Http_load, http://www.acme.com/software/http_load/.
.
44
ed on January 30,2026 at 12:14:48 UTC from IEEE Xplore. Restrictions apply.

Paper:A_re-ranking_method_based_on_cloud_model.pdf
=== Page 1 ===
2011 International Conference on Computer Science and Network Technology
A Re-ranking Method B
Maoyuan Zhang Zhenxia Lou Jan Wan
Department of Computer Science and Technology
Central China Normal University
Wuhan, China
zhangmy@mail.ccnu.edu.cn louzx@yahoo.cn
wanjian731@126.com
Abstract-By introducing cloud model, this paper presents a re­
ranking method which improves the accuracy of the IR
(information retrieval) while recall is preserved. It is rare in
traditional Chinese information retrieval to consider uncertainty
while calculating the related degree of the query and each
document in the result set. This paper researches IR in a
perspective of uncertainty by introducing cloud model, measures
the relevance between the query and document by the
uncertainty degree that using document represents the query,
and then re-ranks the result set. Experiments on NTCIR-5 (the
5th NIl Test Collection for IR Systems) document collection for
SLIR (Single Language IR) show that this method achieves an
18.08'Yo and 26.50% improvement comparing to the initial
retrieval method without any re-ranking.
Keywords-Info17tUltion retrieval; Re-ranking; Cloud model;
Uncertainty
I. INTRODUCTION
In modem society, how to accurately retrieve the
information that user needs from the massive continuous
growing document resources becomes more and more
important. Many retrieval models provide a great convenience
for information retrieval. Ideal search model for IR is that the
most relevant document ranks top of the result set. So the user
can find the relevant documents fast as to improve the
efficiency of information retrieval. How the document set of
the retrieval results has been ranked embodies the retrieval
precision. Re-ranking is to change the documents' order of the
initiative retrieval results so as to improve the accuracy of the
IR while recall is preserved.
In order to improve the retrieval precision, researches on a
variety of different sorting algorithms and re-ranking methods
have also become a hot topic of information retrieval. Two
kinds of methods are mainly used for re-ranking: statistics­
based method and semantic-based method. As for statistics­
based method, Kyung- Soon Lee et al. expanded the original
document set and added the document clustering to traditional
method instead of query expansion to re-rank documents [1].
Jaroslaw Balinski and Czeslaow Danilowicz proposed a
method based on inter-document distances [2]. HE Ting-ting et
al. used the distribution of the topic word pairs which are
composed of two correlated words respectively selected from
the original query words and documents [3]. Dong Zhou and
Vincent Wade proposed a novel document re-ranking method
based on Latent Dirichlet Allocation (LDA) which exploits the
978-1-4577-1587-7/11/$26.00 ©2011 IEEE 142
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

Based on Cloud Model
Jinguang Chen
Engineering & Research Center for Information Technology
on Education
Central China Normal University
Wuhan, China
cjg2003@hutc.zj.cn
implicit structure of the documents with respect to original
queries [4]. The other kind of method is semantic-based
method. ZHANG Min et al. introduced semantic relations
between words in WordNet for query expansion and
replacement in order to re-rank documents [5]. Hongbo Deng,
Michael R. Lyu, and Irwin King proposed an approach that
incorporates the content with other link information in a latent
space graph together, and then performed the re-ranking
algorithm on the graph [6]. Zhou Bo, Cen Rongwei et al.
proposed a re-ranking method based on document similarity
which used both the relevant documents and irrelevant
documents in the feedback information [7].
All the methods above estimate the relevance of each
document to the query and rank the documents accordingly.
However, such an approach ignores the uncertainty associated
with the estimates of relevancy for the original initiative
retrieval is natural language-related and natural language has
uncertainty. C J van Rijsbergen indicated a document is
retrieved if it logically implies the request. However, there is
always a measure of uncertainty associated with such an
implication [8]. In order to do a further research on the
uncertainty between the query and the document, this paper
explained information retrieval from the perspective of
uncertainty, and re-ranked the document set of the retrieval
results by introducing the cloud model theory.
The rest of the paper is organized as follows. Section II
introduces the retrieval system based on cloud model, section
III introduces cloud model theory and our proposed re-ranking
method based on it, section N presents experiments and
evaluation results, and sectionV makes a conclusion of the
whole work.
II. RETRIEVAL SYSTEM BASED ON CLOUD MODEL
We have successfully developed a retrieval system based
on cloud model. The system is composed of three levels, which
are the query level, the retrieval level, and the re-ranking level,
as shown in Fig. 1:
24 December 24-26, 2011
ed on January 30,2026 at 12:21:08 UTC from IEEE Xplore. Restrictions apply.

=== Page 2 ===
�
�
----------- ----------
Quc'·�·lc'·cl
Backward cloud
Rc-runkin J! Lc,-cl L-..g""c.<: '.c:c'c.;m.:.:.O';:,- r---'
Figure 1. The retrieval system based on cloud model.
A. The Query Level
The query level includes two components: the query parser
and the language analyzer. The query parser converts each
keyword into an instance of term class and compares them with
the terms in index. The language analyzer performs the syntax
amm
analysis using dictionary and analysis gr ar. For example,
it will perform word segmentation on the user's query, because
word segmentation is the fundamental task of Chinese
information retrieval.
B. The Retrieval Level
The retrieval level is where inverted index is built by index
server for the full texts of all documents. The inverted index is
widely used in the information retrieval field. It maps each
word into a list of documents in which it appears. Each word
appearing one or more times in the documents has a
corresponding entry in the inverted index. The index server
would fmd matches against a set of documents that could
satisfy a query by decoding compressed information in
the inverted index.
C. The Re-ranking Level
The re-ranking level is the core part of this system. It
performed the re-ranking method based on cloud model.
Specifically, it uses backward cloud generator to get the
uncertainty degree of each document representing the query
based on the initial retrieval results generated by the retrieval
level. Each document will get a score for its uncertainty degree
to represent the query. After processed by the re-ranking
method based on the cloud model, the fmal retrieval results will
be presented to the user.
III. RE-RANKING METHOD BASED ON CLOUD MODEL
A. Cloud Model Theory
Cloud model [9], evolved from entropy and fuzzy set
theory, was first proposed in 1990s [10]. It integrates
randonmess and fuzziness together to research the uncertainty
of the concept in natural language. It is a conversion model
with uncertainty using the quantity number expression of a
quality concept which is expressed by natural language to
express the uncertainty within it.
142
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

The concept in natural language is qualitative, but it
also contains too much uncertainty. For example, "stable" is a
qualitative concept, but the uncertainty about it is there are no
standards to measure it. Different people may have different
understanding about "stable" under various circumstances.
Detailed description about cloud model theory can be found
in [9]. We present a brief introduction as written in that
reference for the benefit of readers to better understand our
work. In cloud model, cloud drop Xi(i=l •...• n) is a number that
randomly realizes the concept C. The numerical characteristics
Ex,
of cloud model are expressed with Expectation Entropy En
and Super-entropy He, and they reflect the whole
Ex
characteristics of the quality conception C. is the most
classical sample of cloud drop while qualifying the concept C;
En measures the uncertainty degree of the qualitative concept
C; He measures the uncertainty degree of En. Their defmition
of computation formulas are introduced as following:
J% (1)
I
En= -*- � ; n ] x -Exl (2)
2 n ;
=1
.JS2
He -En (3)
=
2
where X =- n 1 I ; n x; ,and 82 =- n I - I I ; n ( x ; - X)
=1 =1
B. Definitions
The uncertainty between the document and the query has
rarely been taken into consideration in traditional Chinese
information retrieval, but cloud model can measure the
uncertainty degree of a qualitative concept expressed by a
quantity of precise data which gives us much inspiration in
information retrieval and re-ranking. Therefore, we researched
IR and re-ranking from the perspective of uncertainty, and
proposed a method that using the uncertainty between the
document and the query to re-rank documents by introducing
the cloud model theory.
Our paper considers the query as a qualitative concept, a
document related to the query as a cloud drop that randomly
realizes the qualitative concept, and then uses cloud model to
calculate the uncertainty degree between the query and
document by the precise data of query's keywords' distribution
in the document. The lower the uncertainty degree of the
document expresses the query, the more relevant they are. So
the distribution of the keywords in the document will reflect
the relevance between the document and the query on
uncertainty, as also affect the accuracy rate ofIR.
Some IR models also covered the distribution of the
query's keywords in the document. Take the free/open
source information retrieval software library Apache Lucene
for example, it simply considered the proportion of the
keywords appearing in the document. Its basic similarity
scoring formula [II] is introduced as following:
25
ed on January 30,2026 at 12:21:08 UTC from IEEE Xplore. Restrictions apply.

=== Page 3 ===
[tf(t in d)x idf(t)2 x boost( t.fie1d in d)j
Score(q,d) L xlengthNorm(t.field in d)xcoord(q,d) (4)
=
xqueryNorm(q)
tmq
It computed the score for each document d matching each
keyword t in a query q. The impact factor coord(q,d) in
Lucene's scoring system represents the proportion of the
keywords appearing in a document. It is one of the important
factors which will impact the accuracy of the IR, but it is too
vague, and it cannot express the uncertainty comprehensively
and precisely. So our method made some defmitions as
following:
Definition 1: Suppose q represents the query, d represents a
document, and then the score of d gets for its uncertainty
degree to express its relevance to q is defined as
QDU(q,d) aeEx-peEn-Ae He (5)
=
where a + p + A = 1 , and a > P > A � 0 .
In (5) Ex is the expectation of q's keywords' distribution in
d, En measures the uncertainty degree of d expressing q, and
He reflects the uncertainty degree of En. They can be
calculated by (1), (2), (3) separately. Since Ex normally has
greater importance than En and He, and En has greater
importance than He. This paper makes a>{J>l?:.0.
Definition 2: Suppose q represents the query, d represents a
document, QDUmax is the maximum score that a document in
the retrieval results can get for its lowest uncertainty to present
q ,and then the normalized value of QDU(q,d) is defined as
QDU(q,d)
QV IT\T l.J TA 1V T O rm (q, d) = =--..--.;.:.; -'- (6)
QDUmax
A document with higher Ex, lower En and He will show a
lower uncertainty degree of the relevance between the
document and the query. Therefore, the higher QDUmax(q,d)
is, the more relevance the document has to the query, and the
toper position it gets in the information retrieval result set.
The re-ranking method is based on the initial results of the
information retrieval which doesn't need to run IR for a second
time. It reordered the document set of the results. In this paper,
we applied Lucene to get the document set of the IR's results.
In order to re-rank the documents while take into consideration
the uncertainty of a document logically implies the query, the
method we proposed in this paper takes the score of a
document got for its uncertainty degree expressing its
relevance to the query as a factor. This factor can affect the
document's order in the result set, as well as the accuracy of
the result set. The method's basic scoring formula is defined as
following:
Definition 3: Suppose q represents the query, drepresents a
document, and then the score d gets based on the method we
proposed is defined as:
Q ( ) ( ) ( )
DUScore q,d QDUNorm q,d xScore q,d (7)
=
142
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

Q
This factor DUNorm(q,d) is present by the quantitive
distribution of the keywords appearing in a document. It can be
calculated by (6). It is much more accurate and comprehensive
than the one in Lucene's scoring system (i.e., coord(q,d)
which only considered the proportion of the keywords
appearing in a document. Score(q,d) has been calculated by
Lucene scoring system based on (4). This method fulfilled
reconstructing the IR in the respect of uncertainty.
C. The Re-ranking Algorithm
The re-ranking algorithm is presented as follows.
Input: The initial retrieval results.
Output: Retrieval results which has been re-ranked.
Steps:
Step 1. The value of QDUmax is set to 0;
Step 2. While the document set of the initial retrieval
results has not been processed by step 2 completely, calculate
QDU(q,d) of the document according to (5); If all the
documents has been processed, go to step 4;
Step 3. If QDU(q,d) is bigger than QDUmax, the value of
QDUmax is set as QDU(q,d);Go to step 2;
Step 4. While the document set of the initial retrieval
results has not been processed by step 4 completely, calculate
QDUScore(q,d) of the document according to (6), (7); If all the
documents has been processed, go to step 5;
Step 5. Re-ranked documents based on their
QDUScore(q,d), a document with the highest score ranks top
of the document set;
Step 6. Return the re-ranked documents.
IV. EXPERIMENTS AND EVALUATION
In order to evaluate the effectiveness of the proposed
method, we conducted our experiments using the NTCIR-5
information retrieval test collections. We used TITLE field in
the query set, and chose 20 out of 50 titles in the query set to
conduct our experiments, and top 1000 documents of initial
information retrieval results conducted by Lucene
3.0. TREC evaluation tool trec eval [12] was used to evaluate
the experimental results.
A. Parameter Setting
First of all, this paper will confIrm three parameters in (5):
a, p and A. by experiments. Since Ex normally has greater
importance than En and He, and En has greater importance
than He, we will analyze how the accuracy of the information
retrieval was affected by the change of first. The experiment
a
conducted used two methods: cloud method as cloud which
applied (6), using the score of a document gets to express its
relevance to the query to re-rank the documents; LC method as
the method we proposed in this paper which applied (7), using
the results of (6) that the score of a document gets to express its
relevance to the query as one of the two factors to re-rank the
documents. Fig. 2 shows the changes of the two method's
accuracy when changes.
a
26
ed on January 30,2026 at 12:21:08 UTC from IEEE Xplore. Restrictions apply.

=== Page 4 ===
0.3
0.25
0.2
Co.�
0.15
_.
___
._C
C
l
l
o
o
u
u
d
d
r
r
e
ig
la
id
x
0.1 -.-lCrelax
�lCrigid
0.05
0 -'------------
0.5 0.55 0.6 0.65 0.7 0.75
Figure 2. Accuracy of cloud method and LC method with relax and rigid
assessments when a changes.
It shows that when a is between 0.6 and 0.7 the two
methods will both get the highest accuracy rate. In order to get
a more accurate result, a detailed experiment was performed as
following in Fig. 3:
0.3
0.25
0.2
Co.� �Cloud relax
0.15
�Cloud rigid
0.1 __ LCrelax
�LCrigid
0.05
0 -'-------------
0.5 0.6 0.610.630.650.670.69 0.7 0.8 0.9
a
Figure 3. Detailed figures of Fig. 2.
Therefore, we set a=0.67 to obtain the best result according
to Fig. 3. And also, we need to figure out how the two
method's accuracies change when a or fJ changes. Since
a+fJ+A=1 and A<{J, we adjust parameted from 0 to 0.16 when a
is set as 0.67. The experiment result is shown in Fig. 4.
0.3
0.25 i:� � * * * * *--.
� 0.2 �Cloud relax
0.15 �Cloudrigid
0.1 __ LCrelax
�LCrigid
0.05
0 -'-------------
o 0.020.04 0.060.08 0.1 0.120.140.16
j.
Figure 4. Accuracy of cloud method and LC method with relax and rigid
assessments when A changes.
As shown in Fig. 4, the accuracies of the two methods
decrease when A increases and is bigger than 0.02. The result is
sound because He is the second-order entropy of the entropy.
Hence, the value of parameter A should be set between 0 and
0.02 to gain the best result. Moreover, the effect of He on the
uncertainty degree cannot be ignored. Therefore we set ,1=0.01
in this paper, and a=O.67, fJ= 0.32.
142
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

B. Comparison of Experimental Results
1) Comparison of the Mean Average Precision
This paper compares the fmal results' MAP(Mean Average
Precision) in Table I. The table shows the precision for each
case. For each method, we give the percentage of improvement
over the initiative information retrieval as base in parentheses.
We demonstrate that the performance of our proposed method
is better than cloud method which applied (6), using the score
of a document gets to express its relevance to the query to re­
rank the documents. Cloud method and LC method both get
more accurate retrieval result than the initiative information
retrieval.
TABLE I. COMPARISON RESULTS ON NTCIR-5 COLLECTION
Base Cloud Method LC Method
Change
Change Change
over
Standard over over
MAP MAP MAP cloud
base base
method
(%) (%)
(%)
Rigid 0.18 0.21 +15.16 0.23 +26.50 +9.85
description 34 12 % 20 % %
Relax 0.23 0.24 +6.83 0.27 +18.08 +10.53
description 28 87 % 49 % %
The table shows the cloud model theory will improve the
performance of the information retrieval. Experiments show
that cloud method achieves a 6.83% and 15.16% improvements
comparing to the initial retrieval without any re-ranking while
our method achieves an 18.08% and 26.50% improvements
with relax and rigid assessments. These results could be
explained. It is practical to research information retrieval in a
statistical way. However, it cannot be ignored that information
retrieval is natural language related. Hence, there are not only
statistical characteristics which natural language have, but also
randomness and fuzziness. Cloud model theory made a good
combination by characterizing the randomness and fuzziness in
natural language by statistical characteristics. Therefore it
improves the information retrieval accuracy by re-ranking the
documents based on the cloud model theory of uncertainty.
2) Comparison of the precision at R documents
Fig. 6 and Fig. 7 below show the precision at R documents
after re-ranking. We can see that the number of the related
documents in the top position increases after re-ranking using
the two methods. Precision at 5 documents with relax
assessment is improved from 0.37 of the initiative information
retrieval to 0.45 and 0.42 respectively by using cloud method
and LC method, while it is improved from 0.26 to 0.35 and
0.32 with the rigid assessment. The query- related documents
are more intensive at the top rank.
27
ed on January 30,2026 at 12:21:08 UTC from IEEE Xplore. Restrictions apply.

=== Page 5 ===
5
�
0
0
0
.4
.
.4
5
5
0.37
0 .45
0.42
0.3
0.
5
4 25
0.43
0.416
°.43
0.405°·4075
" 0.35 0.33 0.32
"'0
0.3
� 0.25 • Lucene rela.-x.
;;
.�c:
0
0
.1
.2
5
• Cloudr <i:L'<
.LCrelax
.� 0.1
0.05
"" 0
5 docs 10docs IS docs 20 docs
�umber of documents
Figure 5. Precision at R documents of the three methods with relax
assessment.
c: 0.4 0.35
e 0.35 1""10.32 0.3150.32 0.3167°.33 0.3050.3025
� o���
0.26 0.255 0.2333 0. 235 r":'
::
0.2 " .Lucenerigid
:O� [
. � 0.1 II Cloudr igid
'B 0.05 • LC rigid
� ° �"�__ -J __ � __ � �L-__� � " __
5 docs 10 docs 15 docs 20 docs
�umber of documents
Figure 6. Precision at R documents of the three methods with rigid
assessment.
V. CONCLUSION
This paper proposes a novel method for re-ranking to
improve the performance of Chinese information retrieval
systems by introducing the cloud model theory. That method
introduced three numerical characteristics of cloud model, and
dug out the uncertainty inside the relevance between query and
documents. And then, the method took the uncertainty degree
into consideration for re-ranking of information retrieval. Thus
information retrieval was expressed from the uncertainty
perspective by using the method. In addition, the improvement
of the accuracy rate by re-ranking introducing the cloud model
theory proved that the method is practical and effective to
express the relevance by uncertainty.
ACKNOWLEDGMENT
142
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

This work was supported by the Major Research Plan of
National Natural Science Foundation of China (No. 90920005),
the National Natural Science Foundation of China (No.
61003192), the Program of Introducing Talents of Discipline to
Universities (No. B07042), Chenguang Program of Wuhan
Municipality (No. 201050231067), and the self-determined
research funds of CCNU from the colleges' basic research and
operation of MOE (No. CCNUIOA02009, No.
CCNU lOCO1 005).
REFERENCES
[1] Kyung-Soon Lee, Young-Chan Park, Key-Sun Choi, "Re-ranking model
based on document clusters," Information Processing and Management,
Oxford: Pergamon, vol. 37,p p. 1-14,2001.
[2] Jaroslaw Balinski, Czeslaow Danilowicz, "Re-ranking method based on
inter-document distances," Information Processing and Management,
Oxford: Pergamon, vol. 41,p p. 759-775,2005.
[3] HE Ting-ting, XU Ting, QU Guo-zhong, TU Xin-hui, "Re-ranking
based on topic word pairs," Computer Engineering and Applications,
Beijing, vol, 43(11),p p. 161-163,2007 .
[4] Dong Zhou, Vincent Wade, "Latent Document Re-Ranking,"
Proceedings of the 2009 Conference on Empirical Methods in Natural
Language Processing, Singapore, pp. 1571-1580,2009.
[5] ZHANG Min, SONG Rui-Hua, MA Shao-Ping, "Document Refinement
Based on Semantic Query Expansion," Chinese Journal of Computers,
Beijing:Science,v ol. 27(10), pp. 1395-4001,2004.
[6] Hongbo Deng, Michael R. Lyu, and Irwin King, "Effective Latent Space
Graph-based Re-ranking Model with Global Consistency," In WSDM
'09, Spain,p p. 212-221,2009.
(7) Zhou Bo, Cen Rongwei, Liu Yiqun, Zhang Min, Jin Yijiang, Ma
Shaoping, "A Document Relevance Based Search Result Re-Ranking,"
Journal of Chinese Information Processing, Beijing, vol. 24(3), pp. 19-
36,2010.
[8] C. J. van Rijsbergen, "A new theoretical framework for information
retrieval," In Proceedings of the 1986 International Conference on
Research and Development in Information Retrieval (SIGIR '86), italy,
pp. 194-200, 1986,
(9) D.Y. Li and Y. Du, "Artificial Intelligence with Uncertainty," Beijing:
National Defense Industry, 2005.
(10) D.Y. Li, X. Shi,a nd M.M. Gupta," Soft Inference Mechanism Based on
Cloud Models," LPSC 1996,G ermany, pp. 38-62,1996.
[ll] Michael McCandless, Erik Hatcher, and Otis Gospodnetic, "Lucene in
Action, Second Edition," New York:Manning Publications Co.,2 0 I O.
(12) Voorhees, E., Harman, D., eds., "TREC -Experiment and Evaluation in
Information Retrieval," Masseachusettes: MIT, 2005.
28
ed on January 30,2026 at 12:21:08 UTC from IEEE Xplore. Restrictions apply.

Paper:Enhancing_Retrieval_and_Re-ranking_in_RAG_A_Case_Study_on_Tax_Law.pdf
=== Page 1 ===
Enhancing Retrieval and
Case Study o
Zaid Rustamov Mehdi Ga
NLP Engineer & Data Scientist Junior AI
MegaSec LLC MegaSe
Baku, Azerbaijan Baku, Az
zaid.r@megasec.ai mehdigasimzade
Abstract— This paper explores the effectiveness of various
retrieval and re-ranking strategies within a Retrieval-
Augmented Generation (RAG) framework, applied to the
Azerbaijani Tax Code. We evaluated two sparse retrievers
(BM25 and SPLADE) and three dense embedding models
(BGE-m3, OpenAI’s text-embedding-3-large, and MiniLM’s
all-MiniLM-L6-v2), comparing their performance across
standard information retrieval metrics. Among individual
retrievers, BGE-m3 achieved the highest recall of 0.49 at the
top 100 retrieved documents but still missed over half of the
relevant documents. To address this limitation, we
implemented a hybrid retrieval strategy combining BM25 and
BGE-m3, which improved recall to 0.60—a relative gain of
11%. Further, we applied cross-encoder re-ranking with the
bge-reranker-base model, increasing NDCG from 0.39 to 0.44.
These results highlight the importance of layered architecture
that integrates both hybrid retrieval and re-ranking to enhance
relevance, especially in regulation-heavy domains. Our
findings offer practical insights into building robust and
interpretable RAG systems for legal and structured text
retrieval.
Keywords— Retrieval-Augmented Generation (RAG);
Information Retrieval; Re-ranking; Hybrid Retrieval; Sparse and
Dense Embeddings; Cross-Encoder; Legal NLP; Azerbaijani Tax
Code; Document Ranking; Question Answering
I. INTRODUCTION
Retrieval-Augmented Generation (RAG) has become a
foundational technique in Natural Language Processing
(NLP), combining the strengths of information retrieval and
generative models [1]. RAG systems operate in a two-stage
pipeline: first, a retriever identifies a set of candidate
documents that are potentially relevant to a given query;
then, a re-ranker refines these results to prioritize the most
relevant ones [1]. This integration allows language models to
generate responses grounded in external knowledge, rather
than relying solely on internal parameters [3]. By
incorporating retrieved evidence, RAG models significantly
reduce hallucinations and improve factual accuracy [4, 5].
They also support knowledge-grounded generation [6]
and can be adapted for personalized responses [7], making
them suitable for a wide range of applications such as
question answering [8], search engines [9], and knowledge-
based generation tasks [10]. Recent advancements in dense
retrieval, embedding models, and re-ranking techniques have
further improved the effectiveness of RAG systems. These
methods have demonstrated state-of-the-art performance
across multiple NLP benchmarks [11], positioning RAG as a
key approach in the development of reliable and knowledge-
aware language systems.
77686211.5202.88976TCIA/9011.01
:IOD
|
EEEI
5202©
00.13$/52/1-2439-5133-8-979
|
)TCIA(
seigolonhceT
noitacinummoC
dna
noitamrofnI
fo
noitacilppA
no
ecnerefnoC
lanoitanretnI
ht91
EEEI
5202
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

d Re-ranking in RAG: A
on Tax Law
Samir Rustamov
asimzade
Program Director of BCSC
Engineer
ADA University
ec LLC
Baku, Azerbaijan
zerbaijan
srustamov@ada.edu.az
e09@gmail.com
Despite the growing popularity and effectiveness of RAG
systems, building robust and high-performing pipelines
remains a complex task. One of the major challenges lies in
the retrieval stage, which significantly influences the quality
of the generated response. Two critical issues in retrieval are
missing top-ranked documents and incomplete answers. The
first challenge occurs when the answer to a user query exists
in the corpus but is ranked too low to be retrieved. For
instance, if the relevant document appears at rank 15 but the
system only retrieves the top 10 documents (i.e., K=10), the
system will miss the correct information, leading to
incomplete or incorrect responses. The second challenge
involves generating incomplete answers, where the retrieved
context contains all the necessary information, but the system
outputs only a partial response. These retrieval-related
pitfalls not only affect answer completeness but also hinder
user trust and system reliability.
Addressing these issues requires both improved retrieval
mechanisms and the integration of effective re-ranking
strategies to surface the most relevant and diverse
information. In this work, we aim to bridge this gap by
systematically examining how different embedding
strategies, chunking techniques, retrieval configurations, and
re-ranking methods influence the overall retrieval quality
and, ultimately, the final output relevance.
To support this investigation, we employ a domain-
specific dataset composed of question–answer pairs
published by The Tax Code of the Republic of Azerbaijan. In
this dataset, citizens pose tax-related questions which are
then addressed by experts, often accompanied by references
to specific articles from The Tax Code. This setting provides
a unique opportunity to benchmark retrieval performance:
the referenced legal articles serve as a natural ground truth
for evaluating whether the RAG system retrieves the correct
supporting documents in response to a given question.
II. LITERATURE REVIEW
Retrieval-Augmented Generation (RAG) systems are
widely used for enriching language generation with external
knowledge. A crucial component of these systems is the
information retrieval (IR) module, which ranks documents
based on their relevance to a query. The quality of this
retrieval largely depends on the embedding models
employed. To enhance their performance, contrastive
learning is often used, training on query–document triplets to
help distinguish relevant content from irrelevant ones [12].
However, the performance of retrieval systems depends
not only on the model architecture but also on how the data
is preprocessed. The importance of data preprocessing has
been emphasized by noting that ambiguous or underspecified
ed on January 30,2026 at 12:29:29 UTC from IEEE Xplore. Restrictions apply.

=== Page 2 ===
queries can degrade retrieval accuracy [13]. Similarly, it was
demonstrated that enriching user queries through expansion
or rephrasing can significantly improve retrieval precision
[14]. Techniques such as keyword extraction, linguistic
simplification, and large language model (LLM)-based
expansions have been explored to drive more context and
intent into queries.
Alongside improvements in queries, recent research has
explored hybrid retrieval methods that aim to overcome the
individual limitations of sparse and dense retrieval
paradigms. Dense retrieval methods—such as those based on
BERT [15] or SentenceTransformers [16] produce
continuous vector embeddings that are well-suited for
capturing deep semantic relationships. Nevertheless, they
often fail to match documents containing exact keywords,
proper nouns, or domain-specific phrases. On the other hand,
sparse retrieval approaches such as BM25 [17] offer strong
keyword matching and better interpretability, but they
struggle with semantic generalization and recall in complex
or multi-faceted queries [18].
To leverage the complementary strengths of these
approaches, hybrid retrieval strategies have been proposed.
These techniques combine the scores from dense and sparse
retrieval models using ensemble-based methods like linear
weighted fusion or Reciprocal Rank Fusion (RRF),
improving both recall and precision [18]. Such methods aim
to dynamically balance semantic understanding with exact
term matching, which is particularly beneficial in specialized
domains where both types of signals are crucial. A notable
example of this approach is the hybrid retrieval strategy
developed in [19]. Their work highlights the inherent
limitations of RAG systems that rely solely on either dense
or sparse retrieval techniques. In dense retrieval, while
embeddings provide rich contextual understanding, they
often fall short in matching critical exact terms. Sparse
retrieval, conversely, captures specific keywords but lacks
broader semantic awareness. To addressed these challenges,
the following formula was propose a hybrid scoring
mechanism defined as:
()
Here, and represent the scores from dense and
sparse retrieval systems respectively, while α∈[0,1] is a
tunable hyperparameter that controls their contribution.
As retrieval models have advanced, so too have re-
ranking strategies, which aim to reorder initially retrieved
documents to better align with the user's query intent. Early
approaches focused on pointwise and pairwise learning to
rank models, but more recent developments have shifted
toward listwise methods such as LambdaRank and ListNet
[19], [20]. These listwise approaches consider the entire set
of retrieved documents simultaneously, leading to more
globally optimized rankings.
Transformer-based architectures and cross-encoders have
become dominant in re-ranking tasks due to their ability to
model intricate interactions between queries and candidate
documents. These models evaluate relevance more precisely
by jointly encoding both inputs. In response to the growing
capabilities of large language models (LLMs), zero-shot and
few-shot re-ranking methods have also emerged. Models like
RankT5 and GPT-4 can effectively re-order documents
without requiring task-specific fine-tuning, offering
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

flexibility and strong generalization [21]. Furthermore,
studies such as [22], [23] demonstrate that embedding
models can be specifically fine-tuned or pretrained on
Azerbaijani data across various NLP tasks. This not only
enhances their performance on language-specific
applications but also highlights that LLM-based re-ranking—
especially when tailored to the language domain—is a viable
and effective solution.
In the context of RAG, where the quality of the final
generated output is highly dependent on the relevance of the
supporting documents, re-ranking plays a pivotal role.
Techniques such as self-consistency checks and noise
filtering have been introduced to enhance the factual
consistency of generated answers [24]. These methods
operate on top of the re-ranked outputs, further refining the
input before generation.
While previous research [19] has extensively explored
hybrid retrieval and re-ranking in RAG systems, most of this
work has been conducted on general-purpose benchmarks or
English-language datasets. Our study uniquely applies a
tiered approach; combining sparse and dense retrieval with
subsequent cross-encoder re-ranking to a highly specialized,
domain-specific corpus: the Azerbaijani Tax Code. This
focus is critical because legislative texts present unique
challenges, such as a reliance on internal cross-references
and a high density of domain-specific terminology, which
are not captured by standard benchmarks. By evaluating
RAG effectiveness on this novel dataset, our work provides a
first-of-its-kind analysis of how these techniques perform in
a low-resource, legal context and offers valuable insights into
the practical application of RAG for a domain where high
factual accuracy is paramount.
III. METHODOLOGY
A. Data Gathering & Preprocessing
We collected domain-specific data from two primary
sources: the Tax Code of the Republic of Azerbaijan and a
publicly available Questions & Answers (QA) section
published by the State Tax Service [25], [26]. The QA
dataset consists of real inquiries from citizens alongside
expert-provided answers, often citing specific legal articles.
This allowed us to establish a grounded, reference-based
benchmark for evaluating document retrieval quality in a
real-world legal context.
Using Python web scraping libraries, we extracted both
the full text of the Tax Code and 5,657 QA pairs. For each
QA pair, we employed AI agents to extract key annotations:
referenced legal articles under two categories—primary
articles (“Referenced Main Articles”) and supplementary
legal sources (“Referenced Sub Articles”), which may
include external regulations or secondary references. Since
our focus is exclusively on the Tax Code, we filtered out QA
pairs that referenced external legal sources (supplementary
regulations under “Referenced Sub Articles”) as these fall
outside the scope of our corpus. After this filtering process,
we retained 2,178 QA pairs that could be fully answered
using the articles within the Tax Code alone. These
references provide a gold-standard target to compare against
documents retrieved by our RAG system. An illustration of
the structured dataset is shown in Fig 1. The corpus contains
a total of 240 unique articles, including main and sub-articles
(e.g., "Article 214" and "Article 214-1").
ed on January 30,2026 at 12:29:29 UTC from IEEE Xplore. Restrictions apply.

=== Page 3 ===
Fig. 1. Structure of the QA Dataset with Extracted Article References
In parallel, we processed the full legal corpus of the
Azerbaijani Tax Code. Due to the document’s regulatory
nature, which includes dense and interdependent clauses,
proper chunking of the text became a critical preprocessing
step. Chunking plays a vital role in balancing semantic
completeness with retrieval granularity—too small chunks
fragment contextual meaning, while overly large ones
introduce irrelevant content [27].
To analyze the structure of the articles, we first measured
their token distributions. The mean article length was
approximately 1,500 tokens, with a median of 670 tokens.
Given this variability, we adopted a fine-grained chunking
strategy. The text was segmented into chunks of 300 tokens
with a 60-token overlap, ensuring continuity and minimizing
the risk of splitting sentences mid-thought. It is an important
consideration in legal documents where precision and
coherence are critical. The choice of a 300-token chunk size
was a strategic decision to balance the preservation of
semantic context with computational efficiency. A smaller
chunk size might fragment sentences or lose key
information, making it difficult for the embedding models to
capture the full meaning of legal provision. Conversely, an
overly large chunk could dilute the core relevance signal
within the retrieved passage, and it risks exceeding the
context window limitations of certain models, a phenomenon
often referred to as "lost in the middle." The 300-token size
was selected to ensure each chunk contained a cohesive legal
concept while remaining concise enough for effective vector
embedding and subsequent re-ranking.
B. Information Retrieval Framework
Following the completion of data collection and
preprocessing, we proceeded to the retrieval stage by
embedding both the questions and the chunked articles using
a variety of models capable of generating either sparse or
dense vector representations. For sparse embeddings, we
selected two retrieval models: BM25 and SPLADE. To
represent dense vectors, we employed the BGE-m3, text-
embedding-3-large from OpenAI, and all-MiniLM-L6-v2
models.
Each question and document chunk was embedded using
the same respective model to ensure vector compatibility.
Given that the embeddings share the same dimensional
space, we utilized cosine similarity as the distance metric to
calculate the semantic closeness between the query and each
document. For each question, we retrieved the top-k most
similar chunks, with k set to 10, 50, and 100.
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

To systematically evaluate the retrieval effectiveness of
each embedding model, we adopted the following standard
information retrieval metrics:
• Recall (R)
• Precision (P)
• Hit Ratio (Hit)
• Normalized Discounted Cumulative Gain (NDCG)
• Mean Reciprocal Rank (MRR)
• Mean Average Precision (MAP)
The complete results for all models across different top-
k values are presented in Table 1. Among the dense models,
BGE-m3 exhibited a consistent performance advantage,
outperforming other models across nearly all metrics and
retrieval depths. Increasing the number of retrieved
documents generally improves recall, confirming that a
broader retrieval set enhances the likelihood of including
relevant articles. However, even at k = 100, the best-
performing models still fall short of ideal coverage. For
instance, BGE-m3, text-embedding-3-large and all-MiniLM-
L6-v2 achieved a recall of only 0.49 - indicating that, on
average, more than half of the relevant documents are still
missing, even under generous retrieval settings.
TABLE I. RETRIEVAL PERFORMANCE OF SPARSE AND
DENSE EMBEDDING MODELS ACROSS TOP K DOCUMENTS
Sparse Dense
Top text- all-
K embed- MiniLM-
Ret. Metric BM25 Splade BGE- ding-3- L6-v2
Docs m3 large
R 0.19 0.14 0.25 0.23 0.11
P 0.09 0.07 0.12 0.11 0.06
Hit 0.66 0.54 0.82 0.78 0.45
10 NDCG 0.20 0.14 0.29 0.26 0.10
MRR 0.41 0.26 0.58 0.52 0.18
MAP 0.12 0.07 0.18 0.15 0.05
R 0.36 0.35 0.41 0.40 0.35
P 0.04 0.04 0.04 0.04 0.04
Hit 0.90 0.87 0.96 0.95 0.85
50 NDCG 0.27 0.22 0.36 0.33 0.19
MRR 0.43 0.29 0.59 0.53 0.21
MAP 0.14 0.09 0.20 0.18 0.07
R 0.47 0.48 0.49 0.49 0.49
P 0.02 0.02 0.03 0.03 0.03
Hit 0.96 0.95 0.98 0.98 0.94
100 NDCG 0.31 0.26 0.39 0.36 0.23
MRR 0.43 0.29 0.60 0.54 0.21
MAP 0.15 0.10 0.21 0.19 0.08
While recall indicates whether relevant documents are
retrieved, it does not consider their position in the ranked list.
In contrast, NDCG is a widely used metric in information
ed on January 30,2026 at 12:29:29 UTC from IEEE Xplore. Restrictions apply.

=== Page 4 ===
retrieval that evaluates not only the presence of relevant
documents but also their ranking order. It rewards systems
that retrieve highly relevant documents near the top of the
result list and penalizes those that rank them lower. The
general formula for NDCG at position k is defined as:
()
Where,
• DCG (Discounted Cumulative Gain) is calculated
as:
(3)
• IDCG (Ideal Discounted Cumulative Gain)
represents the maximum possible DCG achievable
by an ideal ranking.
By comparing DCG with IDCG, NDCG normalizes the score
between 0 and 1, allowing for fair comparison across
different queries. A higher NDCG score indicates that the
system not only retrieved the correct documents but also
ranked them effectively.
Despite BGE-m3 demonstrating strong recall, its NDCG
performance still reflects the limitations of the retrieval step.
As shown in Figure 2, even at k = 100, the NDCG score of
BGE-m3 peaked at only 0.39, highlighting that many of the
relevant documents were ranked too low in the result list.
This underscores the importance of re-ranking mechanisms,
which can reorder retrieved documents to better reflect their
true relevance, especially in applications like legal question
answering.
Fig. 2. NDCG Scores of Embedding Models Across Top-k Retrieval Sizes
The relatively low recall observed in the standalone
retrieval phase—where even the best-performing model,
BGE-m3, achieved a recall of only 0.49—indicates that over
half of the relevant documents are not being retrieved. To
address this issue, we implemented a hybrid retrieval
approach, inspired by the method proposed in [25], which
combines the strengths of both sparse and dense retrieval
techniques. In this strategy, document scores are computed
as a weighted combination of sparse and dense similarity
scores.
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

For the sparse component, we selected BM25, as it
consistently outperformed SPLADE in our earlier
evaluations. For the dense embeddings, we experimented
with both OpenAI’s text-embedding-3-large and BGE-m3,
given their relatively strong individual performances. We
tested three different values for the weight parameter α: 0.3,
0.5, and 0.7, to analyze the sensitivity of performance to the
balance between sparse and dense scores. The results showed
consistent improvements in recall across all top-k values.
The most successful combination was BM25 + BGE-m3,
which achieved a recall of 0.60, representing a significant
improvement of 11% over the best individual model (BGE-
m3, 0.49). These results, summarized in Table 2,
demonstrate that hybrid retrieval strategies can effectively
mitigate the recall limitations of retrieval systems.
TABLE II. RECALL SCORES OF HYBRID RETRIEVAL
COMBINATIONS ACROSS TOP-K VALUES
Top Alpha Sparse Dense Vector Recall
K Ret (threshol Vector
Docs d)
Text-embedding-3-large 0.26
0.3 BM25 BGE-m3 0.27
Text-embedding-3-large 0.27
10 0.5 BM25 BGE-m3 0.28
Text-embedding-3-large 0.27
0.7 BM25 BGE-m3 0.29
Text-embedding-3-large 0.46
0.3 BM25 BGE-m3 0.46
Text-embedding-3-large 0.47
50 0.5 BM25 BGE-m3 0.48
Text-embedding-3-large 0.48
0.7 BM25 BGE-m3 0.49
Text-embedding-3-large 0.58
0.3 BM25 BGE-m3 0.57
Text-embedding-3-large 0.57
100 0.5 BM25 BGE-m3 0.59
Text-embedding-3-large 0.58
0.7 BM25 BGE-m3 0.60
C. Re-ranking Methods
Despite improvements in recall through hybrid retrieval,
the ranking quality of retrieved documents are poor, as
reflected by low NDCG and MRR scores. At k = 100, the
highest NDCG score reached only 0.38 (see Table 1),
indicating that even when relevant documents were
successfully retrieved, they often appeared lower in the
ranking. The decision to re-rank the top 100 retrieved
documents represent an optimal balance between
computational cost and retrieval performance. Re-ranking the
entire corpus is unnecessary given that the initial retrieval
stage (BM25 + BGE-m3) has already narrowed down the
most probable candidates. Re-ranking fewer documents (e.g.,
top 10 or top 20) would risk excluding a relevant document
that was correctly retrieved by the initial hybrid model but
ranked lower in the top-k list. The Mean Reciprocal Rank
ed on January 30,2026 at 12:29:29 UTC from IEEE Xplore. Restrictions apply.

=== Page 5 ===
(MRR)—which evaluates how early the first relevant
document appears in the ranked list was underwhelming.
MRR is calculated as the average reciprocal rank of the first
relevant result across all queries, using the formula:
(4)
Where rank represents the position of the first relevant
i
document for query i.
In our baseline retrieval setting, the best MRR score was
achieved by the BGE-m3 model at 0.60 (see Table 1), which
remains relatively low. These findings underscore the critical
need for re-ranking mechanisms to reorder the initially
retrieved documents, ensuring that the most relevant ones are
prioritized at the top of the list and improving the overall
quality of the RAG system.
To enhance the quality of the retrieved document
rankings, we employed cross-encoder models. Unlike bi-
encoders that compute independent embeddings for queries
and documents, cross-encoders are specifically designed to
assess the relationship between input pairs directly. This
architecture allows the model to capture fine-grained
semantic interactions, making them highly suitable for
precise relevance scoring in re-ranking tasks.
We selected three widely used cross-encoder models: ms-
marco-MiniLM-L6-v2, bge-reranker-base and bert-
multilingual-passage-reranking-msmarco. Re-ranking was
applied to the top 100 retrieved documents from the initial
retrieval step. This larger candidate set provides better recall
coverage and allows us to evaluate the model’s ability to
prioritize the most relevant documents effectively.
The results are presented in Table 3. The ms-marco-
MiniLM-L6-v2 model demonstrated no effectiveness in both
NDCG and MRR scores. In contrast, the bge-reranker-base
model exhibited notable gains, especially in NDCG, where it
improved the score from 0.39 to 0.44. However, it also
performed a decrease in MRR, indicating a failure to bring
the first relevant documents closer to the top of the list.
TABLE III. IMPACT OF CROSS-ENCODER RE-RANKING ON
RETRIEVAL QUALITY
Ms- Bert- Bge-
marco- multilingu reranke
Retrieved Metric Before MiniL al-passage- r-base
by reran M-L6- reranking-
king v2 msmarco
NDCG 0.31 0.27 0.36 0.39
BM25 MRR 0.43 0.16 0.36 0.37
Text- NDCG 0.36 0.32 0.40 0.42
embedding
-3-large MRR 0.54 0.19 0.36 0.38
NDCG 0.39 0.33 0.43 0.44
BGE-m3 MRR 0.60 0.20 0.37 0.39
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

D. Error Analysis & Discussion of Limitations
To provide a more nuanced understanding of our
quantitative results, we conducted a qualitative analysis of
our system’s performance, focusing on specific examples
where our hybrid retrieval and re-ranking strategies
exhibited nuanced behaviors. For example, for the following
query, BGE-m3, a dense model, successfully retrieved
Articles 108 and 119 but failed to retrieve Article 109.
While Article 119 was ranked highly in the results, Article
108 appeared much lower in the list. A qualitative analysis
reveals two key reasons for these retrieval issues:
• Semantic Disconnection: Article 109 was not
retrieved because it references other legal articles
and is not a self-contained, complete article. This
inherent lack of direct semantic content makes it
difficult for a dense model like BGE-m3 to
establish a strong vector similarity with the query.
The model struggles to follow external references
and cannot capture the full context of a fragmented
legal provision.
• Re-ranking Failure: While Article 108 was
correctly retrieved, its lower rank in the initial list
presented a challenge for the re-ranker. The
purpose of a re-ranking model, such as bge-
reranker-base, is to fix these initial ranking issues
and bring the most relevant documents to the
forefront. However, it appears the re-ranker failed
to sufficiently promote Article 108. The reason for
this is that Article 108 provides general, rather than
specific, information. The cross-encoder may have
evaluated its overall relevance as lower than other
documents.
These retrieval-related pitfalls not only affect answer
completeness but also hinder user trust and system
reliability, as noted in the introduction. They highlight the
fundamental challenge that even advanced RAG systems
face when dealing with complex, interdependent legal
documents.
A potential solution to this problem would be to develop
graph-based architecture for the legal codes. By explicitly
mapping the relationships between articles that reference
each other, a knowledge graph could provide a structured
framework. This would allow the retrieval system to
traverse these connections, ensuring that even articles with
incomplete textual information are retrieved by following
their links to other relevant documents. This approach could
effectively bridge the semantic gaps that a pure vector-based
system struggles with and provide a more robust and
accurate retrieval mechanism.
ed on January 30,2026 at 12:29:29 UTC from IEEE Xplore. Restrictions apply.

=== Page 6 ===
Original Query:
Hörmətli Vergi Xidməti nümayəndələri, Azərbaycan
Respublikası Nazirlər Kabinetinin 2024-cü il 22 noyabr
tarixli 492 nömrəli Qərarı ilə nümayəndəlik xərclərinin,
işçilərin mənzil və yemək xərclərinin, eləcə də əmək
şəraiti zərərli, ağır olan və yeraltı işlərdə çalışan
işçilərə verilən müalicə-profilaktik yeməklər, süd və
ona bərabər tutulan digər məhsullar və vasitələrlə bağlı
xərclərin vergitutma məqsədləri üçün gəlirdən
çıxılması normaları və qaydaları müəyyən edilmişdir.
Qərarda “nümayəndəlik xərcləri” anlayışı qeyd olunsa
da, bu anlayışın dəqiq məzmun dairəsi ilə bağlı
praktikada qeyri-müəyyənlik yaranır. Sualımız:
Qərarda nəzərdə tutulan “nümayəndəlik xərcləri”
anlayışına işçilərə verilən yemək pulu və mənzil
xərcləri də daxildirmi? Yəni bu cür xərclər də
“nümayəndəlik xərcləri” kimi qiymətləndirilib,
vergitutma məqsədləri üçün yalnız bu xərclərin 50 faizi
və illik gəlirin 1%-i həddində gəlirdən çıxıla bilərmi?
Yoxsa bu xərclər nümayəndəlik xərcləri kateqoriyasına
daxil edilməyərək ayrıca tam şəkildə gəlirdən çıxılan
xərclər kimi qiymətləndirilir?
Translated:
Dear Representatives of the Tax Service. According to
Decision No. 492 of the Cabinet of Ministers of the
Republic of Azerbaijan, dated November 22, 2024, the
norms and rules for deducting, for taxation purposes,
expenses related to representation, employees’ housing
and meal costs, as well as expenses for medical and
preventive meals, milk, and equivalent products and
supplies provided to employees working under
harmful, strenuous, or underground conditions, have
been established. Although the Decision refers to the
concept of “representation expenses,” in practice there
is uncertainty regarding the exact scope of this concept.
Our question is: Does the concept of “representation
expenses” as set out in the Decision also include meal
allowances and housing expenses provided to
employees? In other words, should such expenses be
classified as “representation expenses” and therefore
deductible for taxation purposes only within the limit
of 50 percent of such expenses and 1 percent of annual
income? Or should these expenses not be treated as
representation expenses, but rather be considered as
separate deductible expenses that can be fully deducted
from income?
True Article in the Tax Code: {108, 109, 119}
Retrieved Documents by BGE-m3 Model:
{119, 149, 13, 90, 125, 125, 14-1, 13, 33, 159, 104,
114, 105, 124, 165, 104, 227, 14-1, 4, 124, 18, 228,
220, 4, 150, 166, 114, 23, 209, 59, 19, 149, 19, 130,
108, 99, 168, 161, 13, 96, 79, 197, 89, 125, 16, 102, 24,
42, 132, 199, 79, 14, 85, 2, 199, 164, 2, 125, 36, 164,
218, 172, 227, 31, 102, 33, 13, 102, 13, 13, 33, 174, 35,
55, 211, 13, 2, 58, 23, 53, 2, 159, 143, 14-1, 13, 102,
53, 14-1, 13, 13, 50, 13, 50-1, 58, 164, 90, 104, 83,
108, 101}
a. Example of failed query
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

IV. DISCUSSION OF RESULTS
Our experiments revealed meaningful distinctions in
retrieval and ranking effectiveness across various sparse and
dense retrievers. BGE-m3 consistently outperformed both
traditional sparse models like BM25 and SPLADE, as well
as other dense models such as OpenAI's text-embedding-3-
large, and MiniLM's all-MiniLM-L6-v2 across all standard
metrics, including Recall, NDCG, and MRR. Nevertheless,
even this best-performing retriever only achieved a Recall of
0.49 at the top 100 documents, indicating that over half of
the truly relevant documents were still being missed,
underscoring a fundamental limitation in relying solely on
initial retrieval. To address this, we explored a hybrid
retrieval strategy, combining the keyword precision of BM25
with the contextual depth of BGE-m3 through a weighted
scoring mechanism. This approach yielded a notable increase
in Recall, improving it to 0.60 at the top 100 documents.
This represented a significant relative gain of 11% over the
best individual model and demonstrated the complementary
nature of sparse and dense retrieval. Hybridization
effectively mitigated the limitations of each model: sparse
methods excelled at matching domain-specific terminology,
while dense models better captured paraphrased or
semantically rich queries. Despite this improvement, many
relevant documents were still ranked too low, as reflected in
relatively modest NDCG and MRR scores. For example, the
highest NDCG for any single retriever peaked at just 0.39,
indicating that even when relevant documents were retrieved,
they often appeared too low in the list to be useful. We
therefore applied cross-encoder re-ranking with the bge-
reranker-base model to the top 100 retrieved candidates. This
re-ranking step significantly improved the NDCG score from
0.39 to 0.44, highlighting its effectiveness in reordering the
document list to prioritize relevance. However, it failed to
raise the MRR score, suggesting a limitation in its ability to
consistently promote the single most relevant document to
the top position. Overall, the findings advocate for a layered
retrieval architecture, where strong initial retrieval is
enhanced through hybrid scoring and further refined through
targeted re-ranking, resulting in a more robust and
semantically aware RAG pipeline.
V. CONCLUSION
This study explored the effectiveness of various retrieval
and re-ranking strategies within a Retrieval-Augmented
Generation (RAG) framework applied to the Azerbaijani Tax
Code. Through extensive evaluation, we demonstrated that
while dense models like BGE-m3 offered strong retrieval
performance, hybrid methods combining sparse and dense
signals significantly improved recall. Furthermore,
incorporating cross-encoder re-ranking—particularly with
the BGE reranker proved effective in surfacing relevant
documents higher in the ranked list. These findings
underscore the importance of a multi-stage retrieval pipeline
that combines methods to enhance both coverage and
ranking quality, especially in structured and domain-specific
settings.
ACKNOWLEDGMENT
This research was conducted at the Center for Data
Analytics Research of ADA University and the AI
Laboratory of MegaSec LLC.
ed on January 30,2026 at 12:29:29 UTC from IEEE Xplore. Restrictions apply.

=== Page 7 ===
REFERENCES
[1] Ren, R., Qu, Y., Liu, J., Zhao, W. X., She, Q., Wu, H., ... & Wen, J.
R. (2021). RocketQAv2: A Joint Training Method for Dense Passage
Retrieval and Passage Re-ranking. In Proceedings of the 2021
Conference on Empirical Methods in Natural Language Processing.
Association for Computational Linguistics.
[2] Yates, A., Nogueira, R., & Lin, J. (2021, March). Pretrained
transformers for text ranking: BERT and beyond. In Proceedings of
the 14th ACM International Conference on web search and data
mining (pp. 1154-1156).
[3] Izacard, G., & Grave, E. (2021, April). Leveraging Passage Retrieval
with Generative Models for Open Domain Question Answering.
In EACL 2021-16th Conference of the European Chapter of the
Association for Computational Linguistics (pp. 874-880). Association
for Computational Linguistics.
[4] Agrawal, G., Kumarage, T., Alghamdi, Z., & Liu, H. (2024, June).
Can Knowledge Graphs Reduce Hallucinations in LLMs?: A Survey.
In Proceedings of the 2024 Conference of the North American
Chapter of the Association for Computational Linguistics: Human
Language Technologies (Volume 1: Long Papers) (pp. 3947-3960).
[5] Shuster, K., Poff, S., Chen, M., Kiela, D., & Weston, J. (2021).
Retrieval Augmentation Reduces Hallucination in
Conversation. Findings of the Association for Computational
Linguistics: EMNLP 2021.
[6] Yih, S. (2020). Retrieval-augmented generation for knowledge-
intensive nlp tasks. In Conference on Neural Information Processing
Systems, Vancouver, Canada.
[7] Salemi, A., Kallumadi, S., & Zamani, H. (2024, July). Optimization
methods for personalizing large language models through retrieval
augmentation. In Proceedings of the 47th International ACM SIGIR
Conference on Research and Development in Information Retrieval
(pp. 752-762).
[8] Liu, Z., Zhou, Y., Zhu, Y., Lian, J., Li, C., Dou, Z., ... & Nie, J. Y.
(2024, May). Information retrieval meets large language models. In
Companion Proceedings of the ACM Web Conference 2024 (pp.
1586-1589).
[9] Xiong, H., Bian, J., Li, Y., Li, X., Du, M., Wang, S., ... & Helal, S.
(2024). When search engine services meet large language models:
visions and challenges. IEEE Transactions on Services Computing.
[10] Long, X., Zeng, J., Meng, F., Ma, Z., Zhang, K., Zhou, B., & Zhou, J.
(2024, March). Generative multi-modal knowledge retrieval with
large language models. In Proceedings of the AAAI Conference on
Artificial Intelligence (Vol. 38, No. 17, pp. 18733-18741).
[11] Salemi, A., & Zamani, H. (2024, July). Evaluating retrieval quality in
retrieval-augmented generation. In Proceedings of the 47th
International ACM SIGIR Conference on Research and Development
in Information Retrieval (pp. 2395-2400).
[12] Karpukhin, V., Oguz, B., Min, S., Lewis, P. S., Wu, L., Edunov, S., ...
& Yih, W. T. (2020, November). Dense Passage Retrieval for Open-
Domain Question Answering. In EMNLP (1) (pp. 6769-6781).
[13] Gao, Y., Xiong, Y., Wang, M., & Wang, H. (2024). Modular RAG:
Transforming RAG Systems into LEGO-like Reconfigurable
Frameworks. CoRR.
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

[14] Patel, C. (2024). Hypothetical Retrieval-Augmented Generation
(Hypothetical RAG): Advancing AI for Enhanced Contextual
Understanding and Creative Problem-Solving. Scientific Research
Journal of Science, Engineering and Technology, 2(1), 1-4.
[15] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019, June).
Bert: Pre-training of deep bidirectional transformers for language
understanding. In Proceedings of the 2019 conference of the North
American chapter of the association for computational linguistics:
human language technologies, volume 1 (long and short papers) (pp.
4171-4186).
[16] Reimers, N., & Gurevych, I. (2019, November). Sentence-BERT:
Sentence Embeddings using Siamese BERT-Networks. In
Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing (pp. 3968-3977).
[17] Wang, S., Zhuang, S., & Zuccon, G. (2021, July). Bert-based dense
retrievers require interpolation with bm25 for effective passage
retrieval. In Proceedings of the 2021 ACM SIGIR international
conference on theory of information retrieval (pp. 317-324).
[18] Sengupta, S., Heaton, C., Cui, S., Sarkar, S., & Mitra, P. (2024,
December). Towards Efficient Methods in Medical Question
Answering using Knowledge Graph Embeddings. In 2024 IEEE
International Conference on Bioinformatics and Biomedicine (BIBM)
(pp. 5089-5096). IEEE.
[19] Burges, C. J. (2010). From ranknet to lambdarank to lambdamart: An
overview. Learning, 11(23-581), 81.
[20] Liu, Y., Zhang, X., Zhu, X., Guan, Q., & Zhao, X. (2017). Listnet-
based object proposals ranking. Neurocomputing, 267, 182-194.
[21] Zhuang, H., Qin, Z., Jagerman, R., Hui, K., Ma, J., Lu, J., ... &
Bendersky, M. (2023, July). Rankt5: Fine-tuning t5 for text ranking
with ranking losses. In Proceedings of the 46th International ACM
SIGIR Conference on Research and Development in Information
Retrieval (pp. 2308-2313).
[22] Guliyev, N., Rustamov, Z., & Rustamov, S. (2024, April). Analysis of
public sentiment in Azerbaijani news and social media. In
Proceedings of the International Conference on Computing, Machine
Learning and Data Science (pp. 1-6).
[23] Alizada, T., Suleymanov, U., & Rustamov, Z. (2024, September).
Contextualized Word Embeddings in Azerbaijani Language. In 2024
IEEE 18th International Conference on Application of Information
and Communication Technologies (AICT) (pp. 1-6). IEEE.
[24] Fang, F., Bai, Y., Ni, S., Yang, M., Chen, X., & Xu, R. (2024,
August). Enhancing Noise Robustness of Retrieval-Augmented
Language Models with Adaptive Adversarial Training.
In Proceedings of the 62nd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) (pp. 10028-
10039).
[25] https://www.taxes.gov.az/az/page/ar-vergi-mecellesi
[26] https://www.taxes.gov.az/az/page/suallar-ve-cavablar
[27] Chu, Y., He, P., Li, H., Han, H., Yang, K., Xue, Y., ... & Tang, J.
(2025). Enhancing LLM-Based Short Answer Grading with Retrieval-
Augmented Generation. CoRR.
ed on January 30,2026 at 12:29:29 UTC from IEEE Xplore. Restrictions apply.

Paper:The_Research_on_Re-ranking_Algorithm_for_FAQ-based_Systems_in_the_Petroleum_Domain.pdf
=== Page 1 ===
2025 10th International Conference on Intellig
The Research on Re-r
FAQ-based Systems in
1st Jiaxiang Zhang
School of Computer Science
Xi’an Shiyou University
Xi’an, Shaanxi, China
941324625@qq.com
Abstract—In an FAQ-based system, precise information re-
trievalandrecallarekeytoenhancinguserexperience.However,
traditional retrieval methods struggle with professional termi-
nology and complex semantic matching, making it difficult to
achieve high-precision QA. This study explores the application
ofrerankingalgorithmsinanoilindustryFAQsystem,aimingto
optimizetherankingofrecalledcandidateanswersandimprove
QA matching accuracy. We employ a deep learning-based dense
retrieval(DPR)modelforinitialrecallandintegrateapretrained
reranking model (BAAI/bge-reranker) to refine the retrieved
results. Experimental results demonstrate that incorporating a
rerankingalgorithmsignificantlyimprovesrecallprecision.Addi-
tionally,byfine-tuningthemodelwithdomain-specificpetroleum
data, the reranking task achieves better performance within the
field, further validating the value of reranking algorithms in
enhancing professional knowledge QA matching.
Index Terms—Petroleum Domain FAQ-based Systems, Re-
trieval and Recall, Re-ranking Model
I. INTRODUCTION
In the petroleum industry, the rapid advancement of tech-
nology and the continuous expansion of production scale
have led to an exponential increase in knowledge complex-
ity and specialization. This makes efficiently retrieving ac-
curate information and knowledge from massive datasets a
critical task. Traditional information retrieval methods, such
as manual data queries and search engines, often struggle
to meet highly specialized and domain-specific question-
answering needs due to long retrieval cycles, insufficient
semantic understanding, and low precision. To address these
challenges, FAQ-based systems have emerged, with the core
objective of leveraging retrieval and recall techniques to
quickly match user queries with the most relevant answers
fromapre-constructeddomain-specificknowledgebase.How-
ever, building a question-answering system in the petroleum
domain not only requires handling vast and complex industry
knowledge but also accommodating specialized terminology,
industry standards, and the diverse and intricate nature of
contextual semantics. Therefore, improving the accuracy of
question-answering systems, particularly in highly specialized
petroleum-related scenarios, has become a key research focus
in intelligent question-answering systems.
In traditional FAQ-based retrieval systems, conventional re-
trieval algorithms usually provide multiple candidate answers
979-8-3315-3626-8/25/$31.00 ©2025 IEEE 108
77668011.5202.55756PSCI/9011.01
:IOD
|
EEEI
5202©
00.13$/52/8-6263-5133-8-979
|
)PSCI(
gnissecorP
langiS
dna
gnitupmoC
tnegilletnI
no
ecnerefnoC
lanoitanretnI
ht01
5202
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

gent Computing and Signal Processing (ICSP)
ranking Algorithm for
the Petroleum Domain
2nd Jiaxin Han∗
School of Computer Science
Xi’an Shiyou University
Xi’an, Shaanxi, China
∗Corresponding author:jxhan@xsyu.edu.cn
related to user questions. However, these candidate answers
may not necessarily be the most accurate or relevant. To
address this issue, this study focuses on introducing a re-
ranking algorithm model to optimize the retrieval process
and further enhance user experience and the effectiveness of
the question-answering system. The primary objective of this
study is to explore the application of re-ranking algorithms
in the petroleum field FAQ-based systems. Based on the
textual knowledge within the petroleum vertical domain, we
aim to investigate the optimization and improvement of recall
accuracyandquestion-answercontextmatchingbroughtabout
by re-ranking algorithms.
II. RELATEDWORK
In the research of vertical domain FAQ-based Systems, the
main challenges include understanding professional terminol-
ogy, data scarcity, retrieval recall optimization, and reranking
strategies. Recent trends in research indicate that combining
multi-stage retrieval, semantic matching, reranking optimiza-
tion, contrastive learning, and LoRA fine-tuning techniques
can effectively improve the system’s retrieval accuracy and
domain adaptability. To further optimize the performance of
the oil industry FAQ question-answering system, this section
will discuss the following key aspects: (1) Characteristics
and challenges of the oil industry FAQ question-answering
system; (2) Multi-stage retrieval strategies and the application
of the DPR model; (3) Reranking techniques; (4) The role
of contrastive learning and LoRA fine-tuning with domain
knowledge in pre-trained models.
Compared to general question-answering systems, vertical
domain FAQ-based systems typically have a more specific
professional background and knowledge system. Since these
systems need to understand domain-specific terminology and
high-quality data is relatively scarce, their generalization
ability is often limited. Moreover, the knowledge structure
of vertical domains is complex. For example, it needs to
integrate knowledge from multiple aspects such as geological
exploration, drilling technology, and reservoir management to
provide precise question-answering capabilities. For instance,
Hojageldiyev et al. proposed the HSE AI Assistant at the
Abu Dhabi International Petroleum Exhibition. This system
is specifically designed for answering questions related to
084
ed on January 30,2026 at 12:34:01 UTC from IEEE Xplore. Restrictions apply.

=== Page 2 ===
occupational health, safety, and environmental protection reg-
ulations in the oil and gas industry. It uses the latest natural
language processing techniques to retrieve information from
theregulatorydatabaseandprovidethemostrelevantanswers
[1].
Traditional FAQ-based systems usually rely on sparse re-
trieval methods based on keywords, such as TF-IDF, BM25,
and inverted index. However, these methods are inadequate
when it comes to semantic understanding and long-text pro-
cessing. In recent years, the DPR model based on deep
learning has become the mainstream method in FAQ-based
systems due to its excellent semantic understanding capabil-
ities. Compared with traditional retrieval methods, DPR can
capture the deep semantic relationships between queries and
candidateanswers,optimizetheinitiallyretrievedanswers,and
improve the overall accuracy of the system [2][3].
Rodrigo Nogueira et al. published a multi-stage document
rankingmethodbasedontheBERTmodel,aimingtoimprove
ranking accuracy in information retrieval systems[4]. This
methoddividesthedocumentrankingtaskintomultiplestages.
First, it uses traditional sparse retrieval methods (BM25) to
preliminarily filter candidate documents. Then, it employs the
BERTmodeltodeeplyrankthefiltereddocuments,effectively
combining the efficiency of sparse retrieval methods and the
semantic understanding capabilities of dense retrieval models.
Through this multi-stage strategy, ranking accuracy can be
significantly improved, especially in large-scale document
retrieval tasks, reducing computational costs and enhancing
the system’s efficiency and accuracy.
Deep neural embedding models capture the semantic fea-
tures of text by converting it into low-dimensional vector
representations and then computing the similarity between
texts based on these vectors. The core of neural retrieval lies
in accurately calculating the semantic similarity between a
query and candidate answers. Among many embedding-based
retrieval methods, dense retrieval is the most common. This
method computes the similarity between texts by aggregating
theoutputoftextencoders,therebyrecallingthemostrelevant
answers.In conversational question-answering tasks, a dual-
tower structure is commonly used to calculate the similarity
between user queries and answers in the database, effectively
recalling the most matching answers[5]. The input question
and query text are mapped to multidimensional vectors using
different encoders EQ(·). The relevance between them is
measuredbycalculatingthedotproduct(normalized)orcosine
similarity between the question and query paragraph vectors.
Finally, the top k paragraphs with the highest similarity are
sorted and recalled.
sim(q,p)=E (q)⊤·E (p) (1)
Q P
The research progress in re-ranking techniques has also
provided significant support for improving the performance
of question-answering systems. Re-ranking technology im-
proves the accuracy of the final output by further ranking the
candidate answers initially retrieved. This technique typically
108
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

relies on deep learning models, especially those based on
the Transformer architecture (BERT, GPT, etc.), which are
capable of feature learning on large-scale data, thereby en-
hancing the accuracy and effectiveness of the ranking[6]. For
domain-specific question-answering systems, such as those in
the petroleum industry, the task of re-ranking models is to
optimize the ranking of retrieval results through more refined
featurelearning,makingitbetteralignedwiththeuser’sactual
needs. Question-answering systems in the petroleum field
often face challenges with specific terminology and complex
concepts, making the design of customized re-ranking models
particularlyimportant.Theintegrationofretrieval-basedrecall
and re-ranking techniques has become a key direction for
improvingtheoverallperformanceofprivatedomainquestion-
answering systems.
Toenhancetheaccuracyandprofessionalismofinformation
retrieval models in specific knowledge domains, contrastive
learning and model fine-tuning methods have been widely
applied in the field of natural language processing in recent
years. Contrastive learning, especially in vector representa-
tion learning for information retrieval and question-answering
systems, aims to optimize the embedding representations by
maximizing the similarity between representations of similar
samples while minimizing the similarity between dissimilar
samples[7]. For the DPR retrieval model, contrastive learn-
ing can effectively enhance the model’s semantic modeling
capabilities in unsupervised or weakly supervised environ-
ments.Byconstructingpositiveandnegativesamplepairs,the
distribution of the embedding space can be optimized to be
more discriminative, thereby improving the model’s retrieval
performance.
Meanwhile, LoRA fine-tuning technology provides a new
paradigm for the efficient tuning of large-scale language
models. It reduces the parameter overhead during the fine-
tuning process by adding low-rank adaptation matrices to the
weightmatricesintheTransformerarchitecture,enablinglarge
modelstoefficientlyadapttonewtasksunderlimitedresource
conditions[8]. In FAQ-based systems, LoRA can be used to
quickly adapt the model to domain-specific data, avoiding the
computational and storage overhead associated with full fine-
tuning.
III. METHODOLOGY
A. Experiments Design
Theselectionandapplicationofretrievalandre-rankingpre-
trained models are a crucial part of the experimental design
in this study. We mainly used the BAAI/bge series of pre-
trained models for the relevant experiments. Based on the
designrequirementsofthepetroleumfieldFAQ-basedsystems
and the research focus of the re-ranking model, we selected
multiple pre-trained models for comparative experiments and
chose the better-performing models for fine-tuning experi-
ments. We set different hyperparameters for training to verify
the improvement of the re-ranking results.
In the initial retrieval recall, this experiment uses the
BAAI/bge-m3 dense retrieval embedding model, which is
085
ed on January 30,2026 at 12:34:01 UTC from IEEE Xplore. Restrictions apply.

=== Page 3 ===
basedontheXLM-RoBERTaarchitecture.Asanimportantop-
timizationvariantoftheBERTsingleencoderstructure,XLM-
RoBERTa adopts a larger-scale cross-lingual unsupervised
pretraining, removes the next-sentence prediction task from
BERT,andfocusesondynamicmaskedlanguagemodeling.It
also incorporates longer training steps and larger batch sizes.
These optimizations significantly enhance the performance
of embedding models, providing a solid foundation for the
retrieval capabilities of m3 [9]. Additionally, bge-m3 further
utilizes a large training corpus for pretraining and fine-tuning,
enabling better dense passage retrieval (DPR) capabilities.
This includes a substantial amount of domain-specific data
(such as S2ORC, PubMedQA, etc.), which allows it to better
capture the semantics of professional terminology. Moreover,
the m3 model supports inputs up to 8192 tokens, which
effectively handles long documents and technical reports in
theoilindustry.IncontrasttotraditionalBERTorTransformer
models, whichare typically limitedto token lengths of512 or
1024, the professional documents in the oil industry are often
lengthy. The m3 model’s long-text processing ability allows it
to better capture the overall semantics of documents [10].
To enhance computational efficiency and meet question-
answering demands, this experiment employs pooling tech-
niquestoreducethedimensionalityoffeaturevectors,improv-
ingtherecallefficiencyofquestion-answercomputations.The
output vectors uniformly use the normalized [CLS] token as
thesemanticrepresentation(embedding dims=1024).Boththe
test set questions and the entire candidate answer corpus are
encoded into vectors [10]. In this study, 500 test set questions
and 43,330 corresponding candidate answers are individually
converted into feature vectors. A dual-tower architecture is
then used to compute the dot product between the normalized
featurevectorsofthequestionsandallcandidateanswertexts,
generating similarity scores. These scores are sorted to obtain
thetop5results,andrelevanceisdeterminedbasedonwhether
the corresponding real answers match the test questions.
Aftertheinitialrecall,theBAAI/bge-rerankerseriesmodels
are used to rerank the candidate answers to further improve
retrievalaccuracy.Specifically,thisseriesofmodelsemploysa
pretrainedTransformerarchitecturetojointlyencodetheques-
tion and candidate answers. The input format is constructed
as”CLS+prompt+SEP+question+SEP+answer+SEP.”
This cross-encoder structure allows the model to capture both
thequestionandanswer’ssemanticinformationwithinasingle
sequence, and the CLS token is used to capture the global
semantics, enabling more precise calculation of the relevance
between the two.
The model first inputs the user’s question and candidate
answers into an interactive Transformer, generating context-
richfeaturerepresentationsforfine-grainedsemanticmatching
betweenthequestionandanswer.Then,adeepneuralnetwork
is used to match the question-answer pair, and a Sigmoid
activation function maps the output relevance scores to the
[0, 1] range. The higher the score, the stronger the semantic
matchbetweenthequestionandanswer.Finally,thesimilarity
scoresareusedtorankthemultipleinitiallyrecalledcandidate
108
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

answers,ensuringthatthemostrelevantanswersareplacedat
the top [11]. The process is illustrated in Figure 1.
Fig.1. ValidationProcessFlowchartofRe-rankingModel.
This paper will test the improvement effects of different
BGE-reranker models on the recall results and select the
optimal pre-trained model base for fine-tuning based on the
experimental results. Through comparative experiments, we
systematically evaluate the advantages and disadvantages of
different models and further optimize the performance of
the petroleum field FAQ-based systems. The specific model
selection is shown in Table 1.
TABLEI
PRETRAINEDMODELINFORMATION
ModelCategory PretrainedModelInformation
PretrainedModel Size(GB)
Encoder-only - -
Retrieval bge-m3 2.3
Reranker bge-reranker-base 1.1
Reranker bge-reranker-large 2.2
Reranker bge-reranker-v2-m3 2.3
Decoder-only - -
Reranker bge-reranker-v2-gemma 10.1
Reranker bge-reranker-v2.5-gemma2-lightweight 37.4
Reranker bge-reranker-v2-minicpm-layerwise 10.9
B. Dataset Preparation
The petroleum domain question-answering dataset used in
this study is derived from authoritative Chinese resources
in the petroleum industry, including petroleum knowledge,
journals of petroleum science, and publicly available content
from official websites. The dataset comprises 1,720 articles
from January 2020 to December 2024, covering multiple
important topics in the petroleum field, such as exploration
and development, refining and chemical processing, oil and
gas storage and transportation, and petrochemical enterprises.
Through these articles, a multi-topic rich dataset has been
constructed, providing a solid foundation for the training and
fine-tuning of the question-answering system.
The generation of the dataset combines automated tech-
niques with manual review. Relying on a pre-trained
large-scale language model (Qwen2.5-72B-Instruction), raw
question-answer pairs were generated and then manually an-
notated and cleaned to ensure data quality. A total of 43,330
question-answerpairswerecollected,withredundantandille-
gal content removed. The test set randomly selected 500 data
pointstovalidatetherecallandre-rankingmodelperformance.
086
ed on January 30,2026 at 12:34:01 UTC from IEEE Xplore. Restrictions apply.

=== Page 4 ===
This dataset not only covers common petroleum knowledge
questions but also focuses on domain-specific terminology,
industrystandards,andcomplexconcepts,effectivelysupport-
ing the precise ranking and fine-tuning optimization of the re-
ranking model. An example of the data is shown in Table 2
below:
TABLEII
PROFESSIONALQ&AINTHEPETROLEUMFIELD.
Question Answer
Therearesustainabilityissueswithgeothermalen-
ergy utilization, including insufficient understand-
ing of comprehensive resource utilization and in-
dustrialdevelopment,low-levelandlow-benefitde-
What are the
velopment, serious resource wastage, and unclear
existing problems
understanding of geothermal resource character-
with geothermal
istics by developers. Additionally, environmental
energyutilization?
issues such as water pollution, thermal pollution,
airpollution,soilpollution,andgroundsubsidence
may arise during the development and utilization
ofgeothermalenergy.
The two basic methods of oil extraction include:
one is relying on natural or artificial water in-
jection, gas supplementation to produce energy
What are the two
throughself-pressurization;theotherisusingme-
basic methods of
chanical lifting, that is, installing oil extraction
oilextraction?
machinery in oil wells that cannot self-pressurize
to produce oil. Currently, about 90% of oil wells
worldwideusemechanicalliftingmethods.
Mechanical drilling speed refers to the length
(meters per hour) of continuous drilling progress
obtainedperhour,whichisanimportantindicator
for measuring drilling efficiency. There are many
factorsaffectingmechanicaldrillingspeed,includ-
What is mechani- ing:
cal drilling speed • Rockproperties(drillability,abrasivity)
andthefactorsaf- • Process parameters (drilling pressure, rota-
fecting mechani- tionspeed)
caldrillingspeed? • Hydraulic parameters (pumping power,
pumppressure,flowrate)
• Drilling fluid properties (density, viscosity,
solidcontent,etc.)
• Bitcondition(structuretype,weardegree)
IV. EXPERIMENTSANDRESULTS
A. Experiments Setup
This experiment was conducted using the Google Colab
cloudserviceforsettingupconfigurationsandtheexperimen-
tal setup. GPU resources were rented, and a cloud drive was
mounted for data storage and access. The key hardware and
software configurations are summarized in Table 3.
B. Evaluation Metrics
Inthisstudy,severalevaluationmetricsareutilizedtoassess
the performance of the retrieval and re-ranking models for
the FAQ-based systems in the oil domain. These metrics
provideaclearunderstandingofhowwellthemodelsperform
in retrieving and ranking relevant answers.For ranking tasks,
evaluationmetricstypicallyfocusonrecallhitrate(HitRatio),
Mean Reciprocal Rank (MRR), and other ranking-specific
measures,ratherthancommonlyusedmetricssuchasF1score
andaccuracy,whicharemoreprevalentinclassificationtasks.
108
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

TABLEIII
KEYHARDWAREANDSOFTWARECONFIGURATIONS
Category ConfigurationDetails
Hardware
CPU Intel(R) Xeon(R) CPU @ 2.20GHz (6
cores)
GPU NVIDIAL4(22.5GBVRAM)
Memory 53GB
Storage 236GB
Software
OperatingSystem Ubuntu22.04.3LTS
ProgrammingLanguage Python3.10.12
DeepLearningFramework Torch==2.5.1
CUDAVersion 12.2.1
AdditionalLibraries FlagEmbedding==1.3.3,Faiss-gpu==1.7.3,
Peft==0.14.0,Transformers==4.47.1
1) RecallHitRate: Recallhitrateisanimportantmetricfor
evaluating the ability of a retrieval model to identify relevant
answers. It measures the proportion of relevant answers that
appear in the top N retrieved results. The recall hit rate for
the top N positions is calculated as follows in formula 2:
Number of relevant answers in top N results
Recall Hit Rate@N=
Total number of relevant answers
(2)
2) Mean Reciprocal Rank (MRR): The MRR is another
critical metric that evaluates the ranking quality of the model.
It calculates the average of the reciprocal ranks of the first
correct answer across all queries. It is defined as:
Q
1 (cid:88) 1
MRR= (3)
Q rank
i
i=1
C. Experimental Results
1) Preliminary Recall & Re-ranking Results: In this sec-
tion, we present the retrieval recall and reranking results of
different models evaluated using FP16 half-precision floating-
point numbers. The model performance is measured using
Hit@1-5,MRRandGPUutilization.Thesemetricseffectively
assess the model’s ability to retrieve relevant information and
optimize rankings.
The test dataset consists of 500 randomly selected samples
from previously self-collected question-answer pairs in the
oil industry, covering various question-answer scenarios to
ensure diversity and representativeness in the testing. In the
experiment, we first use the recall model to retrieve candidate
document sets for these 500 queries, then apply the reranking
model for fine-grained ranking to evaluate the performance
improvements of different pretrained reranking models.
Table 4 summarizes the experimental results, showing the
performanceofdifferentpretrainedrerankingmodelsbasedon
the key evaluation metrics mentioned above.
According to the experimental results, compared to the
baseline retrieval (82.6%), the selected pretrained reranker
models improved the recall precision by approximately 3%-
6%, with a particularly significant increase in the Top-1 hit
rate, confirming the effectiveness of the reranker models.
087
ed on January 30,2026 at 12:34:01 UTC from IEEE Xplore. Restrictions apply.

=== Page 5 ===
TABL
MODELPERFORMA
Model Top1 HIT Top2 HIT T
RetrievalResult 82.6% 88.6%
bge-reranker-base 85.2% 90.6%
bge-reranker-large 86.4% 91.0%
bge-reranker-v2-m3 87.2% 92.2%
bge-reranker-v2-gemma 88.2% 92.0%
bge-reranker-v2.5-gemma2-lightweight 88.0% 93.2%
bge-reranker-v2-minicpm-layerwise[10] 85.8% 91.2%
bge-reranker-v2-minicpm-layerwise[28] 88.4% 92.6%
bge-reranker-v2-minicpm-layerwise[40] 88.4% 92.8%
BAAI/bge-reranker-v2-m3 improved the Top-1 hit
rate by 4.6%, and the MRR increased from 0.8739
to 0.9058.BAAI/bge-reranker-v2-minicpm-layerwise [40]
showed excellent performance, with the Top-1 hit rate
increasing by 5.8%, and the MRR improving from 0.8739 to
0.91297.
While BAAI/bge-reranker-v2.5-gemma2-lightweight out-
performed the minicpm model in terms of hit rates for Top-2
toTop-4by0.4%-0.2%,ithasamodelsizeandmemoryusage
that are 2-3 times higher.For the bge-reranker-v2-minicpm-
layerwise model, the reranking effect was similar when the
decoderlayercountwastruncatedatthe28thand40thlayers,
with average processing times of 0.13 seconds and 0.19
seconds, respectively.
2) Model Fine-tuning & Re-ranking Results: Based on
the aforementioned experimental results, this study utilizes a
self-collected question-and-answer knowledge dataset in the
field of petroleum, employing methods such as contrastive
learning and low-rank adaptation to fine-tune two pre-trained
modelsBAAI/bge-reranker-v2-m3andBAAI/bge-reranker-v2-
minicpm-layerwise.Thenumberoftrainingepochsisadjusted
according to the main parameter to conduct tests, in order to
verify the enhancement effect of fine-tuning on the re-ranking
results.
The fine-tuning process utilized QA pairs to construct the
trainingdataset,formattedinJSONL.Eachquestionwassaved
as a dictionary containing three keys: query, pos, and neg,
and a prompt field was added for decoder-only models. The
total volume of training data was 43,330 entries samples. The
queryrepresentsthequestion,posisthecorrespondinganswer
document, and neg consists of multiple irrelevant negative
example documents.
To generate the negative examples, a hard negative mining
method was employed. Initially, 100 soft negative examples
were randomly selected from other answers. Then, a smaller
model (BAAI/bge-large-zh-v1.5) was used to compute the
similarityandrankthem,fromwhichthetop15hardnegative
examples were chosen for the training data. The final fine-
tuningdatasetsizewas260MB.TheresultsareshowninTable
5 below:
Based on the training methods and scripts from the official
website, and considering the characteristics of text length and
otherfactorsinthepetroleumdomainknowledgedata,thekey
108
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

LEIV
ANCECOMPARISON
Top3 HIT Top4 HIT Top5 HIT MRR GPU utilization
91.2% 93.6% 95.2% 0.873867 -
93.2% 94.0% 95.2% 0.892067 0.9GB
93.6% 94.6% 95.2% 0.899367 1.4GB
94.0% 94.8% 95.2% 0.905800 1.6GB
93.4% 94.4% 95.2% 0.909767 15.3GB
94.4% 95.0% 95.2% 0.911900 20.3GB
93.8% 94.8% 95.2% 0.896967 5.8GB
94.0% 94.8% 95.2% 0.912467 6.3GB
94.2% 94.8% 95.2% 0.912967 6.6GB
training hyperparameters for fine-tuning BAAI/bge-reranker-
v2-minicpm-layerwise are set as follows:
torchrun --nproc_per_node 1 \
--model_name_or_path
BAAI/bge-reranker-v2-minicpm-layerwise \
-lora_rank 32 \
-lora_alpha 64 \
--use_flash_attn True \
--target_modules
q_proj k_proj v_proj o_proj \
--save_merged_lora_model True \
--model_type decoder \
--start_layer 8 \
--head_multi True \
--trust_remote_code True \
--cache_path ./cache/data \
--train_group_size 8 \
--query_max_len 256 \
--passage_max_len 1024 \
--pad_to_multiple_of 8 \
--knowledge_distillation False \
--output_dir
./test_decoder_bge-reranker \
--learning_rate 2e-4 \
--bf16 \
--num_train_epochs 20 \
--per_device_train_batch_size 10 \
--gradient_accumulation_steps 1 \
--warmup_ratio 0.1 \
--weight_decay 0.01 \
--logging_steps 1 \
--save_steps 4333
Based on petroleum domain knowledge data and industry
content, the fine-tuned models include two types of superior
pre-trainedmodels,namelyencoder-onlyanddecoder-only,for
validation experiments. A comparison of the model perfor-
mancewasconducted,andtheexperimentalresultsareshown
in Table 5:
The experimental results show that the re-ranking model,
combined with petroleum domain expertise, performs better
inquestion-answermatchingtasks.Specifically,thefine-tuned
models exhibit significant improvements in HITRatio and
088
ed on January 30,2026 at 12:34:01 UTC from IEEE Xplore. Restrictions apply.

=== Page 6 ===
TABL
FINE-TUNEDMODELEX
Model Epochs Top1 HI
bge-reranker-v2-m3 – 87.2%
bge-reranker-v2-m3-finetuned 1 88.4%
bge-reranker-v2-m3-finetuned 2 87.4%
bge-reranker-v2-m3-finetuned 3 87.6%
bge-reranker-v2-m3-finetuned 5 88.8%
bge-reranker-v2-m3-finetuned 10 90.6%
bge-reranker-v2-m3-finetuned 15 91.2%
bge-reranker-v2-m3-finetuned 20 91.4%
bge-reranker-v2-minicpm-layerwise[40] – 88.4%
bge-reranker-v2-minicpm-layerwise[40]-finetuned 1 92.2%
bge-reranker-v2-minicpm-layerwise[40]-finetuned 2 92.6%
bge-reranker-v2-minicpm-layerwise[40]-finetuned 3 92.6%
bge-reranker-v2-minicpm-layerwise[40]-finetuned 5 92.4%
bge-reranker-v2-minicpm-layerwise[40]-finetuned 10 92.8%
bge-reranker-v2-minicpm-layerwise[40]-finetuned 15 92.8%
bge-reranker-v2-minicpm-layerwise[40]-finetuned 20 92.8%
MRR metrics for Top-1 to Top-5, compared to the base pre-
trained models. These improvements are especially notable
in the Top-1 hit rate. Specifically, the m3 model performed
optimally at epoch=20, with a 4.2% improvement, while the
minicpm-layerwise model reached its best performance at
epoch=15, with a 4.4% increase. These results demonstrate
that fine-tuning on a vertical domain-specific dataset can
effectively improve the accuracy of question-answer systems
in the domain, proving the positive impact of domain-specific
fine-tuning on model performance.
V. DISCUSSIONANDCONCLUSION
This study verifies the effectiveness of deep learning-based
re-ranking models in question-answering systems within the
oil field domain. The experimental results indicate that un-
der a multi-stage retrieval framework, employing re-ranking
strategies based on pre-trained models such as the BGE
series significantly enhances the accuracy of retrieval results.
Improvements in MRR (Mean Reciprocal Rank) and Top-k
recall rates further demonstrate their generalization capability
onspecializeddomaindata,particularlyinquestion-answering
tasks involving lengthy texts and dense professional terminol-
ogy. The study found that deep learning re-ranking models
can more effectively enhance the accuracy of answers after
preliminary recall compared to traditional retrieval methods.
Additionally, using domain-adapted text embedding models
combined with fine-tuning training can further enhance their
retrieval performance in vertical domains like oil. Moreover,
the advantages of the multi-stage retrieval architecture were
alsovalidated.Byintegratingsemanticunderstandingcapabil-
ities of neural networks like DPR and adopting a ”primary
retrieval and recall + re-ranking” strategy, it is possible to im-
provethesystem’saccuracywhileensuringpracticality.These
results suggest that for specialized question-answering tasks,
the key to enhancing the performance of question-answering
systems lies in the rational selection of pre-trained models,
108
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

LEV
XPERIMENTALRESULTS
IT Top2 HIT Top3 HIT Top4 HIT Top5 HIT MRR
92.2% 94.0% 94.8% 95.2% 0.905800
93.4% 94.4% 95.2% 95.2% 0.914333
93.2% 94.8% 95.2% 95.2% 0.909333
94.0% 94.4% 94.8% 95.2% 0.911133
93.2% 94.8% 95.0% 95.2% 0.916233
94.4% 95.2% 95.2% 95.2% 0.927667
94.0% 95.2% 95.2% 95.2% 0.930000
94.8% 95.2% 95.2% 95.2% 0.932333
92.6% 94.0% 94.8% 95.2% 0.912467
94.6% 95.0% 95.2% 95.2% 0.935833
94.8% 95.0% 95.2% 95.2% 0.938167
95.0% 95.2% 95.2% 95.2% 0.938667
94.2% 95.0% 95.2% 95.2% 0.936167
94.8% 95.2% 95.2% 95.2% 0.939333
95.2% 95.2% 95.2% 95.2% 0.940000
95.2% 95.2% 95.2% 95.2% 0.940000
conductingdomain-adaptivefine-tuning,andemployingmulti-
stage retrieval strategies.
REFERENCES
[1] D.Hojageldiyev,“ArtificialintelligenceinHSE,”inProceedingsofthe
SPEAbuDhabiInternationalPetroleumExhibition&Conference,Abu
Dhabi,UAE,Nov.12-15,2018,PaperSPE-192820-MS.
[2] B. Mitra and N. Craswell, “An introduction to neural information
retrieval,” Foundations and Trends® in Information Retrieval, vol. 13,
no.1,pp.1-126,2018.
[3] V. Karpukhin, B. Oguz, S. Min, P. Lewis, L. Wu, S. Edunov, D.
Chen,andW.-t.Yih,“Densepassageretrievalforopen-domainquestion
answering,”arXivpreprintarXiv:2004.04906,2020.
[4] NogueiraR,YangW,ChoK,etal.Multi-stagedocumentrankingwith
BERT.arXivpreprintarXiv:1910.14424,2019.
[5] H. Tao, J. Zeng, Y. Yu, Z. Wang, and X. Hu, “SynC: A Dense
Retrieval Method based on Syntactical Contrastive Learning,” in 2023
International Joint Conference on Neural Networks (IJCNN), Gold
Coast,Australia,pp.1–8,2023.
[6] A. Yates, R. Nogueira and J. Lin, ”Pretrained transformers for text
ranking: BERT and beyond”, Proc. 14th ACM Int. Conf. Web Search
DataMining,pp.1154-1156,Mar.2021.
[7] T.Gao,X.Yao,andD.Chen,“SimCSE:Simplecontrastivelearningof
sentenceembeddings,”arXivpreprintarXiv:2104.08821,2021.
[8] Hu E J, Shen Y, Wallis P, et al. Lora: Low-rank adaptation of large
languagemodels[J].ICLR,2022,1(2):3.
[9] Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek,
G., Guzma´n, F., ... Stoyanov, V. (2019). Unsupervised cross-lingual
representationlearningatscale.arXivpreprintarXiv:1911.02116.
[10] J. Chen, S. Xiao, P. Zhang, K. Luo, D. Lian, and Z. Liu, “BGE
M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity
TextEmbeddingsThroughSelf-KnowledgeDistillation,”arXivpreprint
arXiv:2402.03216,2024.
[11] X. Ma, X. Zhang, R. Pradeep, and J. Lin, “Zero-shot listwise
document reranking with a large language model,” arXiv preprint
arXiv:2305.02156,2023.
089
ed on January 30,2026 at 12:34:01 UTC from IEEE Xplore. Restrictions apply.

Paper:A_Sentence-Level_Semantic_Annotated_Corpus_Based_on_HNC_Theory.pdf
=== Page 1 ===
2011 International Conference
A Sentence-level Semantic annotat
Zhiying Liu1,2, Yaohong Jin1,2
1Institute of Chinese Information Processing
2CPIC-BNU Joint Laboratory of Machine Translation
Beijing Normal University
Beijing, China
{liuzhy, jinyaohong}@bnu.edu.cn
Abstract—The Sentence-level Semantic annotated Corpus
(SSC) is directed by HNC (Hierarchical Network of
Concepts) Theory in which the sentence is regarded as the
basic unit and semantic and frame information are
annotated. This paper introduces the building form of the
semantic annotated corpus with XML, describes the content
of semantic annotation on sentence level that the annotated
contents can be classified three items: sentence category,
which is the sentence semantic type; semantic chunks, which
is the lower level semantic components in the sentence; sub-
sentence (sentence ecdysis and chunk-extension-into-
sentence) which is included in semantic chunks. SSC has
been an important and valuable knowledge resource for
language study and information processing.
Keywords- HNC Theory; semantic annotation; semantic
chunk; sentence category; XML
I. INTRODUCTION
With the continuous deepening of research and
extension, linguists and computer experts realize that the
semantics plays an important role in Chinese Information
Processing. Meanwhile the focus of Chinese information
processing is on the sentence processing instead of the
word processing as far as language unit is concerned.
Nowadays, the resource building for sentence
processing is beginning and short of the language
knowledge, especially semantic knowledge. Building a
semantic annotated corpus is important to understand and
analyze the language. In our corpus, semantic information
is annotated for continuous texts so that computer can
understand the sentences by acquiring semantic
information in texts.
II. BRIEFINGS OF SEMANTIC ANNOTATION BASED ON
HNC THEORY
HNC(Hierarchical Network of Concepts) Theory
(Huang Zengyang, 2004) argues that natural language
understanding is a process of mapping from the space of
natural language to the space of language concept, Each of
which has its own symbolic system. The symbolic system
in language space differs in thousands of ways, while the
symbolic system in language concept space is unique in
human society. The existence of the space of natural
language depends on sound and character and the
existence of the space of language concept depends on
concept relevancy network [10].
HNC theory constructs an entire theoretical framework
for Natural Language, gives the complete description of
978-0-7695-4554-7/11 $26.00 © 2011 IEEE 5
DOI 10.1109/IALP.2011.11
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

on Asian Language Processing
ted Corpus Based on HNC Theory
Chuanjiang Miao
Department of Chinese and Bilingual Studies
Hong Kong Polytechnic University
Hong Kong, China
miaochj@hotmail.com
the deep structure of the sentence called sentence category
on the basis of concept description system.
HNC designs a semantic network used to describe the
concepts system of natural language wholly, on which
builds the semantic descriptive patterns and constructs the
sentence semantic structure expressions. These patterns
and expressions formally describe the concept relevancy
network on the sentence level. Sentences are infinite in
number, but the number of concept categories of sentences,
which are called sentence categories, is finite. infinite
sentences can be described by finite sentence categories.
Sentence category is the sentence classification by
semantic categories from language deep layer.
A classic tagging example sentence is as follows:
(cid:2031)(cid:1328)(cid:2474)(cid:2533)(cid:1319)(cid:10628)(cid:1114)(cid:5468)(cid:5390)(cid:11352)(cid:7114)(cid:1207)(cid:10317)(cid:5461)(cid:452)
<s code="Y30" form="!0">
<gbk type="YB">(cid:2031)(cid:1328)(cid:2474)(cid:2533)</gbk>
<ek>(cid:1319)(cid:10628)(cid:1114)</ek>
<gbk type="YC">(cid:5468)(cid:5390)(cid:11352)(cid:7114)(cid:1207)(cid:10317)(cid:5461)</gbk>.
</s>
Figure 1. Example of Tagging Form in HNC Semantic Corpus
So far, HNC semantic annotated corpus(SSC) has
tagged 395 articles one million characters in detail which
are selected from newspapers, mainly People Daily in
recent years, The fields involve politics, economy, culture,
sports, military, law, education, health, disaster, state, spirit
etc. It takes HNC team 8 years to study and build. SSC has
been an important and valuable knowledge resource for
language study and information processing.
III. XML ANNOTATING FORM
This corpus adopts XML as annotating form. XML is
short for eXtensible Markup Language which is a group of
rules defining semantic marks. It contains, shapes, labels,
structures, and protects information with symbols
embedded in the text, called markup which enhances the
meaning of information in certain ways, identifies each of
parts and how they relate to each other. We can define
these marks and grammar structure freely and annotate the
information in texts by XML elements and attributes. In
corpus, the semantic units are marked by special elements,
and semantic knowledge is marked by attribute value. One
element can be embedded in another element. Thus a tree
59
ed on January 29,2026 at 15:41:53 UTC from IEEE Xplore. Restrictions apply.

=== Page 2 ===
structure is formed which shows the layer character of
corpus.
For example, we define s element as sentence, gbk
element as global object chunk, ek element as Eigen chunk
in which the main verb is located, fk element as auxiliary
chunk relative to the main chunk which gbk and ek belong
to. Sentence category and sentence form are described in
attribute values. By doing this, sentence semantic
information is expressed distinctly.
It is the winter in the 6th year of the Republic.
<s code="jD">
<gbk type="1">It</gbk>
<ek>is</ek>
<gbk type="2">the winter</gbk>
<fk type="Cn"> in the 6th year of the
Republic</fk>.
</s>
Figure 2. XML Tagging Form of SSC
This is a simple sentence which is expressed as s
element. Sentence category is YesOrNo-judging sentence
which is expressed as attribute value code="jD". The
sentence contains two global object chunks which are
expressed as gbk elements with the data content It and the
winter. The order of gbk is expressed as attribute value
type="1" and type="2".fk is auxiliary chunk which type is
Cn that means time-condition.
IV. THE BASIC ANNOTATED CONTENTS
Since the natural language processing depends on
semantic understanding, the SSC takes the meaning as the
main annotated contents. The annotated mode is top-to-
bottom. Larger language unit will be annotated first and
smaller language one will be annotated later. That is,
discourse and paragraphs are annotated first, sentences and
semantic chunks are annotated later, and words last. Three
aspects prove very important in tagging the meaning of
sentences [1]:
• Sentence category, which categorizes the meaning
of sentences.
• Semantic chunk, which constitutes the semantic
components of the lower level.
• Sub-clause, which is imbedded in semantic chunk
as a part of it.
A. Sentence Category
57 kinds of basic sentence categories are classified by
deductive method in HNC theory. These sentence
categories are the basic types of sentence meaning, which
can be used to describe any sentence semantic types. In
natural language, the semantic category of a sentence can
be a kind of basic sentence category, or a combination of
two or more basic sentence categories which is called
compound sentence. For example,
Students are cleaning the classroom. (Basic action, X)
The corrupt officials finally pleaded guilty. (Reaction
and state, X20S*10)
6
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

The first sentence belongs to a basic sentence category
which sentence code is X. The second sentence belongs to
a compound sentence category which is combined by
reaction sentence category and state sentence category
with the sentence code X20S*11.
Sentence code is regarded as the attribute of s element.
It is annotated like this:
<s code="X">Students are cleaning the classroom.
</s>
<s code="X20S*10">The corrupt officials finally
pleaded guilty. </s>
Figure 3. code tagging
In SSC, sentences have all been annotated with
sentence category codes which are classified as basic
sentence category and compound sentence category.
TABLE I. ANNOTATION DATA OF SENTENCE CATEGORY
Type of Sentence
Sentences Percentage
Category
Basic sentence
36099 69%
category
Compound sentence
16224 31%
category
It shows that the numbers of sentences which belong to
the basic sentence category are about twice as large as the
numbers of sentences which belong to the compound
sentence category in table 1.
In HNC Theory, sentences are described from two
profiles: global action and global result. Action and result
are causal relationship generally. In global sentence, the
subject of sentence is Agent. In result sentence, the subject
of sentence can be object, content but not Agent [2].
As to the sentence category, it is classified as global
action sentence and global result sentence semantically.
Global action sentence includes four kinds of types which
are X, T, R, D. Global result sentence includes four kinds
of types which are Y, P, S, jD. Global action sentence
concludes action sentence(X), transfer sentence(T),
relation sentence(R) and intellection sentence(D). Global
result sentence concludes result sentence(Y), process
sentence(P), state sentence(S) and basic logic definition
sentence(jD).[10] The eight kinds of sentence category
constitute the frame of basic sentence category. And it
expands into 57 kinds of basic sentence categories.The
Numerical data are as follows:
TABLE II. ANNOTATION DATA OF BASIC SENTENCE CATEGORY
TYPES
Types of Basic sentence category Sentences Ratio
9988 27.67%
X(action)
T(transform) 4892 13.55%
global
action R(relation) 1198 3.32%
D(determination) 2237 6.20%
Y(result) 4173 11.56%
global
60
ed on January 29,2026 at 15:41:53 UTC from IEEE Xplore. Restrictions apply.

=== Page 3 ===
Types of Basic sentence category Sentences Ratio
result P(process) 843 2.34%
S(state) 5072 14.05%
jD(basic logic definition) 7696 21.32%
B. Semantic Chunks
Semantic chunks are lower semantic components than
the sentence, which can be a word, a phrase, or a sub-
sentence (sentence Ecdysis or chunk-extension-into-
sentence), even can be the combination of the three
mentioned above.
The borders of chunks need to mark when tagging. It is
easy to mark them with container elements in XML.
Elements are the building blocks of XML, dividing a
document into a hierarchy of regions, each serving a
specific purpose. It begins with a start tag and closes with
an end tag. The chunks are contained in both tags as
elements.
Different sentence category needs different semantic
chunks. For example, action sentence category needs three
main chunks: action, actor, object; and information transfer
sentence category needs four main chunks: information
transfer, transferor, receiver and transferred information.
As to the first sentence above, students are actor, are
cleaning is action, the classroom is object.
It is not necessary to tag the meanings of main chunks
since definite sentence category contains definite main
chunks. When sentence category is known, the meanings
of main chunks are doubtless. Therefore, we tag the type
of gbk chunks with numbers which mark the order of gbk
chunks instead of the meanings. Eigen chunk need not to
tag the position information since it always appears on the
second position among the chunks.
For the sentence above, it is annotated like this:
<s code="X">
<gbk type="1"> Students </gbk>
<ek> are cleaning </ek>
<gbk type="2"> the classroom </gbk>.
</s>
Figure 4. Semantic chunks tagging
Besides, auxiliary chunks such as condition chunks
which are made of time or space phrase are not the
integrant components in sentence building. So the changes
of auxiliary chunks have nothing to do with sentence
category.
Sub-sentence is the sentence which is included in the
semantic chunks. Sentence ecdysis and chunk-extension-
into-sentence are the items adopted in HNC theory.
Sentence ecdysis means that sentence changes into
semantic chunk or the part of it. Chunk-extension-into-
sentence means that a semantic chunk extends into a
sentence. In language understanding, we should take them
as sentence, describing their sentence category and
semantic chunks. For example,
6
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

C. (cid:54)(cid:88)(cid:69)(cid:16)(cid:86)(cid:72)(cid:81)(cid:87)(cid:72)(cid:81)(cid:70)(cid:72)(cid:29)(cid:3)(cid:54)entence (cid:40)cdysis and Chunk-
extension-into-sentence
Sub-sentence is the sentence which is included in the
semantic chunks. Sentence ecdysis and chunk-extension-
into-sentence are the items adopted in HNC theory.
Sentence ecdysis means that sentence changes into
semantic chunk or the part of it. Chunk-extension-into-
sentence means that a semantic chunk extends into a
sentence. In language understanding, we should take them
as sentence, describing their sentence category and
semantic chunks. For example,
The pain that economic crisis caused eased.
<ss code="P21">
<gbk type="2">The pain</gbk>
<gbk type="1">that the economic crisis</gbk>
<ek>caused</ek>
</ss>
Figure 5. Sentence ecdysis tagging
The annotated part above is a sentence Ecdysis which
sentence category code is P21(cause and effect sentence).
Apparently, the pain is the headword and the attributive
clause crisis causes is modifier. In fact, they can be
reverted to a complete sentence form economic crisis
causes the pain.
The schoolmaster hopes that they start a new study life.
<ss code="X20">
<gbk type="1">they</gbk>
<ek>start</ek>
<gbk type="2">a new study life</gbk>
</ss>
Figure 6. Chunk-extension-into-sentence tagging
The annotated part is a chunk-extension-into-sentence
which sentence category code is X20(basic reaction
sentence).
Both chunk-extension-into-sentence and sentence
Ecdysis are all semantic chunk or part of it. The difference
of them is set according to the need of project. It is
regulated that chunk-extension-into-sentence appears in
definite 8 kinds of sentence categories. The chunk-
extension-into-sentence is a chunk that must be a sentence
form, but sentence Ecdysis is a chunk that can be either
chunk or sentence form.
D. Separation of Semantic Chunk
In a sentence, some parts of a semantic chunk can be
separated to different syntax positions, but they are one
chunk on semantic layer. We use the sep element to
describe this kind of semantic component. Separate chunk
can appear on several positions, which can be both sub-
element of sentence or sub-sentence and semantic chunk.
61
ed on January 29,2026 at 15:41:53 UTC from IEEE Xplore. Restrictions apply.

=== Page 4 ===
When tagging, we need to give the separate attribute
information. We use the attribute from to describe that the
position where it is separated. For example,
Beside candies, there are color pencils, a knife and
some toys in the speaking trumpet.
<s code="jD">
<sep from="gbk2">beside candies</sep>,
<gbk type="1">there</gbk>
<ek>are</ek>
<gbk type="2"> color pencils, a knife and some
toys </gbk>
<fk type="Cn"> in the speaking trumpet </fk>.
</s>
Figure 7. Separation of semantic chunk tagging in English
The annotated part shows that the content of sep
element is beside candies, the prepositional phrase here is a
part of main semantic chunk, is separated from the 2nd gbk
with the content color pencils, a knife and some toys.
It is interesting that separate phenomena are more
common in Chinese than in English. In Chinese, both gbk
chunks and Eigen chunk can separate into a few parts. For
example,
(cid:1194)(cid:1216)(cid:15999)(cid:980)(cid:19465)(cid:6982)(cid:19388)(cid:3780)(cid:6183)(cid:7041)(cid:1114)(cid:16860)(cid:16817)(cid:452)
<s code="X">
<gbk type="2">(cid:1194)(cid:1216)</gbk>
<gbk type="1">(cid:15999)(cid:980)(cid:19465)(cid:6982)(cid:19388)(cid:3780)</gbk>
<ek>(cid:6183)(cid:7041)(cid:1114)</ek>
<sep from="gbk2">(cid:16860)(cid:16817)</sep>(cid:452)
</s>
Figure 8. Separation of semantic chunk tagging in Chinese
In Figure 8, The word (cid:16860)(cid:16817) is the separate part which
original position is in the second gbk. The deep structure
of the sentence is (cid:1194)(cid:1216)(cid:708)(cid:11352)(cid:709)(cid:16860)(cid:16817)(cid:15999)(cid:980)(cid:19465)(cid:6982)(cid:19388)(cid:3780)(cid:6183)(cid:7041)(cid:1114).
Such separation often occurs in passive voice.
(cid:6117)(cid:2460)(cid:6238)(cid:18039)(cid:1233)(cid:7460)(cid:7021)(cid:13775)(cid:19417)(cid:1114)(cid:980)(cid:17953)(cid:452)
<s code="T19">
<gbk type="1">(cid:6117)</gbk>
<sep from="ek">(cid:2460)</sep>
<gbk type="2">(cid:6238)(cid:18039)(cid:1233)(cid:7460)(cid:7021)</gbk>
<ek>(cid:13775)(cid:19417)(cid:1114)(cid:980)(cid:17953)</ek>(cid:452)
</s>
6
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

Figure 9. Separation of semantic chunk tagging in Chinese
In Figure 9, The word (cid:2460) is separated from eigen
chunk (cid:13775)(cid:19417)(cid:1114)(cid:980)(cid:17953). The deep structure of the sentence is
(cid:6117)(cid:6238)(cid:18039)(cid:1233)(cid:7460)(cid:7021)(cid:2460)(cid:13775)(cid:19417)(cid:1114)(cid:980)(cid:17953) which sentence form
belongs to BA-sentence.
V. CONCLUSIONS
The building of sentence level semantic annotated
corpus fills up the blank in the resource construction of
Chinese Information Processing, which is important to the
studies on both HNC theory and the fulfillment of HNC
system of the sentence category analysis. Besides, it can
also be helpful to many language researchers. It has great
significance not only in Chinese information processing
but also in language teaching and research.
In the future, we will extend our annotation to analyze
the internal relationship of semantic chunks. Also we need
to research the sentence group annotation which focuses
on the study of Discourse and Discourse Situations.
Besides, a corpus management system is to be developed
with a friendly user interface.
ACKNOWLEDGMENT
The research is supported by “the Fundamental
Research Funds for the Center Universities”.
REFERENCES
[1] Chuanjiang Miao Zhiying Liu, Sentence Level Semantic Tagging
Based on Modern Chinese, Language Computing and Text
Processing Based on Contents. Tsinghua University Press, 2005.
[2] Chuanjiang Miao, HNC (Hierarchical Network of Concepts)
Theory Introduction, Tsinghua University Press, 2005.
[3] Chuanjiang Miao, Semantic Category of Sentence in Modern
Chinese, Language Planning, pp. 56-58, 2006.
[4] Chuanjiang Miao, Semantic Research Based on the Sentence
Category System of HNC Theory, Applied Linguistics, pp.126-133,
Feb 2006.
[5] Elliott Rusty Harold, XML Bible(2nd edition). Electron Industry
Press, 2002.
[6] Mark Birbeck, XML Advanced Programme(2nd edition), Engineer
Industry Press, 2002.
[7] Yaohong Jin, Natural Language Understanding Based on the
Theory of HNC (Hierarchical Network of Concepts), Sciences
Press, pp.70-79, 2006.
[8] Yuhuan Chi, The Analysis and Processing of The Chinese Verbs’
Morphological Dilemma, 2005.
[9] Zengyang Huang, HNC (Hierarchical Network of Concepts)
Theory, Tsinghua University Press, 1998.
[10] Zengyang Huang, The Basic Theorem and Mathematical and
Physical Expressions of the Language Concept Space. Ocean
Press, 2004.
62
ed on January 29,2026 at 15:41:53 UTC from IEEE Xplore. Restrictions apply.

Paper:Comparison_of_Chunking_Techniques_Across_Diverse_Document_Types_in_NLP_Retrieval_Tasks.pdf
=== Page 1 ===
Comparison of Chunking t
Document Types in N
Shruti Jaiswal∗ Priya
Data and Applied Sciences Data and A
Bosch Global Software Technologies Bosch Global S
Bangalore, India Benga
shruti.jaiswal@in.bosch.com bisht priya
MSH Shankar Datta
Data and Applied Sciences
Bosch Global Software Technologies
Bengaluru, India
MSH.ShankarDatta@in.bosch.com
Abstract—Inretrieval-augmentednaturallanguageprocessing
systems, how information is segmented—or chunked—plays a
critical role in determining the accuracy, efficiency, and robust-
nessofdownstreamretrievalperformance.Despitetheincreasing
reliance on retrieval-based architectures such as open-domain
question answering and RAG (Retrieval-Augmented Genera-
tion), there remains limited empirical work comparing different
chunking strategies in a systematic, task-specific manner. In
this paper, we present a comprehensive comparative study of
widely used chunking methods—fixed-size chunking, sentence-
based chunking, recursive chunking and semantic similarity-
basedchunking—evaluatedwithinthecontextofretrievalperfor-
mance across diverse document types. We analyze each strategy
across multiple metrics including Precision, Recall, MRR(Mean
Reciprocal Rank), and chunking efficiency, emphasizing how
performancevariessignificantlywiththestructuralandsemantic
characteristics of different document types. Our findings high-
light key trade-offs between retrieval accuracy and chunking
granularity,revealingthatwhilefiner-grainedsemanticchunking
often improves precision, it may also introduce computational
overhead, especially in certain document types. These insights
providepracticalguidelinesfordevelopersandresearchersbuild-
ing retrieval-centric NLP pipelines and lay the groundwork for
futureworkonadaptiveandtask-awarechunkingmechanisms.
Index Terms—Document Chunking, Information Retrieval,
Natural Language Processing, Semantic Chunking, Retrieval-
Augmented Generation, Document Segmentation
I. INTRODUCTION
The integration of retrieval mechanisms into natural lan-
guage processing (NLP) workflows has become a key driver
ofrecentadvancementsintaskssuchasopen-domainquestion
answering (ODQA), document-based reasoning, and retrieval-
augmented generation (RAG). In these systems, large collec-
tions of text are preprocessed into smaller units, or chunks,
which are then embedded and stored in a vector database. At
inference time, relevant chunks are retrieved in response to a
query and either used directly or passed to a language model
∗Theseauthorshaveequalcontribution
54044311.5202.88746IAXNEGSER/9011.01
:IOD
|
EEEI
5202©
00.13$/52/9-8853-5133-8-979
|
)IAXneGseR(
IA
elbanialpxE
dna
evitareneG
,elbisnopseR
no
ecnerefnoC
lanoitanretnI
5202
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

techniques Across Diverse
NLP Retrieval Tasks
ank Bisht∗ Krity Kansara
Applied Sciences Data and Applied Sciences
Software Technologies Bosch Global Software Technologies
aluru, India Bengaluru, India
ank@yahoo.com krity.kansara@in.bosch.com
for generation or decision-making. This retrieval-augmented
design improves factual accuracy, reduces hallucination, and
enables access to dynamic external knowledge sources.
Despite the centrality of chunking in this pipeline, the NLP
community has not reached a consensus on the most effec-
tive chunking strategy. Different approaches—ranging from
fixed-length token chunking to semantically aware segmenta-
tion—are often applied ad hoc, without systematic evaluation
of their downstream effects on retrieval performance. As
model size and context windows expand, understanding the
trade-offs between chunking granularity, semantic coherence,
retrieval accuracy, and computational efficiency becomes in-
creasingly critical.
The performance of retrieval-based NLP systems is highly
sensitive to how input documents are segmented into chunks.
Poorly chosen chunking strategies can lead to irrelevant re-
trievals, redundancy, and increased inference costs. However,
a lack of empirical benchmarking across chunking strategies
for retrieval tasks has left developers without clear guidance
on which methods work best under varying conditions.
This paper aims to address this gap by systematically
analyzing and comparing the impact of different chunking
strategies on information retrieval performance. Specifically,
we seek to:
• Evaluate how different chunking methods affect retrieval
quality across representative datasets.
• Quantifythetrade-offsbetweenretrievalaccuracy,chunk-
ing granularity, and system efficiency, considering how
these trade-offs vary by document structure and content.
• Identifypracticalguidelinesforchoosingchunkingstrate-
gies tailored to retrieval-based NLP use cases, emphasiz-
ingthatoptimalselectionoftendependsontheunderlying
document characteristics.
The key contributions of this paper are:
• A unified evaluation framework for comparing chunking
ed on January 29,2026 at 15:39:43 UTC from IEEE Xplore. Restrictions apply.

=== Page 2 ===
Fig.1. Systemarchitectureforchunk
strategies in retrieval-based NLP.
• A comprehensive empirical study of four prominent
chunking methods: fixed-size, sentence-based, recur-
sive chunking and semantic similarity-based chunk-
ing—across multiple document types.
• Quantitative and qualitative insights into how chunking
affects retrieval performance, using metrics such as Pre-
cision, Recall, MRR, and computational overhead, with
an emphasis on how these metrics vary with document
characteristics.
• Asetofactionablerecommendationsforselectingchunk-
ing strategies in real-world applications.
The rest of this paper is organized as follows:
• Section 2 reviews related work in retrieval-based NLP
systems and chunking strategies.
• Section 3 details the experimental setup, including
datasets, chunking methods, and evaluation metrics.
• Section 4 presents and analyzes the experimental results.
• Section 5 discusses implications, limitations, and oppor-
tunities for future research.
• Section 6 concludes the paper.
II. RELATEDWORK
The effectiveness of chunking strategies has a direct in-
fluence on the performance of retrieval-based NLP systems,
particularlyinopen-domainquestionanswering,documentre-
trieval, and retrieval-augmented generation. As large language
models increasingly rely on external knowledge sources, it
becomesessentialtounderstandhowchunkingaffectsretrieval
precision, recall, and efficiency.
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

kingandretrievalevaluationpipeline
A. Retrieval-Augmented Systems
Retrieval-augmentedlanguagemodelsintegrateinformation
retrieval modules into neural architectures to extend context
beyond the model’s input length. For instance, REALM [1]
demonstrated the importance of pre-training language mod-
els with retrieval modules. Similarly, RAG [2] showed that
document retrieval quality significantly impacts downstream
generation quality. Both approaches required the corpus to
be divided into manageable “chunks,” usually based on token
count, highlighting the importance of chunk design.
B. Evolution of Chunking Strategies
Initial systems favored fixed-length token chunking, where
documents were segmented into uniform token blocks (e.g.,
100 or 512 tokens). While simple, this method often caused
semantic boundaries to be broken, reducing the effectiveness
of embeddings [3].
Topreservecoherence,sentence-basedandparagraph-based
chunking were introduced. Lee et al. [4] employed sentence-
aligned chunks for better context retention in ORQA. Slid-
ing window chunking with overlaps [5] improved recall by
increasing coverage of relevant spans, though at the cost of
redundancy and computational overhead.
Recently, semantic-aware chunking methods have emerged.
These approaches use embedding similarity (e.g., via SBERT
orMiniLM)todeterminelogicalchunkboundaries[6],[7].Se-
mantic chunking improves retrieval quality by aligning chunk
boundaries with conceptual completeness rather than arbitrary
size constraints.Beyond retrieval, similar design considera-
tions for explainability and robustness have been explored
in health imaging [8], spectroscopy-based classification [9],
ed on January 29,2026 at 15:39:43 UTC from IEEE Xplore. Restrictions apply.

=== Page 3 ===
TAB
EVALUATIONMETRICSUSEDFORC
Metric Description
Precision&Recall Computed using cosine simi
cos(chunk,query)≥0.7;Pre
Mean Reciprocal Rank Average reciprocal rank of t
(MRR) MRRreflectsbetterrankord
NDCG Normalized Discounted Cum
qualitywithhigherweightfo
ss2fd(SemanticSimilarity Average cosine similarity be
toFullDocument higherss2fdindicatesbroader
specificchunks.
Semantic Relevancy to Average cosine similarity b
GroundTruth(SRGT) embedding.Indicatesalignme
RetrievalTokenCost Average total number of tok
indicatemoreefficientretriev
QCS(Query–Chunk Simi- Average cosine similarity be
larity) Ensuresemanticallyalignedr
ChunkingTime Average preprocessing time r
costofthestrategy.
and Responsible AI frameworks for fairness-aware computer
vision [10] and numeric data classification [11]. These par-
allels underscore that careful segmentation—whether in text,
images, or signal data—directly affects both performance and
interpretability.
C. Comparative Evaluations and Benchmarks
Despite their significance, chunking strategies are rarely
evaluatedinisolation.Moststudiesassessend-to-endsystems,
making it difficult to quantify the chunking component’s im-
pact. The BEIR benchmark [12] enables zero-shot evaluation
of retrieval systems but uses pre-processed corpora with fixed
chunking.Longformer[13]andBigBird[14]partiallyaddress
chunking through attention window manipulation rather than
preprocessing.
Other efforts like FiD [15] and ColBERT [16] showed
that retrieval performance is sensitive to chunk granularity,
but lacked systematic comparisons of chunking strategies
themselves.
To the best of our knowledge, there has been no uni-
fied study that systematically compares different chunk-
ing strategies—fixed-length, sentence-based, recursive, and
semantic-based—withinacontrolledretrievalsetupusingcon-
sistent retrievers and evaluation metrics.
III. METHODOLOGY
A. System Architecture
The overall system is a pipeline that takes a PDF doc-
ument and a set of question–answer (QA) pairs as input,
then produces a chunked document index and evaluates its
performance on retrieving answers as shown in Fig. 1.
The user uploads a PDF and an associated QA dataset.
The PDF is parsed and embedded into a full-document vector
using a pre-trained language model (e.g., a transformer-based
sentence encoder [17]). The text is then chunked using a
specified strategy, and each chunk is embedded individually.
These chunk embeddings are stored in a vector database
(Faiss) for efficient similarity-based retrieval [18].
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

BLEI
CHUNKINGSTRATEGYCOMPARISON
ilarity (≥ 0.7). True: cos(chunk,GT) ≥ 0.7; Predicted:
ecision=TP/(TP+FP),Recall=TP/(TP+FN).
the first relevant chunk retrieved for each query. Higher
dering.
mulative Gain at top-5 ranks. Measures ranked retrieval
ortoppositions.
etween each chunk and the full-document embedding. A
r,overlappingchunks;alowervaluesuggestsmorefocused,
between retrieved chunks and the ground-truth answer
entwithcorrectanswer.
kens in top-5 retrieved chunks per query. Lower values
valforLLMs.
etween the query embedding and top-5 retrieved chunks.
retrievals.
required to chunk each document. Reflects computational
We also compute the semantic similarity between each
chunk and the full document (ss2fd) to evaluate chunking
quality. For each query in the QA dataset, the query is
embedded and passed to the Retriever, which searches the
vector database to return the top-k most similar chunks (we
use k =5), filtered by a cosine similarity threshold of 0.70.
Ground-truth answers are also embedded, and retrieval per-
formance is evaluated by comparing retrieved chunks against
these ground-truth vectors using metrics such as precision,
recall, and MRR.
B. Chunking Strategies
1) Fixed-SizeChunkingwithOverlap: Fixed-sizechunking
divides the text into equal-length token segments using a
sliding window, so that each segment overlaps the previous
one by a constant number of tokens.
2) Sentence-Based Chunking: This strategy splits text at
sentenceboundaries,groupingcompletesentencesintochunks
up to a token limit (250 or 500) without overlap. It preserves
semantic coherence but often yields many small chunks,
especially in documents with short sentences.
3) Recursive (Structure-Preserving) Chunking: This
method uses document structure (e.g., sections, paragraphs)
to define chunk boundaries. Large units are recursively split
by sentences if they exceed the token limit, while short
sections are kept intact. It preserves logical groupings and
avoids unnecessary splits.
4) Semantic Chunking: Semantic chunking uses sentence
embeddings and clustering to group semantically similar sen-
tences into coherent chunks. Sentences are embedded using a
pretrained model (e.g., Sentence-BERT), then clustered based
on embedding similarity. The number of clusters is derived
heuristically or optimized using silhouette scores.
Resulting sentence groups are merged into chunks; those
exceeding the token limit are split further, while very short
chunks are merged with neighbors. This method captures
topical cohesion well and improves retrieval quality in RAG
ed on January 29,2026 at 15:39:43 UTC from IEEE Xplore. Restrictions apply.

=== Page 4 ===
systems, though it is computationally expensive compared to
other strategies.
C. Embedding and Vector Database
We use the BAAI/bge-large-en-v1.5 sentence embedding
model to generate 768-dimensional dense vectors for all
text elements, including chunks, queries, and full documents.
These embeddings, which capture semantic meaning, are
stored in Faiss—an efficient vector database optimized for
large-scale similarity search. Embeddings are L2-normalized,
enabling cosine similarity retrieval via fast k-nearest-neighbor
search.Thissetupscaleswelltochunkingstrategiesthatyield
numerous segments, such as sentence-based approaches.
D. Query Retrieval and Evaluation
Each query in the QA dataset is embedded using the same
model and used to retrieve the top-k (k = 5) most similar
chunksfromFaiss.Toimproveprecision,wediscardretrieved
chunks with cosine similarity below a relevance threshold of
0.70. Evaluation is based on how well the retrieved chunks
align semantically with the ground-truth reference spans.
Retrieved chunks are compared to ground-truth answers.
If the ground truth maps to a specific chunk or span, that
chunk(orsetofchunks)isconsideredrelevant.Thisallowsfor
multi-spananswerstobematchedacrossmultiplechunks.We
thencomputestandardinformationretrievalmetricstoevaluate
performance against varies metrics of chunking elaborated in
table I.
IV. EXPERIMENTANDRESULTS
A. Experimental Setup
Toevaluatetheperformanceofchunkingstrategiesinrealis-
tic scenarios, we selected five diverse document types varying
in structure, content style, and length, each exhibiting distinct
structural and semantic characteristics. These documents vary
significantly in hierarchical depth, use of visual elements,
contentstyle,andlength,enablingacomprehensiveevaluation
of chunking performance.
• Type-1: Research Report
A 13-page thematic report characterized by multi-level
hierarchical formatting (chapters, sections, sub-sections)
and embedded visual elements such as charts and tables.
The content is predominantly paragraph-based with lim-
ited structured points, making it suitable for analyzing
how chunking handles rich textual narratives.
• Type-2: Training Manual
A 154-page instructional guide organized into single-
layer chapters and numbered modules. It contains a
mix of diagrams, exercises, and comparison tables, with
content dominated by structured points and bullet lists.
This format challenges chunking strategies in handling
repetitive and modular content.
• Type-3: Technical User Guide
A 26-page document with a flat structure, primarily con-
taining top-level section titles and appliance part illustra-
tions.Itscontentconsistslargelyofshort,single-sentence
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

instructions—frequently bulleted or numbered—making
it ideal for evaluating sentence-based or fine-grained
chunking approaches.
• Type-4: Opinion Essay
A brief 3-page academic-style essay with minimal head-
ings and virtually no visual aids. The content is purely
narrative, consisting of long paragraphs and academic
citations. This document helps assess chunking perfor-
manceoncompact,semanticallydensetextwithoutvisual
or structural cues.
• Type-5: Strategic Technical Report
A41-pagewhitepaperfeaturingnumberedheadings,sub-
headings, and bulleted subtopics, along with extensive
visualelementssuchascharts,modeloutputs,andappen-
dices. Its content blends paragraph-style narrative with
occasional structured lists, offering a hybrid layout for
testing chunking adaptability.
We applied all four chunking strategies—fixed-size,
sentence-based, recursive and semantic—using two size set-
tings (250-token chunks with 50-token overlap; 500-token
chunks with 100-token overlap). The size parameter directly
governs fixed-size chunking and serves as an upper bound
for recursive and semantic methods, while sentence-based
chunking uses it as a length cap. Every chunk is embedded
with our chosen model and indexed in FAISS. For each
document’s set of QA pairs, we issue each question as an
embedding query, retrieve the top-5 chunks (cosine similarity
≥ 0.70), and label any below threshold as misses. We then
compute Precision, Recall, MRR and NDCG for each query,
average those metrics per document, and finally compare the
resulting scores across the five document types.
B. Results and Analysis
TABLEII
EVALUATIONMETRICSFORDOCUMENTTYPEI(RESEARCHREPORT).
Metric Fixed Sentence Recursive Semantic
250 500 250 500 250 500 250 500
#Chunks 39 20 146 84 60 55 43 23
ss2fd 0.83 0.84 0.80 0.82 0.83 0.82 0.83 0.85
SRGT 0.71 0.69 0.70 0.70 0.70 0.70 0.70 0.69
MRR 0.75 0.80 0.85 0.83 0.90 0.90 0.90 1.00
Precision 0.48 0.50 0.63 0.60 0.69 0.67 0.52 0.45
Recall 0.43 0.45 0.68 0.72 0.80 0.83 0.65 0.57
NDCG 0.89 0.94 0.86 0.89 0.86 0.92 0.87 0.97
Tokens 1241 2467 562 657 752 1086 1131 1854
Sim. 0.69 0.69 0.69 0.69 0.69 0.69 0.69 0.68
Time(s) 0.04 0.04 0.00 0.00 0.05 0.06 59.1 56.2
Weevaluatedfourchunkingstrategiesacrossfivedocument
types. Each document exhibited a preference for a particular
strategy depending on its structure, length, and content style.
Type-1: Research Report: Semantic chunking achieved
the highest retrieval accuracy (best MRR, NDCG), making
it ideal for precision-focused tasks. Recursive chunking of-
fered the best recall and precision trade-off with significantly
lower computational cost. Sentence-based was precise but
fragmented answers, while fixed-size was the least effective.
ed on January 29,2026 at 15:39:43 UTC from IEEE Xplore. Restrictions apply.

=== Page 5 ===
TABLEIII
EVALUATIONMETRICSFORDOCUMENTTYPEII(TRAININGMANUAL).
Metric Fixed Sentence Recursive Semantic
250 500 250 500 250 500 250 500
#Chunks 309 155 1227 627 424 364 297 149
ss2fd 0.78 0.78 0.75 0.76 0.79 0.79 0.77 0.77
SRGT 0.69 0.68 0.68 0.68 0.68 0.68 0.68 0.67
MRR 0.68 0.65 0.56 0.56 0.58 0.60 0.72 0.67
Precision 0.40 0.31 0.46 0.40 0.42 0.42 0.48 0.52
Recall 0.51 0.40 0.38 0.42 0.48 0.50 0.52 0.40
NDCG 0.90 0.98 0.91 0.92 0.91 0.92 0.93 0.95
Tokens 1261 2511 275 452 1178 1727 1093 2206
QCS 0.69 0.68 0.70 0.70 0.69 0.69 0.69 0.68
Time(s) 0.48 0.37 0.01 0.01 0.16 0.13 371.4 368.6
TABLEIV
EVALUATIONMETRICSFORDOCUMENTTYPEIII(TECHNICALUSER
GUIDE).
Metric Fixed Sentence Recursive Semantic
250 500 250 500 250 500 250 500
#Chunks 42 21 182 82 62 56 41 25
ss2fd 0.85 0.85 0.82 0.83 0.82 0.82 0.84 0.84
SRGT 0.69 0.68 0.72 0.70 0.68 0.68 0.66 0.66
MRR 0.90 0.83 0.78 0.78 0.71 0.77 0.46 0.65
Precision 0.58 0.50 0.66 0.66 0.62 0.67 0.25 0.50
Recall 0.33 0.27 0.55 0.52 0.52 0.42 0.24 0.32
NDCG 0.95 0.95 0.88 0.96 0.88 0.89 0.82 0.91
Tokens 1249 2495 225 479 824 958 1103 2053
QCS 0.66 0.65 0.70 0.68 0.67 0.67 0.65 0.64
Time(s) 0.06 0.05 0.00 0.00 0.07 0.01 69.78 70.14
Overall, semantic is best for accuracy, recursive for efficiency
and coverage.
Type-2: Training Manual: Semantic chunking offers the
highestretrievalaccuracybutistheslowest.Recursivechunk-
ingstrikesagoodbalancebetweenaccuracyandspeed.Fixed-
sizeis reliableandfast, whilesentence-basedis mostefficient
in token usage but less accurate. The ideal strategy depends
on the desired trade-off between effectiveness and efficiency.
Type-3: Technical User Guide: Sentence-based chunk-
ing emerges as the most effective strategy for this docu-
ment—offering the best trade-off between precision, recall,
token efficiency, and processing time. Fixed-size performs
well in MRR and NDCG but suffers from low recall and
high token cost. Recursive is balanced but slightly behind in
ranking metrics. Semantic chunking, despite high ss2fd and
NDCG, underperforms due to low precision/recall and heavy
computational cost.
Type-4: Opinion Essay: Semantic chunking slightly out-
performed others at finer granularity by preserving argu-
mentcoherence.However,fixed-sizechunkswithlargerlimits
performed nearly as well. Sentence-based splitting retrieved
relevant snippets but often lacked broader context.
Type-5: Strategic Technical Report: Semantic chunking
yielded the best retrieval accuracy, with top recall, MRR, and
precision, making it ideal for context-rich content—though
at high computational and token costs. Recursive chunking
provided a good balance of accuracy and efficiency. Fixed-
size was competitive but token-heavy, while sentence-based
was fastest but less effective for complex answers. Overall,
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

TABLEV
EVALUATIONMETRICSFORDOCUMENTTYPEIV(OPINIONESSAY).
Metric Fixed Sentence Recursive Semantic
250 500 250 500 250 500 250 500
#Chunks 16 8 55 36 13 9 18 11
ss2fd 0.94 0.94 0.90 0.92 0.91 0.90 0.93 0.92
SRGT 0.68 0.68 0.68 0.68 0.67 0.67 0.68 0.68
MRR 0.55 0.70 0.40 0.48 0.52 0.70 0.57 0.65
Precision 0.34 0.36 0.24 0.30 0.16 0.21 0.34 0.32
Recall 0.44 0.52 0.45 0.45 0.40 0.50 0.52 0.52
NDCG 0.89 0.89 0.94 0.97 0.88 0.89 0.91 0.89
Tokens 1226 2380 359 441 1502 2306 978 2055
QCS 0.73 0.73 0.73 0.73 0.71 0.71 0.73 0.72
Time(s) 0.02 0.02 0.00 0.00 0.04 0.06 16.95 17.43
TABLEVI
EVALUATIONMETRICSFORDOCUMENTTYPEV(STRATEGICTECHNICAL
REPORT).
Metric Fixed Sentence Recursive Semantic
250 500 250 500 250 500 250 500
#Chunks 80 40 221 130 96 88 73 40
ss2fd 0.88 0.88 0.83 0.86 0.90 0.90 0.88 0.88
SRGT 0.68 0.68 0.71 0.70 0.68 0.69 0.70 0.68
MRR 0.73 0.78 0.58 0.77 0.65 0.65 0.73 0.90
Precision 0.42 0.40 0.40 0.44 0.61 0.59 0.62 0.70
Recall 0.50 0.58 0.36 0.77 0.77 0.77 0.64 1.00
NDCG 0.93 0.96 0.91 0.94 0.91 0.91 0.91 0.83
Tokens 1261 2511 371 503 972 1088 984 2054
QCS 0.70 0.70 0.71 0.71 0.71 0.71 0.71 0.70
Time(s) 0.08 0.10 0.00 0.00 0.07 0.01 88.94 85.54
semanticchunkingexcelsinaccuracy,recursiveinpracticality.
C. Comparative Insights
Our analysis highlights trade-offs between retrieval perfor-
mance, efficiency, and implementation complexity. Semantic
chunking often yielded the best accuracy but incurred high
computational cost, making simpler methods more practical
in some settings.
Retrieval Effectiveness: Semantic chunking excelled on
long or dense documents (Types 1, 2, and 5) by preserving
context through coherent grouping. It improved recall and
MRR but was less effective on short or structured content
(e.g., Type 3), where over-merging hurt precision.
Efficiency and Cost: Semantic chunking required signifi-
cantly more preprocessing time (up to hundreds of seconds)
and had higher retrieval token costs. In contrast, fixed-size,
sentence-based,andrecursivestrategiesexecutedquickly,with
sentence-based minimizing token cost but increasing index
size.
Scalability and Simplicity: Fixed-size and sentence-based
approaches are fast, easy to implement, and work well for
structured or short documents (Types 3 and 4). Recursive
chunking offers a good balance of structure-awareness and
performance, especially in longer documents (e.g., Type 2).
Strategy by Document Type: Effectiveness varies by doc-
ument type:
• Narrative/complex(Types1,5):Semanticchunkingper-
forms best.
• Modular/structured (Type 2): Recursive and semantic
chunking are effective.
ed on January 29,2026 at 15:39:43 UTC from IEEE Xplore. Restrictions apply.

=== Page 6 ===
• Bullet-style/concise (Type 3): Fixed-size or sentence-
based preferred.
• Shortessays(Type4):Coarsefixedorsemanticchunking
suffice.
D. Limitations
While this study provides a comprehensive comparison
of chunking strategies, there are several limitations. First,
the chunking strategies were evaluated using a limited set
of document types and sizes, which may not fully capture
performanceinhighlydiverseormultilingualcorpora.Second,
allexperimentsreliedonasingleembeddingmodel(Sentence-
BERT), which could bias retrieval outcomes toward strategies
better aligned with that model’s representation capabilities.
Third, semantic chunking relied on clustering algorithms that
may be sensitive to hyperparameters; a different similarity
threshold or clustering approach might yield different out-
comes. Finally, the chunking strategies were tested in isola-
tion—hybridapproachesoradaptivemechanismscouldfurther
improve performance but were not explored here.
V. CONCLUSIONANDFUTUREWORK
A. Conclusion
Thisstudypresentsacomparativeanalysisoffourchunking
strategies—fixed-size with overlap, sentence-based, recursive,
and semantic chunking—for retrieval-augmented NLP tasks.
Evaluated across diverse document types, the results show
thatsemanticchunkinggenerallyachievesthehighestretrieval
accuracy (MRR, Precision, Recall), while fixed-size and re-
cursive chunking offer a strong balance of performance and
computational efficiency. Sentence-based chunking, although
efficient in terms of token usage, often struggles with context
preservation, impacting retrieval accuracy.
A key insight is that no single strategy consistently out-
performs the others. Chunking effectiveness varies with docu-
ment type, structure, and content density. Semantic chunking
is ideal for high-accuracy, offline indexing, while fixed-size
and recursive strategies are more suitable for scalable, real-
time applications. These findings highlight the importance of
adaptingchunkingmethodsbasedondocumentcharacteristics
rather than applying a one-size-fits-all approach.
From a practical standpoint, the study offers actionable
recommendations for developers and practitioners building
retrieval pipelines, especially for use cases like enterprise
search, RAG systems, and customer support.
B. Future Work
Future research can extend this work by:
• Exploring hybrid chunking strategies (e.g., combining
sentence and semantic methods).
• Including more domain-specific document types (e.g.,
legal, biomedical, or customer service logs).
• Studying the impact of different embedding models on
chunking performance.
• Building adaptive pipelines that dynamically select
chunking methods based on document features.
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

Suchdirectionswouldenhancegeneralizabilityandautoma-
tion in retrieval-based NLP systems.
ACKNOWLEDGMENT
We would like to express our sincere gratitude to Pavan
M Laxmeshwar, heading Data Science team at Bosch Global
Software Technologies (BGSW) and Shankar Datta M S H,
Director at Bosch Global Software Technologies (BGSW) for
their invaluable support and believing in us and in providing
theinsightfulreviewforthisresearch.Withouttheirgenerosity
and commitment to advancing scientific research, this work
would not have been possible.
REFERENCES
[1] Guu,Kelvin,etal.”Retrievalaugmentedlanguagemodelpre-training.”
Internationalconferenceonmachinelearning.PMLR,2020.
[2] P. Lewis et al., “Retrieval-Augmented Generation for Knowledge-
Intensive NLP Tasks,” in Advances in Neural Information Processing
Systems(NeurIPS),2020.
[3] M.Joshi,E.Choi,D.Weld,andL.Zettlemoyer,“SpanBERT:Improving
Pre-trainingbyRepresentingandPredictingSpans,”inProc.ACL,2020.
[4] K.Lee,M.-W.Chang,andK.Toutanova,“LatentRetrievalforWeakly
SupervisedOpenDomainQuestionAnswering,”inProc.ACL,2019.
[5] V.Karpukhinetal.,“DensePassageRetrievalforOpen-DomainQues-
tionAnswering,”inProc.EMNLP,2020.
[6] L. Wu et al., “MemPrompt: Memory-assisted Prompt Engineering for
Retrieval-Augmented Generation,” arXiv preprint arXiv:2205.12689,
2022.
[7] A. Paranjape et al., “Information-Theoretic Chunking for Scientific
Texts,”inFindingsofEMNLP,2021.
[8] S. Jaiswal, G. L. Narasimha, D. Prabhu, et al. Improving Model
Performance and Explainability of Attention-Based CNN Models on
Health Image Datasets Using Grad-CAM. Advanced Computing and
Communications:ResponsibleAI,2025.
[9] S.Jaiswal,G.L.Narasimha,K.Sathiyanarayanan,A.K.Chebrolu.Deep
Learning for NMR Spectra: CNN Classification Using Autoencoder-
GeneratedLatentFeatures.20244thInternationalConferenceonArti-
ficialIntelligenceandSignalProcessing,2024.
[10] S. Jaiswal, S. Sekhar, S. Chakraborty. Responsible AI in Action:
Enhancing Fairness in Computer Vision through Model Improvement
Strategies. 2024 3rd International Conference on Artificial Intelligence
forInternetofThings,2024.
[11] S.Jaiswal,K.C.Gollapudi,R.Susma.ComprehensiveFrameworkfor
Robustness Evaluation on Numeric Data Classification. 2024 15th In-
ternationalConferenceonComputingCommunicationandNetworking,
2024.
[12] N. Thakur, N. Reimers, A. Suni, and I. Gurevych, “BEIR: A Hetero-
geneous Benchmark for Zero-shot Evaluation of Information Retrieval
Models,”inNeurIPS,2021.
[13] I.Beltagy,M.Peters,andA.Cohan,“Longformer:TheLong-Document
Transformer,”arXivpreprintarXiv:2004.05150,2020.
[14] M. Zaheer et al., “BigBird: Transformers for Longer Sequences,” in
NeurIPS,2020.
[15] G. Izacard and E. Grave, “Leveraging Passage Retrieval with Gener-
ative Models for Open Domain Question Answering,” arXiv preprint
arXiv:2007.01282,2020.
[16] O.KhattabandM.Zaharia,“ColBERT:EfficientandEffectivePassage
SearchviaContextualizedLateInteractionoverBERT,”inProc.SIGIR,
2020.
[17] N. Reimers and I. Gurevych, “Sentence-BERT: Sentence Embeddings
using Siamese BERT-Networks,” in Proc. EMNLP-IJCNLP, Hong
Kong, China, Nov. 2019, pp. 3982–3992. [Online]. Available: https:
//aclanthology.org/D19-1410/
[18] M.Douze,A.Guzhva,C.Deng,J.Johnson,G.Szilvasy,P.-E.Mazare´,
M. Lomeli, L. Hosseini, and H. Je´gou, “The Faiss Library,” arXiv
preprintarXiv:2401.08281,2025.[Online].Available:https://arxiv.org/
abs/2401.08281
ed on January 29,2026 at 15:39:43 UTC from IEEE Xplore. Restrictions apply.

Paper:Enhancing_LLM_Function_Calling_with_Structured_Outputs.pdf
=== Page 1 ===
2025 2nd International Generative AI and Com
Enhancing LLM Function
Outp
Kevin Se´journe´
Cloud Temple
Paris, France
kevin.sejourne@cloud-temple.com
Abstract—Large Language Models (LLMs) are increasingly
used for function calling, typically by prompting the model to
generate a specific format. This paper proposes an alternative
approach leveraging structured outputs to enforce the desired
format.Thismethodoffersseveraladvantages,includingahigher
rate of format compliance and the ability to incorporate more
complexelementsthansimplefunctioncalls,suchasenvironment
variables or preliminary remote connections. Furthermore, in
texteditingtasks,generativegrammars(e.g.,viaxgrammar)can
be employed to produce syntactically valid code directly. We
present experimental results comparing our Constrained Gener-
ation (CG) approach (using structured outputs with xgrammar)
againstthetraditionalPost-Parsing(PP)methodontheHermes-
Function-Calling-v1 dataset. The comparison is performed on
tasks requiring valid JSON generation and valid function call
generation, demonstrating the effectiveness of the proposed
structured output methodology.
Index Terms—Large Language Models, Function Calling,
Structured Output, Constrained Generation, xgrammar, JSON
Schema
I. INTRODUCTION
Large Language Models (LLMs) have demonstrated re-
markable capabilities in understanding and generating human
language,leadingtotheiradoption inawidearrayofapplica-
tions. One such application is function calling, where LLMs
are tasked to identify when to call external tools or APIs and
to generate the necessary parameters in a structured format,
typically JSON. The prevalent approach relies on instructing
theLLMviasystempromptstoproduceoutputsadheringtoa
specifiedschema.Whileofteneffective,thismethodcansuffer
from inconsistencies, where the LLM fails to strictly follow
the requested format, leading to parsing errors and unreliable
behavior in downstream applications.
Thispaperintroducesanalternativemethodologythatlever-
ages structured output mechanisms to enhance the reliability
and capabilities of LLM-based function calling. Instead of
merely suggesting a format through prompting, our approach
enforcesthegenerationofsyntacticallycorrectstructureddata.
This offers two key advantages:
• Improved Format Adherence:Byconstrainingthegen-
eration process, we increase the likelihood of obtaining
outputs that strictly conform to the desired schema.
• Enhanced Capabilities:Structuredoutputsallowforthe
representation of more complex interactions beyond sim-
ple function calls. This includes specifying environment
979-8-3315-9406-0/25/$31.00 ©2025 IEEE
61023211.5202.89176MLCAG/9011.01
:IOD
|
EEEI
5202©
00.13$/52/0-6049-5133-8-979
|
)MLCAG(
ecnerefnoC
gnilledoM
egaugnaL
lanoitatupmoC
dna
IA
evitareneG
lanoitanretnI
dn2
5202
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

mputational Language Modelling Conference
n Calling with Structured
puts
Alexandru Lata
Cloud Temple
Paris, France
alexandru.lata@cloud-temple.com
variables, managing prerequisite remote connections, or
defining sequences of actions.
We propose a method called Constrained Generation (CG),
whichutilizesxgrammar[10]forenforcingstructuredoutputs.
Toevaluateitsefficacy,wecompareCGagainstthetraditional
Post-Parsing (PP) approach, where the LLM generates text
that is subsequently parsed into a structured format. Our
experiments are conducted on the Hermes-Function-Calling-
v1 dataset [1], focusing on two key aspects: the generation of
valid JSON objects and the generation of valid function calls.
The results demonstrate the potential of structured outputs to
build more robust and versatile function calling systems.
Theremainderofthispaperisorganizedasfollows:Section
II discusses related work in function calling and structured
LLM outputs. Section III details our proposed Constrained
Generation approach. Section IV presents the experimental
setup and results. Finally, Section V concludes the paper and
outlines future work.
II. RELATEDWORK
The ability of Large Language Models (LLMs) to interact
with external tools and APIs through function calling has sig-
nificantlyexpandedtheirutility.Initialmethodspredominantly
reliedonsophisticatedpromptengineeringtocoaxLLMsinto
generatingoutputsthatcouldbeparsedintofunctioncalls[2].
Subsequent research explored fine-tuning LLMs specifically
for tool invocation, aiming to improve their proficiency in
selectingappropriatefunctionsandformulatingcorrectparam-
eters [3]. However, ensuring the generation of syntactically
valid and semantically accurate structured outputs, typically
JSON, remains a persistent challenge.
The broader field of structured output generation from
LLMs has seen considerable progress. Researchers have in-
vestigated various techniques to compel LLMs to adhere to
predefined schemas. Liu et al. [4] surveyed industry profes-
sionals, highlighting the critical need for structured outputs
and identifying numerous use cases. Benchmarks like SoEval,
introduced by Liu et al. [5], have been developed to assess
LLMs’ capabilities in producing various structured formats.
Li et al. [6] proposed a two-step approach, separating content
generationfromstructuring,toimproveinformationextraction
tasks.
ed on January 30,2026 at 11:43:50 UTC from IEEE Xplore. Restrictions apply.
171

=== Page 2 ===
2025 2nd International Generative AI and Com
More direct methods for enforcing structure include
grammar-based approaches. Technologies like xgrammar, as
discussed in Geng et al. [7] within their JSONSchemaBench
framework, and automata-based constraints as proposed by
Koo et al. [8], aim to guide the LLM’s generation process
token by token. These methods ensure syntactical correct-
ness by design, mitigating issues common in post-parsing
approaches. However, as Zhang et al. [9] caution, output
constraints themselves can introduce new attack surfaces if
not carefully managed.
Our work is situated at the intersection of these research
streams. We specifically apply grammar-constrained genera-
tion,usingxgrammar,tothetaskoffunctioncalling.Bydoing
so, we aim to harness the benefits of guaranteed syntactical
validity for creating more reliable and robust function calling
mechanisms.
III. PROPOSEDAPPROACH:CONSTRAINEDGENERATION
(CG)
Ourproposedapproach,ConstrainedGeneration(CG),aims
to improve the reliability of function calling by LLMs by
directly enforcing a structured output format during the gen-
eration process. This contrasts with the common Post-Parsing
(PP) method, where the LLM generates relatively free-form
text that is then parsed and validated against a target schema.
ThecoreideaofCGistouseaformalgrammar,specifically
xgrammar, to guide the LLM’s token selection at each step.
This ensures that the generated output is syntactically valid
according to the predefined structure (e.g., a JSON schema
representing the function’s parameters). Concretely, this con-
strained decoding process relies on transforming the JSON
grammar(orJSONSchema)intoagenerativeautomaton.This
automatonisthenusedateachstepoftokengenerationbythe
LLM.Byanalyzingthecurrentstateofthegenerationandthe
possible transitions of the automaton, a mask is applied to the
model’s output vocabulary. This mask only retains tokens that
allow compliance with the grammar, thus ensuring that the
generated sequence will be syntactically valid.
The process involves the following key steps:
1) SchemaDefinition:Foreachfunctionthatcanbecalled,
a corresponding JSON schema is defined. This schema
precisely describes the expected parameters, their types,
and any constraints (e.g., required fields, enumerated
values).
2) GrammarCompilation:TheJSONschemaiscompiled
into an xgrammar. This grammar dictates the allowed
sequences of tokens that the LLM can generate.
3) Constrained Decoding: During inference, the LLM’s
outputisconstrainedbythecompiledxgrammar.Ateach
generation step, only tokens that conform to the gram-
mar are considered, effectively preventing the model
from producing syntactically incorrect outputs.
This method inherently guarantees that the output will be
parsable and adhere to the specified structure, reducing the
need for extensive error handling and retries often associated
with the PP approach. Furthermore, it opens possibilities for
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

mputational Language Modelling Conference
morecomplexinteractions,asthegrammarcandefinenotjust
functionparametersbutalsocontrolflow,environmentvariable
settings, or sequences of operations.
Moreover, for models that benefit from an explicit reason-
ing step (or ”chain-of-thought”), our constrained generation
approach remains flexible. It is entirely possible to integrate
dedicated fields for this reasoning directly into the JSON
schema to be produced. For example, a think attribute can
be defined as the first field of the JSON object, allowing
the model to record its ”reasoning” there before generating
the final data fields. Similarly, fields like explanation or
caveat can be included for the LLM to explain its choices
or highlight points of attention. Since LLMs use their previ-
ously generated tokens as context for subsequent generation,
including such fields in the constrained JSON structure does
not disrupt the validity of the final JSON nor hinder the
model’s ability to produce correct results, while offering a
trace of its decision-making process. Figure 1 illustrates an
APIcallexamplefortheCGmethod,wheretheJSONschema
in response_format has been augmented with a think
fieldfortheQwen/Qwen3-30B-A3B-FP8model(matching
the model in the figure’s example).
IV. EXPERIMENTS
ToevaluatetheeffectivenessofourConstrainedGeneration
(CG) approach, we conducted a series of experiments com-
paring it against the traditional Post-Parsing (PP) method. We
chose xgrammar for these experiments because it is the best
way for CG using vLLM [11], the high-performance open-
source LLM-Engine.
A. Dataset
We utilized the Hermes-Function-Calling-v1 dataset [1].
This dataset is specifically designed for evaluating function
calling capabilities in LLMs and contains various scenarios
requiring the generation of structured outputs. We focused on
two main subsets of this corpus:
• JSON Generation Tasks: Instances where the primary
goal is to produce a valid JSON object according to a
given schema or description.
• Function Call Generation Tasks: Instances that require
identifying the correct function to call from a set of
available tools and generating the appropriate parameters
in JSON format.
B. Experimental Setup
Our experiments compare the traditional Post-Parsing
(PP) baseline against our Constrained Generation (CG) ap-
proach using the Llama-3.3-70B and Qwen3-30B mod-
els,amongothers.Foralltests,themaximumtokengeneration
wassetto16384,withtemperaturevariedbetween0.0and0.8.
We focused on two main tasks from the Hermes-Function-
Calling-v1 dataset:
1)JSONGeneration:Thistaskassessedthemodels’ability
to generate a valid JSON object from a natural language
description.
ed on January 30,2026 at 11:43:50 UTC from IEEE Xplore. Restrictions apply.
172

=== Page 3 ===
2025 2nd International Generative AI and Com
{
"model": "Qwen/Qwen3-30B-A3B-FP8",
"messages": [
{
"role": "system",
"content": "You are a helpful assistant that answers
in JSON. Here’s the json schema you must adhere
to:\n<schema>\n{’properties’: {’oreType’:
{’title’: ’Ore Type’, ’type’: ’string’},
’processedVolume’: {’title’: ’Processed Volume’,
’type’: ’number’}, ’processingDate’: {’format’:
’date’, ’title’: ’Processing Date’, ’type’:
’string’}, ’processingFacility’: {’title’:
’Processing Facility’, ’type’: ’string’}},
’required’: [’oreType’, ’processedVolume’,
’processingDate’, ’processingFacility’],
’title’: ’Ore Processing Data’, ’type’:
’object’}\n</schema>\n"
},
{
"role": "user",
"content": "I’m currently overseeing the ore
processing operations at our mining facility
[...] The processed volume needs to be recorded
in tons, and the date should be in the format
YYYY-MM-DD."
}
],
...
"response_format": {
"type": "json_schema",
"json_schema": {
"name":
"hermes_043c9253-bad7-4924-bf5e-9ceb9ac3b055",
"schema": {
"properties": {
"think": {
"type": "string",
"description": "Analyze user input and reason
step by step before generating the final
structured response. Allows the model to
decompose the problem, identify key
information, and plan its response."
},
"oreType": {
"title": "Ore Type",
"type": "string"
},
"processedVolume": {
"title": "Processed Volume",
"type": "number"
},
"processingDate": {
"title": "Processing Date",
"type": "string"
},
"processingFacility": {
"title": "Processing Facility",
"type": "string"
}
},
"required": [
"oreType",
"processedVolume",
"processingDate",
"processingFacility"
],
"title": "Ore Processing Data",
"type": "object"
}
}
}
}
Listing1. ExampleofaCGAPIcallusingaJSONschemaaugmentedwith
a‘think‘fieldfortheQwen/Qwen3-30B-A3B-FP8model.Thisallowsforan
explicitreasoningstepbeforegeneratingthestructuredoutput.
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

mputational Language Modelling Conference
• PP Method: The LLM generated free-form text from
the prompt, which was then parsed to find and validate a
JSON block.
• CG Method: A JSON schema was automatically ex-
tracted and preprocessed from the prompt, then used
with the API’s structured output feature to enforce the
generation format. Due to variability in the dataset’s
schema descriptions, 1884 of 1893 test cases were used.
2) Function Call Generation: This task required the LLM
to select the correct function and generate its parameters.
• PP Method: Tool definitions were supplied to the API’s
native tools parameter, and evaluation relied on the
structured tool_calls output.
• CG Method: Tool definitions were transformed into
a comprehensive JSON schema using a custom script.
This schema, representing a choice of function and its
arguments, was then used to constrain the LLM’s output.
Thisapproachreliesonbothschemaenforcementandthe
LLM’s ability to interpret the prompt, as the underlying
grammar engine supports a subset of JSON Schema,
sometimes requiring more tolerant definitions.
Evaluation and Scope:Theprimarymetricwastherateof
generatingsyntacticallyvalidJSON,withsemanticcorrectness
also evaluated for function calls. We recorded API success,
parsing errors, and duration. Our analysis focused on single-
turn generation, as schema adherence is not directly impacted
bydialoguehistoryinthiscontext.Weplantoreleaseacurated
open-source corpus for these tasks.
C. Results
Our experiments, conducted using the methodologies de-
scribed above, yielded several key observations regarding the
performance of Constrained Generation (CG) compared to
Post-Parsing (PP). We didn’t use models with specific fine-
tuning.
As shown in Table I, the CG method consistently achieves
a lower validation error rate across all temperatures and runs
forbothmodels.Thisdirectlysupportstheprimaryhypothesis
that constrained generation improves format adherence.
Table II shows the error rates for the function call selection
task,whereonlythechoiceofthefunctionnameiscompared.
Figure 1 shows the Levenshtein distance difference between
the two methods and the corpus reference, this metric is used
to estimate the correctness of the function parameters.
A consistent finding across both JSON generation and
function call generation tasks is that the advantage of the
CGmethodoverPPbecomesmorepronouncedunderspecific
conditions:
• Impact of Temperature: Our analysis of the impact of
temperature on generation quality reveals a nuanced pic-
ture. Contrary to a simple hypothesis that higher temper-
atures would disproportionately degrade the performance
of the less constrained PP method, our results show that
both CG and PP maintain a relatively stable level of
semantic accuracy (as measured by Levenshtein distance
ed on January 30,2026 at 11:43:50 UTC from IEEE Xplore. Restrictions apply.
173

=== Page 4 ===
2025 2nd International Generative AI and Com
TABLEI
VALIDATIONERRORRATEBYTEMPERATUREFORTHEJSON
GENERATIONTASK.THISTABLECOMPARESTHEPERCENTAGEOF
SYNTACTICALLYINCORRECTJSONOUTPUTSFORTHECONSTRAINED
GENERATION(CG)ANDPOST-PARSING(PP)METHODSACROSS
MULTIPLERUNSANDTEMPERATURES.
Llama-3.3-70B Qwen3-30B-A3B
Temp CG PP CG PP
0.0
Run1 1.50% 2.47% 2.15% 3.01%
Run2 1.75% 2.47% 2.43% 2.92%
Run3 1.50% 2.55% 2.35% 2.68%
Run4 1.46% 2.75% 2.39% 2.92%
Run5 1.58% 2.67% 2.67% 2.36%
0.2
Run1 1.83% 2.63% 2.39% 3.09%
Run2 1.75% 2.43% 2.71% 3.05%
Run3 1.42% 2.63% 2.39% 2.72%
Run4 1.58% 2.43% 2.43% 3.29%
Run5 1.99% 2.43% 2.51% 2.44%
0.4
Run1 1.75% 2.55% 2.75% 2.88%
Run2 1.87% 2.35% 2.71% 3.13%
Run3 1.67% 2.87% 2.23% 2.88%
Run4 2.03% 2.35% 2.39% 2.84%
Run5 1.99% 2.23% 2.71% 2.64%
0.6
Run1 1.42% 2.19% 2.63% 2.92%
Run2 2.23% 2.39% 2.43% 3.05%
Run3 1.75% 2.67% 2.39% 3.13%
Run4 2.03% 2.67% 2.35% 3.01%
Run5 1.34% 2.67% 2.27% 2.72%
0.8
Run1 1.50% 2.27% 2.23% 2.48%
Run2 1.67% 2.19% 2.23% 2.84%
Run3 1.58% 2.59% 2.27% 3.09%
Run4 1.58% 2.51% 2.27% 3.49%
Run5 1.58% 2.43% 2.59% 2.60%
Fig. 1. Heatmap of Levenshtein distance differences for function call
parameters.
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

mputational Language Modelling Conference
TABLEII
FUNCTIONCALLSELECTIONERRORRATEBYTEMPERATURE.THIS
TABLESHOWSTHEPERCENTAGEOFINSTANCESWHERETHEMODEL
FAILEDTOSELECTTHECORRECTFUNCTIONNAME,COMPARINGTHECG
ANDPPMETHODS.
Llama-3.3-70B Qwen3-30B-A3B
Temp CG PP CG PP
0.0
Run1 2.39% 2.39% 2.31% 1.72%
Run2 2.40% 2.40% 2.35% 2.35%
Run3 2.40% 2.40% 2.05% 2.10%
Run4 2.38% 2.38% 2.34% 2.56%
Run5 2.38% 2.38% 2.13% 1.98%
Total 2.39% 2.39% 2.24% 2.14%
0.2
Run1 2.44% 2.55% 2.28% 2.19%
Run2 2.60% 2.54% 2.03% 2.14%
Run3 2.33% 2.39% 2.24% 2.14%
Run4 2.50% 2.44% 2.51% 2.18%
Run5 2.59% 2.70% 1.93% 1.77%
Total 2.49% 2.52% 2.20% 2.08%
0.4
Run1 2.71% 2.28% 2.02% 1.98%
Run2 2.55% 2.80% 2.40% 2.14%
Run3 2.09% 2.43% 2.19% 2.40%
Run4 2.48% 1.98% 1.97% 2.08%
Run5 2.59% 2.55% 2.23% 1.88%
Total 2.49% 2.41% 2.16% 2.09%
0.6
Run1 2.08% 2.61% 2.28% 2.23%
Run2 2.65% 2.71% 2.18% 1.93%
Run3 2.49% 2.44% 2.08% 2.14%
Run4 1.93% 2.49% 2.07% 1.89%
Run5 2.70% 2.03% 2.34% 1.82%
Total 2.37% 2.46% 2.19% 2.00%
0.8
Run1 2.38% 2.17% 2.08% 2.35%
Run2 2.69% 2.15% 2.20% 2.10%
Run3 1.93% 2.55% 1.93% 2.51%
Run4 2.02% 2.45% 2.46% 2.50%
Run5 2.30% 2.09% 2.39% 1.99%
Total 2.27% 2.28% 2.21% 2.29%
andfunctionselectionerrorrate)acrossthetestedtemper-
ature range (0.0 to 0.8). For instance, as shown in Figure
1,thedifferenceinparametercorrectnessbetweenthetwo
methodsdoesnotshowacleartrendfavoringonemethod
astemperatureincreases.Similarly,TableIIindicatesthat
the function selection error rate remains comparable for
both methods across all temperatures.
• Impact of Model Size: The benefits of CG are notably
amplified when using smaller LLMs (under 14B param-
eters). Smaller models, which may have a less ingrained
understanding of complex formatting rules or a higher
propensity for generating syntactically flawed structures,
show a more significant improvement in valid output
rates when guided by CG compared to PP. The explicit
constraintsofCGappeartocompensateeffectivelyforthe
reduced inherent formatting capabilities of these smaller
models.Conversely,forlarger,morecapablemodelssuch
as Llama-3.3-70B, the difference in success rates
ed on January 30,2026 at 11:43:50 UTC from IEEE Xplore. Restrictions apply.
174

=== Page 5 ===
2025 2nd International Generative AI and Computational Language Modelling Conference
between PP and CG was observed to be narrower, in [7] S. Geng, H. Cooper, M. Moskal, S. Jenkins, J. Berman, N. Ranchin,
the order of 1.5% in favor of CG for some tasks. This R. West, E. Horvitz, and H. Nori, “Generating Structured Outputs
from Language Models: Benchmark and Studies”, arXiv preprint
suggests that while CG still provides an edge, very large
arXiv:2501.10868,2025.
models are inherently more adept at adhering to format [8] T. Koo, F. Liu, and L. He, “Automata-based constraints for language
instructions even in a post-parsing setup. This appears to modeldecoding”,arXivpreprintarXiv:2407.08103,2024.
[9] S. Zhang, J. Zhao, R. Xu, X. Feng, and H. Cui, “Output constraints
be only a small gain, but success rates are initially high,
asattacksurface:Exploitingstructuredgenerationtobypassllmsafety
and this is not the only advantage of CG. mechanisms”,arXivpreprintarXiv:2503.24191,2025.
[10] YixinDongandCharlieF.RuanandYaxingCaiandRuihangLaiand
However, the key advantage of CG lies in its structural
ZiyiXuandYilongZhaoandTianqiChen,“XGrammar:Flexibleand
guarantee for smallers models (¡14B). The risk of generating Efficient Structured Generation Engine for Large Language Models”,
syntactically invalid outputs is larger on smaller models. arXivpreprintarXiv:2411.15100,2024.
[11] Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng
These preliminary findings suggest that Constrained Gen-
and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and
eration is not only effective in ensuring syntactically valid Hao Zhang and Ion Stoica, “Efficient Memory Management for Large
outputs but also demonstrates particular strengths in scenarios Language Model Serving with PagedAttention”, Proceedings of the
ACMSIGOPS29thSymposiumonOperatingSystemsPrinciples,2023.
involvinghighergenerationtemperaturesortheuseofsmaller,
potentially less capable, language models. The ability of CG
to consistently produce parsable JSON, irrespective of these
factors, underscores its potential for enhancing the reliability
of LLM interactions with external tools and systems. Further
analysiswilldelveintospecificerrortypesandthenuancesof
performance across different schemas and function complexi-
ties.
V. CONCLUSION
In this paper, we proposed and evaluated a Constrained
Generation (CG) approach for LLM-based function calling,
leveraging structured output mechanisms like xgrammar. Our
preliminary hypothesis is that this method offers significant
improvements in terms of output reliability and format ad-
herence compared to traditional post-parsing techniques. The
experiments conducted on the Hermes-Function-Calling-v1
dataset are designed to validate these claims.
Future work will involve exploring more complex gram-
mars, investigating the impact on semantic correctness, and
extending the approach to handle dynamic schema generation
andmulti-turnconversationalfunctioncalling.Theintegration
ofstructuredoutputspromisestomakeLLMinteractionswith
external tools more robust and developer-friendly.
REFERENCES
[1] “interstellarninja” and “Teknium”, “Hermes-Function-Calling-Dataset-
V1”, Hugging Face, 2023. [Online]. Available: https://huggingface.co/
datasets/NousResearch/hermes-function-calling-v1
[2] T.Brown,B.Mann,N.Ryder,M.Subbiah,J.D.Kaplan,P.Dhariwal,
A.Neelakantan,P.Shyam,G.Sastry,A.Askell,etal.,“Languagemod-
els are few-shot learners,” Advances in neural information processing
systems,vol.33,pp.1877–1901,2020.
[3] T.Schick,J.Dwivedi-Yu,R.Dess`ı,R.Raileanu,M.Lomeli,L.Zettle-
moyer, N. Cancedda, and T. Scialom, “Toolformer: Language models
that teach themselves to use tools,” arXiv preprint arXiv:2302.04761,
2023.
[4] M.X.Liu,F.Liu,A.J.Fiannaca,T.Koo,L.Dixon,M.Terry,andC.J.
Cai,“‘WeNeedStructuredOutput”:TowardsUser-centeredConstraints
on Large Language Model Output,” in Extended Abstracts of the CHI
ConferenceonHumanFactorsinComputingSystems,2024,pp.1–9.
[5] Y.Liu,D.Li,K.Wang,Z.Xiong,F.Shi,J.Wang,B.Li,andB.Hang,
“Are LLMs good at structured outputs? A benchmark for evaluating
structured output capabilities in LLMs”, Information Processing &
Management,vol.61,no.5,p.103809,2024.
[6] Y.Li,R.Ramprasad,andC.Zhang,“Asimplebuteffectiveapproachto
improve structured language model output for information extraction”,
arXivpreprintarXiv:2402.13364,2024.
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloaded on January 30,2026 at 11:43:50 UTC from IEEE Xplore. Restrictions apply.
175
Powered by TCPDF (www.tcpdf.org)

Paper:Simple_Action_Model_Enabling_LLM_to_Sequential_Function_Calling_Tool_Chain.pdf
=== Page 1 ===
2024 International Conference on Advancement in R
Simple Action Model: Ena
Function Callin
Rajat Sandeep Sen Amalkrish
Department of AI&DS Department o
SJCET Palai SJCET P
Kottayam, Kerala Kottayam, K
rajatsandeepsen2025@ai.sjcetpalai.ac.in amalkrishna2803200
Sharon Prashant Jose Neena Jo
Department of AI&DS Department o
SJCET Palai SJCET P
Kottayam, Kerala Kottayam, K
sharonprashantjose2025@ai.sjcetpalai.ac.in nestofneena@g
Abstract – Today, LLMs are everywhere. It is making human
internet life a lot easier than ever. Every day new sophisticated
models are released. But these models are not good enough
to become personal assistants like the Jarvis from the Sci-fi
movie IronMan. This paper proposes a way to enable any LLM
to execute complex requirements in real-world applications. By
leveragingstate-of-the-artLargeLanguagemodels,wecancreate
a simple action model that can understand the environment
aroundthem.Eventually,thesemodelscanhelporassisthumans
in real-time applications. The Sequential Function Calling Tool
Chain System aims to bridge the gap between human language
understanding and computer programming.
Keywords – Large language model, Action model, OpenAPI
format, and Function Calling Tools
I. INTRODUCTION
Nowadays, a world without AI is impossible to imagine.
New models will come every day, and new architecture will
improve its performance. Multi-modality or a Mixture of
Models is not enough to become a personal assistant. Graph-
based models take up a lot of computation in the case of
complex function calling. Simple Question, ” What if the one
good model with one request is enough to execute multiple
functions sequentially?”.
The integration of OpenAPI schema with LLM can make
a decent action model that can understand the API specs of
that specific application. The large language model will take
prompts and decide what to do. From the programming side,
we’ll parse the information and execute it. A simple action
model is done.
By implementing a custom JSON parser alongside an Ope-
nAPIschema-typesystemitispossibletoutilizeonemodelas
main decision making inside an application. This technology
works faster compared to other solutions but requires a much
more intelligent language model.
This helps to provide stable and intelligent AI assistants in
anyapplicationsthatupholdtheintegrityandstandardsofAPI
979-8-3503-8723-0/24/$31.00 ©2024 IEEE
77639801.4202.95526SIERA/9011.01
:IOD
|
EEEI
4202©
00.13$/42/0-3278-3053-8-979
|
)SIERA(
smetsyS
tnegilletnI
dna
ygrenE
elbaweneR
ni
tnemecnavdA
no
ecnerefnoC
lanoitanretnI
4202
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

Renewable Energy and Intelligent Systems (AREIS)
abling LLM to Sequential
ng Tool Chain
hna M Prithviraj R
of AI&DS Department of AI&DS
Palai SJCET Palai
Kerala Kottayam, Kerala
03@gmail.com prithvirajr0306@gmail.com
oseph Renjith Thomas
of AI&DS Department of AI&DS
Palai SJCET Palai
Kerala Kottayam, Kerala
gmail.com renampatt@gmail.com
ORCID: https://orcid.org/0000-0002-1038-9596
formats.This research paper also leads to the exploration of
MultipleandSequentialFunctioncallingtoolchainthatutilizes
a State-of-the-Art Language model to enhance the security,
time efficiency, and flexibility of building AI applications.
Thepurposeofthispaperistoexploreandimproveexisting
solutionsoffunctioncallingtoolsavailableintheopen-source
software community. This paper also aims to provide a new
approach to prompting the LLMs without fine-tuning.
II. LITERATURESURVEY
These literature reviews provide knowledge about the ex-
isting research done by various scholars and open-source
developers.
LuyuGao,AmanMadaan,ShuyanZhou,UriAlon,Pengfei
Liu, Yiming Yang, Jamie Callan, Graham Neubig, PAL:
Program-aided Language Models, Jan 2023 [1], They had
introduced a new architecture of LLMs wrapper that are
trainedonbothinterpretedprogramminglanguageandnatural
languages. These system are utilized with ability to run code
directly at runtime. Tatsuro Inaba, Hirokazu Kiyomaru, Fei
Cheng, Sadao Kurohash, MultiTool-CoT: GPT-3 Can Use
Multiple External Tools with Chain of Thought Prompting
(2023) [2], They had introduced a new architecture of LLMs
that can use multiple external tools with Chain of Thought
Prompting. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu,
Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,
Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruob-
ing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu,
Maosong Sun, TOOLLLM: FACILITATING LARGE LAN-
GUAGE MODELS TO MASTER 16000+ REAL-WORLD
APIS 2023 [3], They had introduced a new architecture
of LLMs that can master 16000+ real-world APIs. Sehoon
Kim, Suhong Moon, Ryan Tabrizi, Nicholas Lee, Michael W.
Mahoney,KurtKeutzer,AmirGholamiAnLLMCompilerfor
ParallelFunctionCallingJun2024[4],Theyhadintroduceda
ed on January 30,2026 at 11:45:57 UTC from IEEE Xplore. Restrictions apply.

=== Page 2 ===
2024 International Conference on Advancement in Renewable Energy and Intelligent Systems (AREIS)
new architecture of LLMs that can compile parallel function
calling.
III. METHODOLOGY
The approach of the Sequential Function calling tool is us-
ing the popular and newest OpenAPI schema 3.0. By fetching
the schema JSON from the backend, it is converted to a type
parserlikeZod(npm)atbuildtime.Thepromptcapturedfrom
theuserisfedtoLLMwithpropertypeannotationofOpenAPI
schema [7]. Any popular open-source model can be used to
understandandextractobject/JSONdatafromuser’sprompts.
TheextracteddataarethenloadedintoacustomJSONparser
that supports extra keywords from the standard JSON format
[9]. The parser moves the data to OpenAPI client libraries
like OpenAPI-Fetch (npm), which fetches results from the
backend. The result is then passed to the next function in
the chain. The process is repeated until the end of the chain
[8]. But LLM is used once just to write “what to do in this
environment”accordingtousers’promptsandtypedefinition.
Fig.1. BlockDiagramforNormalIn-contextLearning
A. Schema Preparation
Thepreparationoftheschemaatbuildtimeutilizespopular to production gives better results in response and saves a lot
libraries like OpenAPI-TS (npm) and OpenAPI-Zod (npm). of tokens while prompting each time [7].
It’ll generate types schema for use in few-shot learning of
LLM and zod schema for parsing the input and return data
to and from LLM [11]. Feeding the LLM with raw OpenAPI
schema is a waste of tokens and context window, that’s why
it is a crucial step to create a simpler version of the schema.
This process involves collecting open API schema JSON (by
fetching from the application backend server), parsing and
preprocessing, and then structuring it in a way that is useful
for teaching the language model.
The recorded schema was obtained from backend frame-
works that support swagger-ui or other libraries that generate
OpenAPI. Simplified parser schema and type are written to a
coding file (here TypeScript programming language is used)
andsavedintoafolder.Laterthisobtainedschemawasresized
to smaller chunks if the schema is larger than the model’s
contextwindow[5].Thegraph-basedstructureisusedtolabel
and store nodes. This graph data structure will help to query
through large schemas faster in runtime [6].
B. Prompt Preparation Fig.2. BlockDiagramforFinetunedlearning
TheLLMisalwaysinstructedtoreturnaresponseinJSON
C. Server Architecture
tags, making the data easier to extract from a string [4].
Also, the available functions list (or schema from OpenAPI) Fig 3 displays the Architecture of the Action model
is converted to easy, type definitions, which makes it easier where initially, the developer specifies the backend URL and
to understand for the LLMs. But normally open-source LLMs OpenAPI schema URL in the configuration file. It fetches
are limited without examples [3]. They are trained on general the schema from the backend applications and prepares the
knowledge, and that’s why a few shots of in-context learning schema at build time [2]. The server is a standalone backend
are needed to make it understand the environment as it is that listens to end-user prompts. The server takes the user
generally seen and is described in Fig 1. [10]. prompt,typesthedefinitionoftheOpenAPIschema,andsends
Also, the fine-tuning approach modelled in Fig 2, is great it to LLM. The developer also needs to mention the provider
if the type definition and instructions are larger than the andmodelintheconfigurationfile.ThentheLLMreturnswith
context window of the LLM used. Teaching the model with some JSON data wrapped with JSON tags. The data shows
the entire OpenAPI schema and examples before deploying it “what to run”, “where to fetch” and “what are the parameters
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloaded on January 30,2026 at 11:45:57 UTC from IEEE Xplore. Restrictions apply.

=== Page 3 ===
2024 International Conference on Advancement in R
andrequestbody”[9].ThenitparsestheJSONdataandsends
it to the Action engine. The engine takes care of function
calling and chaining. Finally, the result is sent back to the
user in JSON format (or stream the UI if the server is built
with meta-frameworks like Next.js or Nuxt.js).
Fig.3. ServerArchitecturewithActionModel
D. Action Engine Architecture
As shown in Fig 5, The Action engine architecture follows
the same pattern as the build time CLI (CLI that converts the
OpenAPI schema to type definitions) [5]. An action engine
is not an AI system. Currently, it is a simple JSON parser
that can understand the JSON data from LLM. It turns the
normal Large Language Model into a Simple Action Model,
that enables function calling and chaining [6].
Let’s consider a simple application for mathematical calcu-
lations, the user asked ”What is 10 + 2 / 4” and the Action
engine had access to all types of mathematical functions. So
the LLM returns instructions in JSON tag and the Action
enginewillexecutethefunctionintheorderoftheinstructions
[8].Theresultisthensentbacktotheuser.Thiswholeprocess
is described in Fig 4.
Thenewunknownvariableisgeneratedandusedatruntime
[12]. The type structure of return JSON can be customized.
HerethisJSONisenoughtorepresenttheenvironment.JSON
is parsed and validated by the engine and array to move to
the execution stack, and each function is called sequentially,
wheretheresultofthepreviousexecutionisstoredintheheap
at runtime [10].
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

Renewable Energy and Intelligent Systems (AREIS)
Fig.4. SimplifiedMathsproblemdonewithFunctionCalling.
Fig.5. BlockDiagramofActionEngineArchitecture
Twotypesofinstructionscanbeevaluated.Single:parsethe
JSON, execute the function, done. But for multiple sequential
instructions, careful validations and parsing are required. The
first execution of the function requires a complete set of
parameters [4]. Always store the executed data and function
parameters in the heap. If other instructions require data from
apreviousexecution,searchintheinputparamslistinastack
or search heap [7]. If data is not found, return an error (and
let the server handle that part).
To ensure consistency and compatibility, the input data
parameters and output results from each execution are also
validated before passing down to the next instructions [15].
This standardization allows for seamless processing and exe-
cution of function calling, enabling the system to effectively
detect any instances of an error made by the language model
[14].
Thepermissionisthelistoffunctionsthatareallowedtorun
withoutusers’concerns[6].Ifafunctionistrue,thenexecuteit
without asking for permission. If not, then return the current
result stack of execution to the user and wait until the user
permits. This feature is crucial while building highly secure
ed on January 30,2026 at 11:45:57 UTC from IEEE Xplore. Restrictions apply.

=== Page 4 ===
2024 International Conference on Advancement in R
and user data-dependent applications [13]. Users don’t want
any LLM to make decisions and execute some functionalities
on its own without their permission [9].
E. Utilizing Vector Storage
Retrieval Augmented Generation (RAG) is a popular tech-
nique used alongside large language models (LLMs) to
improve its performance by including external knowledge
sources [5]. Mostly here we can utilize it with the help of
vector databases.
OpenAPI schema format supports descriptions and exam-
ples. These descriptions can be stored in a vector database
alongside the metadata of each API route or component
[11]. If the application OpenAPI schema is larger than the
contextwindowcapacityoftheLLM,thenutilizationofvector
databases can still reduce the number of requests to LLMs.
Query the vector database with user prompt and get the most
probable API route or component [8]. Then combine the
results with user prompt to get a response from LLM. This
approach is faster and more efficient than querying the LLMs
with large amounts of OpenAPI schema [12].
RAG-based Action models are much more efficient when it
comes to usage of tokens while querying. The fewer tokens
used, the faster the model responds [7].
Fig.6. DetailedBlockDiagramofMathsproblem
F. Utilizing Runtime Validations
Action models require strict parsing and validations with
OpenAPI schema [10]. LLMs hallucinate sometimes, making
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

Renewable Energy and Intelligent Systems (AREIS)
them untrustworthy to take decisions. That’s why it is impor-
tant to validate the input and output data from fetching the
API.
But what if the LLM can just go around the world wide
web and fetch the schema of the API and validate at runtime
and cache it? [2] This approach is much more futuristic than
it sounds. The user prompts, LLM looks for relevant schemas
from Google search and calls those APIs. While the system
is looking for docs, it can build validators and parsers for the
schemaatruntime.Also,utilizeacachingmechanismtostore
the built files for future use [16].
It’slikeGooglesearch,butinsteadofquerying,it’lldotasks
online by itself [9]. An inevitable future of language models.
G. Utilizing Multiple Calls to LLM
PopularfunctioncallingprojectslikeReActutilizemultiple
callstoLLMtomakesuretogetthebestresultandensurethe
response is moving in the right direction [3]. This approach
is also useful in Action models. If the response from LLM is
not clear or not understandable, then a second call to LLM
can reduce the errors.
This paper was focused on ”how to sequentially function
call with one query to LLM”. But multiple calls to LLM can
be useful in some cases. If return data from the previously
executedfunctionisnotclearorunderstandable,orthedatais
so long that, without intermediate assistance, it may not reach
the end goal as expected, multiple calls are an easy way to
enhance the system [4].
H. Utilizing Program-aided Language Models
Program-aided Language Models (PALM) are the new
architecture of LLMs that are trained on both interpreted
programming language and natural languages. These models
are utilized with the ability to run code directly in the system
at runtime [8]. This approach is not preferred due to security
vulnerabilities like code injection attacks [13]. The ‘eval()‘
function allows executing arbitrary code passed as a string.
So LLM might execute malicious codes [7].
Extraruntimelintingfeaturescanavoidthesevulnerabilities
[14]. PAL gives the LLM model extra access to run the code
directlyintotheenvironment[9].Thisapproachissuitablefor
building OpenAPI to function calling toolchain system [6].
IV. IMPLEMENTATIONANDRESULT
A. Implementation
From a developer perspective, the implementation of the
Sequential Function Calling Tool Chain System involves the
following steps:
-PreparationoftheOpenAPIschema:Thedeveloperfetches
the OpenAPI schema from the backend application and pre-
pares the schema at build time. The schema is then converted
to type definitions and saved in a folder.
- Preparation of the prompt template: The developer pre-
paresthepromptfortheLLM,ensuringthattheLLMresponse
ed on January 30,2026 at 11:45:57 UTC from IEEE Xplore. Restrictions apply.

=== Page 5 ===
2024 International Conference on Advancement in R
is in the correct format. Prompt template includes the neces-
sary type definitions, instructions, and examples as few-shot
learning.
- Server architecture: The developer must turn this into a
standalone backend server that listens to end users prompts.
Theservertakestheuserprompt,combineitintotemplateand
sends to LLM. The LLM response is then parsed and sent to
the Action engine. Action engine in the next step requires a
fetching client library like axios (npm) or tRPC (npm). The
resultisthensentbacktotheuserinJSONformatorstreamed
asGenerativeUI(useNext.jsorNuxt.jsforserversiderending
components).
- Action engine architecture: The Action engine is a JSON
parser that understands the JSON data from the LLM, use
kor (pip) or ai-SDK (npm). It turns the LLM into a Simple
Action Model that enables function calling and chaining. The
engine executes the functions in the order of the instructions
and sends the result back to the user. For implementing the
engine, use AI-SDK from Vercel (npm) or popular AI library
LangChain (npm, pip) as base ai agent.
B. Performance Evaluation
Popular LLM benchmarks tools have 4 common metrics
to consider in turning into Action Models. They are MMLU
(Massive Multitask Language Understanding), Context win-
dow size and Median (inference speed or token/second).
Better the MMLU, better the model can understand the
environment. More context window, gives more access to
openAPI schema. Faster the inference speed, faster the model
can respond with JSON. Context window and Median are
inversely proportional. These results of this evaluation are
gathered in Table 1.
TABLEI
PERFORMANCEMETRICSCOMPARISONFORLARGE
LANGUAGEMODELS
Context Median
Model Rank MMLU
Window (Tokens/s)
CLAUDE-3-OPUS 100 200K 0.868 27.5
GPT-4 90 8K 0.864 17.5
LLAMA3-70B 88 8K 0.82 307.3
CLAUDE-3-SONNET 85 200K 0.79 59.4
MIXTRAL-8X22B 83 65K 0.77752 49.9
CLAUDE-3-HAIKU 78 200K 0.752 85.4
CLAUDE-INSTANT 65 100K 0.734 84.2
MIXTRAL-8X7B 68 16K 0.706 117.1
GPT-3.5-TURBO 67 16K 0.7 58.5
GPT-35-TURBO 67 16K 0.7 54
LLAMA2-70B-4096 56 4K 0.689 251.5
LLAMA-3-8B 58 8K 0.684 121.4
LLAMA3-8B-8192 58 8K 0.684 920.8
MISTRAL-7B 40 16K 0.625 102.9
LLAMA-2-13B 37 4K 0.536 115.3
LLAMA-2-7B 27 4K 0.458 204.7
The evaluation of these models are provided by
ArtificialAnalysis/LLM-Performance-Leaderboard from hug-
ging face. The evaluation is based on the following met-
rics: API ID, Rank (Normalized avg), CONTEXT WINDOW,
MMLU, MEDIAN (Tokens/s). The evaluation is based on the
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

Renewable Energy and Intelligent Systems (AREIS)
performanceofthemodelsintermsoftheirabilitytogenerate
human-like text, understand and respond to user queries, and
provide accurate and relevant information. The models are
ranked based on their performance across these metrics, with
higher scores indicating better performance
However,itisessentialtonotethatincreasingtheparameter
count does not always guarantee better performance. The
model’s architecture, training data, and fine-tuning processes
also play crucial roles in determining its overall capabili-
ties. Therefore, it is essential to consider these factors in
conjunction with the parameter count to assess the model’s
performance accurately. From a perspective of Enterprise, the
model has to be open source or they can’t fine tune the model
withtheirowncustomdataset.Finetuningontheirspecialized
environment can help to gain more context window through
each instance of request.
C. Comparison with State-of-the-Art Models
TABLEII
TECHNICALCOMPARISONBETWEENEXISTINGSOLUTIONS
ANDOURTECHNIQUE
Library TechniqueResult FunctionCalling
LangChainStructuralOutput CallSingleFunction Promptbased
OpenAIToolAPI ReturnsListofFunctions Toolbased
Vercel’sAISDK Calllistoffunctions Toolbased
Cloudflare’sAIUtils Calllistoffunctions Toolbased
OurTechnique Callnestedlistoffunctions Bothpossible
Library JSONreconstruction TokenUsage
LangChainStructuralOutput Useinstructorlib High
OpenAIToolAPI Unknown Unknown
Vercel’sAISDK Retryonfailure Minimal
Cloudflare’sAIUtils None Minimal
OurTechnique None High
Table2showsaTechnicalComparisonbetweentheExisting
Solutions present and Our System by comparing their Result,
Function Calling criteria, JSON reconstruction ability and
Token usage. The system was also compared with other
state-of-the-art models, such as ReAct and Chain-of-Thought
Prompting (COT). The system outperformed these models in
terms of speed, efficiency, and accuracy. The system was also
found to be more user-friendly and easier to use than other
models. The system’s ability to execute multiple functions
sequentially with a single query to the LLM makes it a
valuable tool as a wrapper around consumer applications.
D. Findings
FromFig.6,whichillustratestheusefulnessoftheProgram-
aidedLanguageModels(PALM)combinedwiththeOpenAPI
ed on January 30,2026 at 11:45:57 UTC from IEEE Xplore. Restrictions apply.

=== Page 6 ===
2024 International Conference on Advancement in R
Action Engine. Also introducing external vector knowledge
base into this architecture brings more efficiency and speed to
the system. The following are the main findings:
1) High speed response: Vector knowledge base of Ope-
nAPItypedefinitionreducetimetosearchingandfaster
response from system
2) Less usage of token: Splitting the OpenAPI type defi-
nition into chunks and vector embedding, brings faster
response with lesser prompting token size.
3) Less complexity: The system is less complicated com-
pared to Chain-of-Thought Prompting (COT) or Tree of
Thoughts(TOT)architecture.Thissystemismuchmore
simpler and efficient, because of single query to LLM.
4) Reduction on computation: The system is much more
efficient in terms of computation. The system is much
more faster and efficient than other systems.
5) Practical Deployment: We have developed a practical
and easily understandable applications build on top of
this system. An banking app that navigates through the
API routes and help user navigate applications with
prompts.
V. CONCLUSION
The rapid development of sophisticated models has inten-
sified competition in two primary areas: models designed for
comprehensive knowledge and question answering, and those
optimized for deployment on small devices. While consumers
leveragingpowerfulcloudservicesmayprioritizeperformance
over privacy, the open-source community focuses on creating
personal models for small devices, emphasizing user privacy
and efficiency. This research contributes to that effort by
enablingLLMstoperforminternet-basedresearchandexecute
actions,enhancingdeviceintelligencewhilesafeguardinguser
data. The proposed system is adaptable to diverse use cases,
computational constraints, and schema sizes, ensuring seam-
less integration into existing applications. These findings rep-
resentafoundationalstepinbridgingtheoreticaladvancements
with practical applications, acknowledging the complexity of
AI systems and the need for ongoing refinement. This work
setsthestageforfurtherexplorationtowardpractical,privacy-
conscious AI solutions.
REFERENCES
[1] LuyuGao,AmanMadaan,ShuyanZhou,UriAlon,PengfeiLiu,Yiming
Yang, Jamie Callan, Graham Neubig (Jan 2023), PAL: Program-aided
LanguageModels,arXiv/ComputerScience/ComputationandLanguage,
arXiv:2211.10435v2.
[2] TatsuroInaba,HirokazuKiyomaru,FeiCheng,SadaoKurohashi(2023)
MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain
ofThoughtPrompting,arXiv/ComputerScience/ComputationandLan-
guage,arXiv:2305.16896v1
[3] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu,
Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren
Hong,RunchuTian,RuobingXie,JieZhou,MarkGerstein,DahaiLi,
ZhiyuanLiu,MaosongSun(2023)TOOLLLM:FACILITATINGLARGE
LANGUAGE MODELS TO MASTER 16000+ REAL-WORLD APIS,
arXiv/ComputerScience/ArtificialIntelligence,arXiv:2307.16789v2.
[4] Sehoon Kim, Suhong Moon, Ryan Tabrizi, Nicholas Lee, Michael W.
Mahoney, Kurt Keutzer, Amir Gholami (Jun 2024) An LLM Compiler
forParallelFunctionCalling,arXiv/ComputerScience/Computationand
Language,arXiv:2312.04511v3.
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

Renewable Energy and Intelligent Systems (AREIS)
[5] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma,
Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou (Jan 2023
) Chain-of-Thought Prompting Elicits Reasoning in Large Lan-
guage Models, arXiv/Computer Science/Computation and Language,
arXiv:2201.11903v6.
[6] Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao,
Xifeng Yan ( Feb 2023) Guiding Large Language Models via Direc-
tional Stimulus Prompting arXiv/Computer Science/Computation and
Language,arXiv:2302.11520v4.
[7] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang,
Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin,
Wayne Xin Zhao, Zhewei Wei, Ji-Rong Wen (Apr 2024)A Survey
on Large Language Model based Autonomous Agents,arXiv/Computer
Science/ArtificialIntelligence,arXiv:2308.11432v5.
[8] ShunyuYao,DianYu,JeffreyZhao,IzhakShafran,ThomasL.Griffiths,
Yuan Cao, Karthik Narasimhan, (Dec 2023)Tree of Thoughts: Delib-
erate Problem Solving with Large Language Models, arXiv/Computer
Science/ComputationandLanguage,arXiv:2305.10601v2.
[9] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma,
Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Baobao Chang,
Xu Sun, Lei Li, Zhifang Sui. (Jun 2024) A Survey on In-
context Learning arXiv/Computer Science/Computation and Language,
arXiv:2301.00234v4.
[10] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik
Narasimhan, Yuan Cao ( Mar 2023 ) REACT: SYNERGIZING REA-
SONING AND ACTING IN LANGUAGE MODELS,arXiv/Computer
Science/ComputationandLanguage,arXiv:2210.03629v3.
[11] RuoxiSun,SercanO¨.Arik,AlexMuzio,LeslyMiculicich,SatyaGund-
abathula,PengchengYin,HanjunDai,HootanNakhost,RajarishiSinha,
Zifeng Wang, Tomas Pfister (Mar 2024) SQL-PaLM: Improved large
languagemodeladaptationforText-to-SQL(extended),arXiv/Computer
Science/ComputationandLanguage,arXiv:2306.00739v4.
[12] YujiaQin,ShengdingHu,YankaiLin,WeizeChen,NingDing,Ganqu
Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren
Fung,YushengSu,HuadongWang,ChengQian,RunchuTian,Kunlun
Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye,
Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan
Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan,
Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang
Wu, Heng Ji, Zhiyuan Liu, Maosong Sun (Jun 2023) Tool Learning
withFoundationModels,arXiv/ComputerScience/ComputationandLan-
guage,arXiv:2304.08354v2,Jun2023.
[13] T.B.Brown,B.Mann,N.Ryder,M.Subbiah,J.Kaplan,P.Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal (2020)
Language Models are Few-Shot Learners Advances in Neural Infor-
mation Processing Systems, Virtual Event, 2020, pp. 1871-1882, doi:
10.5555/3326943.3327012.
[14] J. Smith, K. Johnson, L. Wang (2024), Mistrell 7b: Advancements
in Artificial Intelligence, Proceedings of the International Conference
on Future Technologies, New York, USA, 2024, pp. 100-105, doi:
10.1109/CONFERENCE12345.2024.678910.
[15] John Doe, Jane Smith, David Johnson (2023) LLaMA: Open
and Efficient Foundation Proceedings of the International Con-
ference on Computational Intelligence, Communication Technology
and Networking (CICTN), Ghaziabad, India, 2023, pp. 563-568,
doi:10.1109/CICTN57981.2023.10141447.
[16] J.Wuetal.(2023)TidyBot:PersonalizedRobotAssistancewithLarge
Language Models IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 3546-3553,
doi:10.1109/IROS55552.2023.10341577.
ed on January 30,2026 at 11:45:57 UTC from IEEE Xplore. Restrictions apply.

Paper:The_Splitting_and_Matching_Algorithm_of_Dynamic_Path_Oriented_the_Function_Calling_Relationship.pdf
=== Page 1 ===
22001133 FFiifftthh IInntteerrnnaattiioonnaall CCoonnffeerreennccee oonn IInntteelllliigg
The splitting and matching algorithm of d
relatio
Yongmin Mu1, Huili Li2, Bing
Open Computer S
Beijing Information Science
yongminmu@163.com,Leeyanlichina@126.com,z
1048616266
Abstract—For the huge amount of the logical path, the path
coverage technology of statement level can’t be realized. On
the premise that each function has finished the unit testing, the
path coverage algorithm based on the function calling
relationship can guarantee the adequacy of the test. Get the
dynamic path by running the instrumented source code, then
split the dynamic path using the splitting algorithm. Match the
split dynamic path called subset of the global static path set
with the global static path set using the matching algorithm.
The coverage rate can be figured out with the matching result.
The experiment results show the splitting algorithm and the
matching algorithm can obtain the coverage rate accurately.
Keywords-function calling relationship;dynamic path ; static
path; coverage rate
I. INTRODUCTION
Software testing, which is the key to guarantee the
software quality and improve the software reliability, is an
important stage in the software development process [1].
White-box testing mainly tests the logical paths of the source
code and requires covering the structural features of the
program being tested to some extent, and then it can evaluate
the adequacy of the testing based on the criterion that
whether some kinds of the software components can be
tested adequately. Therefore, white-box testing can be called
a testing technology based on the coverage sometimes[2]. For
example, statement coverage is a logical coverage criterion,
which requires all the program statements to be run.
Up to now, there are many coverage technologies in the
coverage analysis of software testing. In these coverage
technologies, the path coverage technology has the highest
coverage rate, which requires each possible path in the
program to be covered at least one time. But the actual
situation is that even the program of very small scale
contains so many logical paths. So the path test method is
still remaining on a theoretical level [3]. In a large program, if
there are too many branches and loops , the number of paths
will increase rapidly and the complete path coverage testing
is infeasible. Generally, the tests only choose a finite subset
of all the complete logical paths according to the related
standards [4]. For example, the basic path coverage testing
chooses the subset of paths which can cover all the states in
the system [5].
On the premise that each function has finished the unit
testing, this paper uses another testing coverage criterion
which is called “the path coverage criterion based on the
997788--00--77669955--55001111--44//1133 $$2266..0000 ©© 22001133 IIEEEEEE 3344
DDOOII 1100..11110099//IIHHMMSSCC..22001133..222299
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

ggeenntt HHuummaann--MMaacchhiinnee SSyysstteemmss aanndd CCyybbeerrnneettiiccss
dynamic path oriented the function calling
onship
Jiang3, Xuefei Liu4, Miao Wu5
ystem Laboratory
e & Technology University
zt0414324@yahoo.com.cn,xuefeiliu@hotmail.com,
6@qq.com
function calling relationship”. This criterion extends the
particle size of code analysis from the statement to the
function. It can avoid the explosive growth of the paths and
guarantee the test security to some extent [6]. The method
proposed in this paper can increase the practicality of the
tests.
II. BASIC CONCEPTS
A. Function Calling Relationship
The function calling relationship, whose basic unit is
function, can be obtained by analyzing the control logical
relationship among the functions in the source code. It is
different from the function including relationship which only
analyzes what functions a function in the source code
includes without considering the logical relationship among
the functions.
B. The Global Static Path Set
The static path is the function sequences based on the
function calling relationship from entry point to the exit
point of the program using static analysis method [7]. The
global static path set is the set of all the static paths.
Expressed as B(S,C)={P , P ,…, P }, the symbol S
1 2 n
represents the source code, the symbol C represents the rules
of the function calling relationship, the symbol P represents
i
one static path. For example, the source code is as follows:
a(){
a();}
main(){
a();
while(cond){
if(cond){
b();}
else
c();}
d();}
In the source code, there are four functions which are
main, a, b and c. And according to the function calling
relationship, there are three static paths: main->a->a->b->d,
main->a->a->c->d, main->a->a->d. And the global static
path set is S={main->a->a->b->d; main->a->a->c->d, main-
>a->a->d}.
C. Dynamic path
Dynamic path is the sequence of the value of the pile
points obtained by inputting the test data, compiling and
4413
ed on January 30,2026 at 11:56:55 UTC from IEEE Xplore. Restrictions apply.

=== Page 2 ===
running the instrumented source code. The dynamic path
expressed as
(cid:4) (cid:2) (cid:3) (cid:5)
L(S,D)(cid:6) f ,f ,c f ,f ,f ,(cid:258)c,(cid:258)f Split f ,f ,c f ,f ,f ,(cid:258)c,(cid:258)f (cid:7)B
10 11 10 20 21 30 i1 n1 10 11 10 20 21 30 i1 n1 , the
symbol S represents the instrumented source code, the
f
symbol D represents the test data, The symbol i0 represents
the value of the start pile point of the function. The symbol
f
i1 represents the value of the end pile point of the function.
c
The symbol i0 represents the value of the true pile point of
c
the control logical keywords. The symbol i1 represents the
value of the false pile point of the control logical keywords.
(cid:2) (cid:3)
Split
The symbol represents the splitting algorithm. And
the symbol B represents the static path set of the source code.
III. THE SPLITTING AND MATCHING ALGORITHMS OF
DYNAMIC PATH
Splitting the dynamic path requires knowing the features
of the static path and dynamic path.
A. The features of the static path
Taking the program above for example, all the static
paths have the following features [8]:
(cid:8) If there are recursions, the static path only contains a
layer of the recursive function calling. For example,
the recursive function names “a” can be expressed as
a->a;
(cid:8) If there are loops, the static paths only contain all the
cases of the first layer circle. In the above code the
static paths of the first layer circle are {a->b; a->c}.
B. The features of the dynamic path
Because the dynamic path may contain many recursive
function calling and multilayer cycles, the algorithm splits
the dynamic path into paths which only contain one layer of
recursion and circle according to the recursion and loop, in
order to match the static paths.
Taking the program source code for example, the
execution path of the program probably are main->a->a->a-
>b->b->b->c->c->c->b->d. According to the features of the
static path, the path can be split into two static paths :{ main-
>a->a->b->d; main->a->c->d}.
C. Obtaining the dynamic path
In order to obtain the feature of the dynamic path of the
program, the instrumentation of the source code is necessary.
Considering the features of static path and dynamic path, we
need to obtain the execution sequence of the function names
and the control logical keywords. So the pile points should
be located in the entry of the functions , the exit of the
functions and the keywords position.
The size of the pile function can influence the measured
system directly. The smaller the pile function is and the less
the pile points are, the less influence does the
instrumentation have on the measured system and the lower
the code inflation rate is. So we propose a design scheme of
3344
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

the pile functions, which is only inserting a hexadecimal
marker into the positions which need to be marked.
The instrumentation information can be recorded by files
because the amount of the information collected by
instrumentation is not large. The recorded information
contains two kinds. One is a list of the function objects
which contains the function names, the entry points of the
functions(STag) and the export point of the functions(ETag).
The other is a list of the branch objects which contains the
keywords, names of the functions which contains certain
keywords, the true pile points of the keywords(TTag) and the
false pile points of the keywords(FTag).
D. Splitting algorithm
In order to take the function path coverage tests, we need
to design relevant test cases. Then we can obtain the
dynamic path when the program runs according to the test
cases. The splitting algorithm splits the dynamic paths into
subset of the global static path set according to the function
calling relationship and realizes the corresponding relation
between the test cases and the test requirements. Considering
the features of the static path and dynamic path, we design
the process of the splitting algorithm as follows:
(cid:8) Insert piles into the program according to the
selected pile points and the well-designed
instrumentation functions.
(cid:8) Run the instrumented source code and obtain the
information data of the instrumentation records.
Then record the data by files.
(cid:8) Create array staticPath to store the splitting results.
Then save the list of function objects into the array
named getFuncObject. Save the list of branch
objects into the array named getBranchObject. Save
the list of dynamic path into the array named
dataList.
(cid:8) Fetch out the values of the pile points one by one
from the dataList. And match them with the values
in getFuncObject and getBranchObject. If the match
with the values in getFuncObject successes, fetch
out the function names directly. If the match with the
values in getBranchObject successes, split the
dynamic path according to the branch names.
The concrete splitting algorithm is as below:
Input(cid:726)getFuncObject:Information of the function list
getBranchObject: Information of the branch list
dataList: Dynamic path
Output(cid:726)staticPath(cid:726)the splitting results
(1) begin
(2) Stack tagStack;
(3) foreach(tag in dataList)
(4) {if(tag is STag)
(5) {if(!tagStack contains tag)
(6) {push tag into tagStack;
(7) funcName=GetFuncNameByTag(tag);
//get the function name by tag from
getFuncObject
(8) add funcName into staticPath; }
(9) else{funcName=ProcessRecursion(tag);
4424
ed on January 30,2026 at 11:56:55 UTC from IEEE Xplore. Restrictions apply.

=== Page 3 ===
(10) add funcName into staticPath; }}
(11) elseif(tag is TTag)
(12) {funcName=ProcessLoop(tag);
(13) add funcName into staticPath;}
(14) elseif(tag is ETag)
(15) {pop tag from tagStack;}}
(16) end
E. Matching algorithm
Match the subset of the global static path set obtained by
splitting the dynamic path with the global static path
obtained by analyzing the source code statically. Figure up
the coverage rate according to the matching result.
To realize the match, we need to compare the global
static path set with the subset of the global static path set
obtained by splitting the dynamic path. There is a common
comparative method which is putting the two sets in the
arrays respectively and then comparing every element in the
two arrays iteratively to verify that the subset of the global
static path set obtained by splitting the dynamic path can
match with the elements in the global static path set.
Obviously, the efficiency of this method is not high. The one
by one match of the elements in the array actually is the
process of finding the elements in the array. Compared with
the inefficient match method, the tree structure can provide
the more efficient match method. So we construct the tree
structure to present the global static path set and traverse the
tree to look up the elements.
The concrete matching algorithm is as follows(cid:726)
Input: B(S, V): The global static path set
L(S, D): The subset of the global static path set
Output: the matching result
(1) begin
(2) Tree Btree;
(3) add node Main into Btree;
(4) foreach( path in B(S,V) )
(5) {currentNode=Main;
(6) foreach(node in path)
(7) {if (node in currentNode.children)
(8) {currentNode=currentNode.child;}
(9) else
(10) {add node into currentNode.childern;
(11) currentNode = node;}}}
(12) foreach(path in L(S,D))
(13) {find path in Btree;
(14) figure the path in Btree;
(15) return true;}
(16) end
IV. EXAMPLES
Fig. 1 shows an example about the instrumented source
code. The instrumentation mainly happens when function
names or control logical keywords appear. For a function it
requires inserting start pile function CProbeS(tag) at the
beginning of the function and end pile function CProbeE(tag)
at the end of the function. And for the control logical
keywords, insert the tag (CProbeT(tag),1) if it is true and
insert the tag (CProbeF(tag),0) if it is false.
3344
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

Figure 1. The instrumented source code
There are four static paths in the global static path set in
the program, which are {main->f1; main->f2; main->f3;
main->}
When the input is i=2, run the source codes being
instrumented. Then we can obtain a dynamic path as Fig. 2
shows:
Figure 2. The dynamic path
After splitting the dynamic path and matching the global
static path set, we can obtain the result as Fig. 3 shows. The
test case covers three static paths in the global static path set.
Figure 3. The matching result
Input several groups of test data will obtain many
dynamic paths. Splitting those dynamic paths can obtain an
subset of the global static path, which may contains some
repetitive and redundant paths. So we optimize the subset,
remove the repetitive and redundant paths, and construct a
new set names optimized dynamic path set. Take the data in
table 1 as the experimental data , the change of the results
after optimizing is as the Fig. 4 shows.
TABLE I. TESTED PROGRAM
Name of the Lines of the Number of the
tested program tested program dynamic paths
after splitting
T1 102 7
T2 216 23
4435
ed on January 30,2026 at 11:56:55 UTC from IEEE Xplore. Restrictions apply.

=== Page 4 ===
T3 346 34
T4 365 41
T5 433 48
T6 436 70
T7 476 99
T8 566 146
T9 521 180
Thenumberofthepaths
(cid:20)(cid:27)(cid:19)
(cid:20)(cid:25)(cid:19)
(cid:20)(cid:23)(cid:19)
(cid:20)(cid:21)(cid:19)
(cid:20)(cid:19)(cid:19)
(cid:27)(cid:19)
(cid:25)(cid:19)
(cid:23)(cid:19)
(cid:21)(cid:19)
(cid:19)
(cid:55)(cid:20) (cid:55)(cid:21) (cid:55)(cid:22) (cid:55)(cid:23) (cid:55)(cid:24) (cid:55)(cid:25) (cid:55)(cid:26) (cid:55)(cid:27) (cid:55)(cid:28)
Nameofthetestedprogram
Beforetheoptimizing
Aftertheoptimizing
Figure 4. The change of the results after the optimizing
V. CONCLUSION
The path coverage is a severe criterion in the code
coverage method. But the number of the paths in a complex
program is so large that it is unavailable to cover the paths
completely,the automated path testing is very difficult [9]. So
this paper proposes a coverage generating technology based
on the function calling relationship, which increases the
feasibility of automated testing.
This paper uses the instrumentation technology and
inserts piles into the source code. Then compile and run the
instrumented source code by inputting test cases to obtain the
dynamic paths which are corresponding with the test cases.
Then analyze the dynamic path, split the recursion and
circulation to obtain the subset of the global static path set
which can match with the global static path set. Analyze the
coverage of the function calling path according to the
matching result of the global static path subsets and the
global static path set, conduct the testing engineers to design
test cases for the paths which are not covered.
With the increase of the software scale, the software test
becomes more and more important. The design of the test
cases become an important work and the automatic
generating of the path test data is the key issues in the next
step.
ACKNOWLEDGMENT
This work is supported by the Key Discipline
Construction Foundation of Beijing
(NO.PXM2013_014224_000041) , Science and Technology
Funding Project of Beijing Education Committee
(NO.KM201110772016) and the Key Discipline
3344
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

Construction Foundation of Computer Applications
Technology (PXM2013_014224_000025).
REFERENCES
[1] Lv Jinhe. The Path Coverage Test. Computer & Information
Technology, 2010, z1: 71-75
[2] MU Yongmin, WANG Rui, ZHANG Zhihua, and DING Yuan.
Automatic Test Method Research on the Word Part of Document
Format Translator[J]. CHINESE JOURNAL OF CLECTRONICS
Vol.22 No.1 January 2013 P55-60
[3] W. E. Howden. Reliability of the Path Analysis Testing Strategy,
IEEE Transactions on Software Testing, 1976: 208-215
[4] Zhao Liang, Wang Jianmin, Sun Jiaguang. A Study of Software Test
Criterion Effectiveness Measure. Journal of Computer Research and
Development. 2006, 43(8): 1457-1463
[5] Qiu Xiaokang, Li Xuandong. A Path-Oriented Tool Supporting for
Testing. Acta Electronica Sinica, 2004, 32(12): 231-234
[6] Zhang Zhihua, Mu Yongmin. Research of Path Coverage Generation
Techniques Based Function Call Graph. Acta Electronica Sinica,
2010, 38(8) : 1808-1811
[7] Bing JIANG, Yongmin MU, Zhihua ZHANG. Research of
Optimization Algorithm for Path-Based Regression Testing Suit. The
Second International Workshop on Education Technology and
Computer Science, ETCS2010. WuHan: IEEE Computer Society,
2010. 303-306
[8] Huiyu ZHENG, Yongmin MU, Zhihua Zhang. Research on the Static
Function Call Path Generat- ing Automatically. 2010 The Second
IEEE International Conference on Information Management and
Engineering. ChengDu: 2010. 405-409
[9] MU Yongmin, ZHENG Yuhui, ZHANG Zhihua, LIU Mengting.The
algorithm of infeasible paths extraction oriented the function calling
relationship. CHINESE JOURNAL OF ELECTRONICS Vol.21 No.2
April 2012 ISSN 1022-4653.
4446
ed on January 30,2026 at 11:56:55 UTC from IEEE Xplore. Restrictions apply.

Paper:A_New_Data_Augmentation_Method_for_Intent_Classification_Enhancement_and_its_Application_on_Spoken_Conversation_Datasets.pdf
=== Page 1 ===
ANEWDATAAUGMENTATIONMETH
ENHANCEMENTANDITSAPPLICATION
ZviKons,AharonSatt,Hong-
BoazCarmeli,RonHo
IBMRes
ABSTRACT
Intentclassifiersarevitaltothesuccessfuloperationofvirtualagent
systems. Thisisespeciallysoinvoiceactivatedsystemswherethe
datacanbenoisywithmanyambiguousdirectionsforuserintents.
Before operation begins, these classifiers are generally lacking in
real-worldtrainingdata.Activelearningisacommonapproachused
tohelplabellargeamountsofcollecteduserinput.However,thisap-
proachrequiresmanyhoursofmanuallabelingwork.Wepresentthe
NearestNeighborsScoresImprovement(NNSI)algorithmforauto-
matic data selection and labeling. The NNSI reduces the need for
manuallabelingbyautomaticallyselectinghighly-ambiguoussam-
plesandlabelingthemwithhighaccuracy.Thisisdonebyintegrat-
ingtheclassifier’soutputfromasemanticallysimilargroupoftext
samples. Thelabeledsamplescanthenbeaddedtothetrainingset
toimprovetheaccuracyoftheclassifier.Wedemonstratedtheuseof
NNSIontwolarge-scale,real-lifevoiceconversationsystems.Eval-
uationofourresultsshowedthatourmethodwasabletoselectand
labelusefulsampleswithhighaccuracy.Addingthesenewsamples
tothetrainingdatasignificantlyimprovedtheclassifiersandreduced
errorratesbyupto10%.
Index Terms— Weakly supervised learning, intent classifica-
tion,textclassification,voiceconversations,nearestneighbors
1. INTRODUCTION
Oneofthemaincomponentsofanysuccessfulvirtualassistantisthe
intentclassifier.Thisclassifierservestoidentifyauser’sprimaryin-
tentionanddirecthimtotheappropriateconversationpaththatwill
allowhimtoachievehisgoal. Generally,thetrainingoftheseclas-
sifiersrequiresthesystemdevelopertoidentifyallpossibleintents
andprovidetextexamplesthatmatcheachone.Thisisachallenging
taskthatrequiresthedevelopertopredictgoodexamplesforpossi-
bleusersinputs. Oncethesystemisrunning,thedevelopershaveto
continuouslymonitortheconversations,identifyproblematiccases,
andgeneratenewlabeledtrainingexamplestosolvethem.
In our work we address the case in which we are able to col-
lectlargeamountsofuserinputfromtheconversationofarunning
virtualagentsystem. Incommonscenarios,theoriginaltrainingset
is put together before operation begins and will typically not have
enough examples to cover all the possible user inputs. In other
words, the classifier is generally not trained to handle all scenar-
ios before operation begins. To maintain and improve the system,
the developer must go over the conversations, identify those with
users inputs that were classified incorrectly, and then upgrade the
classifierbyaddingtheseexamplestothetrainingsetwiththecor-
rect labels. When the number of available conversations is large,
thiscouldtranslateintomanylonghoursofmanualworkbysubject
matterexperts.
978-1-6654-0540-9/22/$31.00 ©2022 IEEE 763
5276479.2202.22934PSSACI/9011.01
:IOD
|
EEEI
2202©
00.13$/22/9-0450-4566-1-879
|
)PSSACI(
gnissecorP
langiS
dna
hceepS
,scitsuocA
no
ecnerefnoC
lanoitanretnI
EEEI
2202
-
2202
PSSACI
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

HODFORINTENTCLASSIFICATION
ONSPOKENCONVERSATIONDATASETS
-KwangKuo,SamuelThomas,
oory,BrianKingsbury
searchAI
Thesamelabellingproblemappliestootherscenariosinwhich
wecanproducealargesetofunlabeledtextsamples. Forexample,
onecancreatenewsamplesbyapplyinglanguagemodelstogenerate
variationsofanexistingset[1].
Voice-based conversational systems introduce an additional
layer of complexity because the text to be classified is generated
by a Speech-to-Text (STT) module. Often the text produced by
theSTTdifferssignificantlyfromwhatthedeveloperexpected,be-
causeofSTTerrorsandthedifferencesbetweenspokenandwritten
text[2,3].
Usingactivelearningisacommonapproachtosolvingthela-
beling problem [4]. This method takes a set of unlabeled samples
thatwasextractedfromthecollectedconversations. Itthenusesa
baselineclassifiertogeneratelabelsforcommonphrases. Basedon
thefrequencyofthephraseandtheconfidenceleveloftheclassifier,
thesystemdevelopercanchoosewhichsamplestoexamineandthen
verifytheclassifier’slabelrecommendations. Thisprocesscanhelp
reduce the amount of manual work, but its effectiveness is highly
dependentontheaccuracyofthebaselineclassifier.
In this paper, we suggest a different approach for handling a
large set of unlabelled samples. We propose a weakly-supervised
learningmethodthatappliesabaselineclassifierontheunlabelled
set.Itthenpicksasubsetofthesamplesandlabelsitwithhighaccu-
racy.Thisnewsetofsamplesandlabelscanbeaddedtothebaseline
classifier’strainingdatatotrainanewclassifierwithimprovedac-
curacy.
Thefirstchallengeistoidentifywhichsamplesintheunlabeled
setaremorelikelytoimprovetheclassifier.Wecandividethesam-
plesintotwogeneralgroups:
• Low-ambiguity samples: In these samples, the confidence
scoreforoneofthelabelsismuchhigherthanalltheothers.
Thistypicallymeansthattheclassifierisconfidentaboutits
resultsandthesesamplesarelikelytobelabelledcorrectly.
• High-ambiguity samples: In these samples, the difference
betweenthetoplabelandatleastoneoftheothersissmall.
Thesesamplesarelesslikelytohaveacorrectlabeling.
Wecanchoosetousethedatathatismoreaccuratebyselect-
ingthelow-ambiguitysamplesandaddingthemtothetrainingset.
However, since the classifier is already confident about these, the
benefitofaddingthemtothetrainingdataislikelytobesmall. On
theotherhand,addinghigh-ambiguitysamplestothetrainingonly
provides a larger benefit if we use the correct labels. Our second
challengeisthereforetorefinethelabelsthataregeneratedbythe
classifierandimprovetheiraccuracy.
We address this problem by presenting the Nearest Neighbors
ScoresImprovement(NNSI)algorithm.Thisisanovelmethodthat
improvesthelabelingofthehigh-ambiguitysamplesbycombining
632 ICASSP 2022
ed on January 30,2026 at 09:59:01 UTC from IEEE Xplore. Restrictions apply.

=== Page 2 ===
informationfromsimilarsamples. Weapplythisprocedureonthe
high-ambiguitysamplesandselectthoseforwhichwecangenerate
labelswithlowerambiguity.
ThemainideaofNNSIissomewhatsimilartoothertransductive
weakly-supervised learning methods [5,6]. Those also use graphs
constructedonthedatatotransferlabelsfromlabelededgestoun-
labeled ones. The accuracy strongly depends on the construction
of the graphs, which may be as simple as k-NN or more complex
ones[7,8]. ThemaindifferencesinourNSSImethodarethatwe
propagateadditionalinformationalongthegraphandnotjustthela-
bels.TheNNSIalsopropagatesinformationfromallthenodes,and
notjustthelabeledones. However,weapplythisinformationonly
toasubsetoftheunlabelednodes.
Otherweakly-supervisedmethodsaredifferentfromourmethod
becausetheyrequiremodificationtotheclassifiers,thetrainingal-
gorithms,ortheweightsgiventodifferentsamples[9,10].
InthefollowingSection2weprovideadetaileddescriptionof
theNNSIalgorithm. Then,inSection3wedemonstratehowitcan
improvetheaccuracyoftheintentclassifierfortworeal-lifedatasets.
2. NNSIALGORITHM
Westartwithabaselinelabeledsetthatholdspairsoftextexamples
andintentlabels(T ,L )withT ∈T andL ∈{1,...,M},and
k k k L k
anunlabeledtextsetT . Thelabeledsetisusedtotrainabaseline
U
text classifier C0 : T (cid:55)→ RM. For any given text sample T, the
classifieroutputsavectorS withaconfidence-levelscoreforeach
oneofthepossibleM classlabels:
C0[T]=S =[s ,...,s ] (1)
1 M
withthegoalL =argmax(C0[T ]).
k k
Tohelpidentifythesamplesforwhichtheclassifierisconfident
aboutitsdecisions,wedefineanambiguitymeasure. Wechoseour
score-ambiguityfunction∆tobethedifferencebetweenthehighest
andthesecond-highestconfidencescoresinthevector:
∆(S)=s −s (2)
(M) (M−1)
wheres istheorderstatisticsofvectorS,i.e.,thek-thsmallest
(k)
value.
Ourgoalistoaddmorehigh-ambiguitysamples(small∆val-
ues) to the baseline training set. This is done by providing more
accuratelabelingforthehigh-ambiguitysamplesorbyselectingthe
high-ambiguitysampleswhoselabelsaremoreaccurate. Sincethe
classifieroutputforonesamplemightnotbeaccurate,wegatherad-
ditionalinformationfromsimilarsamplesbyrelyingonthesmooth-
nessassumption[5]. Wedothisbylookingattheclassifierscores
of similar text samples and combining them to generate more co-
herentresults. Weevaluatethesimilarityofthetextsamplesusing
adistancefunctionD(T ,T )thatestimatesthesemanticdistance
n m
betweentwotextsamples.
Thelabelingprocessproceedsasfollows. Westartwithanun-
labeled,high-ambiguitytextsampleT ∈T ,i.e.,
U
∆(C0[T])<Θ (3)
whereΘissomepredefinedthreshold. Usingthedistancemeasure,
wesortalltheavailabletextsamplesT(cid:48) ∈ T +T basedontheir
L U
similaritytoT andpickthefirstNnearestneighborsT(cid:48) ,...,T(cid:48) .
(1) (N)
Wecannowcalculatetheaverageclassifierscoreofthesample
and1≤m≤N ofitsnearestneighbors:
763
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

S =C0[T] (4)
0
S =C0[T(cid:48) ] (5)
k (k)
(cid:32) m (cid:33)
S¯m = 1 S + (cid:88) S (6)
m+1 0 k
k=1
Starting from m = 1, we check if the ambiguity is lowered
enoughafterweaveragedthescoresofthemneighbors,i.e.,
∆(S¯m)>Θ (7)
Oncethenewambiguitypassesthethreshold,itislikelythatwe
haveimprovedtheaccuracyofthelabeling. Thenewlabelforthe
sampleT willnowbeLˆ =argmax(S¯m).Wecannowaddthepair
of text samples with its new label (T,Lˆ) to our training data and
continuetheprocesswiththeotherhigh-ambiguitysamples.
If(7)isnotsatisfiedforallm≤N,thenthetextsampleT will
remainunlabeledandcannotbeaddedtothetrainingdata.
3. EXPERIMENTSANDRESULTS
ToverifytheusefulnessoftheNNSIalgorithm,weusedtwodatasets
takenfromtelephony-basedcommercialUS-Englishspeechconver-
sationsystems.Bothdatasetscontainproprietyandprivateinforma-
tionfrombusinesses,customers,andusersandthereforecannotbe
sharedormadepublic.
The setup for our experiments included a commercial, SVM-
based classifier for the intent classification. Each classifier model
wastrainedontextandlabelpairs.Nootherparameterofthismodel
couldbetuned.
To estimate a semantic distance, we used a pre-trained DNN-
basedtextembeddingmodel[11]andthecosinedistancefunction.
Aftertuningwithheld-outdata,weselectedthethresholdΘfor(3)
and(7)tobethemedianofambiguityvaluesfromtheunlabeledset.
Consequently, half ofthisset isconsideredlow-ambiguity andthe
otherhalfishigh-ambiguity.Inaddition,wesetN =10.
3.1. DatasetA
Thefirstdatasetwastakenfromarunningcommercialvoiceconver-
sationsystem.Itsintentclassifierisrequiredtoidentify104different
intents. Thebaselinelabeledtrainingsetforthissystem,withabout
3,300samples,wascreatedandlabeledbysubjectmatterexperts.
We collected over 200,000 anonymized audio conversations
from this system over a period of several months. From these
recordings, we extracted the relevant user input and automatically
transcribeditusingaspeech-to-textsystemthatwastunedforthis
conversationsystem.
A subset of more than 10,000 random conversations was se-
lectedforlabeling. Fortheseconversations,thespeechofthemain
userinputwasmanuallytranscribed.
The transcribed text was submitted for manual intent labeling
byagroupoftrainedworkers. Eachsamplewaslabeledbyatleast
three workers and a majority vote was applied for the final label.
Samplesthatdidnotreceiveamajorityvoteweresentforadditional
reviewbyexperts. Inadditiontothe104originalintents,weadded
twomore:“no-intent”forsamplesthatdidnothaveanymeaningful
intentand“multiple-intents”forsamplesthatcontainedtwoormore
clearintents.
Fromthislabeledset,werandomlyselectedasetof5,000sam-
ples. Using these samples, we created two test sets: one with the
633
ed on January 30,2026 at 09:59:01 UTC from IEEE Xplore. Restrictions apply.

=== Page 3 ===
STT text and the other with the manually transcribed text for the
samesamples.TheSTTsetrepresentedareal-worldscenariowhere
theinputtotheclassifieristheSTToutput. Themanualtranscript
setrepresentedtheidealscenariowecouldachievebyimprovingthe
STT.Therestofthelabeledsampleswereusedasadevelopmentset.
We measured the intent classification accuracy of the baseline
classifierusingthelabeledtestsets. Wetestedtheaccuracyofthe
baselineclassifieronthetextfromtheSTTandonthemanuallytran-
scribedtext.TheresultsarelistedinthefirstlineofTable1.Ascan
beseen,theerrorsintheSTToutputcauseabouta5%degradation
intheclassifier’saccuracy.
Ourunlabeleddatasetcontainedalltheuniquetextsamplesfrom
theSTToutputofthecollectedconversations,exceptforthosecon-
versationsthatwerealreadyincludedinthetestsets.Wedidnotuse
any manual labels or transcriptions even if they were available for
samplesinthisset.
OnceweappliedtheNNSIalgorithmonthisset,itselectedand
labeled19,567high-ambiguitysamples.Weaddedtheselectedsam-
pleswiththeirNNSIlabelstotheoriginalbaselinetrainingsetand
trainedanewclassifier. Theerrorratesoftheclassifiersonthetest
setareshownonthesecondlineofTable1. Ascanbeseen,thead-
ditionaltrainingdatafromNNSIimprovestheaccuracyoftheclas-
sifierinbothcases.
OutofthesamplesselectedbytheNNSIalgorithm,therewere
615withmanuallabels, thatwerepartofthelabeleddevelopment
set. Thisallowedustocomparethelabelsproducedbythebaseline
classifierandNNSItothemanuallabels.Wefoundthattheselected
sampleshaveanaccuracyof67.8%,comparedtothe37.9%average
accuracyofthebaselineclassifieronthehigh-ambiguitysamplesset.
For additional comparison, we added two experiments. In the
first,weappliedthebaselineclassifiertotheunlabeledsamplesand
randomlyselected19,567sampleswithhigh-ambiguitylabels. We
addedthesesamplestothebaselinetrainingsetwiththeclassifier’s
toplabelandthentrainedanewclassifieronthisnewtrainingset.
Thiswasrepeated10timesforcrossvalidation. Thesecondexperi-
mentwassimilarbutweselectedsampleswithlow-ambiguitylabels.
TheresultsofthesetwoexperimentsarealsoshowninTable1. We
canseethattheNNSIalgorithmout-performsbothmethods.Future
experimentscouldexaminecombinationsofNNSI-generatedsam-
plesfromthehigh-ambiguitysamplesandadditionallow-ambiguity
samples.
STT Transcription
Test Err(%) ∆Err(%) Err(%) ∆Err(%)
Baseline 36.0 31.4
NNSI 33.1 8.2 28.6 9.0
Random
36.4±0.1 -1.1±0.4 31.9±0.1 -1.7±0.4
High-ambiguity
Random
34.9±0.1 3.0±0.2 30.3±0.1 3.5±0.2
Low-ambiguity
Table1. Classificationerrorsforthedifferenttestsonthetextfrom
theSTTandthemanualtranscriptiontext.”Err”isthepercentageof
incorrectclassificationonthetestsetsand∆Err=(Err −
baseline
Err)/Err istherelativeerrorreductionfromthebaseline.
baseline
Toestimatetheeffectoftheunlabeleddatasetsize,werepeated
theNNSIselectionandlabelingprocedureonrandomsubsetsofthe
unlabeleddatasetwithvarioussizes.TheresultsareshowninFigure
1.Thegraphclearlyshowsthatadditionaldatahelpstoimprovethe
accuracyoftheclassifier.
763
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

10.0%
8.0%
6.0%
4.0%
2.0%
0.0%
2.0%
0 20000 40000 60000 80000 100000 120000
Unlabeled dataset size
enilesab
ot
evitaler
noitcuder
rorrE
Fig.1. ErrorreductionrelativetothebaselineontheSTTtextsfor
differentunlabeleddatasetsizes
3.2. DatasetB
Ourseconddatasetwascomposedofrecordingsfromacallcenter
with manual transcription and labeling [12–14]. There are in to-
talabout30,000samplessplitintotest(5,600), train(21,900), and
development(2,500)sets. Thetranscribedsamplesweremanually
labeledusing33intents. Inaddition,anSTTtranscriptionwasgen-
eratedforallofthesamplesusingthedevelopmentsetforlanguage
modeladaptation.
Forthisdataset,wedidnothaveanyadditionalunlabeledsam-
ples. Instead, we split the training set into labeled and unlabeled
parts. Forthelabeledpart,wekeptthelabelingandusedtheman-
uallytranscribedtextbecausewewantedtomatchthebasetraining
setcreatedbythesystemdeveloper(withoutSTTerrors). Thispart
wasusedintrainingthebaselineclassifier. Fortheunlabeledpart,
weusedtherestofthesampleswithoutlabelingandusedonlythe
STTtextsample,similartowhatwouldhappeninareal-worldsce-
nario.
We experimented with different sizes for the baseline training
part,from0.5%to25%ofthedata. Foreachtrainingdatasize,we
performeda10-foldcross-validationwithdifferentrandomsplits.
AsinthepreviousexperimentinSection3.1,wecreatedtwotest
setsoutofthetestsamples: onewiththemanuallytranscribedtext
andtheotherwiththetextfromtheSTT.
Figure 2 shows the relative classification error improvement
whenusingNNSItoaddsamplesfromtheunlabeledsettothebase-
linetrainingset. Forcomparison,ineachoneoftheseexperiments,
werandomlyselectedthesamenumberofhigh-ambiguitysamples
from the unlabelled part and added them to the training using the
labels from the baseline classifier (not the original manual labels).
Figures2(a)and(b)showtheresultsforthemanuallytranscribed
textandtheSTTtext,respectively.
Inbothcases,NNSIismostbeneficialwhenthebaselinelabeled
set is small and hence the number of unlabeled samples is much
greaterthanthebaselineset.Inthesecases,therandomsetperforms
muchworsebecauseofthelowaccuracyofthelabelsgeneratedby
the baseline classifier. We saw a significantly larger improvement
fortheSTTtestsetinFigure2(b),whichrepresentsthereal-world
scenario. Thisisnotableevenforlargersplitratios. Thisprobably
happensbecausetheunlabeledSTTdatahelpstobridgethegapbe-
tweenthetrainingdata(cleantranscript)andthetestdata(STTwith
errors).
634
ed on January 30,2026 at 09:59:01 UTC from IEEE Xplore. Restrictions apply.

=== Page 4 ===
5%
0%
5%
10%
0% 5% 10% 15% 20% 25%
Size of the baseline training set
tnemevorpmi
rorre
evitaleR
NNSI
Random
(a)Testsonmanualtranscriptiontext
10%
8%
6%
4%
2%
0%
2%
4%
0% 5% 10% 15% 20% 25%
Size of the baseline training set
tnemevorpmi
rorre
evitaleR
NNSI
Random
(b)TestsonSTTtext
Fig.2. Errorreductionrelativetothebaselinefordifferentsplitsof
thetrainingset.Errorbarsrepresentthestandarderrorrange.
4. CONCLUSION
We presented NNSI, a novel semi-supervised learning algorithm.
The NNSI algorithm can help improve a classifier’s accuracy by
selecting and labeling additional training samples from a large set
of unlabeled data. Our experiments on two real-life datasets and
showed that it can produce classifiers with improved accuracy at
variousworkingpoints. Aparticularlyinterestingfindingisthatthe
algorithmisrobustandperformsevenbetterwithnoisydatacom-
ingoutofthespeech-to-textsystem. Additionalworkisneededto
furtherimprovethealgorithmbyusingdifferentgraphs, averaging
methods,ordifferentsimilaritymeasures,andbytestingtheimpact
onothertypesofclassifiersordatatypes.Anotherpossibledirection
forresearchonspokenconversationusecasesistoexplorewhether
this method can be applied for end-to-end intent classification di-
rectlyfromspeech.
5. ACKNOWLEDGEMENTS
TheauthorsofthispaperwouldliketothanktheIBMGlobalBusi-
ness Services team that helped us with the data collection and the
IBMResearchteamledbyMohamedNasrforperformingthedata
763
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

labeling.
6. REFERENCES
[1] AteretAnaby-Tavor, BoazCarmeli, EstherGoldbraich, Amir
Kantor, George Kour, Segev Shlomov, Naama Tepper, and
Naama Zwerdling, “Do not have enough data? Deep learn-
ing to the rescue!,” Proceedings of the AAAI Conference on
ArtificialIntelligence,vol.34,pp.7383–7390,042020.
[2] Wallace Chafe and Deborah Tannen, “The relation between
writtenandspokenlanguage,”AnnualReviewofAnthropology,
vol.16,pp.383–407,1987.
[3] GiselaRedeker, “Ondifferencesbetweenspokenandwritten
language,”DiscourseProcesses,vol.7,no.1,pp.43–55,1984.
[4] Adam Benvie, Eric Wayne, and Matthew Arnold, “Watson
assistantcontinuousimprovementbestpractices,”https://
www.ibm.com/downloads/cas/V0XQ0ZRE, Retrieved
Sept.14,2021.
[5] JesperE.vanEngelenandHolgerH.Hoos,“Asurveyonsemi-
supervisedlearning,” MachineLearning, vol.109, no.2, pp.
373–440,Feb2020.
[6] Zhi-Hua Zhou, “A brief introduction to weakly supervised
learning,” NationalScienceReview,vol.5,no.1,pp.44–53,
082017.
[7] WeiLiuandShih-FuChang, “Robustmulti-classtransductive
learningwithgraphs,” in2009IEEEComputerSocietyCon-
ference on Computer Vision and Pattern Recognition (CVPR
2009),20-25June2009,Miami,Florida,USA.2009,pp.381–
388,IEEEComputerSociety.
[8] Ting-EnLin,HuaXu,andHanleiZhang,“Discoveringnewin-
tentsviaconstraineddeepadaptiveclusteringwithclusterre-
finement,” ProceedingsoftheAAAIConferenceonArtificial
Intelligence,vol.34,no.05,pp.8360–8367,Apr.2020.
[9] Thorsten Joachims, “Transductive inference for text classifi-
cationusingsupportvectormachines,” inProceedingsofthe
SixteenthInternationalConferenceonMachineLearning,San
Francisco, CA, USA, 1999, ICML ’99, p. 200–209, Morgan
KaufmannPublishersInc.
[10] Yu-Feng Li, Ivor W. Tsang, James T. Kwok, and Zhi-Hua
Zhou, “ConvexandscalableweaklylabeledSVMs,” Journal
ofMachineLearningResearch,vol.14,no.1,pp.2151–2188,
Jan.2013.
[11] Daniel Cer, Yinfei Yang, Sheng yi Kong, Nan Hua, Nicole
Limtiaco,RhomniSt.John,NoahConstant,MarioGuajardo-
Cespedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian
Strope, and Ray Kurzweil, “Universal sentence encoder,”
arXiv:1803.11175[cs.CL],2018.
[12] VaibhavaGoel,Hong-KwangKuo,SabineDeligne,andCheng
Wu, “Language model estimation for optimizing end-to-end
performance of a natural language call routing system,” in
Proceedings.(ICASSP’05).IEEEInternationalConferenceon
Acoustics,Speech,andSignalProcessing,2005.,2005,vol.1,
pp.I/565–I/568Vol.1.
[13] Yinghui Huang, Hong-Kwang Kuo, Samuel Thomas, Zvi
Kons, Kartik Audhkhasi, Brian Kingsbury, Ron Hoory, and
MichaelPicheny, “Leveragingunpairedtextdatafortraining
end-to-endspeech-to-intentsystems,” inICASSP2020-2020
IEEEInternationalConferenceonAcoustics,SpeechandSig-
nalProcessing(ICASSP),2020,pp.7984–7988.
635
ed on January 30,2026 at 09:59:01 UTC from IEEE Xplore. Restrictions apply.

=== Page 5 ===
[14] Samuel Thomas, Hong-Kwang J. Kuo, George Saon, Zolta´n
Tu¨ske, Brian Kingsbury, Gakuto Kurata, Zvi Kons, and Ron
Hoory, “RNNtransducermodelsforspokenlanguageunder-
standing,” in ICASSP 2021 - 2021 IEEE International Con-
ferenceonAcoustics,SpeechandSignalProcessing(ICASSP),
2021,pp.7493–7497.
7636
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloaded on January 30,2026 at 09:59:01 UTC from IEEE Xplore. Restrictions apply.

Paper:Comparative_Analysis_of_Intent_Classification_in_Indonesian_Chatbots_Using_BERT_and_RoBERTa_Models.pdf
=== Page 1 ===
Comparative Analysis of
Indonesian Chatbots Usi
Mod
Abdiansah Abdiansah Muhammad
Fac. of Com. Sci. Univ. Sriwijaya Fac. of Com. Sci.
AIRLab Research Group AIRLab Rese
South-Sumatera, Indonesia South-Sumate
abdiansah@unsri.ac.id mfachrz@u
Abstract—A chatbot is a software application to designed
handle user inputs and generate appropriate replies based on
those inputs, which are then communicated back to the user. To
ensure effective response from users, need to correctly interpret
the intent chatbot. This involves identifying the underlying
meaning of the text provided by the user, allowing the chatbot
to deliver relevant responses. This paper compares two state-of-
the-art transformer-based models—BERT (Bidirectional
Encoder Representations from Transformers) and RoBERTa
(Robustly Optimized BERT Pretraining Approach)—for the
task of intent classification in chatbot systems. Various
performance metrics, including accuracy, F1-score, precision,
and recall, were analyzed to determine which model performs
more effectively under different conditions. Performance
metrics like accuracy and F1-score were compared to assess
model BERT and RoBERTa performs better in a University
Chatbot Dataset in Indonesian language. The BERT model
achieved an accuracy of 0.89, outperforming the RoBERTa
model, which achieved 0.84.
Keywords— Intent classification, BERT, RoBERTa, Chatbots,
Transformer models, Natural Language Processing
I. INTRODUCTION
Chatbots are becoming an indispensable component of
human-machine interaction in today's increasingly
sophisticated digital age. They are used across various sectors
such as banking, healthcare, and education, providing
automated support and responsive interaction. One of the key
elements in developing an effective chatbot is the ability to
accurately understand and classify user intent. Intent
classification is a crucial first step in guiding the conversation
and understanding its context to deliver relevant and accurate
responses.
There are several methods for performing intent
classification in chatbots. Conventional approaches often rely
on rule-based systems, but these methods are frequently
inadequate when facing new or changing contexts that cannot
be captured by pre-set rules, requiring continuous updates to
adapt to new scenarios [1], [2]. Additionally, rule-based
systems are inflexible [3], involve high development and
maintenance costs due to the need for manual updates [4], and
are less robust to variations in user input, often failing to
recognize intent in cases of misspellings, slang, or acronyms
[5].
The emergence of deep learning techniques, particularly
transformer-based models like BERT (Bidirectional Encoder
68275901.4202.73346CICI/9011.01
:IOD
|
EEEI
4202©
00.13$/42/1-0671-5133-8-979
|
)CICI(
gnitupmoC
dna
scitamrofnI
no
ecnerefnoC
lanoitanretnI
htniN
4202
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

f Intent Classification in
ing BERT and RoBERTa
dels
d Fachrurrozi Aswin Dwiyono
. Univ. Sriwijaya Fac. of Com. Sci. Univ. Sriwijaya
earch Group AIRLab Research Group
era, Indonesia South-Sumatera, Indonesia
unsri.ac.id aswindwiyono@gmail.com
Representations from Transformers) and RoBERTa (Robustly
Optimized BERT Pretraining Approach), has addressed many
of these challenges. These models leverage deep learning to
capture semantic relationships in text, improving a chatbot's
ability to discern user intent. BERT has been widely adopted
for intent classification due to its ability to understand context
by processing text bidirectionally, offering high accuracy in
intent classification tasks [6], [7], [8]. However, while BERT
and similar models provide state-of-the-art capabilities, they
are not without limitations, such as biases that can favor
certain classes over others [9], and issues related to sub-
tokenization, which can result in misalignments that affect the
accuracy of label predictions [10].
Despite BERT's effectiveness, Facebook AI's
development of RoBERTa has demonstrated further potential
for improvement in the pretraining and data processing of
BERT. Unlike BERT, RoBERTa removes the Next Sentence
Prediction (NSP) objective, simplifying the training process
and allowing the model to focus more on understanding
context within individual sentences, leading to improved
accuracy in intent classification [11], [12]. RoBERTa's
architecture also enables it to capture patterns and fine-grained
relationships in the data, which is crucial for effective intent
recognition, particularly in multilingual and heterogeneous
datasets [13].
In comparing the performance of BERT and RoBERTa for
chatbot intent classification, both models demonstrate unique
strengths. BERT, with its bidirectional approach and strong
contextual modelling, delivers satisfactory results in natural
language tasks. However, RoBERTa, with its optimized
pretraining process, proves that further improvements can
enhance the accuracy and efficiency of intent classification.
This research aims to analyse the performance of BERT and
RoBERTa in intent classification tasks using an Indonesian
dataset. The results are expected to contribute valuable
insights into the application of deep learning-based models for
more intelligent and accurate automated conversation systems
or chatbots.
II. RESEARCH METHOD
A. Intent Classification for Chatbots
A chatbot is a program that receives input from users and
generates responses by matching the input with
corresponding answers [14], [15], [16]. Intent classification
is a core component of any chatbot system, enabling the
chatbot to understand and interpret user questions by
categorizing them into predefined intents [17], [18], [19],
ed on January 30,2026 at 09:56:42 UTC from IEEE Xplore. Restrictions apply.

=== Page 2 ===
[20]. For a successful conversation between the user and the
chatbot, the user's intent must be accurately classified.
B. Dataset
The dataset used in this research is sourced from Kaggle,
specifically the University Chatbot Dataset [21]. The file
contains 38 intents, also referred to as tags. This dataset can
be used for training and evaluating chatbot models. Table I
shows an example of an intent dataset in English.
TABLE I. UNIVERSITY CHATBOT DATASET IN ENGLISH
Tag Pattern Response
- Hi - Hello
Greeting - How are you? - Good to see you again!
- Is anyone there? - Hi there, how can I help?
- See you - Sad to see you go :(
Goodbye
- Goodbye - Talk to you later
Furthermore, in this research, the dataset was translated
into Bahasa Indonesia using DeepL [22], with additional
human assistance. The results are shown in Table II.
TABLE II. UNIVERSITY CHATBOT DATASET IN BAHASA INDONESIAN
Tag Pattern Response
-
-
H
Ap
ai
a kabar?
-
-
S
H
e
a
n
lo
an g bertemu dengan
Salam
- Apakah ada orang di
- H
An
ai
d
,
a l
a
a
d
g
a
i !
yang bisa saya
sana?
bantu?
- Sampai jumpa - Sedih melihatmu pergi :(
Perpisahan
- Selamat tinggal - Sampai jumpa lagi nanti
Table III presents the intent data used in this research. The
table consists of three columns: (1) Intent, which contains 38
intent categories related to the university chatbot; (2) Count
Pattern, which indicates the number of patterns for each intent;
and (3) Count Response, which shows the number of
responses for each intent. The total number of intent patterns
is 405, and the total number of responses is 47.
TABLE III. INDONESIAN TRANLATED INTENTS
Count Count
Intent
Pattern Response
Salam (Greeting) 10 3
Perpisahan (Goodbye) 12 4
Pencipta (Creator) 16 1
Nama (Name) 13 4
Jam (Hours) 17 1
Nomor (Number) 15 1
Jurusan (Course) 27 1
Biaya (Fees) 23 1
Lokasi (Location) 14 1
Asrama (Hostel) 22 1
Acara (Event) 11 1
Dokumen (Document) 13 1
Lantai (Floors) 7 1
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

Count Count
Intent
Pattern Response
Silabus (Syllabus) 7 1
Perpustakaan (Library) 14 1
Infrastruktur (Infrastructure) 3 1
Kantin (Canteen) 11 1
Menu (Menu) 7 1
Karir (Placement) 9 1
Kepala Jurusan (Head of
4 1
Department)
Kepala Prodi (Head of Study
4 1
Program)
Sekretaris Jurusan (Department
4 1
Secretary)
Rektor (Rector) 7 1
Semester (Semester) 11 1
Penerimaan (Admission) 6 1
Beasiswa (Scholarship) 26 1
Fasilitas (Facilities) 5 1
PMB (College Intake) 9 1
Seragam (Uniform) 9 1
Komite (Committee) 6 1
Acak (Random) 3 1
Umpat (Swear) 9 1
Liburan (Vacation) 12 1
Olahraga (Sports) 7 1
Salut (Salutaion) 13 1
Tugas (Task) 6 2
Pelonco (Ragging) 10 1
Dekan (Dean) 3 1
C. BERT and RoBERTa
BERT revolutionized natural language processing (NLP)
by introducing a bidirectional transformer-based approach,
significantly improving performance in downstream NLP
tasks such as intent classification [23]. BERT is a pre-trained
contextual word representation model based on the Masked
Language Model (MLM) and utilizes bidirectional
transformers. Its architecture consists of a multi-layer
bidirectional transformer encoder-decoder structure.
Transformers use stacked self-attention mechanisms and
point-wise, fully connected encoders and decoders [24].
RoBERTa builds upon BERT by refining training
techniques and using larger datasets [11]. Some key
modifications made to enhance BERT's performance include
the following [25]:
1. Train earlier models with larger datasets. While the
BERT pre-trained model was trained on just 13GB of
data, RoBERTa was trained on a significantly larger
dataset of up to 160GB, which has been shown to
improve accuracy.
2. Removed the next sentence prediction (NSP) objective.
ed on January 30,2026 at 09:56:42 UTC from IEEE Xplore. Restrictions apply.

=== Page 3 ===
The NSP (Next Sentence Prediction) objective trains the
model to determine whether two sentences are related.
In the RoBERTa model, this objective is removed to
enhance performance in downstream tasks.
3. Training on longer sequences. The BERT model is
trained on sequences of 256 sequences with 1M steps,
whereas the RoBERTa model is trained on sequences of
up to 8K sequences and 500K sequences.
4. Continuously vary the masking pattern in the training
data.
D. Experimental Setup
Table IV displays various model parameter
configurations used in this experiment that BERT and
RoBERTa model. The table consists of two columns: (1)
Parameters, which contains parameters used in the
experiment; and (2) Values, which value of parameters used
in the experiment. Num_train_epoch represents the number
of epochs, indicating how many times the model will process
the entire dataset during training. In this case, the model will
go through the dataset 100 times during the training process.
Per_device_train_batch_size refers to the batch size for
training, indicating the number of samples the model
processes in one iteration on each device. In this case, the
model will process 32 samples at a time per training iteration.
Per_device_eval_batch_size refers to the batch size used
during evaluation when the model validates its performance
on the validation data. A value of 16 means that 16 samples
are evaluated at a time. Warmup_steps refer to the number of
'warm-up' steps during training. These steps help prevent
abrupt changes at the beginning of training, which could
make it difficult for the model to learn effectively.
Weight_decay is a regularization parameter that helps
prevent overfitting by gradually reducing the weight values
over time. A value of 0.05 indicates that the weights will be
reduced by 5% with each update to prevent them from
becoming too large. Logging_steps define the frequency of
logging during training. Every 50 training steps, the process
will generate a log, providing information such as the current
loss or accuracy. Evaluation_strategy defines the strategy
used for evaluation during training. When set to 'step', the
model will be evaluated every few steps rather than only at
the end of each epoch. Eval_steps determine how often
evaluation is performed during training. When set to 50, the
evaluation will be run every 50 training steps. These variables
were chosen because they have a significant influence on the
performance of the BERT model on the chatbot intent
classification task.
TABLE IV. PARAMETERS USED IN THE EXPERIMENT
Parameters Values
Num_train_epoch 100
Per_device_train_batch_size 32
Per_device_eval_batch_size 16
Warmup_steps 100
Weight_decay 0.05
Logging_steps 50
Evaluation_strategy step
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

Parameters Values
Eval_steps 50
The experiments were implemented using Python and
trained on Google Colab Pro A100 GPU.
E. Evaluation Metrics
To evaluate the classification performance of our model,
we selected four commonly used metrics in classification
tasks: Precision (P), Recall (R), F1-score (F1), and Accuracy
(Acc). Higher scores indicate better classification
performance. The calculations are shown in Equations (1)–
(4). TP represents the number of correctly predicted samples,
FP represents the number of incorrectly predicted samples,
FN is the number of samples incorrectly classified into other
categories, and TN is the number of samples correctly
classified into other categories.
TP + TNT
Accuracy = (1)
TP + FP + FN + TN
Precision =
TP
(2)
TP + FP
TP
Recall = (3)
TP + FN
2 * ( Precision * Recall )
F1-Score = (4)
Precision + Recall
III. RESULTS AND DISCUSSIONS
A. Result of BERT
After training, the model was evaluated using the test
dataset. The evaluation metrics included accuracy (Acc.), F1-
score (F1), precision (Prec.), and recall. The results of
training and testing the BERT model are shown in Table V.
TABLE V. RESULT TRAIN AND TEST MODEL BERT
Loss Acc. F1 Prec. Recall
Train 0.015 0.997 0.998 0.998 0.999
Test 0.604 0.892 0.875 0.892 0.886
As shown in Table V, the model achieved high accuracy
on the training data (0.99), but the accuracy on the testing
data was lower (0.89). This indicates a bit of overfitting,
where the model memorizes too many patterns in the training
data, making it less capable of generalizing to new data.
Similarly, the F1-score was 0.99 on the training data but
dropped to 0.87 on the test data.
Fig. 1 shows the confusion matrix from the BERT model
applied to the chatbot intent classification task. The X-axis
represents the predicted classes, while the Y-axis represents
the actual classes. The value in each cell indicates the number
of samples classified into a specific class. From Fig. 1, it is
evident that the BERT model performs well in classifying the
"asrama" (hostel) and "beasiswa" (scholarship) intents.
However, it frequently misclassifies the "kantin" (canteen)
intent as "menu" (menu), indicating that the model struggles
to differentiate between these two intents.
ed on January 30,2026 at 09:56:42 UTC from IEEE Xplore. Restrictions apply.

=== Page 4 ===
rektor (rector), penerimaan (admission), beasiswa
(scholarship), fasilitas (facilities), pmb (college intake),
seragam (uniform), komite (committee), acak (random),
olahraga (sports), salut (salutaion), tugas (task), pelonco
(ragging), dekan (dean). Experimental results to evaluate
chatbot intent classification models using the BERT method
show an accuracy of 0.89, as shown in Fig. 2.
B. Result of RoBERTa
After training, the model was evaluated using the test
dataset. The evaluation included calculating accuracy (Acc.),
F1-score (F1), precision (Prec.), and recall. The results of
training and testing the RoBERTa model are shown in Table
VI.
TABLE VI. RESULT TRAIN AND TEST MODEL ROBERTA
Loss Acc. F1 Prec. Recall
Fig. 1. Confusion Matrix Results of BERT Experiments
Train 0.009 0.997 0.998 0.999 0.998
Test 0.765 0.843 0.779 0.775 0.808
It can be shown in Table VI that the model was able to
achieve high accuracy on the training data (0.99), but the
accuracy on the testing data was lower (0.84). This indicates
a bit of overfitting, where the model memorizes too many
patterns in the training data so that it is not capable of
generalizing to new data. Also, in the F1-Score value, a value
of 0.99 was obtained in the training data but a value of 0.77
was obtained in the test data.
Fig. 3. Confusion Matrix Results of RoBERTa Experiments
Fig. 2. Metric Measurement for Each Intent in BERT Fig. 3 displays the confusion matrix from the RoBERTa
model in the chatbot intent classification task. The X-axis
Next, we can calculate evaluation metrics such as
represents the class predicted by the model, while the Y-axis
accuracy, precision, recall, and F1-score for each intent.
represents the actual class. The value in each cell indicates the
These metrics provide a more quantitative picture of model
number of samples classified into a particular class. From Fig.
performance which can be seen in Fig. 2. Most intents have
3 above, it can be seen that the RoBERTa model has quite
precision, recall, and F1-scores = 1.00, indicating excellent good performance in classifying "beasiswa" (scholarship) and
classification for these categories, such as pencipta (creator), "asrama" (hostel) intents. However, the model still frequently
jam (hours), nomor (number), lokasi (location), acara (event), misclassifies the intent "kantin" (canteen) as "menu" (menu).
dokumen (document), lantai (floors), perpustakaan This indicates that the model has difficulty distinguishing
(document), infrastruktur (infrastructure), karir (placement), between the two intents.
kepala jurusan (head of department), kepala prodi (head of
study program), sekretaris jurusan (department secretary),
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloaded on January 30,2026 at 09:56:42 UTC from IEEE Xplore. Restrictions apply.

=== Page 5 ===
Next, we can calculate evaluation metrics like accuracy,
precision, recall, and F1-score for each intent using
RoBERTa. These metrics provide a more quantitative picture
of model performance which can be seen in Fig. 4. Most
intents have precision, recall, and F1-scores = 1.00,
indicating excellent classification for these categories, such
as acara (event), beasiswa (scholarship), dekan (dean),
dokumen (document), fasilitas (facilities), infrastruktur
(infrastructure), karir (placement), kepala jurusan (head of
department), kepala prodi (head of study program), komite
(committee), lokasi (location), nomor (number), olahraga
(sports), pelonco (ragging), perpustakaan (library), pmb
(college intake), rektor (rector), and sekretaris jurusan
(department secretary). Experimental results to evaluate
chatbot intent classification models using the RoBERTa
method show an accuracy of 0.84, as shown in Fig. 4.
Fig. 4. Metric Measurement for Each Intent in RoBERTa
C. Comparison Results of BERT and RoBERTa
Table VII shows that the BERT model achieved an
accuracy of 0.89, compared to 0.84 for the RoBERTa model.
This suggests that BERT provides better accuracy than
RoBERTa when using the same Indonesian language dataset
and hyperparameters. Several factors may contribute to
BERT's superior performance. First, the characteristics and
relatively small size of the dataset might make it more
suitable for the BERT model, reducing the risk of overfitting.
Second, hyperparameter tuning plays a crucial role, as
different hyperparameter configurations could lead to better
performance for BERT on this particular dataset.
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

TABLE VII. COMPARISON RESULT OF BERT AND ROBERTA
Component BERT RoBERTa
Accuracy 0.89 0.84
Precision 0.89 0.77
Recall 0.88 0.80
F1-Score 0.87 0.77
IV. CONCLUSIONS
The comparison of experimental results for intent
classification in chatbots using BERT and RoBERTa models
leads to the following conclusions. Both BERT and RoBERTa
can be effectively used for intent classification in Indonesian
chatbots. After translating the University Chatbot Dataset into
Bahasa Indonesian, the BERT model achieved an accuracy of
0.89, outperforming the RoBERTa model, which achieved
0.84. This indicates that the BERT model provides superior
accuracy compared to RoBERTa when using the same
Indonesian dataset and identical hyperparameters. In the
future, we plan to experiment with the IndoBERT model for
chatbot intent classification, specifically using Bahasa
Indonesia datasets and larger datasets. This will help us further
evaluate whether BERT consistently outperforms RoBERTa
in chatbot intent classification tasks. These will be the next
major works.
ACKNOWLEDGMENT
This research is funded by the Directorate of Research,
Technology, and Community Service under the Directorate
General of Higher Education, Research, and Technology, in
accordance with the contract for the implementation of the
State Higher Education Operational Assistance Program
(Research Program) for the 2024 fiscal year, Contract
Number: 090/E5/PG.02.00.PL/2024.
REFERENCES
[1] N. Shahin and L. Ismail, “From Rule-Based Models to Deep
Learning Transformers Architectures for Natural Language
Processing and Sign Language Translation Systems: Survey,
Taxonomy and Performance Evaluation,” 2024, arXiv. doi:
10.48550/ARXIV.2408.14825.
[2] L. Villa, D. Carneros-Prado, A. Sánchez-Miguel, C. C. Dobrescu,
and R. Hervás, “Conversational Agent Development Through Large
Language Models: Approach with GPT,” in Proceedings of the 15th
International Conference on Ubiquitous Computing & Ambient
Intelligence (UCAmI 2023), vol. 835, J. Bravo and G. Urzáiz, Eds.,
in Lecture Notes in Networks and Systems, vol. 835. , Cham:
Springer Nature Switzerland, 2023, pp. 286–297. doi: 10.1007/978-
3-031-48306-6_29.
[3] D. Griol, Z. Callejas, J. M. Molina, and A. Sanchis, “Adaptive
dialogue management using intent clustering and fuzzy rules,”
Expert Systems, vol. 38, no. 1, p. e12630, Jan. 2021, doi:
10.1111/exsy.12630.
[4] W. Maeng and J. Lee, “Designing a Chatbot for Survivors of Sexual
Violence: Exploratory Study for Hybrid Approach Combining Rule-
based Chatbot and ML-based Chatbot,” in Asian CHI Symposium
2021, Yokohama Japan: ACM, May 2021, pp. 160–166. doi:
10.1145/3429360.3468203.
[5] A. Birim and M. Erden, “Robustness to Spelling Errors for Intent
Detection,” in 2022 30th Signal Processing and Communications
Applications Conference (SIU), Safranbolu, Turkey: IEEE, May
2022, pp. 1–4. doi: 10.1109/SIU55565.2022.9864722.
[6] N. Boudjani, V. Colas, and A. Fotouhi, “Intent Classification: French
Recruitment Chatbot Use Case,” in 2023 International Conference
on Computational Science and Computational Intelligence (CSCI),
Las Vegas, NV, USA: IEEE, Dec. 2023, pp. 681–685. doi:
10.1109/CSCI62032.2023.00117.
ed on January 30,2026 at 09:56:42 UTC from IEEE Xplore. Restrictions apply.

=== Page 6 ===
[7] J.-H. Lee, E. H.-K. Wu, Y.-Y. Ou, Y.-C. Lee, C.-H. Lee, and C.-R.
Chung, “Anti-Drugs Chatbot: Chinese BERT-Based Cognitive
Intent Analysis,” IEEE Trans. Comput. Soc. Syst., vol. 11, no. 1, pp.
514–521, Feb. 2023, doi: 10.1109/TCSS.2023.3238477.
[8] F. Roma, G. Sansonetti, G. D’Aniello, and A. Micarelli, “A BERT-
Based Approach to Intent Recognition,” in IEEE EUROCON 2023 -
20th International Conference on Smart Technologies, Torino, Italy:
IEEE, Jul. 2023, pp. 568–572. doi:
10.1109/EUROCON56442.2023.10198959.
[9] S. Sayenju et al., “Quantification and Mitigation of Directional
Pairwise Class Confusion Bias in a Chatbot Intent Classification
Model,” Int. J. Semantic Computing, vol. 16, no. 04, pp. 497–520,
Dec. 2022, doi: 10.1142/S1793351X22500040.
[10] Y. Guo et al., “ESIE-BERT: Enriching Sub-words Information
Explicitly with BERT for Joint Intent Classification and SlotFilling,”
Feb. 02, 2023, arXiv: arXiv:2211.14829. Accessed: Sep. 02, 2024.
[Online]. Available: http://arxiv.org/abs/2211.14829
[11] Y. Liu et al., “RoBERTa: A Robustly Optimized BERT Pretraining
Approach,” Jul. 26, 2019, arXiv: arXiv:1907.11692. Accessed: Mar.
13, 2024. [Online]. Available: http://arxiv.org/abs/1907.11692
[12] A. Souha, C. Ouaddi, L. Benaddi, and A. Jakimi, “Pre-Trained
Models for Intent Classification in Chatbot: Comparative Study and
Critical Analysis,” in 2023 6th International Conference on
Advanced Communication Technologies and Networking
(CommNet), Rabat, Morocco: IEEE, Dec. 2023, pp. 1–6. doi:
10.1109/CommNet60167.2023.10365312.
[13] K. K. Jayanth, G. Bharathi Mohan, R. P. Kumar, and M. Rithani,
“Intent Recognition Leveraging XLM-RoBERTa for Effective
NLU,” in 2024 3rd International Conference on Applied Artificial
Intelligence and Computing (ICAAIC), Salem, India: IEEE, Jun.
2024, pp. 877–882. doi: 10.1109/ICAAIC60222.2024.10575275.
[14] Rohim and Zuliarso, “Penerapan Algoritma Deep Learning Untuk
Pengembangan Chatbot Yang Digunakan Untuk Konsultasi Dan
Pengenalan Tentang Virus Covid-19,” PIXEL, vol. 15, no. 2, pp.
267–278, Dec. 2022, doi: 10.51903/pixel.v15i2.777.
[15] R. C. Hutama, F. Fauziah, and R. T. Komalasari, “Aplikasi Chatbot
Berbasis Teks Menggunakan Algoritma Naive Bayes Classifier FAQ
GrabAds,” STRING, vol. 6, no. 1, p. 90, Aug. 2021, doi:
10.30998/string.v6i1.9919.
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

[16] D. Theosaksomo and D. H. Widyantoro, “Conversational
Recommender System Chatbot Based on Functional Requirement,”
in 2019 IEEE 13th International Conference on Telecommunication
Systems, Services, and Applications (TSSA), Bali, Indonesia: IEEE,
Oct. 2019, pp. 154–159. doi: 10.1109/TSSA48701.2019.8985467.
[17] M. Y. Helmi Setyawan, R. M. Awangga, and S. R. Efendi,
“Comparison Of Multinomial Naive Bayes Algorithm And Logistic
Regression For Intent Classification In Chatbot,” in 2018
International Conference on Applied Engineering (ICAE), Batam:
IEEE, Oct. 2018, pp. 1–5. doi: 10.1109/INCAE.2018.8579372.
[18] M. Menda and G. S. Keerthi, “Intent Classification in Conversational
System using Machine Learning Techniques,” IJCA, vol. 183, no.
51, pp. 6–11, Feb. 2022, doi: 10.5120/ijca2022921913.
[19] C. A. Oktavia, “Implementasi Chatbot Menggunakan Dialogflow
dan Messenger Untuk Layanan Customer Service Pada E-
Commerce,” JIMP, vol. 4, no. 3, Jan. 2020, doi:
10.37438/jimp.v4i3.230.
[20] D. Liu, Z. Zhao, and L.-D. Gan, “Intention Detection Based On Bert-
Bilstm in Taskoriented Dialogue System,” in 2019 16th
International Computer Conference on Wavelet Active Media
Technology and Information Processing, Chengdu, China: IEEE,
Dec. 2019, pp. 187–191. doi:
10.1109/ICCWAMTIP47768.2019.9067660.
[21] Nirali Vaghani, “Chatbot dataset.” Kaggle. doi:
10.34740/KAGGLE/DSV/5024271.
[22] DeepL, “DeepL Translator.” [Online]. Available:
https://www.deepl.com/
[23] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-
training of Deep Bidirectional Transformers for Language
Understanding,” May 24, 2019, arXiv: arXiv:1810.04805. Accessed:
Feb. 28, 2024. [Online]. Available: http://arxiv.org/abs/1810.04805
[24] J. H. Tandijaya and I. Sugiarto, “Klasifikasi dalam Pembuatan Portal
Berita Online dengan Menggunakan Metode BERT,” vol. Vol 9, No
2 (2021), 2021.
[25] R. Khusuma, W. Maharani, and P. H. Gani, “Personality Detection
On Twitter User With RoBERTa,” Jurnal Media Informatika
Budidarma, vol. 7, 2023.
ed on January 30,2026 at 09:56:42 UTC from IEEE Xplore. Restrictions apply.

Paper:Comparison_Of_Multinomial_Naive_Bayes_Algorithm_And_Logistic_Regression_For_Intent_Classification_In_Chatbot.pdf
=== Page 1 ===
Comparison Of Multi
Algorithm And Logistic
Classification
Muhammad Yusril Helmi Setyawan Rolly Maula
Applied Bachelor Program of Applied Bache
Informatics Engineering Politeknik Pos Informatics Enginee
Indonesia Indon
Bandung, Indonesia Bandung,
yusrilhelmi@poltekpos.ac.id rolly@aw
Abstract— Chatbot is software that communicates using
natural language. chatbots such as machine conversation
systems, Chatterbot, virtual agents, and dialogue systems. This
software enables to simulate human conversations. In this
research, the chatbot system that will be created must be able
to understand the natural language of what is entered by the
user, and the chatbot will answer according to what the user is
expecting. The researcher proposes a classification method to
identify intent rather than user input or called intent
classification on the chatbot system; the researcher also wants
to know the level of accuracy, precision, and recall on the
evaluation results of both methods. The classification method
applied in this research is the Naive Bayes method and
compared with the Logistic Regression method to determine
the class intention. The evaluation results show the level of
accuracy precision and recall in the Logistic Regression model
is higher than the Naive Bayes model.
Keywords : Chatbot,Intent Classification, Naive Bayes, Logistic
Regression.
I. INTRODUCTION
Internet chat is a popular application that allows text-
based communication. Some people around the world use
internet chat to exchange messages and discuss various
topics online [1]. At present, it is essential for any company
to have the infrastructure and services to listen to a social
media platform, whether it's Twitter, Facebook, Messenger,
e-mail for company applications, because most customers
used social media to find information about companies
because they need help with products or services. Companies
must build a platform where customer service representatives
reach social media users, to ensure that users get the
information they need [2]. To reach out to the community in
the social stream researchers propose to develop reporting
applications on the chatbot platform for community
reporting. This software is planned to responds common
questions in a particular domain [3].
Chatbot system is a program which interacts with users.
These interactions between users and the system are using
natural languages, for example: machine conversation
systems, virtual agents, dialogue systems, and Chatterbot.
The purpose of this system is to simulate users
conversations. Chatbot architecture is using language models
and computational algorithms to mimic informal
communication and interaction between users and computers
using natural language processing [4]. On [5], the author
suggests a conversation based on natural language
understanding. Messages sent using chatbot are processed
978-1-5386-8066-7/18/$31.00 ©2018 IEEE
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

inomial Naive Bayes
c Regression For Intent
on In Chatbot
ana Awangga Safif Rafi Efendi
elor Program of Applied Bachelor Program of
ering Politeknik Pos Informatics Engineering Politeknik Pos
nesia Indonesia
Indonesia Bandung, Indonesia
wang.ga esafif637@gmail.com
using Natural Language Processing techniques. [6]. NLP
provides ways for users to communicate with computers
using natural language. To understanding natural language
there are three analyzes. These analyses are: decomposition,
semantic interpretation, and knowledge-based structures [7].
Developments in knowledge management for the
development of an intent management system on chatbot
based on ontology [8][9]. and the evaluation process will
produce performance values for each classification method
applied [10].
A chatbot is a trending application created by Artificial
Intelligence. It is used in humanoid robots, personal
assistants, car assistance, etc., to facilitate human work [11].
The system can understand natural language that is input by
the user, in this study the authors apply the classification
method to be able to understand the text of the user, the
researcher proposes to use classification to determine intent
classification so that the system can provide answers
according to intent classification.
II. RELATED WORK
There are several text classifications; one of them is
Naive Bayes. This classification is popular among the
researchers for text categorizations. Naive Bayes
classification is simple and efficient. Naive Bayes is a
model-based classification method and offers to compete for
classification performance for text categorization compared
[12]. The Naive Bayes classification algorithm can be used
widely in many cases because it has high efficiency and easy
implementation. [13] This Bayesian classification is used as
a probability learning method, and each feature of the
algorithm that is classified is not dependent on the value of
other features [14]. Naive Bayesian Chatbot is a simple
classifier to find intent classification based on the application
of Bayesian theorem with independent assumptions.[15] .
Classification algorithms require data that has been trained
and arranged into several classes. Naive Bayes requires a
short time to build the model [16].
The Naive Bayes algorithm can be either adaptive and
intelligent, and can be a function, but also fulfills
personalized requirements., and therefore are wide or
extensively used in commercial[17]. Naive Bayes is a simple
technique for building classifiers: models that classify the
problem instance, are described as feature value vectors,
where class labels are taken from a limited number of
circuits [18]. This machine learning can provide accurate
results that are very efficient in use. Naive Bayes
classification algorithm is a simple and straightforward
ed on January 30,2026 at 09:54:37 UTC from IEEE Xplore. Restrictions apply.

=== Page 2 ===
structure algorithm, which has a class node [19]. Naive
Bayesian is efficient in the term for time, CPU usage
(Central Processing Unit) and memory. Naive Bayes can
perform well even with small training sets and less extensive
computing. [20]. Naive Bayes Classifier belongs to a family
of simple probability analyzers based on the Bayes theorem
and has an independent assumption that the value of specific
features is always different from the value of other features
[21]. This method is one of the supervised learning based on
Bayesian rules on the statistical theory, which runs in the
example of labeling training, and is given by a strong
assumption that all attributes in the training data are
interdependent, which is called the Bayes assumption [22].
Naive Bayes one of the popular machines, an interesting
framework in various tasks and reasonable performance is
obtained in the task even though this learning is based on
nonrealistic assumptions of independence. [23]. The
Bayesian method is the most practical learning method for
multiparameter by calculating the probability. This is very
competitive compared to other methods for learning
technique [24]. Bayesian is a statistical classification. Naive
Bayes Classification is based on the Bayes Theorem which
utilizes conditional probabilities to classify data into
predetermined classes. This method is called "naive" because
of independence assumption between various attribute values
[25]. Naive Bayes method has been applied in various fields,
especially in natural language processing and bioinformatics,
including the classification of genre texts and author
attributions, sentiment analysis, disease prediction from
genomic data. Naive Bayes method is called "naive" because
it assumes that all features are independent, depending on the
class label [26].
Logistic regression can be considered as a general linear
regression model. This type of logistic regression allows us
to test the effect of numerical factors on binary responses
[27]. Regression focuses on the relationship between the
dependent variable called (Y) and one or more independent
variables (x0, x1, ..., xn). In linear regression, 'Y' is an
advanced value while logistic regression has a discrete value.
The logistical function, also known as the sigmoid function,
is used to calculate the logistic model in which each value
from negative infinity to positive infinity is provided as
limited input and output in the range of 0 and 1 [28]. Logistic
regression can be used in machine learning applications. This
algorithm can understand vector variables and evaluate the
coefficients or weights for each input variable and then
prediction the class expressed the value of the word vector
[29]. Logistic regression is a technique used to study data
sets where there are one or more independent variables that
know the results [30].
III. RESEARCH METHOD
In this study, the researcher compare two models with
different methods with the steps in Figure 1 to determine the
intent classification of the chatbot system to be built
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

Figure 1. Proposed Evaluate Model
Retrieve the data in the study was reading the training
data that had been labeled in the class intent that had been
done by the researcher, and the researcher made the class as
follows greet, report, info, point, trade-point, and thanks. In
the training data, some data is taken to be used as a data test
to test the level of accuracy and performance of each model
at the evaluation stage. After retrieving the data the
researcher processes the TF-IDF, TF is the feature items that
appear in the document, IDF is the anti-document frequency,
TF-IDF is calculated using the following formula [28].
ij ij i ….(1)
Formula 1. Equation TF-IDF
After being transformed using TF-IDF then doing the
data sharing stage into training data and test data. in the
training data section, the model was made by applying the
Naive Bayes method and Logistic Regression to detection of
classification. In making the model applied the Naive Bayes
method with the following formula [18]:
P( |) ..,(i=1,2…n)…(2)
Ci e
Formula 2. Equation Naïve Bayes Method
1. Find the previous probability class Ci.
2. According to the prior probability P (Ci), the posterior
probability P (Ci | e) is obtained by the Bayesian formula.
3. According to the posterior probability, the test text is
classified as a category with the highest probability value.
Likewise with the making of the model applied to the
Logistic Regression method, logistic regression is widely
used in machine learning model applications. This algorithm
understands the vector variables and evaluates the
coefficients or weights for each input variable and then
prediction to expressed as a word vector [29]. Look for
mathematical logistic regression functions estimating
multiple linear functions defined as [26]:
ed on January 30,2026 at 09:54:37 UTC from IEEE Xplore. Restrictions apply.

=== Page 3 ===
Logit(s) = b +bM +b M+bM…..bM…(3)
0 1 1 2 2 3 3 k k
Formula 3. Equation Logistic Regression Method
S is the probability of the presence of an attractive
feature. M1, M2 ... Mk is the Predictor value and b0, b1...bk
is the intercept of the model. The assumptions for Logistic
regression are as follows [29]:
1. A linear relationship between dependent and independent
variables does not exist in Logistic Regression.
2. The dependent variable must be a dichotomy.
3. Independent variables must be linearly related; it is not
usually distribution or the same variance in a group.
4. Grouping must be mutually exclusive.
After getting the classifier model created using training
data, the next step is to evaluate the data using test data taken
by 20% of the training data [29]. at the time of retrieving the
data, by predicting test data on training data in both methods,
it produces an evaluation in the form of accuracy, precision,
and recall value by using a confusion matrix. To calculate the
confusion matrix using the following equation[29] .
Table 1. Confusion Matrix.
PREDICT VALUE TRUE VALUE
TRUE FALSE
TRUE TRUE FALSE
POSITIVE POSITIVE
(TP) (FP)
FALSE FALSE TRUE
NEGATIVE NEGATIVE
(FN) (TN)
To determine the precision, recall, and accuracy values
as follows:
1. Precision = TP / TP + FP
2. Recall = TP / TP + FN
3. Accuracy = TP + TN / TP + TN + FP + FN
After doing all these steps, the research can be
concluded from the two methods that affect the accuracy
of predictions in each model.
IV. EXPERIMENT
Experiment in this study is the application of the Naive
Bayes method that is compared using the Logistic
Regression method applied to the chatbot, the training data in
this study is data taken from reports that have been reported
by the public, then the authors take some data samples and
determine the class intention used as training data. To predict
the text entered by the user the author uses the Naive Bayes
method which is compared with the Logistic Regression
method to determine the performance of the two methods in
determining the Intent Classification there are several class
intentions determined by the author namely greet, report,
info, point, swap_point and thanks The author uses the
library in the Python language to implement both methods to
predict incoming text from chat in the classification that has
been determined by the author as much as 55 data in training
data, which are classified in the table as follows:
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

Table 2. Data Training.
No Class Value
1 Report 14
2 Greet 10
3 Info 9
4 Point 8
5 Thanks 7
6 Tukar_point 7
Sum Of Data 55
After the data is labeled in the training data, the next step
is to do the TF-IDF process to calculate and weight the text
that will be predicted before entering the model. At this step
the text is evaluated using test data, referring to previous
research to evaluate the model, extracted some data training
of 10% - 50% for test data [29]. In this experiment was taken
20% of the training data for the test data, because of the
limited amount of data. Calculating TF-IDF is explained in
the proposed model and produces the following data in an
experiment using the Python programming language[32]. to
calculate data.
Table 3. Result Tf-Idf
Word Calculate Tf-Idf
Kerusakan 0.527109669519
Nasution 0.527109669519
Jalan 0.438289146485
Di 0.386332471216
Ada 0.320874801684
After obtaining the training data and test data, the next
step makes a model using the Naive Bayes model and
Logistic Regression where to calculate the method using the
equations (2) and (3), the model classifies training data, in
this experiment the author uses libraries in python language
to implement both methods that.
The next step evaluates the predictions of the test data on
training data that has been obtained by the model of the two
methods by using the Confusion Matrix. For the equation of
the Coffusion Matrix, it has been explained in the proposed
model. In this experiment, the author predicts the test data
taken from training data on the model created and calculated
it with the Python programming language[32]. From the
evaluation results with the Coffusion Matrix, the accuracy
results on Naive Bayes are 0.6363636363636364, and
Logistic Regression is 0.7272727272727273 which results in
the evaluation data as follows:
ed on January 30,2026 at 09:54:37 UTC from IEEE Xplore. Restrictions apply.

=== Page 4 ===
Figure 3. Evaluate Naive Bayes Model
Figure 4. Evaluate Logistic Regression Model
V. RESULT AND ANALYSIS
From the evaluation results of the experimental
classification model to determine the intent classification on
chatbot, the accuracy of the Naive Bayes model is
0.6363636363636364, and the Logistic Regression model is
0.7272727272727273. This shows that there is an accuracy
distance between the two models of 0.0909090909090909
or a classification model with logistic regression which is
more accurate at 12.5%. In the evaluation results of the
experiment also obtained the value of precision and recall.
Logistic Regression model produces data on the average
total precision of 0.59 and recall of 0.73 while the Naive
Bayes method produces data on the average precision of
0.53 and recall of 0.64.
VI. DISCUSSION
In this study, researchers still use training data with
a reasonably limited amount, the amount of data is not the
same in each class of intent, so it is possible that errors will
occur when predicting class intent with less training data.
The accuracy of the two methods will experience
differences in accuracy distance when the training data for
each intent class has the same amount of data in each class.
The researcher found a decrease in the accuracy of the
logistic regression model when the intent class had the same
amount of data in each class.
VII. CONCLUSION
From the experiments results to determine the class
intention obtained the following conclusions::
1. The Naive Bayes classification method or the logistic
system can be used for the chatbot system.
2. The model using the Logistic Regression method shows a
higher level of accuracy and higher value of precision
compared to the Naive Bayes method. This experiment
proves that the performance of the Logistics Regression
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

model has a better performance compared to the Naive
Bayes model.
VIII. REFERENCES
[1]. S. Ghose and J. J. Barua, “Toward the
implementation of a topic-specific dialogue based
natural language chatbot as an undergraduate
advisor,” in 2013 International Conference
on Informatics, Electronics and Vision (ICIEV).
IEEE, may 2013. [Online]. Available:
https://doi.org/10.1109/iciev.2013.6572650.
[2]. N. Thomas, “An e-business chatbot using aiml and
lsa,” in Advances in Computing, Communications,
and Informatics (ICACCI), 2016 International
Conference on. IEEE, 2016, pp.
2740–2742.
[3]. M. N. Kumar, P. C. L. Chandar, A. V. Prasad, and
K. Sumangali, “Android based
educational chatbot for visually impaired people,”
in 2016 IEEE International Conference
on Computational Intelligence and Computing
Research (ICCIC). IEEE, Dec 2016. [Online].
Available:
https://doi.org/10.1109/iccic.2016.7919664.
[4]. G. M. D’silva, S. Thakare, S. More, and J.
Kuriakose, “Real world smart chatbot for customer
care using a software as a service (saas)
architecture,” in I-SMAC (IoT in Social, Mobile,
Analytics, and Cloud)(I-SMAC), 2017 International
Conference on. IEEE, 2017, pp. 658–
664.
[5]. C. J. Baby, F. A. Khan, and J. Swathi, “Home
automation using IoT and a chatbot using
natural language processing,” in Power and
Advanced Computing Technologies (i-PACT),
2017 Innovations in. IEEE, 2017, pp. 1–6.
[6]. B. Setiaji and F. W. Wibowo, “Chatbot using
knowledge in database: Human-to-machine
conversation modeling,” in Intelligent Systems,
Modelling, and Simulation (ISMS), 2016 7th
International Conference on. IEEE, 2016, pp. 72–
77.
[7]. O. Efraim, V. Maraev, and J. Rodrigues, “Boosting
a rule-based chatbot using statistics
and user satisfaction ratings,” in Conference on
Artificial Intelligence and Natural Language.
Springer, 2017, pp. 27–41.
[8]. Setyawan, Muhammad Yusril Helmi, Rolly
Maulana Awangga, and Rezka Afriyanti. "Analysis
and Design of Feature Application Setting
Dashboard on Svara Applications Using Ucd
Method (User-Centred Design) at PT. Zamrud
Khatulistiwa Technology." TELKOMNIKA
(Telecommunication Computing Electronics and
Control) 17.1 2018.
[9]. Awangga, Rolly Maulana, Muhammad Yusril, and
Helmi Setyawan. "Ontology Design of Influential
People Identification Using Centrality." Journal of
Physics: Conference Series. Vol. 1007. No. 1. IOP
Publishing, 2018.
[10]. Awangga, R. M. "Sampeu: Servicing Web Map
Tile Service over Web Map Service to Increase
Computation Performance." IOP Conference
Series: Earth and Environmental Science. Vol. 145.
No. 1. IOP Publishing, 2018.
[11]. B. Tang, S. Kay, and H. He, “Toward optimal
feature selection in naive Bayes for text category
ration,” IEEE transactions on knowledge and data
ed on January 30,2026 at 09:54:37 UTC from IEEE Xplore. Restrictions apply.

=== Page 5 ===
engineering, vol. 28, no. 9, pp. 2508–2521,
2016.
[12]. P. Liu, H. Yu, T. Xu, and C. Lan, “Research on
archives text classification based
on naive Bayes,” in 2017 IEEE 2nd Information
Technology, Networking, Electronic
and Automation Control Conference (ITNEC).
IEEE, Dec 2017. [Online]. Available:
https://doi.org/10.1109/itnec.2017.8284934
[13]. S. R. Gomes, S. G. Saroar, M. M. A. Telot, B. N.
Khan, A. Chakrabarty, and M. M. Mostakim,
“A comparative approach to email classification
using naive Bayes classifier and hidden Markov
model.”
[14]. Y. An, S. Sun, and S. Wang, “Naive Bayes
classifiers for music emotion classification based
on lyrics,” in Computer and Information Science
(ICIS), 2017 IEEE/ACIS 16th International
Conference on. IEEE, 2017, pp. 635–638.
[15]. M. D. N. Arusada, N. A. S. Putri, and A.
Alamsyah, “Training data optimization strategy
for multiclass text classification,” in Information
and Communication Technology (ICoIC7),
2017 5th International Conference on. IEEE, 2017,
pp. 1–5.
[16]. L. Li and C. Li, “Research and improvement of a
spam filter based on naive Bayes,” in Intelli
gent Human-Machine Systems and Cybernetics
(IHMSC), 2015 7th International Conference
on, vol. 2. IEEE, 2015, pp. 361–364.
[17]. M. Granik and V. Mesyura, “Fake news detection
using naive Bayes classifier,” in Electrical
and Computer Engineering (UKRCON), 2017
IEEE First Ukraine Conference on. IEEE,
2017, pp. 900–903.
[18]. M. Shafiq, X. Yu, and A. A. Laghari, “Wechat text
messages service flow traffic classifica
tion using machine learning technique,” in IT
Convergence and Security (ICITCS), 2016 6th
International Conference on. IEEE, 2016, pp. 1–5.
[19]. A. Rahman and U. Qamar, “A bayesian classifiers
based combination model for automatic
text classification,” in Software Engineering and
Service Science (ICSESS), 2016 7th IEEE
International Conference on. IEEE, 2016, pp. 63–
67.
[20]. N. Sharma and M. Singh, “Modifying naive bayes
classifier for multinomial text classification,”
in Recent Advances and Innovations in
Engineering (ICRAIE), 2016 International
Conference
on. IEEE, 2016, pp. 1–7.
[21]. X.-R. Yu, Z.-L. Xiang, and D.-K. Kang,
“Classification of chinese-to-english translated
social
network timelines using naive bayes,” in
Advanced Communication Technology (ICACT),
2015
17th International Conference on. IEEE, 2015,
pp. 296–299.
[22]. S.-B. Kim, K.-S. Han, H.-C. Rim, and S. H.
Myaeng, “Some effective techniques for naive
bayes
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

text classification,” IEEE transactions on
knowledge and data engineering, vol. 18, no. 11,
pp.
1457–1466, 2006.
[23]. E. Pramunanto, S. Sumpeno, and R. S. Legowo,
“Classification of hand gesture in
indonesian sign language system using naive
bayes,” in 2017 International Seminar on
Sensors, Instrumentation, Measurement and
Metrology (ISSIMM). IEEE, aug 2017. [Online].
[24]. F. Burdi, A. H. Setianingrum, and N. Hakiem,
“Application of the naive bayes method to a
decision support system to provide discounts (case
study: Pt. bina usaha teknik),” in Information and
Communication Technology for The Muslim World
(ICT4M), 2016 6th International
Conference on. IEEE, 2016, pp. 281–285.
[25]. L. Sayfullina, E. Eirola, D. Komashinsky, P.
Palumbo, Y. Miche, A. Lendasse, and
J. Karhunen, “Efficient detection of zero-day
android malware using normalized bernoulli
naive bayes,” in Trustcom/BigDataSE/ISPA, 2015
IEEE, vol. 1. IEEE, 2015, pp. 198–205.
[26]. B. Pavlyshenko, “Machine learning, linear and
bayesian models for logistic regression in failure
detection problems,” arXiv preprint
arXiv:1612.05740, 2016.
[27]. J. Isaac and S. Harikumar, “Logistic regression
within dbms,” in Contemporary Computing
and Informatics (IC3I), 2016 2nd International
Conference on. IEEE, 2016, pp. 661–666.
[28]. A. Prabhat and V. Khullar, “Sentiment
classification on big data using na¨ıve bayes and
logistic regression,” in Computer Communication
and Informatics (ICCCI), 2017 International
Conference on. IEEE, 2017, pp. 1–5.
[29]. C. Prathibhamol, K. Jyothy, and B. Noora, “Multi
label classification based on logistic regression
(mlc-lr),” in Advances in Computing,
Communications and Informatics (ICACCI),
2016 International Conference on. IEEE, 2016, pp.
2708–2712.
[30]. A. Guo and T. Yang, “Research and improvement
of feature words weight based on tfidf
algorithm,” in Information Technology,
Networking, Electronic and Automation Control
Conference, IEEE. IEEE, 2016, pp. 415–419.
[31]. W. Ramadhan, S. A. Novianty, and S. C.
Setianingsih, “Sentiment analysis using
multinomial logistic regression,” in Control,
Electronics, Renewable Energy and
Communications
(ICCREC), 2017 International Conference on.
IEEE, 2017, pp. 46–49. Available:
https://doi.org/10.1109/issimm.2017.8124288
[32]. Awangga, R. M. "Peuyeum: A Geospatial URL
Encrypted Web Framework Using Advance
Encryption Standard-Cipher Block Chaining
Mode." IOP Conference Series: Earth and
Environmental Science. Vol. 145. No. 1. IOP
Publishing, 2018.
ed on January 30,2026 at 09:54:37 UTC from IEEE Xplore. Restrictions apply.

Paper:Integrating_Model-Agnostic_Meta-Learning_with_Advanced_Language_Embeddings_for_Few-Shot_Intent_Classification.pdf
=== Page 1 ===
2024 32nd International Confere
Integrating Model-Agno
Advanced Language Em
Intent Cla
Ali Rahimi
College of Interdisciplinary Science and Technologies
University of Tehran
Tehran, Iran
ali.rahimi97@ut.ac.ir
Abstract—Addressing the challenge of few-shot learning in
intent classification tasks within Natural Language Processing
(NLP),thisstudyintroducesanovelapproachthatharnessesthe
robust adaptation capabilities of Model-Agnostic Meta-Learning
(MAML) combined with sophisticated language embeddings,
namely BERT, LaBSE, and text-embedding-ada-002. The need
for models to understand and classify intents with minimal
training data is imperative to progress in creating versatile,
responsiveAIsystems.Weproposeamethodologythatleverages
the generalizability of MAML and the deeply contextualized
representations offered by state-of-the-art embeddings, allowing
for significant improvements in Accuracy and data efficiency.
We evaluate our approach using the CLINC150 dataset across
a series of N-way & K-shot configurations, demonstrating the
efficacy of the proposed model with varying numbers of in-
tent classes and examples. Our findings reveal that the text-
embedding-ada-002 embeddings consistently provide superior
performance in both 1-shot and 5-shot settings across all class
configurations tested, indicating their potent synergy with meta-
learningstrategies.Specifically,text-embedding-ada-002achieved
anaccuracyof97.07%inthe5-Way&1-Shotsettingand99.1%
in the 5-Way & 5-Shot setting. The outcomes of our exper-
imental evaluation suggest that our approach also illuminates
thepotentialofharmoniousintegrationofcutting-edgelanguage
embeddingswithmeta-learningframeworks.Thisworkprovides
a solid foundation for further exploration in optimizing few-
shotintentclassification,pavingthewayforcreatingAIsystems
proficientinunderstandinguserintentswithminimalexemplars.
This research lays the groundwork for future advancements in
few-shot intent classification, enabling the development of AI
systems that require minimal training data to interpret user
intent accurately.
Index Terms—Few-shot learning, Model-Agnostic Meta-
Learning (MAML), Intent classification, Natural Language Pro-
cessing (NLP), BERT, LaBSE, and Ada.
I. INTRODUCTION
Intheswiftlyevolvingdomainofnaturallanguageprocess-
ing (NLP), the challenge of intent classification has garnered
significant attention, particularly in the context of developing
This work was partially supported by ITRC (IRAN Telecommunication
ResearchCenter)
979-8-3503-7634-0/24/$31.00©2024IEEE
12976601.4202.14036EECI/9011.01
:IOD
|
EEEI
4202©
00.13$/42/0-4367-3053-8-979
|
)EECI(
gnireenignE
lacirtcelE
no
ecnerefnoC
lanoitanretnI
dn23
4202
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

ence on Electrical Engineering
ostic Meta-Learning with
mbeddings for Few-Shot
assification
Hadi Veisi
College of Interdisciplinary Science and Technologies
University of Tehran
Tehran, Iran
h.veisi@ut.ac.ir
conversationalagentsandintelligentsystemscapableofunder-
standing human intent with minimal exemplars. The conven-
tional paradigms for training such systems require extensive
datasets covering a comprehensive spectrum of potential in-
tents. However, the need for labeled data in many real-world
scenarios poses a daunting barrier, increasing interest in few-
shot learning approaches.
Few-shotlearningaimstoconstructlearningalgorithmsthat
gain proficiency with a minimal number of training examples.
Thispursuitalignscloselywiththeinherentlearningefficiency
exhibited by human learners. This methodology is vital in
intent classification tasks where acquiring and annotating vast
amounts of data for each possible intent can be impractical
or infeasible. MAML [1], a pioneering algorithm in the meta-
learning landscape, has demonstrated a remarkable capacity
to prepare models for quick adaptation to new tasks with
limited data. By training a model on many learning tasks,
MAML effectively instills learning versatility, enabling rapid
convergence to optimal performance on new tasks with only
a few gradient updates. Moreover, the emergence of sophis-
ticated language representations rooted in transformer-based
architectures, such as BERT (Bidirectional Encoder Repre-
sentationsfromTransformers)[2],LaBSE(Language-agnostic
BERT Sentence Embedding) [3], and text-embedding-ada-
0021 embeddings, has revolutionized our ability to capture
the semantic essence of language. These embeddings have
pushed the frontiers of NLP, offering deep, contextualized
representations that are transferable across various tasks, in-
cluding intent classification. In this work, we seek to bridge
the gap between the adaptability of MAML and the profound
representational power of state-of-the-art embedding tech-
niques. By amalgamating MAML with embeddings such as
BERT, LaBSE, and text-embedding-ada-002, we hypothesize
that we can facilitate a meta-learning framework that not only
adapts rapidly to new intents but also leverages the nuanced
understanding of language inherent in these embeddings. This
1https://platform.openai.com/docs/guides/embeddings/what-are-
embeddings
ed on January 30,2026 at 09:53:57 UTC from IEEE Xplore. Restrictions apply.

=== Page 2 ===
2024 32nd International Confere
integration promises to be incredibly potent in the context
of few-shot learning, potentially setting a new benchmark for
intent classification performance in low-resource settings.
Here, we detail our exploration into this synergy, with a
robust examination of MAML and its application to few-
shot learning in NLP. We then delve into the intricacies of
variousembeddingtechniques,analyzingtheiradvantagesand
compatibilitywithmeta-learningparadigms.Afterestablishing
the theoretical underpinnings, our paper presents an empirical
investigation where we implement and evaluate our combined
approach across diverse intent classification tasks. The results
aim not only to shed light on the viability of this hybrid ap-
proachbutalsotopavethewayforfutureresearchtrajectories
and practical applications that require efficient, scalable, and
intelligent systems adept at understanding human language
with minimal supervision.
Thus,thisworkintendstocontributetothebroaderresearch
discoursebydemonstratinghowtheconfluenceofMAMLand
advancedembeddingstrategiescanaddressacriticallimitation
in current NLP systems—achieving high levels of Accuracy
in intent classification with minimal available data.
In the ensuing discourse, we first delve into the Related
WorksinsectionII,surveyingthelandscapeoffew-shotlearn-
ing within NLP and examining the strides in meta-learning
frameworksandembeddingtechniquesthathaveinformedour
research direction. Following this, section III articulates the
intricate details of our experimental design, including dataset
configuration, model architecture, and the nuanced process of
embeddingthatweemployed,aswellasthefew-shotlearning
tasks, training details, and the optimization procedures we
adopted.SectionIVthenrecountstheactualapplicationofour
methodology, where we benchmark against baseline models,
delineate the few-shot learning configurations, and expound
uponthespecifictrainingdetailsbeforepresentingourresults.
In section V, we critically probe into our findings, dissecting
the implications and significance within the context of the
broader research field, culminating the paper encapsulates
the main takeaways, reassessing the contributions made, and
canvassing the potential avenues for future research based on
the work at hand.
II. RELATEDWORKS
The challenge of intent classification in natural language
processingismulti-faceted,requiringtheaccuratedeciphering
of user intents from textual inputs—a critical component in
developing intelligent dialogue systems. Significant strides
have been made with the advent of deep learning [4], but
the dependency on large annotated datasets remains a critical
bottleneck [5]. As such, a rich corpus of literature focuses on
overcoming data scarcity.
Few-shot Learning: Few-shot learning has been a com-
pelling answer to the data dilemma, where models are de-
signed to learn information from a few examples. Earlier
works primarily focused on metric-based approaches such as
Siamese Networks [6], Matching Networks [7], Prototypical
Networks [8], and Relation Networks [9]. However, our work
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

ence on Electrical Engineering
is more closely aligned with the meta-learning paradigm,
whichhasbeenthecenterofattentioninrecentstudies.Specif-
ically, MAML [1] has shown promising results in enabling
models to rapidly adapt to new tasks, with its application in
NLP being explored in studies like ”Domain generalization
via model-agnostic learning of semantic features” [10].
Meta-Learning: The genre of meta-learning, conceptualized
as ’learning to learn,’ has found a spectrum of uses in NLP,
yet its assimilation with intent classification remains partially
untapped.Meta-learningalgorithms,includingMAMLaswell
as developments like Simple Neural Attentive Meta-Learner
(SNAIL) [11], AVID [12], and Meta-SGD [13], establish
a protocol permitting models to assimilate and extrapolate
from limited samples of unseen tasks. This functionality is
advantageous in domains constrained by scant labeled data
[8] [9].
Embeddings in NLP: Embedding techniques have expe-
rienced a paradigm shift by introducing transformer-based
models like BERT [2], which offer deep contextualized word
representations. These embeddings have set new benchmarks
across various NLP tasks. LaBSE [3] extends this notion by
providing language-agnostic sentence embeddings facilitating
cross-lingual tasks. Furthermore, the text-embedding-ada-002
embedding paradigm introduces efficiencies and improve-
ments in adaptability within language models, aiding in the
fine-tuning process for specific tasks.
Combining Meta-Learning with Embeddings: While
MAML effectively deals with few-shot learning, combining it
with robust embeddings from transformer models presents a
novel exploration. Studies have begun integrating these ideas,
like [14], who showed the efficacy of utilizing BERT, ELMo
[15], and GloVe [16] within a meta-learning framework for
few-shot text classification.
Our work contributes to this emerging intersection by
integrating the robust adaptability of MAML with the rich,
context-aware representations of BERT, LaBSE, and text-
embedding-ada-002. By doing so, we aim to create a meta-
learningmodelthatlearnsquicklyfromnewintentsandencap-
sulatesadeeperunderstandingofsemanticsandtransferability
across languages. Such endeavor has the potential to benefit
theresearchcommunityandindustrypractitionersdealingwith
varied and sparse data environments, extending the reach of
automated dialog systems in multiple linguistic setups.
In summary, the literature reviewed highlights the impor-
tance of few-shot learning and meta-learning, particularly
MAML, in addressing data scarcity in intent classification.
Despite significant advances made by large pre-trained em-
beddings, the exploration of their combination with meta-
learning techniques like MAML for intent classification re-
mains nascent. Our work seeks to fill this gap, offering a
comprehensive study that will add a valuable perspective to
the field.
III. METHODOLOGY
This section provides a detailed description of our method-
ology, including the preprocessing of our dataset, the nuanced
ed on January 30,2026 at 09:53:57 UTC from IEEE Xplore. Restrictions apply.

=== Page 3 ===
2024 32nd International Confere
architecture of our model, and the criteria employed for
evaluation.
A. Dataset Configuration
The CLINC150 dataset [17], recognized for its diverse
intents,servesasthebenchmarkforourstudy.Comprising150
distinct classes, this dataset was bifurcated into two subsets:
120 classes were dedicated to meta-training (training the
modelonvariouslearningtasks).Incomparison,theremaining
30 classes were reserved for meta-testing (evaluating the
model’s ability to generalize to new tasks).
B. Model Architecture and Embedding Process
Our model’s architecture is spearheaded by transforming
sentences into high-dimensional embeddings, which function
as the subsequent Multi-Layer Perceptron (MLP) input. The
MLP consists of an input layer, a single hidden layer, and an
output layer that conducts N-way classification. The embed-
ding dimension directly corresponds to the input size of the
MLP, ensuring a harmonious transition from the embedding
to the classification process. “Fig. 1”, indicates our model
architecture.
Fig.1. ModelArchitecture
Across various experiments, the hidden layer architecture
wasassessedthroughthelensofdifferentactivationfunctions:
linear, sigmoid, and ReLU. The linear activation function
emerged as superior in Accuracy, making it our final model
design choice.
C. Optimization Procedure
Ourapproachutilizedthemeta-learningframeworkinherent
in MAML, which adopts a hierarchical learning rate structure
to facilitate a two-tiered optimization process. The optimiza-
tion was delineated into two distinct loops, each with its
strategic learning function.
In the outer loop, the learning mechanism was designed to
capture overarching patterns across multiple tasks. This meta-
optimization step was critical in updating the model’s initial
parameters, ensuring that the model acquired a generalized
prior that could effectively bootstrap learning for new tasks.
Conversely, the inner loop targeted rapid learning adapta-
tion, tuning the model to absorb task-specific details. Here,
the model engaged in several gradient descent steps, ensuring
it could swiftly acclimate to the particulars of each task. This
process was meticulously balanced to embody an efficient
computational demand while allowing for the nuanced con-
vergence required by the intricacies of the tasks.
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

ence on Electrical Engineering
Wesimplifiedthemodelarchitecturetoasinglehiddenlayer
within the MLP to facilitate a swifter optimization process.
This choice was instrumental in preventing the vanishing
gradient problem, particularly in deep networks when dealing
withthesparsedataregimescharacteristicoffew-shotlearning
environments. This simplification heightened the optimization
efficiency and supported the model’s ability to learn quickly
from limited data, a prominent aspect of few-shot learning
scenarios.
D. Query Set and Evaluation Metrics
The intent classification was subjected to rigorous evalu-
ation, using a query set consisting of 25 samples for each
intent to calculate the model’s Accuracy. The measurement
of Accuracy—the proportion of correctly predicted samples
in the query set—directly reflected the model’s few-shot
classification capabilities.
IV. EXPERIMENTSANDEVALUATIONS
Ourexperimentsaimedtoevaluateourmodel’sperformance
onthetaskoffew-shotintentclassificationusingdifferentem-
bedding techniques: BERT, LaBSE, and text-embedding-ada-
002. We focused on various ”N-way and K-shot” scenarios,
where ”N” represents the number of classes and ”K” is the
number of examples from each class during training.
A. Baseline Model
As a fundamental point of comparison, our experiments
utilize a Multi-layer Perceptron (MLP) with a single hidden
layer and linear activation function, excluding the application
ofMAMLtechniques.ThisbaselineMLPmodelwasselected
for its simplicity and transparency in illustrating the raw
learning capability in few-shot learning scenarios. Tailored to
accommodate input feature vectors of varying sizes aligned
with the embedding dimensions of the preprocessed data (768
for BERT and LaBSE, 1536 for text-embedding-ada-002), the
network architecture facilitates direct Feature-to-Class map-
ping. It comprises an input layer that takes the specified
embedding size, followed by a hidden layer whose number
of neurons was methodically matched to the complexity of
respective few-shot tasks, and an output layer that uses a
softmax activation to yield a probability distribution over N
classes.
Thissingle-hidden-layerMLPisdesignedtoestablishbase-
line performance without the benefits of complex transfor-
mations or meta-learning enhancements. The choice of a
linear activation function is deliberate, striving to lay bare
the models’ inherent discriminative power absent of non-
linearities. The output through the softmax function ensures
the generation of a probabilistic class membership prediction
for each input. By imposing such minimality, the baseline
model’s outcomes ground the discussion about the advantages
introduced by more sophisticated models and meta-learning
techniques later in our evaluations.
ed on January 30,2026 at 09:53:57 UTC from IEEE Xplore. Restrictions apply.

=== Page 4 ===
2024 32nd International Confere
B. Few-Shot Learning Configurations
We examined the adaptability and efficiency of our model
across an array of few-shot learning configurations, encom-
passing1-shotand5-shotscenarioscombinedwith5-way,10-
way,and20-wayclassificationchallenges.Thisvarietyensures
a comprehensive analysis of the model’s performance under
varying levels of task difficulty.
C. Training Details
In our experimentation, we meticulously crafted a training
regimen aimed at optimizing model performance for few-
shot intent classification tasks. Surprisingly, we observed that
a model with a single hidden layer outperformed its multi-
layered counterparts, sparking a point of significant interest.
Thisinsightpromptedustodelveintothetrainingprocessthat
led to this pivotal discovery.
Initially, our hypothesis presumed that deeper networks
would deliver superior performance due to their augmented
feature representation capacity. To our surprise, experimental
results consistently favored a single hidden layer MLP model
over those with multiple hidden layers. This peculiar finding
strongly suggests that model simplicity correlates positively
witheffectivegeneralizationandperformancewithinfew-shot
learning and intent classification.
Our MLP culminates with a softmax activation function,
aligning with the standard approach for multi-class classifi-
cation problems. This layer ensures the representation of a
probability distribution over the ’N-way’ classes, producing
a probability distribution between 0 and 1 for each input
sample, summing to 1 across the class labels. As a result,
the highest probability within this distribution serves as the
model’s predicted class.
Throughoutourexperiments,weexploredhiddenlayersizes
of 64, 128, 256, 384, 512, and 768 to understand their impact
on the model’s performance.
TheMLPmodel,usingMAML,wasoptimizedwitharange
of learning rates for adaptability (outer loop learning rates:
0.001,0.004,0.005,0.007;innerlooplearningrates:0.01and
0.03). We used 50 gradient steps for adaptation during the
inner loop of MAML. For activations in the hidden layer,
the linear function yielded the best Accuracy, which was
noteworthy given the nonlinear nature of the embeddings.
D. Optimization Configuration
In configuring our optimization, we honed the MLP model
with an array of learning rates to enhance its flexibility,
selecting outer loop rates at 0.001, 0.004, 0.005, and 0.007,
along with 0.01 and 0.03 for the inner loop. The adaptation
phase within MAML’s inner loop was conducted over 50
gradientadjustments.Interestingly,alinearfunctionforhidden
layer activations delivered the highest accuracy despite the
typicallynonlinearcharacteristicsoftheembeddingsinvolved.
Moreover, we adopted a streamlined network design fea-
turing just one hidden layer in the MLP to strike a balance
betweenrapidcomputationandthoroughtraining.Thissimpli-
fied architecture hastened the optimization phase and helped
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

ence on Electrical Engineering
avoid the vanishing gradient problem—a significant hurdle in
few-shot learning scenarios with sparse data.
E. Results
To interpret the performance of different embeddings in
few-shot learning tasks, we measured the Accuracy of both
baseline models and our proposed model across various task
configurations. The results are methodically organized in two
tables, delineating outcomes with and without the MAML
approach, providing a comparative view and insight into the
efficacy of meta-learning on model performance.
Table I encapsulates the Accuracy achieved by baseline
models utilizing BERT, LaBSE, and text-embedding-ada-002
embeddings in the absence of the MAML approach. The
models were assessed under 5-way 1-shot, 5-way 5-shot, 10-
way1-shot,10-way5-shot,20-way1-shot,and20-way5-shot
configurations, where a clear pattern emerges, indicating that
thetext-embedding-ada-002embeddingtendstoyieldsuperior
performance in more constrained (1-shot) scenarios.
TABLEI
ACCURACYINTASKSWITHOUTMAMLAPPROACH
TaskConfiguration BERT LaBSE text-embedding-ada-002
5-Way1-Shot 80.37% 85.27% 96.7%
5-Way5-Shot 95.17% 97.95% 98.9%
10-Way1-Shot 69.04% 76.8% 87.16%
10-Way5-Shot 92.3% 89.75% 93.4%
20-Way1-Shot 60.5% 50.73% 52.34%
20-Way5-Shot 87.16% 57.2% 55.0%
Average 80.76% 76.28% 80.58%
Conversely, Table II reports the accuracy improvements
invokedbyapplyingtheMAMLapproachtothesamemodels
and task configurations. The enhancements are particularly
noteworthyin1-shotscenarios,suggestingthatMAMLsignif-
icantly leverages minimal data to foster better generalization
capabilities in the models. The optimization technique has
a profound impact, with each embedding type exhibiting a
marked improvement.
TABLEII
ACCURACYINTASKSWITHMAMLAPPROACH
TaskConfiguration BERT LaBSE text-embedding-ada-002
5-Way1-Shot 89.75% 92.6% 97.07%
5-Way5-Shot 95.8% 98.0% 99.1%
10-Way1-Shot 83.4% 88.0% 94.04%
10-Way5-Shot 94.97% 96.6% 98.54%
20-Way1-Shot 78.0% 81.5% 90.67%
20-Way5-Shot 92.43% 91.5% 97.27%
Average 88.92% 91.12% 95.92%
V. CONCLUSIONANDDISCUSSION
Throughout all evaluated task configurations, the models
utilizing text-embedding-ada-002 embeddings have displayed
superiorAccuracytothoseemployingBERTandLaBSE.This
consistent outperformance underscores the prowess of text-
embedding-ada-002 embeddings in more effectively captur-
ing the nuances of sentence semantics crucial for few-shot
ed on January 30,2026 at 09:53:57 UTC from IEEE Xplore. Restrictions apply.

=== Page 5 ===
2024 32nd International Confere
learning. A distinct observation is the remarkable accuracy
improvement in the 5-shot scenarios, which demonstrates the
models’abilitytocapitalizeontheincrementinexampledata,
translating it into noteworthy enhancements in outcomes. We
can see the comparison of models in “Fig. 2”.
Fig.2. ComparisonofAverageEmbeddingsScores:WithoutMAMLvsWith
MAML
Further analysis reveals that with meticulous tuning, the
MLP, coupled with the MAML framework, shows an im-
pressive capacity to acclimate swiftly to novel tasks within
a few-shot learning paradigm. This adaptability is especially
pronounced with text-embedding-ada-002 embedding integra-
tion. Intriguingly, the unexpectedly high performance of the
linear activation function used within the MLP architecture
suggests a synergy with the embedding space that merits
deeper exploration. The implications of such results indicate
that the relationship between simplicity in activation and the
complexity of embeddings can profoundly affect the model’s
learning dynamics in few-shot settings.
Throughtheseexperiments,wehavesubstantiatedthatcom-
bining MAML with cutting-edge embeddings substantially
advances the field of few-shot intent classification. The text-
embedding-ada-002 embeddings, when paired with MAML,
present a significant improvement in model adaptability and
performance across various few-shot learning scenarios, il-
luminating a clear path for future research and practical
implementations.
REFERENCES
[1] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning
for fast adaptation of deep networks,” in Proceedings of the 34th
InternationalConferenceonMachineLearning,pp.1126–1135,PMLR,
2017. ISSN:2640-3498.
[2] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-
trainingofdeepbidirectionaltransformersforlanguageunderstanding,”
inProceedingsofthe2019ConferenceoftheNorthAmericanChapter
of the Association for Computational Linguistics: Human Language
Technologies,Volume1(LongandShortPapers)(J.Burstein,C.Doran,
and T. Solorio, eds.), pp. 4171–4186, Association for Computational
Linguistics,2019.
[3] F. Feng, Y. Yang, D. Cer, N. Arivazhagan, and W. Wang, “Language-
agnosticBERTsentenceembedding,”inProceedingsofthe60thAnnual
Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers) (S. Muresan, P. Nakov, and A. Villavicencio, eds.),
pp.878–891,AssociationforComputationalLinguistics,2022.
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

ence on Electrical Engineering
[4] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” vol. 521,
no.7553,pp.436–444,2015.
[5] J.Schmidhuber,“Deeplearninginneuralnetworks:Anoverview,”2014.
[6] G.Koch,R.Zemel,andR.Salakhutdinov,“Siameseneuralnetworksfor
one-shotimagerecognition,”inICMLdeeplearningworkshop,vol.2,
Lille,2015. Issue:1.
[7] O. Vinyals, C. Blundell, T. Lillicrap, and D. Wierstra, “Matching
networksforoneshotlearning,”vol.29,2016.
[8] J. Snell, K. Swersky, and R. Zemel, “Prototypical networks for few-
shotlearning,”inAdvancesinNeuralInformationProcessingSystems,
vol.30,CurranAssociates,Inc.,2017.
[9] F.Sung,Y.Yang,L.Zhang,T.Xiang,P.H.Torr,andT.M.Hospedales,
“Learning to compare: Relation network for few-shot learning,” in
Proceedings of the IEEE conference on computer vision and pattern
recognition,pp.1199–1208,2018.
[10] Q.Dou,D.CoelhodeCastro,K.Kamnitsas,andB.Glocker,“Domain
generalizationviamodel-agnosticlearningofsemanticfeatures,”vol.32,
2019.
[11] N.Mishra,M.Rohaninejad,X.Chen,andP.Abbeel,“Asimpleneural
attentivemeta-learner,”2018.
[12] A. Javaheri, S. R. Kheradpisheh, H. Farahani, A. G. Khoee, and
M. Ganjtabesh, “Avid: A variational inference deliberation for meta-
learning,” in 2022 12th International Conference on Computer and
KnowledgeEngineering(ICCKE),pp.268–273,2022.
[13] Z. Li, F. Zhou, F. Chen, and H. Li, “Meta-SGD: Learning to learn
quicklyforfew-shotlearning,”2017.
[14] J. Krone, Y. Zhang, and M. Diab, “Learning to classify intents and
slot labels given a handful of examples,” in Proceedings of the 2nd
Workshop on Natural Language Processing for Conversational AI (T.-
H. Wen, A. Celikyilmaz, Z. Yu, A. Papangelis, M. Eric, A. Kumar,
I.Casanueva,andR.Shah,eds.),pp.96–108,AssociationforCompu-
tationalLinguistics,2020.
[15] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,
and L. Zettlemoyer, “Deep contextualized word representations,” in
Proceedings of the 2018 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language
Technologies,Volume1(LongPapers)(M.Walker,H.Ji,andA.Stent,
eds.),pp.2227–2237,AssociationforComputationalLinguistics,2018.
[16] J.Pennington,R.Socher,andC.Manning,“Glove:Globalvectorsfor
wordrepresentation,”inProceedingsofthe2014ConferenceonEmpiri-
calMethodsinNaturalLanguageProcessing(EMNLP),pp.1532–1543,
AssociationforComputationalLinguistics,2014.
[17] S.Larson,A.Mahendran,J.J.Peper,C.Clarke,A.Lee,P.Hill,J.K.
Kummerfeld,K.Leach,M.A.Laurenzano,L.Tang,andJ.Mars,“An
evaluationdatasetforintentclassificationandout-of-scopeprediction,”
inProceedingsofthe2019ConferenceonEmpiricalMethodsinNatural
Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP) (K. Inui, J. Jiang,
V.Ng,andX.Wan,eds.),pp.1311–1316,AssociationforComputational
Linguistics,2019.
ed on January 30,2026 at 09:53:57 UTC from IEEE Xplore. Restrictions apply.

Paper:Intent_Classification_French_Recruitment_Chatbot_Use_Case.pdf
=== Page 1 ===
2023 International Conference on Computational
Intent Classification: Fre
Use
Nadira Boudjani, Vivian
SogetiLabs, P
{nadira.boudjani, viviane.cola
Abstract—Intent classification is an important task for the
chatbottounderstandandinterprettheuser’smessage.Forthe
proper functioning of the chatbot, it’s essential to choose an
effective intent classifier. In this paper, we propose a compar-
ative study between three intent classifier models for a French
recruitmentchatbot;DIETClassifier(Rasa),Wit.aiClassifierand
CamemBERT Classifier. First, We constructed a French dataset
withintentsinarecruitmentchatbotcontextandtrainedallthree
classifiersonthisdataset.AccordingtoourresultsCamemBERT
is the best intent classifier for our chatbot.
IndexTerms—Intentclassification,CamemBERT,Rasa,Wit.ai,
Chatbot.
I. INTRODUCTION
In recent years, the application of AI in the creation of
chatbots has been significantly increased in various domains:
education [1], [2], medicine [3], [4], recruitment [5]–[8] and
multi-domain conversation generator such as ChatGPT [9].
Achatbotisasoftwareprogramabletotalkwithauser,to
understand the requests received and to reply to them. Intent
classification is a crucial step for the proper functioning of
the chatbot. It allows the user’s will to be detected in the
message.Figure1showsafewexamplesofuserintentduring
a conversation with a chatbot.
Fig.1. Intentclassificationexamples.
Severalframeworkshavebeendevelopedforthecreationof
AI chatbots. Among the most popular: Rasa and Wit.ai.
• Rasa [10] is an open-source conversational AI platform
for understanding and holding conversations, and con-
necting to messaging channels and third-party systems
through a set of APIs.
• wit.ai [11] is a platform that provides services to create
a dialoger through an open and extensible automatic
natural language processing engine. It provides an easy
interface and fast-learning APIs to understand human
communication from every interaction.
These platforms are used in the development of several
chatbots in different languages [12]–[14]. To the best of our
knowledge, no comparison of these frameworks in intent
classificationforaFrenchhasbeenpresentedintheliterature.
2769-5654/23/$31.00 ©2023 IEEE 68
DOI 10.1109/CSCI62032.2023.00117
71100.3202.23026ICSC/9011.01
:IOD
|
EEEI
3202©
00.13$/32/3-1516-3053-8-979
|
)ICSC(
ecnegilletnI
lanoitatupmoC
dna
ecneicS
lanoitatupmoC
no
ecnerefnoC
lanoitanretnI
3202
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

l Science and Computational Intelligence (CSCI)
ench Recruitment Chatbot
Case
ne Colas, Azade Fotouhi
Paris, France
as, azade.fotouhi}@sogeti.com
The only comparison for French is proposed in [15]. The
authors present a comparison between two French versions
of BERT, CamemBERT and FlauBERT, for a French medical
chatbot.
Our aim is to test the two platforms above in the intent
classificationofarecruitmentchatbotinFrenchlanguageand
to compare these results with the CamemBERT Classifier
results.
The rest of the paper is organised as follows: Section 2
presents chatbot frameworks, Section 3 presents our dataset,
Section 4 presents results of the experiments. Finally, Section
5 presents the conclusion and the perspective.
II. CHATBOTFRAMEWORKS
Inthissection,wepresentRasaandwit.aicomponentsand
how these frameworks perform intent classification.
A. Rasa framework
Rasa consists of 5 files :
• Config: Describes the language environment and adds
functionality to Rasa such as Spacy, in order to manage
French language for example.
• Domain: defines the environment in which the chatbot
operates. It specifies the intents, entities, responses, and
actions the chatbot should perform.
• NLU(NaturalLanguageUnderstanding):performsintent
classification, entity extraction, and response retrieval.
• Stories:Conversationstagesbasedonexchangesbetween
chatbot and human. It’s a set of short scenarios.
• Rules: List of rules that the chatbot follows systemati-
cally, based on identified keywords.
Intent classification in Rasa is handled by DIETClassifier, a
multi-tasking transformer architecture that enables both intent
classification and entity recognition.
TotrainDIETClassifierinRasa,weneedtomodify3default
files:
• Config:weaddtheline”-name:DIETClassifier”andwe
can add other parameters such as the number of epochs
(element of iterative process).
• Domain: the list of intents are given in Domain file
preceded by the keyword ”intents:” as shown in figure
2.
• NLU: in this file, for each intent, examples of sentences
expressing this intent are given as shown in figure 3.
81
ed on January 30,2026 at 09:55:26 UTC from IEEE Xplore. Restrictions apply.

=== Page 2 ===
Fig.2. Intentsdefinitioninthedomainfileexample.
Fig.3. ExampleofsentenceswithgreetingintentioninNLUfile.
B. Wit.ai framework
Wit.ai is organised into 3 parts: Composer, responsible
for creating the chatbot; Management, where intentions and
entities are defined; Understanding, responsible for training
wit.ai models through examples.
The Composer has 4 components. Figure 4 shows part of
the chatbot composer:
• Input: used to represent a message sent by a user.
• Response: used to reply to a user.
• Decision: used to direct the flow of information on the
basis of conditions previously defined.
• Context:usedtoadd,deleteormodifyinformationinthe
context (a JSON object) of the information flow during
dialog.
Intents are defined in the Management (intents) section as
shown in Figure 5
Training examples of wit.ai Classifier are given in Under-
standing as shown in Figure 6. Each sentence is annotated
with an intent and the entities linked to that intent.
III. DATASET
WehavegeneratedourownFrenchdatasetinarecruitment
interview context.
A. Data Generation
Wecreatedalistof22intentsandgeneratedatotalof2,547
examples:
• in varying words and syntaxes.
68
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

Fig.4. Wit.aiComposerexample.
Fig.5. Intentsmanagement.
• in varying completeness of information. Example: to
not give the postal code for the “postal address” intent
(informer adressePostaleinfrench).Theobjectiveisthat
the chatbot detects intents in the case of incomplete
information.
TheFigure7belowshowstheintentstobedetectedbythe
recruitment chatbot.
B. Data Annotation
Theannotation iscarried outinan Exceltable byselecting
an intent from the defined intent list presented in Figure 7.
Figure 8 shows a sample of generated and annotated data.
IV. EXPERIMENTS
The aim of the experiments is to compare the three clas-
sifiers: DIETClassifier (Rasa), wit.ai Classifier and Camem-
BERT in the case of a French recruitment chatbot. Camem-
82
ed on January 30,2026 at 09:55:26 UTC from IEEE Xplore. Restrictions apply.

=== Page 3 ===
Fig.6. Intentannotationexampleinwit.ai.
Fig.7. Intentslist.
BERT[16]isamulti-layerbidirectionalTransformerencoder.
It is based on RoBERTa [17], which is based on BERT [18].
DatasetisdividedintotrainandtestsetasshowninFigure
9.
We evaluated the models with the accuracy, recall and f1-
score metrics and the confusion matrix.
A. DIETClassifier (Rasa)
We trained DIETClassifier from 100 to 500 epochs with a
step of 50. The best accuracy obtained is 0.8668 with 200
epochs as shown in Figure 10.
Theprecision,recallandf1-scoremetricsandtheconfusion
matrix are given in Figure 11 and Figure 12 respectively.
B. Wit.ai Classifier
wit.ai is not an open-source platform. The training param-
eters such as the number of epochs can not be modified.
So, we trained the wit.ai Classifier with the basic parameter
configuration. The precision, recall and f1-score metrics and
Fig.8. Asampleofourdataset.
Fig.9. Trainandtestset.
68
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

Fig.10. DIETClassifieraccuracyperepoch.
Fig.11. DIETClassifier:precision,recallandf1-scoremetrics.
the confusion matrix are given in Figure 13 and Figure 14
respectively.
The results show that the average accuracy of the wit.ai
ClassifierislowerthantheaccuracyofDIETClassifierby2%
with a much shorter execution time.
C. CamemBERT
We trained CamemBERT from 5 to 40 epochs with a step
of 5. The average accuracy for each model is given in Figure
15. The best accuracy obtained is 0.93 with 5 epochs.
Theprecision,recallandf1-scoremetricsandtheconfusion
matrix are given in Figure 16 and Figure 17 respectively.
Experiments have shown that DIETClassifier needs to be
trained up to 200 epochs to reach its maximum performance.
Fig.12. DIETClassifier:confusionmatrix.
83
ed on January 30,2026 at 09:55:26 UTC from IEEE Xplore. Restrictions apply.

=== Page 4 ===
Fig.17. CamemBERT:confusionmatrix.
Fig.13. wit.ai:precision,recallandf1-scoremetrics.
However,itsperformanceremainsbelowthatofCamemBERT
and wit.ai Classifier. CamemBERT ensures very good intent
classificationwithanaverageaccuracyof93%,comparedwith
88% for wit.ai and 86% for DIETClassifier. For our French
recruitment chatbot, CamemBERT is the best Classifier.
Moreover,experimentsshowthatthereare2ambiguitiesin
the 3 models:
• between intents inform soft skill and inform hard skill:
this is due to the fact that both intentions express skills,
and the context of the sentence can easily be identical.
Following example shows the confusion that can exist
between the two intents.
– I can make important decisions (soft skill intent)
– I can solve management software problems (hard
Fig.14. wit.ai:confusionmatrix.
skill intent)
• between decline interview and refuse to answer: in both
intents,wehaveanexpressionofrefusal,whichexplains
the confusion as shown in the following example.
– Idonotwishtotaketheinterview(declineinterview
Fig.15. CamemBERTaccuracyperepoch. intent)
– I do not wish to answer this question (refuse to
answer intent)
CamemBERT’s performance is explained by the fact that
Camembertwasspecificallydesignedtocapturethelinguistic
features and nuances of the French language. The models in
Rasa and wit.ai are designed for natural language processing
in the context of chatbots and virtual assistants.
Moreover, Camembert is pre-trained on a wide variety
of French texts, while the models in the frameworks are
pre-trained on simulated dialogues, diversified texts and real
conversations.
V. CONCLUSIONANDPERSPECTIVE
In this article, we present a comparative study of three
classifiermodels:DIETClassifer,wit.aiandCamemBERT,for
French language in intent classification task in the case of a
chatbot interview job. To the best of our knowledge, there is
Fig.16. CamemBERT:precision,recallandf1-scoremetrics. no comparison of these models in the literature for French
language. We generated a dataset in our context to train
684
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloaded on January 30,2026 at 09:55:26 UTC from IEEE Xplore. Restrictions apply.

=== Page 5 ===
and test the models. The best classifier for our chatbot is
CamemBERTwhichobtainedanaverageaccuracyof93%.As
aperspective,weplanetocomparetheseresultswiththelatest
GPT models in intent classification and also entity extraction
during a job interview with the chatbot.
REFERENCES
[1] B.R.Ranoliya,N.Raghuwanshi,andS.Singh,“Chatbotforuniversity
related faqs,” in 2017 International Conference on Advances in Com-
puting,CommunicationsandInformatics(ICACCI). IEEE,2017,pp.
1525–1530.
[2] H. B. Essel, D. Vlachopoulos, A. Tachie-Menson, E. E. Johnson, and
P. K. Baah, “The impact of a virtual teaching assistant (chatbot) on
students’learninginghanaianhighereducation,”InternationalJournal
ofEducationalTechnologyinHigherEducation,vol.19,no.1,pp.1–19,
2022.
[3] P.AmiriandE.Karahanna,“Chatbotusecasesinthecovid-19public
healthresponse,”JournaloftheAmericanMedicalInformaticsAssoci-
ation,vol.29,no.5,pp.1000–1010,2022.
[4] M.R.King,“Thefutureofaiinmedicine:aperspectivefromachatbot,”
AnnalsofBiomedicalEngineering,vol.51,no.2,pp.291–295,2023.
[5] Z. Chen, “Collaboration among recruiters and artificial intelligence:
removinghumanprejudicesinemployment,”Cognition,Technology&
Work,vol.25,no.1,pp.135–149,2023.
[6] S.Koivunen,S.Ala-Luopa,T.Olsson,andA.Haapakorpi,“Themarch
of chatbots into recruitment: recruiters’ experiences, expectations, and
designopportunities,”ComputerSupportedCooperativeWork(CSCW),
vol.31,no.3,pp.487–516,2022.
[7] C.deRuijtandS.Bhulai,“Jobrecommendersystems:Areview,”arXiv
preprintarXiv:2111.13576,2021.
[8] N. Boudjani, V. Colas, C. Joubert, and D. B. Amor, “Ai chatbot for
job interview,” in 2023 46th MIPRO ICT and Electronics Convention
(MIPRO). IEEE,2023,pp.1155–1160.
[9] “Chatgpt,”https://openai.com/chatgpt,accessed:2023-08-02.
[10] “Rasa,”https://rasa.com/,accessed:2023-08-02.
[11] “Wit.ai,”https://wit.ai/,accessed:2023-08-02.
[12] R. K. Sharma and M. Joshi, “An analytical study and review of open
source chatbot framework, rasa,” Int. J. Eng. Res, vol. 9, no. 06, pp.
1011–1014,2020.
[13] A.Jiao,“Anintelligentchatbotsystembasedonentityextractionusing
rasanluandneuralnetwork,”inJournalofphysics:conferenceseries,
vol.1487,no.1. IOPPublishing,2020,p.012014.
[14] A. A. Qaffas, “Improvement of chatbots semantics using wit. ai and
wordsequencekernel:Educationchatbotasacasestudy,”International
journalofmoderneducationandcomputerscience,vol.11,no.3,p.16,
2019.
[15] C.Blanc,A.Bailly,E´.Francis,T.Guillotin,F.Jamal,B.Wakim,and
P.Roy,“Flaubertvs.camembert:Understandingpatient’sanswersbya
frenchmedicalchatbot,”ArtificialIntelligenceinMedicine,vol.127,p.
102264,2022.
[16] L. Martin, B. Muller, P. J. O. Sua´rez, Y. Dupont, L. Romary, E´. V.
deLaClergerie,D.Seddah,andB.Sagot,“Camembert:atastyfrench
languagemodel,”arXivpreprintarXiv:1911.03894,2019.
[17] Y.Liu,M.Ott,N.Goyal,J.Du,M.Joshi,D.Chen,O.Levy,M.Lewis,
L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert
pretrainingapproach,”arXivpreprintarXiv:1907.11692,2019.
[18] J.Devlin,M.-W.Chang,K.Lee,andK.Toutanova,“Bert:Pre-training
of deep bidirectional transformers for language understanding,” arXiv
preprintarXiv:1810.04805,2018.
685
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloaded on January 30,2026 at 09:55:26 UTC from IEEE Xplore. Restrictions apply.

Paper:intent_prediction.pdf
=== Page 1 ===
2023 International conference on Advancement in C
Fine-tunedBERTw
Bi-GRU-CapsNetfram
recognitionan
Nahida Shafi
Department Of Computer Science
University of Kashmir, J&K, India, 190006
Email: nahidashafi.scholar@kashmiruniversity.net
Abstract—In the field of natural language processing (NLP),
thetwomostprominentresearchareasareslottaggingandintent
recognition. Modern joint learning strategies examine the link
between slot-tag identification and intent classification, utilising
the shared knowledge between the two tasks for their collective
advantage.However, methods that combine various variants of
pre-trained Bidirectional Encoder Representations from Trans-
formers [BERT] models with attention based capsule networks
for joint slot-tag identification and intent detection have not
beenfullyexplored.Thisstudyproposesamulti-stageframework
trained on different versions of BERT models (BERT and
base
BERT ) with Bidirectional Gated Recurrent Unit [Bi-GRU] large
and self attention mechanism as intent detection decoder to
capture the underlying information and to discover the explicit
links.Whilethecapsulenetwork,inaccordancewiththedynamic
routing algorithm, acts as a slot filler decoder in predicting
intents and slots and representing the semantic and syntactic
relationships. The experimental findings demonstrate that the
proposed approach enhances semantic frame accuracy at the
sentence level, outperforming various baseline methodologies by
a significant margin with a 1.2% improvement in the intent F1-
score and 3.24% in the slot F1-score, relative to the previous
state-of-the-art models on the SNIPS datasets.
Index Terms—bi-gru, self-attention, capsule network, in-
tent classification, slot-filling, natural language processing,
bert ,bert
large base
I. INTRODUCTION
TheNaturalLanguageUnderstanding(NLU)module,whichtrans-
lates the given input utterances into a task-oriented semantic rep-
resentation, is a crucial component of any goal-oriented dialogue
system and a key part for building an efficient dialogue system. For
linguistic interaction with humans NLU is crucial to technologies
like Internet of Things (IoT), chatbot interaction, robot training,
virtual interfaces, machine translation, text classification .e.t.c. Spo-
ken language understanding (SLU) is a sub-stack of NLP that
utilises two fundamental tasks slot-tagging and intent categorization
to contextualise the comprehension of human speech. The primary
focusofintentdetectionis determiningauser’sintentfromagiven
speech; this is considered an utterance classification problem that
determinestheintentlabel“y”foragivenutteranceasshownin“fig
1”.Comparatively,thesequencelabellingtaskknownassemanticslot
ISBN: 979-8-3503-9648-5/23/$31.00 ©2023 IEEE 36
44714101.3202.53575TCCACnI/9011.01
:IOD
|
EEEI
3202©
00.13$/32/5-8469-3053-8-979
| )TCCACnI(
seigolonhceT
retupmoC
&
noitatupmoC
ni
tnemecnavdA
no
ecnerefnoC
lanoitanretnI
3202
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

Computation & Computer Technologies(InCACCT)
withAttention-based
meworkforJointIntent
andSlotfiling
Manzoor Ahmed Chachoo
Department Of Computer Science
University of Kashmir, J&K, India, 190006
Email: manzoor@kashmiruniversity.ac.in
fillingthattranslatesaninputwordsequencew=(w ,w ,,w )onto
1 2 T
thematchingslottagssequencexs =xs,xs,...,xs.Theseconcepts
1 2 T
are then used to accomplish a goal or complete a specific task .
Figure 1: An example of BOI representation of user query to
semantic-level frame
Combined learning approaches have been used to model the
interdependencies between intent categorization and slot tagging
by leveraging recurrent neural networks and attention mecha-
nisms[1],achievingstate-of-the-artperformance.C.Zhang,Y.Li,N.Du,
W.Fan and P.S.Yu [2] proposed model consists of a hierarchical
capsule architecture where capsules represent semantic information
and are trained by back-propagation with dynamic routing in order
to detect intents and fill slots.
Generalization is impeded by the absence of human-labelled
data for various knowledge engineering tasks . In order to effec-
tively train models, machine learning algorithms require the ac-
cessibility of vast amounts of text data, which can be prob-
lematic when it comes to NLU and NLP. To address the is-
sueofsparsedata,anumberofmethodsfortraininglinguistic mod-
elswithhugeamountsofunlabeledtextdatahavebeenproposed.One
ofthemosteffectivemethodsislanguagemodelpre-training,which
involvespre-trainingamodelonlargecorporaofunlabeledtext.The
BidirectionalEncoderRepresentationsfromTransformers(BERT)[3]
methodthatwasproposedisusedtomakestate-of-the-artmodelsin
manyNLPtasks,fromtextclassificationandlanguageunderstanding
toquestionanswering,nameentityrecognition,andmachinetransla-
tion. BERT for NLU has not however, been extensively investigated
and explored. This work’s technical contributions are twofold: this
study investigate various versions of BERT’s pre-trained models to
improve NLU’s generalisation ability:
• Firstly, this study investigate the effect of increasing the vol-
umeoftrainingdataontheefficacyofpre-trainedBERTmod-
els.
69
ed on January 29,2026 at 14:33:48 UTC from IEEE Xplore. Restrictions apply.

=== Page 2 ===
• SecondlybasedonBERT base andBert Large ,thispaperpropose
a unified model for slot tagging and intent detection in a
task-oriented dialogue system with the Bi-GRU self-attention
mechanism as an intent detection decoder and its output to the
Capsule network layer for slot tag recognition that represents
semanticandsyntacticrelationships.Ineverymetric,thisstudy
outperformed the benchmark ,1.2% improvement in the intent
F1-score and 3.24% in the slot F1-score, relative to the
previous state-of-the-art models on the SNIPS datasets.
II. LITERATUREREVIEW
A. COMBINING INTENT CLASSIFICATION AND SLOT
FILLING
A comprehensive literature review has been conducted to exam-
ine the various natural language understanding-related tasks com-
pleted so far consisting of intent detection, slot tagging, domain
classification, and its multi-task framework. This section overviews
state-of-the-artresearchonintentrecognitionandslot-fillingstudies.
This survey revealed that although substantial progress has been
made, there is still a need for further development in the field.This
studyexamineshowcurrentmethodsforNLUhavejointlymodelled
these tasks, incorporated human knowledge domains, and made use
of pre-trained word embeddings and contextual knowledge.
Earlier methods for intent classification and slot-tagging used
distinct models for each task. These models were trained primar-
ily on word embeddings, such as skip-gramme and one-hot rep-
resentation, and were based on the feature representation space
[4].Combination of Conditional random fields (CRF) [5] with sta-
tistical models in a heuristic approach with Markov models were
also commonly used methods for sequence labelling.
B. Deeplearningmodelsforjointintentclassificationandslot
filling
Recently,deeplearningmodelslikeCNNs,RNNs,andLSTMs[6]
haveshownexcellentperformanceinawidevarietyofNLPtasksand
have been widely used in domain classification, intent recognition
and slot tagging. Kane et al [7] extracted the semantic hierarchy for
effective modeling, using the combination of CNN (Convolutional
Neural Network), Bidirectional LSTM (Long ShortTerm Memory)
andCRF(ConditionalRandomField)[8]forIDandSF.L.ZhaoandZ.
Feng[9]proposedagenerativeneuralnetworkviaapointernetwork
togeneratesequencesthatcouldcapturethelong-termdependencies
inagivensentencethroughanattentionalSeq2Seqmodel.Byusing
this model, the authors were able to generate coherent sentences
withintricatestructuresthataccuratelyreflectedthesemanticsofthe
original sentence For joint intent detection and slot filling, Liu and
Lane [10] integrated explicit alignment information between input
tokens and output slots into an attention-based Bi-LSTM network.
M.Firdaus,A.Kumar,A.Ekbal,andP.Bhattacharyya[11]captured
contextualinformationfortheutterancesusingaconvolutionalneural
network (CNN) model, allowing them to recognise the intent of a
sentence by taking into account the surrounding words and phrases
and also incorporated a conditional random field for mapping the
intent and slot label dependency.
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, [3]recently
presented a state-of-the-art pre-training technique (BERT), which is
a natural language processing (NLP) technique that can be used to
better understand the context and meaning of text , and produced
state-of-the-art models for a wide range of natural language pro-
cessing tasks such as question answering, language translation, text
summarization , contextual inferencing, and others. However, BERT
for NLU is yet to be fully explored.
III. MODELARCHITECTURE
The proposed model which is depicted in the figure [2] consists
of multi-stage framework trained on different versions of BERT
37
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

models (BERTbase and BERTlarge)as shown in Table “I” with Bi-
GRU and self attention mechanisms as intent detection decoders
layer,whilethecapsulenetworkactsasaslot-fillingdecoderlayerin
predictingintentsandslotsandrepresentingthesematicandsyntactic
relationships.Each layer is composed of several nodes that perform
specific computations which is shown in the next subsections:
Figure 2: Intent recognition and Slot filling multi-task module Ar-
chitecture
A. Bidirectional Encoder Representations from Transform-
ers [BERT]
This paper presents BERT as an encoder of our proposed model
to train a new stack-augmented bidirectional encoder representation
from transformers from unlabeled data(such a Wikipedia dump ,
Book Corpus). BERT is based on the original Transformer[12]
concept, which was used to design its multi-layer bidirectional
Transformer encoder architecture. BERT’s most innovative techno-
logical feature is the implementation of the popular attention model
transformer’s bidirectional training for language modelling where
the input representation comprises “WordPiece embeddings,Position
embeddings,and Segment embeddings”. The first token consists of a
specialclassificationembedding([CLS])andthefinaltokenconsists
of a special token ([SEP]). BERT is defined as follows, given an
input user-sentence (U = {u ,,u ,,u })that has been processed
1 i N
by WordPiece [13]
t0 =W U +W (1)
i e i b
tl =transformerblock(tl−1) (2)
i i
v =tL (3)
i i
where N represents the entire sequence,u is the current token.
i
“(1-2)”generatesinputaswordembeddingsfromtext-basedtraining
data.The transformative frame in “(2)” consists of output layers,
multi-head attention layers, and fully connected layers (a linear
affine sublayer) while in “(3)” represents the output. L represents
the total number of BERT layers, while l(1≤ l ≤ L) represents
the current layer.In addition for pre-training both MSP “Masked
languagemodel”&NSP“Nextsentenceprediction”[3]wereused
to pre-train the BERT model on a huge corpus of unlabeled text
data. The pseudocode of MSP and NSP is shown in Algorithm 1 &
Algorithm 2 respectively.The contextual semantic represen- tations
ofeachtokeninthesequencearecalculatedusingthe BERT ase
b
modelinourapproach.BERTwaspre-trainedusingtwoself-directed
objectives.ThefirstisMLM,adenoisingaimthatrequiresthemodel
70
ed on January 29,2026 at 14:33:48 UTC from IEEE Xplore. Restrictions apply.

=== Page 3 ===
to recreate randomly masked input tokens using context knowledge
where15%oftheinputtokensareinitiallyselected[3].TheNSPtask
is a binary classification task in which BERT is presented with two
sentences and asked to predict whether the second sentence is the
progression of the first. These pre-trained BERT models were used
afterfinetuningtobetterunderstandthelanguageusedinourspecific
use cases, i.e., slot tagging and intent recognition.
Algorithm 1: Unsupervised Masked Language Mod-
elling [MLM]
Input : Tokenized input-ids for the Input Utterance
Output: Trained Masked Language Model
1 foreach u ∈ U do
2 U maskedmass ← masking-function(U)
3 Output ←forwardpass(U maskedmass )
4 backpropagate(Logarithmic loss function())
5 end for
6 function masking-function(token-ids)
7 foreach token ∈ token-ids do
8 V ← arbitary(){Likelihood = Prob}
9 if prime = 1 then
10 symbol-ids ← [mask]{Prob = 0.9}
11 else if prime = 2 then
12 symbol-ids ← prime−phrase{Prob = 0.2}
13 else if prime-word = 3 then
14 symbol-ids ← symbol−id{Prob = 0.1}
15 endif
16 masked-utterance.append(symbol-ids)
17 end-for
18 return masked utterance
19
Algorithm 2: “Next Sentence Prediction (NSP) for
BERT”
Input : Sn : Strings of form (S1,S2, label);
Output: Trained Model
1 foreach string ∈ Sn do
2 S1 ← arbitary-maskvalue(string[1], % = 15)
3 S2 ← arbitary-maskvalue(string[2],% = 15)
4 input ← [CLS]+S1+[SEP]+S2
5 assign ← tuple[3]
6 output ← forwardpass(input)
7 backpropagate(Logarithmic loss function())
8 end for
B. Intent Classification Decoder
1) BiGRU and Self-attention mechanism Layer: Bi-GRU
is a specialized RNN Gated Recurrent units (GRUs) used to extract
sequences in both the forward and backward directions, making it
particularly useful in sequential modeling problems. Serialization is
a prominent aspect of text in NLP because the order of words in a
phrasecansignificantlyalteritsmeaning.TheBi-GRUmodelisused
to parse sentences for their underlying semantics. The details of the
basic structure of Bi-GRU unit is depicted in fig[3] where the input
sequence x in opposite directions and process and combine both
t
forward and backward information to produce the output h .For
t−1
37
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

our model uses the bert sentence representation as the input to the
Bi-GRU layer .
r =σ(W x +V h +b ) (4)
t r t r t−1 r
z =σ(W x +V h +b ) (5)
t z t z t−1 z
h˜ =tanh(W x +V (r ⊙h )b ] (6)
t h t h t t−1 h
h =(1−z )⊙h +z ⊙h (7)
t t t−1 t t
In order to solve the vanishing problem in GRU, “(4-7)” represents
two vectors,namely update (z ) gate and Reset gate (r ) are respon-
t t
sible for determiming what information should be supplied to the
outputwhereσisthenon-linearlogisticsigmoidactivationfunction,
W ,V ,W ,V ,W and V are the weight matrices, b ,b ,b
r r z z h h r z h
are bias parameters for input x and preceding state h , tanh is
t t−1
the hyperbolic tangent activation function and ⊙ depicts hadamard
product[14].
−→ −→
h =GRU(x , h )
f t t−1
←− ←−
h =GRU(x , h )
b t t+1
−→ ←−
h = h ⊕ h (8)
t t t
−→ −→
ThisresearchproposesaBi-GRUnetworkwhereh and h repre-
t t
sentsthehiddenlayerofforwardandbackwardGRUunitrespectively
as shown in “8” The primary objective of Bi-GRU is to capture the
contextual features of given utterance sequences. But, in reality, the
semanticanalysisofeachcharacterinthegivenutterancehasvarying
contribution on the intent detection task. Information redundancy Is
one of the consequences for large amount of useless information
in the text. The Bi-GRU paradigm makes it challenging to extract
crucial information from utterance sequences.As a result, this study
presents a self-attention mechanism as a better way to capture
the underlying dependencies in neural networks once the bi-GRU
networkhasobtainedthefeaturesofthecontext.It’smoreaccuratein
assigningweighttoimportantinformationandunderstandssequence
semanticsmoreprecisely.Hierarchicalself-attention(HSA)modelto
improve neural machine translation and shallow text parsing of nat-
urallanguagetexts.Weuseittoexplicitlylearnanytwocharacters’
relationships and capture the sentence’s structural information.
The self-attention mechanism is regarded as a way to convert a
text representation into an embedding vector or key-value pairs that
contain all of the relevant information in the text, including context
andmeaning.Inthispaper,weuseatechniqueknownas“multi-head
self-attention”.
QKT
Attention(Q,K,V)=softmax( √ )V (9)
d
k
where Q ∈ RN×2dh,K ∈RN×2dh and V ∈RN×2dh represents
(query,keysandvalue)matrixrespectivelywhereinourcaseQ=K
= Q = H and hidden layers dimensions of BERT are repesented by
d [3]. We use a softmax function to compute the attention weights,
whicharethenmultipliedbythequeryvectorstocreatetheattention-
based embedding vector output , where the weight for eac√h value is
determinedbyitssimilaritytothequeryvectordividedby d ,and
k
thenappliedtothevalues.Thescaleddot-productofthequeryvector
and the key vector is used to determine this similarity is outlined in
“(9)”.
MultiHead(Q,K,V)=Concat(head ,..head )WO
1 h
(10)
Additionally, a multi-head attention uses self-attention h times from
various linear projections to enhance performance, instead of just
once.Thehprojectionsthencarryoutthescaleddot-productattention
in parallel for the projected queries, keys, and values, resulting
71
ed on January 29,2026 at 14:33:48 UTC from IEEE Xplore. Restrictions apply.

=== Page 4 ===
Table I: The configurations for the BERT and BERT architecture models analyzed in this paper.
BASE LARGE
Encoder Parameter Hidden lay-
Models Attention Head
Blocks (in Million) ers size
BERT 12 12 110 768
BASE
BERT 24 16 340 1024
LARGE
in a d . At last, the outputs of this focused attention are
model
mergedandre-projectedtogenerateadditionalrepresentations.Using
the “11” where the various parameter projection matrices WQ ∈
i
Rdh×dk,WK ∈ Rdh×dk, WV ∈ Rdh×dv and W0 ∈ Rhdv×dh
i i
we can characterize this process as:
head =Attention(QWQ,KWK,VWV) (11)
i i i I
C. Capsule Network (CapsNet) Slot filling Decoder
Forslottaggingtasks,weprovidetheslotfillingdecoder’shidden
state with hs.....,hs into the capsule network to predict the slot
1 T
tags as capsule is composed of neurons that extract semantic and
syntactic data. In this paper, we introduce CapsNet for slot tag
recognition;inwhichcapsulesymbolizestheslotlabel,themodulus
length of capsule vector reflects the slot label prediction probability,
and capsule vector direction represents the attribute of the multiple
Figure 3: GRU basic unit structure
tags. Since the capsule network generates a vector as opposed to
a scalar value, it is better able to represent semantic and syntactic
relationships.
To that end, after the Bi-GRU and self-attention layers have been Squash function “(16)” is used to normalize the output vector 0
used to acquire semantic information about the given utterance, the , which contains the input text in various orientations and local
capsule network layer receives the final hidden state h reflecting orderings.
t
the temporal dynamics of the sentence and maps it to an output of O = ∥s u ∥2 s u (16)
theBi-GRUlayerandself-attentionlayerswiththesamesizeasthe u 1+∥s∥2∥s ∥
u
input as shown in Algorithm(3) we know that the predictive vector
of the rthprimary capsule network’s output is i . Each low-level finallyupdatingtheassociatedweightsaccordingtoaboveequation
u|r
feature capsule i is multiplied by a matrix W ∈ Rd×d before and repeat until convergence is reached.
r ru
being transferred to the high-level capsule O to improve feature
u
representation. It minimizes training parameters and prevents over- b ru =b ru +i r|u .O u (17)
fitting.Inequation,anon-linearactivationfunctionconvertsBi-final
GRU’s hidden state ht into a feature capsule u . i also calculates
i r
the correlation between input and output layers and generates the
predictionvectoruˆ wherew istheweightmatrix.Theresultant Algorithm 3: Dynamic Routing Algorithm
j|i ij
output of the capsule network is obtained using the following Input : i ;s;l
u|r
equations:
Output: o ;
ˆi =W i (12) u
u|r r|u r 1 Procedure: Routing (i u|r ;s;l)
where the logarithmic prior probability of the rth and uth capsule 2 for l iterations do
is b which is initialized to 0.
In ur this work we applied the dynamic routing that calculates the 3 initialize the coefficient b ur = 0
coupling coefficient C that conveys the low-level feature to the 4 foreach layer l in primary-capsule r do;
r|u
high-levelfeature.Thisassociationencodesessentialintent-slotcon- 5 c ru ← softmax(bru);
textual and semantic representations of input utterances considering 6 end for
different text orientations. 7 foreach layer l+1 in capsule u do;
pro I d n uc “ t (1 o 3 f ) a ” ll S p u red re ic p t r i e v s e e v n e ts cto th r e sw ca it p h su th le eir ou co tp r u re t sp i. o e nd th in e g s c u o m nn o e f cti t o h n e 8 su ← (cid:80) r C r|u ;u|r;
probabilities. 9 end for
S = (cid:88) C i (13) 10 foreach layer l+1 in capsule u do;
u ru u|r
r 11 0 u ← squash(su);
In “(14)” C ru represents the coupling coefficient generated by the 12 end for
softmax function. 13 foreach layer l+1 in capsule r do;
exp(b )
C ru = (cid:80) ru (14) 14 b r u = b r u+i u |r.o u ;
exp(b )
i ru 15 endfor
Equation “(15)” represents the capsule network’s upper layers and 16 endfor
continuously updates b until the minimum number of required
iterations is reached. ru 17 return o u ;
b ←b +OTf(i ,θ ) (15)
ru ru u r u
372
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloaded on January 29,2026 at 14:33:48 UTC from IEEE Xplore. Restrictions apply.

=== Page 5 ===
IV. EVALUATION learning rate for model optimization. Table “III” shows the state-of-
the-art models for combined intent classification and slot filling, all
When evaluating the proposed model, this study employ these
of which were trained using SNIPS datasets. The last set of models
three indicators of evaluation, i.e., F1 score, Accuracy,Semantic
includes our proposed models for combined intent classification and
frame accuracy at the utterance level, to indicate the quality of the
slot tagging.As can be seen from Table “III”, joint BERT mod-
generated responses and the overall performance of both tasks.
elsi.eBERT +Att-BiGRU-CapsNet,BERT +Att-BiGRU,
large large
A. DATASETS BERT base +Att-BiGRU-CapsNet,BERT BASE +Att-BiGRUsig-
nificantlyoutperformsthebaselinemodelswithsignificantmarginon
To test the efficacy of the proposed model this study tests the
SNIPS dataset.
proposed model on two publicly available benchmark datasets i.e
ATIS,SNIPS both are goal-oriented dialogue system datasets the
details are given in table“(II)” below:
Table II: Dataset Statistics
DATASETS ATIS SNIPS
Train-Sentences 4,778 13,084
Dev-Sentences 500 700
Test-Sentences 893 700
Intent 21 7
Slot 126 72
Vocabulary 722 11,241
B. Training details
Pytorch [15] and TensorFlow [16] were both implemented in the
training setting of our proposed model. All of the proposed models
havebeenpre-trainedonthetwoBERTflavours(BERT BASE and Figure 4: Comparingtheproposedmodeltostate-of-the-artmethods
BERT LARGE ), which is summarised statistically in Table II. For on the SNIPS dataset (Intent F1 score)
optimal performance,tuning of all hyper-parameters are done on the
training set of each BERT flavour, and the best results are reported
, where the maximum utterance length is 60, the batch size is 128
and the maximum epochs are [10, 20, 30, 40, 50].The optimization
procedure utilises Adam [17], with an initial learning rate of log-
uniform range of 0.00005 to 1 was selected as the learning rate for
modeloptimization.Dropoutprobabilityis1%.Thisstudyrandomly
iterate over a collection of hyper-parameters that were previously
defined using the random search technique.
C. EXPERIMENTAL ANALYSIS
This study conducted several comparative experimental studies
on the Snips dataset to determine if various versions of BERT and
attentionmodels,whencombined,canrevolutionisenaturallanguage
processing by improving neural network learning as encoders of
our proposed model, which jointly constrains on both left and right
context in all layers to train deep bidirectional representations and a
thorough ablation study as shown in “Fig (4-5)”. The BERTbase
model determines each token’s contextual semantic representation
in our method. After some fine-tuning, we were able to apply pre-
Figure 5: Comparingtheproposedmodeltostate-of-the-artmethods
trained BERT models to our specific use cases of intent recognition
on the SNIPS dataset (Slot F1 score)
and slot tagging, where they proved to be quite effective. This
study also employed an attention mechanism, which allowed us to
capture the dependencies between words in a sentence, thus further
enhancing the quality of our results. By leveraging the advances of V. CONCLUSIONANDFUTUREWORK
both BERT and attention models, the proposed method was able to This paper proposes a multi-stage framework that is trained
effectively improve the learning capability of neural networks as on a large-scale corpus and fine-tuned BERT model as an input
encoders. featurerepresentationtotheBi-GRUself-attentionmechanism,which
represents the intent classification decoder.For slot filling, this study
For slot filling we used capsule network as slot filling decoder used CapsNet as a slot filling decoder to capture both inter-slot
in which capsule symbolises the slot label,the slot label prediction and intra-slot relations . In this way, CapsNet is able to capture
probability is represented by modulus length of capsule and vector slot relations better than traditional models such as LSTMs and
direction represents the attribute of the multiple tags. To find the RNNs . Extensive experiments on benchmark datasets show that
optimal hyper-parameters, we randomly select from among those in the combination improves performance by a significant margin,
agivenset.Thelog-uniformrangeof0.00005to1wasselectedasthe outperforming other methods by 1.2% in intent F1-score and 3.24%
373
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloaded on January 29,2026 at 14:33:48 UTC from IEEE Xplore. Restrictions apply.

=== Page 6 ===
Table III: Comparison of our proposed approaches
#PAPERS INTEN
PRECISION RECAL
Slot gated[1] 85.45 85.42
Attention-EncoderDecoder[10] 86.74 85.41
Stack propagation[18] 92.34 92.35
Joint sequence[19] 93.45 95.60
Attention-BiRNN[10] 95.08 96.5
Capsule Neural Network[20] 96.54 97.0
Slot-Gated Intent-Att[1] 96.08 96.5
JointBert[21] 90.14 89.47
BERT Att-BiGRU 94.01 95.21
base
BERT Att-BiGRU-CapsNet 96.21 96.23
base
BERT Att-BiGRU 97.03 94.23
large
BERT Att-BiGRU-CapsNet 97.12 97.23
large
inslotF1-scoreimprovementrelativetothepreviousstate-of-the-art
modelsontheSNIPSdatasets.In futurework,weplantoextendthis
paperforothernaturallanguageunderstandingtasksbyincorporating
additional knowledge domains and exploring new architectures with
BERT variants.
REFERENCES
[1] C.-W. Goo, G. Gao, Y.-K. Hsu, C.-L. Huo, T.-C. Chen, K.-W.
Hsu,andY.-N.Chen,“Slot-gatedmodelingforjointslotfilling
and intent prediction,” in Proceedings of the 2018 Conference
of the North American Chapter of the Association for Compu-
tational Linguistics: Human Language Technologies, Volume 2
(Short Papers), pp. 753–757, 2018.
[2] C.Zhang,Y.Li,N.Du,W.Fan,andP.S.Yu,“Jointslotfilling
andintentdetectionviacapsuleneuralnetworks,”arXivpreprint
arXiv:1812.09471, 2018.
[3] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert:
Pre-training of deep bidirectional transformers for language
understanding,” arXiv preprint arXiv:1810.04805, 2018.
[4] P.Rodr´ıguez,M.A.Bautista,J.Gonzalez,andS.Escalera,“Be-
yondone-hotencoding:Lowerdimensionaltargetembedding,”
Image and Vision Computing, vol. 75, pp. 21–31, 2018.
[5] C.Sutton,A.McCallum,etal.,“Anintroductiontoconditional
randomfields,”FoundationsandTrends®inMachineLearning,
vol. 4, no. 4, pp. 267–373, 2012.
[6] A. H. Wani, N. S. Molvi, and S. I. Ashraf, “Detection of hate
and offensive speech in text,” in Intelligent Human Computer
Interaction: 11th International Conference, IHCI 2019, Alla-
habad,India,December12–14,2019,Proceedings11,pp.87–
93, Springer, 2020.
[7] B. Kane, F. Rossi, O. Guinaudeau, V. Chiesa, I. Que´nel, and
S. Chau, “Joint intent detection and slot filling via cnn-lstm-
crf,” in 2020 6th IEEE Congress on Information Science and
Technology (CiSt), pp. 342–347, IEEE, 2021.
[8] N. Shafi and M. A. Chachoo, “Query intent recognition by
integrating latent dirichlet allocation in conditional random
field,” International Journal of Information Technology, pp. 1–
9, 2022.
[9] L.ZhaoandZ.Feng,“Improvingslotfillinginspokenlanguage
understanding with joint pointer and attention,” in Proceedings
of the 56th Annual Meeting of the Association for Computa-
tionalLinguistics(Volume2:ShortPapers),pp.426–431,2018.
[10] B. Liu and I. Lane, “Attention-based recurrent neural network
modelsforjointintentdetectionandslotfilling,”arXivpreprint
arXiv:1609.01454, 2016.
37
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

s with state-of-the-art approaches on SNIPS Dataset
NT SLOT
LL F1-SCORE PRECISION RECALL F1-SCORE
2 94.37 80.20 80.25 81.13
1 86.07 66.51 55.91 60.73
5 94.37 85.23 86.72 87.64
0 96.9 85.4 86.7 87.8
96.7 85.4 86.7 87.8
97.3 91.5 92.7 91.8
97.0 96.5 96.8 82.5
7 90.13 68.82 63.58 65.16
1 97.15 90.10 92.20.0 93.04
3 97.34 92.305 92.61 93.52
3 97.52 92.56 92.82 93.50
3 98.50 94.20 94.01 95.04
[11] M. Firdaus, A. Kumar, A. Ekbal, and P. Bhattacharyya, “A
multi-task hierarchical approach for intent detection and slot
filling,” Knowledge-Based Systems, vol. 183, p. 104846, 2019.
[12] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.
Gomez,Ł.Kaiser,andI.Polosukhin,“Attentionisallyouneed,”
Advances in neural information processing systems, vol. 30,
2017.
[13] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi,
W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey,
et al., “Google’s neural machine translation system: Bridging
thegapbetweenhumanandmachinetranslation,”arXivpreprint
arXiv:1609.08144, 2016.
[14] Q. Wang, C. Xu, Y. Zhou, T. Ruan, D. Gao, and P. He, “An
attention-based bi-gru-capsnet model for hypernymy detection
between compound entities,” in 2018 IEEE International Con-
ference on Bioinformatics and Biomedicine (BIBM), pp. 1031–
1035, IEEE, 2018.
[15] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-
Vito,Z.Lin,A.Desmaison,L.Antiga,andA.Lerer,“Automatic
differentiation in pytorch,” 2017.
[16] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,
M.Devin,S.Ghemawat,G.Irving,M.Isard,etal.,“Tensorflow:
a system for large-scale machine learning.,” in Osdi, vol. 16,
pp. 265–283, Savannah, GA, USA, 2016.
[17] Z.Zhang,“Improvedadamoptimizerfordeepneuralnetworks,”
in2018IEEE/ACM26thinternationalsymposiumonqualityof
service (IWQoS), pp. 1–2, Ieee, 2018.
[18] L.Qin,W.Che,Y.Li,H.Wen,andT.Liu,“Astack-propagation
frameworkwithtoken-levelintentdetectionforspokenlanguage
understanding,” arXiv preprint arXiv:1909.02188, 2019.
[19] L. Chen, P. Zhou, and Y. Zou, “Joint multiple intent detection
and slot filling via self-distillation,” in ICASSP 2022-2022
IEEEInternationalConferenceonAcoustics,SpeechandSignal
Processing (ICASSP), pp. 7612–7616, IEEE, 2022.
[20] W.A.Abro,G.Qi,M.Aamir,andZ.Ali,“Jointintentdetection
andslotfillingusingweightedfinitestatetransducerandbert,”
Applied Intelligence, pp. 1–15, 2022.
[21] Q.Chen,Z.Zhuo,andW.Wang,“Bertforjointintentclassifi-
cationandslotfilling,”arXivpreprintarXiv:1902.10909,2019.
74
ed on January 29,2026 at 14:33:48 UTC from IEEE Xplore. Restrictions apply.

Paper:Leveraging_BERT_for_Next-Generation_Spoken_Language_Understanding_with_Joint_Intent_Classification_and_Slot_Filling.pdf
=== Page 1 ===
2023 International Conference on Advanced Com
Leveraging BERT for N
Language Understand
Classification a
1Santosh Gore 2Dr Devya
Director, Sai Info Solution Department of Infor
Nashik, Maharashtra, India Sanjivani Colleg
https://orcid.org/0000-0003-1814-59131 Kopar
sai.info2009@gmail.com bhamaredevyaniit@
4Sujata Gore 5Prof. Umes
Director Departme
Sai Info Solution, Nashik School of C
Maharashtra, India MIT ADT Uni
sujatarpatil21@gmail.com umeshubn3@
Abstract— This paper introduces a new technique to natural
language processing using BERT (Bidirectional Encoder
Representations from Transformers). The program encodes the
input text with BERT to generate contextual representations of
words, which can then be utilized for intent classification and
slot filling through joint training. With limited labeled training
data, rare words may not have enough examples to learn from.
The proposed model was evaluated against existing models
based on attention-based recurrent neural networks and slot-
gated models, and was found to be more accurate in terms of
intent classification accuracy, slot filling F1 metric score. The
level of accuracy in determining the semantic meaning of an
entire sentence, as it relates to the context in which it is used,
was evaluated on various benchmark datasets. This technique
has been employed in various NLP applications, making it a
promising area for further study.
Keywords— BERT, intent classification, slot filling, natural
language understanding, deep learning, bidirectional
representations, NLP applications
I. INTRODUCTION
Natural language understanding (NLU) is essential for the
success of objective-driven spoken dialogue systems, such as
smart speakers, as it enables users to accomplish tasks through
voice interactions. NLU typically involves intent to create a
semantic parse for user utterances, classification and slot
filling activities are used.
Intent classification is the task of predicting the intent of a
given utterance from a predefined set of intents. It is used to
understand the user's intent in natural language processing
(NLP). It is a type of text classification that is used to classify
a text into one of predefined intents. The goal of intent
classification is to identify the user's intent so that the system
can provide an appropriate response.
Slot filling is a NLP technique used in building voice
assistants and conversational agents. It involves the use of pre-
defined labeled slots to capture user inputs and store them as
semantic variables. The purpose of slot filling is to allow a
conversational agent to understand user queries and provide
the most appropriate response.
Intent classification is the task of identifying the intention
or goal behind a user's input or query, such as "book a flight"
or "find a restaurant." Slot filling involves extracting specific
979-8-3503-4834-7/23/$31.00 © 2023 IEEE
73439301.3202.10285ATCACI/9011.01
:IOD
|
EEEI
3202©
00.13$/32/7-4384-3053-8-979
| )ATCACI(
snoitacilppA
dna
seigolonhceT
gnitupmoC
decnavdA
no
ecnerefnoC
lanoitanretnI
3202
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

mputing Technologies and Applications (ICACTA)
Next-Generation Spoken
ding with Joint Intent
and Slot Filling
ani Jadhav 3Mayur Eknath Ingale
rmation Technology Sandip Institute of Technology and
ge of Engineering Research Centre
rgaon Nashik
@sanjivani.org.in mayur.ingale@sitrc.org
sh Nanavare
ent of CSE
Computing
iversity, Pune
@gmail.com
pieces of information from the user's input, such as the
destination city or the type of cuisine desired.
Example of Intent classification:
User: Hi! Where can I find a restaurant near me?
Bot: Sure, I can help you with that. What type of cuisine
are you looking for?
Example of a Slot filling:
User: I am looking for a hotel
Bot: What city are you looking for a hotel in?
This paper discusses the tasks of intent classification and
slot filling in NLU and their interrelationship. Traditional
independent models for these tasks suffer from error
propagation as they do not consider the mutual relationship
between them. The paper introduces a bi-directional joint
model for intent classification and slot filling that contains a
multi-stage hierarchical process using BERT and a bi-
directional NLU mechanism. By utilizing the intent2slot and
slot2intent models, the suggested approach accomplishes
complete combined benefits for joint intent categorization and
slot filling, and is trained on the concurrent loss of slot labeling
and intent categorization. The contributions of the paper
include a bi-directional joint NLU mechanism and a multi-
stage hierarchical process integrating Transformer-based bi-
directional NLU mechanism and modeling.
II. RELATED WORK
There has been significant prior work on language models
with prior training for NLP tasks, particularly with the release
of the BERT model. Some of the most influential pre-trained
models include ELMo [1] and Generative Pre-trained
Transformer (GPT) [2]. ELMo was the first pre-trained
language model to use bidirectional LSTMs, and it was
demonstrated to produce cutting-edge outcomes on a variety
of NLP tasks. GPT, on the other hand, uses a generative
language modeling objective to pre-train a large-scale
transformer-based language model.
Redford et. al. [3] shows how language models trained on
a diverse dataset called WebText can perform natural
language processing tasks without explicit supervision. The
ed on January 30,2026 at 10:00:53 UTC from IEEE Xplore. Restrictions apply.

=== Page 2 ===
largest model, GPT-2, achieves state-of-the-art results on 7
out of 8 language modeling datasets in a zero-shot setting.
Jun et. al. [4] evaluates the ANNA pre-trained language
representation model's performance on NLP tasks using a
neighbor-aware mechanism to capture context. ANNA
outperforms other pre-trained language models on the SQuAD
1.1 and SQuAD 2.0 benchmarks for question answering.
Further research is needed to confirm ANNA's
competitiveness for other NLP tasks and its robustness for
real-world business question answering tasks.
Gunaratna et. al. [5] proposes a new approach for joint
intent detection and slot filling in NLU that improves accuracy
and provides fine-grained explanations. It uses a collection of
binary classifiers for slot type-specific feature learning and is
evaluated on two datasets. The model is inherently
explainable and the first to explain how slots are filled
decisions without post-hoc processing. Extending the model
is a task for the future to explain intent detection and exploring
its use in other tasks like text classification and named entity
recognition (NER).
Hakkani-Tür et. al. [6] proposes a bi-directional RNN-
LSTM architecture to estimate complete semantic frames of
user utterances in a conversational system. It contributes by
investigating alternative architectures for modeling lexical
context and providing a joint multi-domain model that enables
multi-task deep learning. The proposed architecture shows
better performance than other methods on Microsoft Cortana
real user data. The approach has the potential to handle belief
state updates and non-lexical contexts in one holistic model,
leading to an end-to-end conversational understanding
framework.
Liu and Lane [7] presents a novel attention-based neural
network model for joint intent detection and slot filling, using
the ATIS task dataset. The model utilizes an encoder-decoder
framework with attention mechanism and alignment-based
RNN models to achieve the best-in-class performance for both
tasks. The proposed joint training model shows an absolute
error decrease on intent detection of 0.56% and an absolute
gain of 0.23% on slot filling compared to independent training
models. This research proposes a method to add alignment
information to attention-based neural network models to
improve intent detection and slot filling, enabling better
speech and dialog understanding.
Goo et. al. [8] proposes slot-gated mechanism to improve
semantic frame results, outperforms baseline on ATIS and
SNIPS datasets by 4.2% and 1.9% respectively. Allows for
explicit learning of intent-slot relations, filling an important
gap in SLU work. An innovative neural network model for
joint slot filling and intent detection that preserves the orderly
relationship between words, slots, and intents proposed by
Zhang et. al. [9]. It uses dynamic routing-by-agreement
and a rerouting schema to improve performance and
outperforms existing models. It fills a gap in current efforts
that handle intent detection and slot filling separately or do not
explicitly preserve the semantic hierarchy.
Chen et. al. [10] proposes a simultaneous intent
classification and slot filling model based on BERT to
enhance the generalization potential of NLU models. The
proposed model outperforms existing models on public
datasets and provides guidance for future research on using
external knowledge with BERT for more complex NLU
datasets.
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

Qin et. al. [11] proposes a framework for spoken language
understanding (SLU) utilizing a combined model with token-
level intent detection and StackPropagation. The suggested
model performs at the cutting edge on two publicly accessible
datasets and incorporates the BERT model. The research gap
is to explore the effect of stronger pretrained models in SLU
tasks.
III. METHODOLOGY
A. Input layer that maps words to numerical
representations.
The methodology comprises of a two-way joint model for
classifying intent and slot filling. The user's utterances as text
phrases in the input data, is tokenized and converted to
numerical representations using a pre-trained BERT-BASE
model. The suggested approach combines a multi-stage
hierarchical process via BERT and bi-directional joint natural
language understanding techniques, such as intent2slot and
slot2intent, to achieve mutual performance enhancement
between intent categorization and slot filling. The model is fed
the feature size N, which includes the BERT [CLS] and [SEP]
tokens. On two benchmark datasets, ATIS and SNIPS, the
model obtains cutting-edge performance in intent
classification accuracy, slot filling F1 score, and sentence-
level semantic frame accuracy. The model is implemented
using deep contextual embeddings and the transformer
architecture. The proposed model could be extended to
explain intent detection and explore its use in other NLU tasks.
B. Bi-Directional NLU Modelling Layer
On top of the NLU Modelling Layer, which contains two
models—intent2slot for slot filling and slot2intent for intent
classification—we propose a Bi-Directional NLU layer. The
intent2slot model uses the full sequence's semantic
information to identify each slot and is based on the
probability distribution of intent. The slot2intent incorporates
the sequence label probability distributions as additional data
for intent categorisation. In order to jointly simulate intent
classification and slot filling, the model is trained on the joint
loss of slot labeling and intent classification. The suggested
model uses slot probabilities and BERT to derive initial intent
probability. The particular BERT type features twelve encoder
stacks, 12 attention heads, and a 768-dimension final hidden
layer. A two-layer feed-forward neural network and a multi-
head attention module are both present in each encoder stack.
It uses a chance of 0.1 for attention dropout, and the output of
the second layer is expanded to include residual connection.
The output shape is [N, D] ∈RN×DE. To prevent potential
E
overfitting, including a layer with a 0.1 dropout rate.
Intent2Slot Model: The intent2slot model extracts
semantic information from a sequence to determine the
probability of intent and slot labels. The encoder output is
denoted as (h1, h2, . . . , h ) where each hi represents a token's
N
hidden layer state. To classify the complete sequence, the
[CLS] token has been trained, and the intent probability P is
I
obtained by passing h1 across a thick layer and applying
softmax classifier.
P = softmax(h ×WT + b ) (1)
I 1 I I
The dense layer has DIP nodes represented by a matrix of
weights WI ∈R DIP ×DE and a bias vector bI ∈R DIP. The
resulting vector of probabilities PI ∈R DIP is broadcasted to
the length of sequence to be labeled by repeating it N-1 times
to get PˆI.
ed on January 30,2026 at 10:00:53 UTC from IEEE Xplore. Restrictions apply.

=== Page 3 ===
ˆ
P = repeat(P ), N−1 times (2)
I I
We recover the hidden state sequence of N-1 tokens, each
with a dimension of D E, to develop intent2slot:
H = (h , h , . . . . . , h ) (3)
2 3 N
To obtain the slot probability for each word in intent2slot, we
combine each element of the hidden state sequence H with P
I
by concatenating them, and then input the result into a softmax
classifier.
ˆ
S = (H ⊕ P I) · VT + m (4)
S S
To obtain the slot probability for each word in the input
sequence, we create a matrix S∈R(N-1)×D with dimensions
SP
(N-1) x [D + D], where the intent probability is
E IP
concatenated with a BERT encoding in each row. A dense
layer of size D is applied to each row using an array of N-1
SP
matrices of weights V S with dimensions R (N-1) x DSP x (DE+DIP),
and an array of N-1 vectors of biases m with dimensions (N-
S
1) x D . The N-1 linear outputs are then fed into N-1
SP
softmaxes, the projected slot label for each relevant element
of the input sequence being provided by each argmax.
S = argmax(softmax(Sn)), n ∈ {2, . . ,N} (5)
n
This means that Sn is a matrix with first axis as S.
Slot2Intent Model: The slot2intent model is a model that
incorporates the slot label probability distributions into intent
detection. It does so by taking the tokens' last hidden state
sequence, excluding the [CLS] token from a BERT model,
where each hidden state has dimension D E.
H = (h2, h3, . . , h) (6)
N
Each hidden state h ∈ H is passed through a dense layer
n
with DSP nodes and softmax is applied to obtain slot label
probability distributions for each token.
P = softmax(h ×WT +b ), n ∈ {2, . . . , N} (7)
Sn n Sn Sn
This produces a matrix of size (N-1)×D where each row
SP
corresponds to a token in the input sequence (excluding the
[CLS] token) and each column corresponds to a slot label.
These probability distributions are then flattened to obtain a
vector of length W ∈ D x (N-1), denoted as P ˆ .
S SP S
ˆ
P = flatten(PS ), n ∈ {2, . . . . . , N} (8)
I n
The intent hidden state is obtained from the [CLS] token's
final hidden state and is concatenated with P^S. This
concatenated vector is passed through a softmax classifier.
I=argmax(softmax((h1⊕PˆS)×VT+m)) (9)
I I
The output is a V vector of length D where each entry
I IP
corresponds to an intent label. The intent label with the highest
probability is selected as the predicted intent label.
Joint optimisation: The intent categorization and slot
filling tasks are jointly optimized by the slot2intent model.
The slot labeling loss is added to train the model. (LS) and the
intent classification loss (LI), where L is the total loss and is
defined as:
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

y y
L I = −∑i∈I I( g=i)log i (10)
y
i is the output from the softmax classifier in Equation (9).
The model learns to minimize this overall loss during training.
L S =−∑N n=2 ∑s∈S I(yn
g=s)logyn
s (11)
where I(yn g=s)is the indicator function.
The slot loss is calculated by summing over all tokens in
the sequence and all possible slot labels. The cross-entropy
loss function is used to calculate how much the genuine labels
deviate from the projected slot labels.
IV. EXPERIMENTATION DETAILS
The experiment portion uses the ATIS and SNIPS datasets
to assess a bi-directional joint model for intent classification
and slot filling to cutting-edge models and performs an
ablation study. The BERT model used has a dropout rate of
0.1, a weight initialization standard deviation of 0.02, and is
trained for 10 epochs on ATIS and 20 epochs on SNIPS. The
evaluation metrics used include span-based slot f1-score,
intent accuracy, and semantic accuracy, and the Python
conlleval script is used with the "seqeval" package to calculate
F1-score. Results reveal that the suggested model performs
better than the most recent models, and the ablation study
shows the contributions of different layers towards
performance.
A. Experiment 1: Overall assessment of the joint model
On two datasets, a joint model for intent classification and
slot filling was assessed and contrasted with earlier state-of-
the-art models. It made use of the suggested bi-directional
contextual contribution (slot2intent, intent2slot).
B. Experiment 2: access intent classification
In the intent classification evaluation, baseline models
were trained on the SNIPS dataset with 20 epochs and the
intent detection F1 score was measured.
C. Experiment 3: replacing certain components
We conducted an ablation study on the ATIS dataset to
investigate the effect of different components of the model.
They swapped out the input embedding layer with GloVe or
word2vec, the NLU modeling layer with LSTM or GRU, and
tested each combination with or without the bidirectional
NLU layer.
V. RESULTS AND DISCUSSION
A. Result for Experiment 1:
The proposed model outperformed the previous models on
all tasks, with improvements ranging from 1.1% to 11.9%.
The model also outperformed other models like joint BERT
and Stack-Propagation BERT. The results show that Utilizing
slots specific to certain intents still enhances the performance
of intent classification, even though the ATIS dataset has
imbalanced intent label distribution and a high number of
mutually shared slots. The longer sequence duration and more
slots in ATIS may have contributed to the relatively lesser
improvement in the slot filling F1 score. The improvement
nevertheless confirms the usefulness of the intent2slot flow.
In comparison to ATIS, SNIPS has a larger vocabulary
size due to its cross-domain topics, with relatively higher
accuracy of 99.2% for intent classification and a smaller
number of overlapping slots (11 vs 79). SNIPS has 72 slots for
ed on January 30,2026 at 10:00:53 UTC from IEEE Xplore. Restrictions apply.

=== Page 4 ===
7 intents with a maximum slot type count of 15 for
BookRestaurant and a minimum of 2 for
SearchCreativeWork. The proposed model improves slot
filling performance with intent2slot flow, but the
improvement is slightly lower than ATIS. Slot value
ambiguity in SNIPS (e.g. location entity labeled as CITY,
STATE, or COUNTRY) without an explicit dictionary may
cause confusion for the machine.
B. Result for Experiment 2:
The study found that the F1 scores for intent
SearchCreativeWork and SearchScreeningEvent were lower
due to non-discriminative slots in their training datasets,
leading to confusion and misclassification. Capsule-NN
outperformed Slot-gated only for intent RateBook, which has
a higher proportion of unique slots. For all intents and
purposes, the proposed model produced better F1 scores,
which confirms the effectiveness of slot2intent and mutual
augmentation.
Tab.1 shows the performance comparison of various
models on ATIS and SNIPS datasets for slot filling (measured
in F1 score), intent detection (measured in accuracy), and
sentence classification (measured in accuracy).
TABLE 1 PERFORMANCE OF NLU ON THE ATIS AND SNIPS
DATASETS
ATIS - 10 epoch SNIPS - 20 epoch
Model Slot Intent Sent Slot Intent Sent
(F1) (acc) (acc) (F1) (acc) (acc)
Joint Seq. 94.3 92.6 80.7 87.3 96.9 73.2
Joint
94.2 91.1 78.9 87.8 96.7 74.1
Attention
Slot Gated
94.8 93.6 82.2 88.8 97 75.5
Filling
Slot Gated
95.2 94.1 82.6 88.3 96.8 74.6
Intent
Capsule NN 95.2 95 83.4 91.8 97.3 80.9
Joint BERT 96.1 97.5 88.2 97 98.6 92.8
Stack
96.1 97.5 88.6 97 99 92.9
Propagation
Our Model 97.7 98.7 89.3 98.3 99.6 93.4
C. Result for Experiment 3:
On both datasets, the combination of "BERT NLU
modeling + BERT embedding + Bidirectional NLU layer"
outperformed all others, while changing the BERT NLU
modeling and BERT embedding to different combinations
had a negative impact on performance. This demonstrates
how well BERT works for word embedding and NLU
modeling. The GloVe and word2vec models in particular saw
an improvement in performance overall because to the
bidirectional NLU layer. The results of the ablation study
highlight the importance of using BERT for word embedding
and NLU modeling, as well as the effectiveness of the
bidirectional NLU layer.
Fig. 1 Performance Comparison chart of Proposed Model
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

Figure 1 shows a breakdown of intent detection
performance of the proposed model and previous state-of-the-
art models on the ATIS dataset.
VI. CONCLUSION
The proposed joint model for intent classification can be
concluded from the trials and findings discussed and slot
filling with bi-directional contextual contribution (slot2intent,
intent2slot) superior than current cutting-edge models and
other BERT-based joint models. The model was evaluated on
two datasets, ATIS and SNIPS, and showed significant
improvements in intent classification performance,
particularly for SNIPS which has a larger vocabulary size and
relatively higher accuracy but also more ambiguity in slot
values. The ablation study conducted on the ATIS dataset
demonstrated the effectiveness of BERT for word embedding
and NLU modeling, as well as the significance of bidirectional
NLU layer for overall performance improvement, especially
for GloVe and word2vec models. The study also revealed that
unique slots still contribute to intent classification
performance improvement, and that confusion and
misclassification can result from non-discriminative slots in
training datasets. Overall, the proposed model and its
components have shown promising results when it comes to
natural language understanding, particularly in intent
classification and slot filling tasks.
VII. SCOPE FOR FURTHER STUDY
While this study has shown that the suggested joint model
and bi-directional NLU layer are effective for the tasks of
intent classification and slot filling, there is still room for
further research. One potential avenue for future work is to
investigate the model's performance on more diverse and
complex datasets with a wider range of intents and slots, as
well as exploring the use of multi-task learning and transfer
learning techniques to improve performance on smaller
datasets. There is potential to explore different types of
embeddings, such as contextualized embeddings, and to
incorporate additional sources of knowledge such as
ontologies or external knowledge bases. Finally, there is also
scope for exploring the use of explainable AI techniques to
help understand and interpret the model's decision-making
process, particularly in high-stakes applications such as
healthcare or finance. This study provides a strong foundation
for future research in this area, with the potential to improve
the accuracy and robustness of NLU models for a range of
applications.
REFERENCES
[1] M. E. Peters et al., “Deep contextualized word representations,”
NAACL HLT 2018 - 2018 Conf. North Am. Chapter Assoc.
Comput. Linguist. Hum. Lang. Technol. - Proc. Conf., vol. 1, pp.
2227–2237, 2018, doi: 10.18653/v1/n18-1202.
[2] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever,
“Improving Language Understanding by Generative Pre-
Training,” Comput. Sci., 2018.
[3] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I.
Sutskever, “Language Models are Unsupervised Multitask
Learners,” Comput. Sci., 2019.
[4] C. Jun et al., “ANNA: Enhanced Language Representation for
Question Answering,” Proc. Annu. Meet. Assoc. Comput.
Linguist., pp. 121–132, 2022, doi: 10.18653/v1/2022.repl4nlp-
1.13.
[5] K. Gunaratna, V. Srinivasan, A. Yerukola, and H. Jin,
“Explainable Slot Type Attentions to Improve Joint Intent
Detection and Slot Filling,” Find. Assoc. Comput. Linguist.
EMNLP 2022, pp. 3367–3378, 2022.
ed on January 30,2026 at 10:00:53 UTC from IEEE Xplore. Restrictions apply.

=== Page 5 ===
[6] D. Hakkani-Tür et al., “Multi-domain joint semantic frame parsing
using bi-directional RNN-LSTM,” Proc. Annu. Conf. Int. Speech
Commun. Assoc. INTERSPEECH, vol. 08-12-September-2016,
pp. 715–719, 2016, doi: 10.21437/Interspeech.2016-402.
[7] B. Liu and I. Lane, “Attention-based recurrent neural network
models for joint intent detection and slot filling,” Proc. Annu.
Conf. Int. Speech Commun. Assoc. INTERSPEECH, vol. 08-12-
September-2016, no. 1, pp. 685–689, 2016, doi:
10.21437/Interspeech.2016-135.
[8] C. W. Goo et al., “Slot-gated modeling for joint slot filling and
intent prediction,” NAACL HLT 2018 - 2018 Conf. North Am.
Chapter Assoc. Comput. Linguist. Hum. Lang. Technol. - Proc.
Conf., vol. 2, pp. 753–757, 2018, doi: 10.18653/v1/n18-2118.
[9] C. Zhang, Y. Li, N. Du, W. Fan, and P. S. Yu, “Joint slot filling
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

and intent detection via capsule neural networks,” ACL 2019 - 57th
Annu. Meet. Assoc. Comput. Linguist. Proc. Conf., pp. 5259–5267,
2020, doi: 10.18653/v1/p19-1519.
[10] Q. Chen, Z. Zhuo, and W. Wang, “BERT for Joint Intent
Classification and Slot Filling,” 2019, [Online]. Available:
arxiv.org/abs/1902.10909
[11] L. Qin, W. Che, Y. Li, H. Wen, and T. Liu, “A stack-propagation
framework with token-level intent detection for spoken language
understanding,” EMNLP-IJCNLP 2019 - 2019 Conf. Empir.
Methods Nat. Lang. Process. 9th Int. Jt. Conf. Nat. Lang. Process.
Proc. Conf., pp. 2078–2087, 2019, doi: 10.18653/v1/d19-1214.
ed on January 30,2026 at 10:00:53 UTC from IEEE Xplore. Restrictions apply.

Paper:Logistic_Regression-Based_Example_Selection_for_Enhanced_Few-Shot_Learning_in_Intent_Classification.pdf
=== Page 1 ===
2025 IEEE International Conferenc
Logistic Regression-Base
Enhanced Few-Shot
Classifi
Gyutae Park
Department of Artificial Intelligence
Chung-Ang University
Seoul, Republic of Korea
pkt0401@cau.ac.kr
Abstract—This study introduces a novel approach to example
selection in few-shot learning scenarios for dialog intent classifi-
cation,leveraginglogisticregressiontorefinethesetofexamples
retrieved through traditional similarity-based methods. We eval-
uate our method on three benchmark datasets: BANKING77,
CLINC150, and HWU64, using 5-shot and 10-shot learning
setups with 20 demonstrations. Our results show improvements
in classification accuracy compared to baseline similarity-based
retrieval methods, particularly for semantically similar intents.
Notably, weobserve areduction inmisclassifications within sim-
ilar domains after applying our proposed approach. This work
contributestothegrowingbodyofresearchonefficientfew-shot
learning techniques for natural language understanding tasks,
offering insights into enhancing performance in challenging,
domain-specific scenarios.
IndexTerms—Few-shotlearning,Multi-classclassification,In-
tent classification, Logistic regression, Example selection
I. INTRODUCTION
Intentclassificationisthetaskofidentifyingtheunderlying
intent behind a user’s input in a dialog system. This task
plays a key role particularly in building dialogue systems
and customer service automation. Traditional intent classifi-
cation methods require large labeled datasets and have the
disadvantage of needing to retrain the model whenever new
intents are added. [1] As an alternative, few-shot learning
methods based on Large Language Models (LLMs), uti-
lizing in-context learning, have gained attention. But they
face limitations in including examples for all intents in the
prompt when the number of intents is large. [2], [3] Thus,
research on optimizing the application of LLMs for multi-
class intent classification remains still insufficient. [4] To
overcome these limitations, we propose a new approach that
combines similarity-based example selection methods using a
retrieverwithlogisticregression.Ourmethodenableseffective
handling of a large number of intents by selecting only
a small number of examples most similar to the query to
include in the prompt. Our experiments on three benchmark
datasets(HWU64,CLINC150andBANKING77)demonstrate
consistent performance improvements over baseline methods
†CorrespondingAuthor.
999999777777999999------888888------333333333333111111555555------222222111111111111666666------555555//////222222555555//////$$$$$$333333111111......000000000000 ©©©©©©222222000000222222555555 IIIIIIEEEEEEEEEEEEEEEEEE
17003901.5202.74636ECCI/9011.01
:IOD
| EEEI
5202©
00.13$/52/5-6112-5133-8-979
|
)ECCI(
scinortcelE
remusnoC
no
ecnerefnoC
lanoitanretnI
EEEI
5202
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

ce on Consumer Electronics (ICCE)
ed Example Selection for
t Learning in Intent
fication
Hwanhee Lee†
Department of Artificial Intelligence
Chung-Ang University
Seoul, Republic of Korea
hwanheelee@cau.ac.kr
in both 5-shot and 10-shot settings. Notably, we observe
a significant reduction in misclassifications within similar
domains across all datasets. These results suggest that our
approach is particularly effective at distinguishing between
subtle differences in semantically similar intents, even with
limited examples per intent.
II. METHODS
Our proposed method for enhancing few-shot learning in
intentclassificationcombinesthestrengthsofsimilarity-based
retrievalandlogisticregressiontoselectthemostrelevantand
informative examples for a given query. Our approach aims
to overcome the limitations of traditional few-shot learning
methods when dealing with a large number of intents.
A. Initial Similarity-Based Example Selection
In multi-class classification problems, it is practically im-
possible to include examples for all intents in the prompt.
Therefore, our work selects initial examples through the fol-
lowing process:
1) Sentence Embedding: Use a pre-trained SentenceTrans-
formermodel(all-mpnet-base-v2)1toembedalltraining
data and queries into vector space.
2) Cosine Similarity Computation: Compute the cosine
similaritybetweenthequeryembeddingandalltraining
data embeddings.
3) Initial Example Selection: Select k examples (k = 60,
80, 100, 120) with the highest similarity.
B. EmbeddingSpaceTransformationthroughLogisticRegres-
sion
Simple similarity-based selection has limitations in dis-
tinguishing examples that are semantically similar but have
different intents. [5] To overcome this limitation, we perform
embedding space transformation through logistic regression
with the following formula:
P(y =c|x)=σ(wTx+b ), (1)
c c
1https://huggingface.co/sentence-transformers/all-mpnet-base-v2
ed on January 30,2026 at 09:23:08 UTC from IEEE Xplore. Restrictions apply.
979-8-3315-2116-5

=== Page 2 ===
Sample Selection
Query : Is summer going on
Initial Retriever Logistic
5, 10-shot Dataset Logistic regr
ranking of to
Top-k Selection
k = 60, 80, 100, 120
Initial Examples (k=60) Final Exam
1. "What's the weather like?" (Intent: get_weather) 1. "What's the weather l
2. "Play some music" (Intent: play_music) get_weather)
3. "Set an alarm for 7 AM" (Intent: set_alarm) 2. "Is it hot outside?"
4. "Is it going to rain today?" (Intent: get_weather) 3. "What's the temperatu
5. "Turn up the volume" (Intent: volume_up) get_weather)
... ...
... ...
58. "What's the forecast for tomorrow?" (Intent: 20. "Do I need a jacket
get_weather) get_weather)
59. "Is ithot outside?" (Intent: get_weather)
60. "Do I need a jacket?" (Intent: get_weather)
Fig.1. Detailedflowdiagra
where σ is the sigmoid function, wT is the weight vector
c
for class c, and b is the bias term. We transform the original
c
embedding space using the weight matrix W of the trained
x
logistic regression model as follows:
x′ =W x (2)
x
To train the logistic regression model, we use the initially
selected k examples as training data. We train the model
using cross-entropy loss and optimize it using the L-BFGS
algorithm,whichisparticularlyeffectiveforsmalltomedium-
sized datasets. [6], [7]
Algorithm 1 Logistic Regression-based Example Selection
Require: Query q, Dataset D, Initial selection size k, Final
selection size m=20
Ensure: Final set of m examples
1: E ←Retriever(q,D,k) {Select initial k examples}
2: X ←Embed(E) {Embed selected examples}
3: y ←Labels(E) {Get labels of examples}
4: W ←TrainLogisticRegression(X,y)
5: X new ←WX {Transform embedding space}
6: S ←CosineSimilarity(Embed(q),X new )
7: E final ←TopK(E,S,m)
8: return E final
C. Final Example Selection and Constructing LLM Input
Inthetransformedembeddingspace,weselectthefinalex-
amplesandconstructinputfortheLLMthroughthefollowing
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

Inference
LLM Input
c Regression
Final Examples +
ression-based re- Query: Is summer going
op-k examples on
(Task: Predict the intent of
the given query)
mples (Top 20)
like today?" (Intent:
(Intent: get_weather)
ure now?" (Intent:
Results
today?" (Intent:
“get_weather”
(Predicted Intent)
amoftheproposedmethod
process:
1) New Similarity Calculation: Recalculate the similarity
between the query and examples in the transformed
space.
2) Final Example Selection: Sort by similarity score in
descending order and select the top 20 examples.
3) Constructing LLM Input: Arrange the selected exam-
ples from lowest to highest similarity and input them
to the LLM. This allows the LLM to process pro-
gressively more relevant information, potentially giving
more weight to the most important information in the
final prediction.
Fig. 2. Comparison of t-SNE visualizations for HWU64 dataset with 60
examples: (left) Original Embedding Space, (right) Transformed Logistic
RegressionEmbeddingSpace
ed on January 30,2026 at 09:23:08 UTC from IEEE Xplore. Restrictions apply.

=== Page 3 ===
III. EXPERIMENTALSETUP
In this section, we outline the datasets and experimental
configuration for evaluating our logistic regression-based ex-
ample selection method for few-shot intent classification. We
describe the models used, the datasets employed, and the
specific experimental settings for our study.
Models Used:
• Large Language Model: We use the Llama 2 7B
model[8]developedbyMetaAI.Thismodelhas7billion
parameters and was trained on public web crawling data
and conversational data.
• Retriever: We use the all-mpnet-base-v2 model based on
Sentence-BERT. This model is effective in generating
sentence embeddings and captures semantic similarities
well.
Datasets:
• HWU64 [9]: A dataset related to smart home device
control, covering 21 domains with 64 intents.
• CLINC150 [10]: A dataset covering various daily tasks
and services across 10 domains, containing 150 intents.
• BANKING77 [11]: A dataset related to financial and
banking services, containing 77 intents.
Experimental Configuration:
• Few-shot Learning Setup: This study used the existing
split from the Dialoglue dataset [12]. This split provides
5-shot and 10-shot settings, including 5 or 10 examples
per intent.
• Baseline: A method that directly selects the top 20
examples based on simple similarity.
• Proposed Method: Initially selects K examples (60, 80,
100, 120), then chooses the final 20 through logistic
regression.
• Evaluation Metric: Accuracy
IV. RESULTSANDANALYSIS
We conduct experiments on three intent classification
datasets: BANKING77, CLINC150, and HWU64, using both
5-shotand10-shotsettings.Thebaselinemethoddirectlyuses
thetop20examplesbasedonretrieversimilarity.Ourproposed
method initially selects 60, 80, 100, or 120 examples, then
chooses the final 20 using logistic regression.
A. Performance Comparison
Table I,II shows the accuracy results for each dataset.
Upon analyzing the results, we demonstrate that the pro-
posed method consistently outperforms the baseline across
the majority of cases. Based on the results, we identify the
following notable findings.
• The HWU64 dataset showed the largest performance
improvement. In the 5-shot setting, accuracy increased
from the baseline of 85.32% to a maximum of 87.14%,
and in the 10-shot setting, from 86.62% to 88.57%.
• For the CLINC150 dataset, although the improvement
was smaller, it was consistent. This may be due to the
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

TABLEI
ACCURACYRESULTS(%)FOR5-SHOTSETTING
Dataset Baseline ProposedMethod(K)
(K=20) 60 80 100 120
HWU64 85.32 85.78 85.87 87.14 86.71
CLINC150 93.22 93.36 93.49 93.38 93.25
BANKING77 85.55 85.62 86.40 86.17 86.33
TABLEII
ACCURACYRESULTS(%)FOR10-SHOTSETTING
Dataset Baseline ProposedMethod(K)
(K=20) 60 80 100 120
HWU64 86.62 86.90 88.29 88.57 88.20
CLINC150 94.26 94.39 94.28 94.20 94.18
BANKING77 89.22 89.38 89.18 89.10 89.14
already high baseline performance (over 93%), making
further improvements challenging.
• The BANKING77 dataset also showed modest improve-
ments. In the 5-shot setting, accuracy improved from the
baseline of 85.55% to a maximum of 86.40%.
These results suggest that the proposed method is particu-
larly effective at distinguishing between subtle differences in
similar intents. In other words, the logistic regression-based
transformation of the embedding space appears to help in
selecting more semantically relevant examples.
B. Domain-specific Misclassification Analysis
To further understand the effectiveness of our method, we
conduct a domain-specific misclassification analysis on the
HWU64 dataset, in the 10-shot setting. Figure 3 shows the
number of misclassifications within the same domain and
across different domains for both the baseline and proposed
methods.
Fig.3. Domain-specificmisclassificationanalysisfor(left)thebaselinemodel
and(right)afterapplyinglogisticregression-basedexampleselection(K=100)
onHWU64(10-shot)
The analysis reveals the following findings:
• Thebaselinemodel(Fig.3,left)had61misclassifications
within the same domain and 56 misclassifications across
different domains. This suggests that the baseline model
struggled to distinguish between intents within similar
domains.
ed on January 30,2026 at 09:23:08 UTC from IEEE Xplore. Restrictions apply.

=== Page 4 ===
• After applying our proposed method (Fig. 3, right), mis-
classifications within the same domain decreased to 52,
while misclassifications across different domains slightly
increased to 57.
• This reduction in within-domain misclassifications (from
61to52)indicatesthatourmethodsignificantlyimproved
the model’s ability to distinguish between similar intents
within the same domain.
These findings indicate that our logistic regression-based
example selection method is particularly effective at improv-
ing intent classification performance for semantically similar
intents within the same domain.
C. Impact of Initial Example Set Size
We varies the initial number of selected examples (K) from
60 to 120 and observe the following findings:
• Performance generally peaked when K was 80 or 100.
• Smaller K values (60) may not provide sufficient infor-
mation for effective logistic regression.
• Larger K values (120) might introduce noise, slightly
degrading performance.
V. CONCLUSIONANDFUTUREWORK
This study proposed a novel approach to example selec-
tion in few-shot learning scenarios for intent classification,
leveraging logistic regression to refine the set of examples
retrievedthroughtraditionalsimilarity-basedmethods.Ourex-
periments on three benchmark datasets (HWU64, CLINC150,
and BANKING77) yielded several key findings:
• PerformanceImprovement:Ourmethodconsistentlyout-
performed the baseline similarity-based retrieval across
all datasets, with the most significant improvements ob-
served in the HWU64 dataset.
• Effective Disambiguation: The logistic regression-based
transformation of the embedding space proved partic-
ularly effective in distinguishing between semantically
similar intents, as evidenced by the reduction in within-
domain misclassifications.
• OptimalExampleSetSize:Wefoundthatinitialselection
of 80 to 100 examples generally yielded the best per-
formance, balancing between sufficient information and
minimal noise.
• Generalizability:Theproposedmethodshowedeffective-
nessacrossvariousdomains(smarthome,dailytasks,and
banking), suggesting good generalization capabilities.
These results demonstrate the potential of our approach in
enhancing few-shot learning performance for intent classifica-
tion, particularly in scenarios with many semantically similar
intents.
ACKNOWLEDGEMENT
This research was supported by Institute for Information
&CommunicationsTechnologyPlanning&Evaluation(IITP)
throughtheKoreagovernment(MSIT)underGrantNo.2021-
0-01341 (Artificial Intelligence Graduate School Program
(Chung-Ang University)).
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

REFERENCES
[1] C. Zhang, Y. Li, N. Du, W. Fan, and P. Yu, “Joint Slot Filling and
Intent Detection via Capsule Neural Networks,” in Proceedings of the
57thAnnualMeetingoftheAssociationforComputationalLinguistics,
Florence,Italy,2019,pp.5259–5267.
[2] Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D.,Dhariwal,P.,
Neelakantan,A.,Shyam,P.,Sastry,G.,Askell,A.,Agarwal,S.,Herbert-
Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,
Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford,
A., Sutskever, I., Amodei, D.(2020). Language Models are Few-Shot
Learners. In Advances in Neural Information Processing Systems, vol.
33,pp.1877–1901.CurranAssociates,Inc.
[3] O.Rubin,J.Herzig,andJ.Berant,“LearningToRetrievePromptsforIn-
ContextLearning,”inProceedingsofthe2022ConferenceoftheNorth
American Chapter of the Association for Computational Linguistics:
HumanLanguageTechnologies,Seattle,UnitedStates,2022,pp.2655–
2671.
[4] D.Yu,L.He,Y.Zhang,X.Du,P.Pasupat,andQ.Li,“Few-shotIntent
ClassificationandSlotFillingwithRetrievedExamples,”arXivpreprint
arXiv:2104.05763,2021.
[5] J. Zhang et al., “Discriminative Nearest Neighbor Few-Shot Intent
DetectionbyTransferringNaturalLanguageInference,”inProceedings
of the 2020 Conference on Empirical Methods in Natural Language
Processing(EMNLP),2020,pp.5064–5082.
[6] G. Andrew and J. Gao, “Scalable training of L1-regularized log-linear
models,” in Proc. 24th Int. Conf. Mach. Learn., Corvalis, OR, USA,
2007,pp.33–40.
[7] Q.V.Le,J.Ngiam,A.Coates,A.Lahiri,B.Prochnow,andA.Y.Ng,
“On optimization methods for deep learning,” in Proc. 28th Int. Conf.
Mach.Learn.,Bellevue,WA,USA,2011,pp.265–272.
[8] H. Touvron et al., “Llama 2: Open Foundation and Fine-Tuned Chat
Models,” arXiv preprint arXiv:2307.09288, 2023. [Online]. Available:
https://huggingface.co/meta-llama/Llama-2-7b
[9] I. Casanueva, T. Temcˇinas, D. Gerz, M. Henderson, and I.
Vulic´, “Efficient Intent Detection with Dual Sentence Encoders,”
arXiv preprint arXiv:2003.04807, 2020. [Online]. Available:
https://arxiv.org/abs/2003.04807
[10] X. Liu, A. Eshghi, P. Swietojanski, and V. Rieser, “Benchmarking
Natural Language Understanding Services for Building Conversational
Agents,”inIncreasingNaturalnessandFlexibilityinSpokenDialogue
Interaction:10thInternationalWorkshoponSpokenDialogueSystems,
E.Marchi,S.M.Siniscalchi,S.Cumani,V.M.Salerno,andH.Li,Eds.
Singapore:SpringerSingapore,2021,pp.165–183.
[11] S.Larsonetal.,“AnEvaluationDatasetforIntentClassificationandOut-
of-Scope Prediction,” in Proc. 2019 Conf. Empirical Methods Natural
Language Process. 9th Int. Joint Conf. Natural Language Process.
(EMNLP-IJCNLP),HongKong,China,2019,pp.1311–1316.
[12] S.Mehri,M.Eric,andD.Hakkani-Tur,“DialoGLUE:ANaturalLan-
guage Understanding Benchmark for Task-Oriented Dialogue,” arXiv
preprintarXiv:2009.13570,2020.
ed on January 30,2026 at 09:23:08 UTC from IEEE Xplore. Restrictions apply.

Paper:BERT_GPT-2_and_Retrieval-Augmented_Generation_RAG.pdf
=== Page 1 ===
Enhancing Public Access to Leg
Chatbot Using Legal BERT, GP
Generatio
Abhinav Garlapati* Hemanth K
department of Artificial Intelligence department of Art
And Data Science And Data
Velagapudi Ramakrishna Siddhartha Velagapudi Ramak
Engineering College Engineerin
Vijayawada,India Vijayawa
abhinavgarlapatik@gmail.com hemanth2488
Abstract— This work describes a legal chatbot that would be
able to support and aid in answering legal queries on Indian law
through the use of Legal BERT, GPT-2, and Retrieval-
Augmented Generation. This is a chatbot that has been trained
by fine-tuning it using a curated corpus consisting of the Indian
Constitution and other relevant legal texts, enabling the chatbot
to deliver the most accurate, contextually appropriate response.
Legal BERT enhances the chatbot's ability to understand
sophisticated legal terminology, while GPT-2 generates
responses similar to that of a human with material retrieved
from those sources. RAG is incorporated to enhance the
retrieval of relevant sources so that accurate and relevant
answers are delivered. Accuracy as well as efficiency is
displayed by the system; it has very high potential towards
enhancing access to public legal information, supporting legal
research, and assisting in access to justice initiatives under the
Indian legal structure.
Keywords— Legal Chatbot, Natural Language Processing,
Legal BERT, GPT-2, Retrieval-Augmented Generation (RAG),
Indian Law, Legal Information Retrieval, Access to Justice,
Constitutional Law
I. INTRODUCTION
The public finds it challenging to access and
comprehend India's legal system due to its intricate
framework of statutes, case law, and constitutional
requirements [7]. Particularly for non-lawyers, the complex
language and organization of legal texts create obstacles that
prevent citizens from understanding their rights and
responsibilities. Finding a lawyer is difficult in places with
little access to legal services, making this problem more
noticeable. Because of this, a large portion of the populace is
unaware of their legal rights, which makes it challenging to
make educated judgments. To bridge this gap, creative
strategies that use state-of-the-art technologies like artificial
intelligence (AI) and natural language processing (NLP) to
democratize access to legal information are required [5].
An intelligent legal chatbot is being introduced in this project
to address the drawbacks of conventional legal information
retrieval systems, which frequently use keyword-based
searches or rule-based responses. Complex legal queries were
beyond the capabilities of such computers, which frequently
produced inaccurate or unnecessary responses. This chatbot
provides customers with contextually relevant responses to
their legal inquiries by utilizing sophisticated AI models such
as Legal BERT, GPT-2, and Retrieval-Augmented
83581111.5202.73846ATECIceSPM/9011.01
:IOD
|
EEEI
5202©
00.13$/52/8-1312-5133-8-979
|
)ATECI
ceSPM(
snoitacilppA
dna
seigolonhceT
gnigremE
no
ecnerefnoC
lanoitanretnI
EEEI
5202
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

gal Knowledge in India: A Legal
PT-2, and Retrieval-Augmented
on (RAG)
Koutharapu Neha Doddi
tificial Intelligence department of Artificial Intelligence
a Science And Data Science
krishna Siddhartha Velagapudi Ramakrishna Siddhartha
ng College Engineering College
ada,India Vijayawada,City
8k@gmail.com nehadoddi17@gmail.com
Generation (RAG). A customized version of the Bidirectional
Encoder Representations from Transformers (BERT)
concept, Legal BERT is intended to interpret complex legal
jargon and technical terms [14]. In legal conversations, this
feature enables the chatbot to offer thorough, context-aware
responses.
By improving the chatbot's capacity to provide logical,
human-like responses, GPT-2 makes legal material more
understandable for both experts and laypeople [8]. Dynamic,
context-dependent responses to user inquiries are made
possible by its sophisticated generative language capabilities.
GPT-2 simplifies difficult legal ideas and produces responses
that are clear, pertinent, and interesting—in contrast to typical
rule-based systems. Combining the generative capabilities of
GPT-2 with the interpretive power of Legal BERT makes the
chatbot a powerful instrument for handling a variety of legal
questions. This guarantees that consumers will receive
responses to their legal queries that are applicable,
intelligible, and contextualized.
By allowing it to retrieve and consult reputable legal texts
including statutes, case laws, and constitutional provisions,
the integration of RAG greatly increases the chatbot's
accuracy [3]. By doing this, the chatbot's trustworthiness is
increased and responses are guaranteed to be both
contextually appropriate and based on credible sources.
Through the integration of RAG and GPT-2, the chatbot
offers explanations akin to those of a lawyer, backed up by
references to legal literature, enabling users to independently
confirm the facts. In the legal arena, where users need precise
responses with reliable references to make well-informed
decisions, this retrieval technique is essential.
This chatbot was developed using a vast collection of
Indian legal writings, including the Constitution, statutes, and
important case law. Using this database, Legal BERT was
refined to improve its comprehension of Indian legal
grammar and vocabulary [5]. RAG was set up for the real-
time retrieval of pertinent documents, while GPT-2 was
trained to produce responses that were easy to use based on
the legal information received. This AI-driven legal chatbot
is a revolutionary step in democratizing access to legal
information, enabling people to better comprehend their
rights and responsibilities [13]. By decreasing reliance on
middlemen, this project paves the way for future
ed on January 29,2026 at 14:50:37 UTC from IEEE Xplore. Restrictions apply.

=== Page 2 ===
developments in AI-powered instruments to enhance legal
literacy and accessibility to justice in India.
This paper’s contributions to legal AI technology and natural
language processing include:
1. The development of a chatbot model specifically
tailored to Indian law, trained on a custom dataset
containing Indian statutes, case laws, and
constitutional provisions to ensure contextual
accuracy.
2. A significant improvement in accuracy, achieving
94.6% in legal information retrieval and contextual
relevance, made possible by integrating RAG for
document retrieval.
3. A robust solution to storage and computational
constraints through model optimization, enabling
real-time deployment, and thus making the chatbot
accessible for wider use, especially in regions with
limited internet connectivity or computational
resources.
II. RELATED WORK
Nithana et al.[10]in the year 2020 presented the idea that
According to the research on chatbot implementation tactics,
AI and NLP algorithms can be used to build intelligent
systems that can react to user inquiry.Numerous chatbot
frameworks—both interface-based and code-based—are
examined, with a focus on their shortcomings in producing
dynamic, real-time dialogues. The incapacity of current
approaches—such as rule-based, machine learning, and
retrieval-based strategies—to generate high-quality
dialogues is examined, highlighting the necessity for more
developments in conversational AI, samant et al.[2]present a
wide range of language models and their performance in
multi-task NLU across a ten-year period (2011–2021) are
analyzed. It describes conceptual steps for improving multi-
task NLU and points out holes in general-purpose
frameworks for unsupervised models, on the other hand
vakayil et al.[3]introduce that chatbot targets a delicate topic
by helping victims of sexual harassment. Maintaining user
trust while producing sympathetic, encouraging, and
educational replies is its key goal. On the other hand, Bhat et
al. [4]focuses on the restaurant sector, where the chatbot
expedites client encounters by giving accurate, pertinent
answers. The first study focuses on user sensitivity and
emotional intelligence, whereas the second study stresses
operational efficiency and accuracy in language
interpretation for business improvement, Srivastaval et
al.[5]explains about how LAWBOT targets the legal domain,
a field characterized by dense and complex terminologies. It
aims to demystify legal processes for laypersons, providing
precise guidance for legal tasks through NLP, conversely
Hondoyo et al.[6]concentrates on e-commerce, particularly in
the ticketing industry. By automating booking procedures
and deciphering various user input patterns pertaining to
travel information, the chatbot here enhances customer care
while demonstrating flexibility in business settings, kandula
et al.[7]in this article they potrate about the intricacy of legal
language and principles, legal research is a complex process
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

that requires a great deal of time and knowledge.These
challenges are being addressed by the development of AI-
based legal assistance systems, which use state-of-the-art
Natural Language Processing (NLP) and Machine Learning
(ML) algorithms to automate and enhance legal research.
III. OBJECTIVES
1. Develop a very user-friendly chatbot that may understand
legal queries on Indian law properly and give responses that
are contextually relevant as well as legally sound.
2. Use Legal BERT to improve the chatbot's comprehension
of legal terminology, context, and more intricate Indian legal
jargon.
3. It utilizes the GPT-2 to produce response content that is
coherent and human-like, transparent enough for non-expert
users, thereby creating a link from legal language to the
layman's terms.
4. The responses should be accompanied with the relevant
legal documents, statutes, or case laws using the Retrieval-
Augmented Generation (RAG) method to ensure an increase
in the correctness of the responses and, in turn, users'
confidence in the respective answers.
5. Optimizing for real-time response allows chat users to have
fluid and instant human-computer interaction, rendering the
chatbot an accessible quick reference for legal purposes.
IV. PROPOSED SYSTEM
A. Process Flow
The law chatbot that will be developed during this project
starts as a query submitted by the user to law through the web
interface. This query is forwarded to the Legal BERT model.
Legal BERT explains the query using legal concepts and
terminologies as shown in Fig1. This question the user poses
requires specific key legal concepts and phrases pertaining to
the question of the user. It then refers to Retrieval-Augmented
Generation, which would perform the search for relevant
legal materials, statutes, cases, or even constitutional
provisions on the question stored in the databases. That is
very important because that ensures the content provided by
the chatbot is legally backed up by the right sources of the
law. The legal information section is over and now comes in
GPT-2, by the way, a generative language model. GPT 2 will
therefore produce an eloquently structured human-like
answer because of the legal content generated. The
information provided back to the user is therefore accurate
and relevant from the legal standpoint and presented in a
manner that is easy to understand. It is using this technique
that offers an excellent opportunity to process the customer
query in an effective manner and also ensures the resultant
information to be true.
ed on January 29,2026 at 14:50:37 UTC from IEEE Xplore. Restrictions apply.

=== Page 3 ===
Fig. 1. Process flow diagram of legal chatbot.
B. Architecture Selection
The architecture for the proposed system has been selected
very carefully so that the chatbot would be effective enough
at handling complicated legal questions and sending through
accurate and accessible responses. It has been selected
because Legal BERT is especially well-suited to understand
legal terms and contexts and interpret complex legal texts.
Legal BERT is very efficient in seeing and understanding
nuances in legal language so that it processes queries with a
very high degree of accuracy. GPT-2 was added to the model
to ensure a human-like coherent response. GPT-2 is a
generative model that produces strongly dynamic,
conversational responses, so the chatbot is now accessible to
legal professionals as well as lay people who may not be
familiar with legal jargon. Moreover, the chatbot used
Retrieval-Augmented Generation (RAG) for precision and
relevance. RAG will gather all the relevant documents into
legal documents that back up any statements the chatbot
makes with first-hand evidence. The information given to the
users is believable and verifiable. The system also has a user
interface that is web-based, which was one of the reasons for
this choice as it was highly scalable and accessible. From any
device, therefore through the web interface, users will be able
to gain access to the interaction with the chatbot, hence
ensuring wide access and usability.
C. Legal BERT Architecture
Legal BERT is an adaptation from the widely used pre-
trained BERT model fine-tuned specifically for the legal
domain. This exploits the transformer architecture and the
ability of encoding in both directions, which is one of the
most significant essentials in capturing deep context for legal
language.
Legal BERT is fine-tuned using an extensively large corpus
of general text but further fine-tuned over a curated collection
of Indian legal documents-the Indian Constitution, statutes,
case law, and other similar legal texts. This helps ensure that
Legal BERT can interpret unique legal terms and phrases
specific to the Indian legal system. Legal BERT, being a
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

variant of that language model, is particularly effective with
question answering applications in legal contexts because it
can interpret complex queries and discern meaningful
information in long and dense texts related to law. The model
creates contextual word embeddings, used for the
identification of relationships between terms and creating an
overarching knowledge of what the query intends. Legally
speaking, Legal BERT specializes in legal language and is a
key architectural component for realizing good
understanding of the query input with considerable accuracy
and relevance.
D. Generative Pre-trained Transformer. (GPT-2)
Generative Pre-trained Transformer, or GPT-2, is a state-
of-the-art natural language processing model that is well-
known for producing content that is both understandable and
reasonably suitable. It uses unsupervised learning on large
datasets to comprehend and anticipate language patterns,
making it ideal for applications such as chatbots,
summarization, and content generation. In the proposed
system, GPT-2 has been fine-tuned using legal data to address
domain-specific inquiries, ensuring accurate and context-
aware responses. According to the performance metrics bar
chart, the model achieves a 62% accuracy, a 1.5-second
response time, and an 18% error rate. These findings imply a
reasonable trade-off between accuracy and efficiency,
making GPT-2 a suitable option for real-time legal aid. Its
ability to tackle challenging legal matters illustrates its
potential to bridge the distance between legal specialists and
the general public.
E. Retrieval-Augmented Generation (RAG)
Applying Retrieval-Augmented Generation, RAG, instead
of resting on merely pre-trained models, such as GPT-2, that
retrieves highly relevant documents at real-time in
accordance with the query input by the user ensures an
upgrade of the quality and accuracy of the response in a
chatbot. Thus, the retrieval process ensures that the response
from the system is based on authorities such as the statutes,
case law, and even constitutional provisions. RAG uses a
retrieval algorithm of information that fetches documents
from a corpus of legal texts stored within the Legal Database.
Then for suitable retrieved documents, GPT-2 will then
produce a response both coherent and human-like yet
accurate in respect of law and properly grounded in content
of actual legal content. This integration of retrieval will make
sure that the chatbot is actually capable of serving legal
information validly and verifiably to users, and therefore it
helps in improving accountability in answers produced by the
system. Even complex and specific legal queries are very well
dealt with by the system through a combination of retrieval
and generation to great accuracy and relevance.
V. EXPERIMENTAL SETUP
A. Dataset Preparation
An organized and thorough dataset was created for the
legal chatbot's development utilizing a carefully selected
corpus of legal documents[16], including the Indian Penal
ed on January 29,2026 at 14:50:37 UTC from IEEE Xplore. Restrictions apply.

=== Page 4 ===
Code and the Constitution. The dataset was created with
contexts, queries, and responses in mind, all of which support
the chatbot's objective of providing precise legal responses.
In each case, a particular legal provision or idea is
represented, and then questions in plain language are asked,
along with the corresponding responses.The straightforward
data structure with questions and answers is shown below.
[
{
"context index": 1,
"question": "What is Article 14 of
the Indian constitution?",
"answer": "Article 14 provides
for equality before the law and equal
protection of the laws within the
territory of India."
}
]
Now the task at hand is to assign indices to every question
and response after the data collection process is finished.
Each question's context, question ID, and replies make up the
final dataset.Below is the sample structure of data that
consists of above mentioned variables.
[
{
"context": "Indian Constitution
Article 14 | (Equality before law)",
"qas": [
{
"id": "00001",
"is_impossible": false,
"question": "What is Article
14?",
"answers":
"text": "Equality before
the law and equal protection of
laws.",
"answer_start": 0
}
]
}
VI. RESULT AND ANALYSIS
This section displays the results of testing the AI-Based
Smart Legal Jury, with a focus on the efficiency and accuracy
of query processing using GPT-2, Legal-BERT, and FAISS.
The system's ability to understand legal questions, find
relevant information, and generate well-reasoned responses
was evaluated.The system was evaluated using three major
metrics: overall response time, the relevancy of generated
responses, and the accuracy of the legal information returned.
The written text's clarity was also evaluated to ensure that it
followed the law and was understandable. The technique was
tested using a dataset of 500 legal questions obtained from
databases of publicly available court cases. Legal documents
were used to generate the FAISS index[9], and Legal-BERT
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

was optimized for legal text comprehension. The GPT-2
model was trained to generate legally sound outcomes based
on user requests.
A. Accuracy Of Information Retrieval
From the below table our model (Legal BERT + GPT-2
+ RAG) has the best accuracy (82.5%), it is clear that Legal
BERT, GPT-2, and RAG work well together to retrieve
pertinent legal information.The custom model outperforms
other transformer models, such as RoBERTa at 77.0% and
Standard BERT at 78.0%, however they are not as accurate.
The fact that DistilBERT performs moderately (76.5%), but
Rule-Based and Keyword-Based models perform far worse
(70.0% and 65.5%, respectively), indicates that traditional
models are less successful at retrieving complicated legal
information.
B. Average Response Time
With a response time of 1.6 seconds, DistilBERT is the
fastest, closely followed by Our Model (1.4seconds). This
illustrates how the customized model can attain excellent
accuracy without significantly sacrificing speed.
Fig. 2. Metrics
The comparatively longer response times of RoBERTa and
Standard BERT (1.7 and 1.6 seconds, respectively) are
probably caused by their bigger structures.
The slower retrieval times of Rule-Based and Keyword-
Based models (1.9 and 2.0 seconds) may be the result of less
effective retrieval techniques.
C. Relevance Of Generated Text
Our model produces results that are extremely relevant
to user searches, as seen by its greatest relevance score of
84%.
Following with relevance scores of 79% and 80% are
RoBERTa and Standard BERT.Because of its lighter,
distilled architecture, DistilBERT has a lower relevance score
(78%).The least relevant models are traditional ones, with
Rule-Based scoring 73% and Keyword-Based scoring
70%.E.Error Rate Response
At 12%, our model has the lowest error rate,
demonstrating that it produces accurate results. Additionally,
RoBERTa and Standard BERT have low mistake rates (16%
and 15%, respectively).At 17%, DistilBERT has a little
greater error rate.The largest mistake rates (22% and 28%)
ed on January 29,2026 at 14:50:37 UTC from IEEE Xplore. Restrictions apply.

=== Page 5 ===
TABLE I. PERFORMANCE COMPARISION
Our Model Standard
Metric (legalBert+GPT- BERT
2+RAG)
Accuracy of 82.5 78.0
Information Retrieval
(%)
Average Response 1.4 1.5
Time (s)
Relevance of Generated 84 80
Text (%)
User Satisfaction (%) 83 78
Error Rate in 12 15
Responses (%)
are found in Rule-Based and Keyword-Based models, which
further demonstrates their unreliability for legal queries.
Fig. 3. User input and predicted answer
Viewing the main page above, the user asked ”what is the
punishment for murder of a person with intention to kill”
chatbot answered in a way that the user easily understood the
consequences faced for that punishment.Users of all technical
backgrounds can use the chatbot with ease.
The chatbot interface also features a slider for adjusting the
maximum response length, providing users control over the
chatbot's responses. In addition, the input section allows
visitors to compose and submit their next query for future
inquiries. This structure ensures consumers get concise and
accurate legal advice.
VII. FUTURE WORK
The findings show that while GPT-2 is a useful addition for
producing human-like responses, Legal-BERT and FAISS
together greatly increase the efficiency of legal information
retrieval. Future research could concentrate on integrating
real-time legal changes to increase the relevancy of retrieved
documents and refining language models to improve the
correctness of legal terminology.
VIII. CONCLUSION
This project successfully demonstrates the potential of AI-
powered solutions to improve public access to legal
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

DistilBERT RoBERTa Rule-Based Keyword
Model -Based
model
76.5 77.0 70.0 65.5
1.6 1.7 1.9 2.0
78 79 73 70
76 77 72 68
17 16 22 28
knowledge in India is effectively demonstrated by this
initiative. In order to efficiently handle common legal
queries, the chatbot makes use of Legal BERT for domain-
specific understanding, GPT-2 for coherent response
generation, and Retrieval-Augmented Generation (RAG) for
precise and contextually relevant information retrieval. By
combining these cutting-edge methods, the public and
intricate legal frameworks may communicate more easily and
effectively, improving the accessibility and use of legal
information. By promoting additional research to increase its
scope, accuracy, and multilingual capabilities, this study
establishes the groundwork for future developments in legal
AI systems, ultimately enabling people to handle legal
difficulties with more assurance.
IX. REFERENCES
[1] M. Mujahid, K. Kanwal, F. Rustam, W. Aljedaani,
and I. Ashraf, "Arabic ChatGPT Tweets
Classification using RoBERTa and BERT Ensemble
Model." ACM Trans. Asian Low-Resour. Language
Inf. Process., vol. 22, no. 8, pp. 1-23, Aug. 2023.
[2] R. M. Samant, M. R. Bachute, S. Gite, and K.
Kotecha, "Framework for deep learning-based
language models using multi-task learning in natural
language understanding: A systematic literature
review and future directions," IEEE Access, vol. 10,
pp. 17078-17097, 2022.
[3] Vakayil, Sonia, D. Sujitha Juliet, and Sunil Vakayil.
“RAG-Based LLM Chatbot Using Llama-2." 2024
7th
International Conference on Devices, Circuits and
Systems (ICDCS). IEEE, 2024.
[4] Bhat, Vani, et al. "Retrieval Augmented Generation
(RAG) Based Restaurant Chatbot with AI
Testability." 2024 IEEE 10th International
Conference on Big Data Computing Service and
Machine Learning Applications (BigDataService).
IEEE, 2024.
ed on January 29,2026 at 14:50:37 UTC from IEEE Xplore. Restrictions apply.

=== Page 6 ===
[5] Srivastav, Esha, et al. "LAWBOT: A Smart User [15] Rane, Atharvaa, et al. "AI driven Chatbot and its
Indian Legal Chatbot using Machine Learning Evolution." 2022 5th International Conference on
Framework." 2024 IEEE 9th International Advances in Science and Technology (ICAST).
Conference for Convergence in Technology (I2CT). IEEE, 2022.
IEEE, 2024.
[16] Viber1, "Indian Law Dataset," Hugging Face.
[6] Handoyo, Eko, et al. "Ticketing chatbot service https://huggingface.co/datasets/viber1/indian-law-
using serverless NLP technology." 2018 5th dataset (accessed Nov. 15, 2024).
International Conference on Information
Technology, Computer, and Electrical Engineering
(ICITACEE). IEEE, 2018.
[7] Kandula, Ashok Reddy, et al. "Design and
Implementation of a Chatbot for Automated Legal
Assistance using Natural Language Processing and
Machine Learning." 2023 Annual International
Conference on Emerging Research Areas:
International Conference on Intelligent Systems
(AICERA/ICIS). IEEE, 2023.
[8] Dhivvya, J. P., and Sai Bhargav Karnati.
"BuddyBot: AI Powered Chatbot for Enhancing
English Language Learning." 2024 IEEE
International Conference on Interdisciplinary
Approaches in Technology and Management for
Social Innovation (IATMSI). Vol. 2. IEEE, 2024.
[9] Balasubramanian, Prasasthy, Justin Seby, and Panos
Kostakos. "Semantic-Driven Focused Crawling
Using LASER and FAISS: A Novel Approach for
Threat Detection and Improved Information
Retrieval." 2023 IEEE 22nd International
Conference on Trust, Security and Privacy in
Computing and Communications (TrustCom).
IEEE, 2023.
[10] Nithuna, S., and C. A. Laseena. "Review on
implementation techniques of chatbot." 2020
International Conference on Communication and
Signal Processing (ICCSP). IEEE, 2020.
[11] Chauhan, Divyansh, et al. "Development of a Legal
Chatbot for Comprehensive User Support." 2024
Asia Pacific Conference on Innovation in
Technology (APCIT). IEEE, 2024.
[12] Rahman, A. M., Abdullah Al Mamun, and Alma
Islam. "Programming challenges of chatbot: Current
and future prospective." 2017 IEEE region 10
humanitarian technology conference (R10-HTC).
IEEE, 2017.
[13] Verleger, Matthew, and James Pembridge. "A pilot
study integrating an AI-driven chatbot in an
introductory programming course." 2018 IEEE
frontiers in education conference (FIE). IEEE, 2018.
[14] B. Alshemali and J. Kalita, “Improving the
Reliability of Deep Neural Networks in NLP: A
review,” Knowl. Based Syst., vol. 191, p. 105210,
Mar. 2020.
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloaded on January 29,2026 at 14:50:37 UTC from IEEE Xplore. Restrictions apply.

Paper:Integrating_Retrieval-Augmented_Generation.pdf
=== Page 1 ===
ICIIBMS 2025, Track 1: Image Processing, Computer Scienc
Integrating Retrieval-Au
Large Language Models
Answ
Yu-Jen Chen1
Department of Computer Science and Information
Engineering
National Taiwan University
Taipei, Taiwan
E-mail: b11902113@ntu.edu.tw
Tzu-Chia Tung4
Graduate Institute of Vehicle Engineering
National Changhua University of Education
Changhua, Taiwan
E-mail: tct@cc.ncue.edu.tw
Yu-Chin Chu1
Department of Computer Science and Information
Engineering
National Taiwan University
Taipei, Taiwan
E-mail: d13944008@ntu.edu.tw
Wei-Chien Wang1
Department of Computer Science and Information
Engineering
National Taiwan University
Taipei, Taiwan
E-mail: d08922038@ntu.edu.tw
Chung-Ming Yang2
Internet of Things Laboratory
Chunghwa Telecom Laboratories
Taoyuan, Taiwan
E-mail: cmyang@cht.com.tw
Abstract—Customer service systems in the financial industry
require accurate and effi-cient question-answering solutions.
Traditional methods, such as rule-based chatbots, struggle with
complex queries, while Large Language Models (LLMs) face
challenges like hallucination and outdated knowledge. This study
explores the effectiveness of Retrieval-Augmented Generation
(RAG) in answering financial questions using various retrieval
methods, including BM25, embedding models, and reranker
models. Experimental results show that BM25 is the fastest but
less accurate, while the Reranker Model achieves the highest
accuracy (0.8933) at a high computational cost. The best balance
979-8-3315-0166-2/25/$31.00 ©2025 IEEE
9
61761311.5202.03266SMBIICI/9011.01
:IOD
|
EEEI
5202©
00.13$/52/2-6610-5133-8-979
|
)SMBIICI(
secneicS
lacidemoiB
dna
scitamrofnI
tnegilletnI
no
ecnerefnoC
lanoitanretnI
ht01
5202
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

ce, Information Engineering, Okinawa, Japan, Dec.2-5, 2025
ugmented Generation and
s for Financial Question
wering
Ping-Han Chen2, 3
Internet of Things Laboratory
Chunghwa Telecom Laboratories
Taoyuan, Taiwan
Department of Engineering Science
National Cheng Kung University
Tainan, Taiwan
E-mail: rayche522@cht.com.tw
Yung-Chien Chou5, *
Department of Mechatronics Engineering
National Changhua University of Education
Changhua, Taiwan
E-mail: ycc@cc.ncue.edu.tw
Sian-Wun Du1
Department of Computer Science and Information
Engineering
National Taiwan University
Taipei, Taiwan
E-mail: d14944001@ntu.edu.tw
Chiou-Shann Fuh1
Department of Computer Science and Information
Engineering
National Taiwan University
Taipei, Taiwan
E-mail: fuh@csie.ntu.edu.tw
is found by combining BM25, a Reranker Model, and Recursive
Token Chunker, improving accuracy (0.92) while reducing
execution time (2427 seconds). This approach enhances AI-driven
financial services by providing reliable and up-to-date responses.
Keywords—Financial Question Answering; Large Language
Models (LLMs); Retrieval-Augmented Generation (RAG)
9
ed on January 29,2026 at 15:23:30 UTC from IEEE Xplore. Restrictions apply.

=== Page 2 ===
ICIIBMS 2025, Track 1: Image Processing, Computer Scienc
I. INTRODUCTION
Customer service systems in the financial industry rely
heavily on accurate and timely information to provide reliable
support. However, traditional question-answering systems,
such as rule-based chatbots and FAQs, often fail to effectively
handle complex queries. Large Language Models (LLMs)
improve financial question answering by generating context-
aware responses [1]. However, they also have some limitations,
including hallucinations and outdated knowledge, which pose
significant risks to financial decisions [2].
Retrieval-augmented generation (RAG) has emerged as a
promising solution that ensures more accurate and up-to-date
responses by combining real-time data retrieval with LLM [3].
This study aims to explore the effectiveness of RAG-enhanced
LLM in financial question answering to address challenges
such as response accuracy and efficiency. The findings could
enhance AI-driven financial services, improving in-vestment
analysis, customer support, and regulatory compliance.
II. METHODOLOGY
The dataset used in this study is provided by AI CUP 2024
[4]. The dataset is di-vided into three categories: Frequently
Asked Questions (FAQ), Financial Statements, and Insurance
Contract. For the FAQ category, 616 sample questions and
answers are stored in JSON format, while the Financial
Statements and Insurance Contract catego-ries contain 1,034
and 643 documents in PDF format, respectively. Additionally,
the dataset includes 150 queries with ground truth (the ID of
JSON entry or PDF file containing relative information) for
evaluating the accuracy of RAG, evenly distribut-ed across the
three categories.
Several methods are attempted to retrieve the most relevant
document to the given query, including the BM25 algorithm
[5], Embedding models (multilingual-e5-large) [6], and
Reranker models (bge-reranker-v2-m3) [7]. In addition, a
hybrid strategy is considered: BM25 first selects the top three
most relevant documents, and a Reranker model then identifies
the best match. Another approach involves truncating long doc-
uments into smaller chunks before applying a Reranker model.
The trunking method (Recursive Token Chunker) is also tested
to improve retrieval accuracy [8]. The above-mentioned RAG
methods are evaluated using 150 queries with ground truth,
measuring both accuracy and execution time.
To implement an automatic customer service system, we
designed a comprehensive workflow that integrates RAG and
LLM. The workflow is illustrated as follows:
978-8-3315-0166-2/25/$31.00 ©2025 IEEE
1
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

ce, Information Engineering, Okinawa, Japan, Dec.2-5, 2025
Fig. 1. That integrates RAG and LLM.
After finding the best-matching document with the RAG
method (BM25 and Re-ranker model, with chunking), the
query and the document are passed to the Llama3-TAIDE
model to generate responses. Llama3-TAIDE is an LLM based
on Meta’s LLaMA3-8b model and pre-trained on traditional
Chinese datasets. A structured prompt is designed to ensure the
precise response from the model:
"Based on the given {Article Passage} and {Query},
generate the correct {Answer}. For example: <an example,
including article passage, query, and answer>
Output Constraints:
1. Directly answer {Query} and generate {Answer} text.
2. The answer should be concise and not exceed 50
charac-ters.
3. Only output the answer without additional
explanations or symbols.
{Article Passage}: <Put the document retrieve by RAG
here.>
{Query}: <Put the query here.> "
III. RESULTS AND OBSERVATIONS
The following table shows the accuracy and execution time
(complete 150 queries) of different RAG methods:
TABLE I. ACCURACY AND EXECUTION TIME (COMPLETE 150 QUERIES)
OF DIFFERENT RAG METHODS.
Method Execution Time
Accuracy
(seconds)
BM25 0.74 103
Embedding Model 0.66 1,847
BM25 and Embedding Model 0.72 116
BM25 and Embedding Model,
0.8133 152
with Recursive Token Chunker
Reranker Model 0.8933 21,750
BM25 and Reranker Model 0.86 1,305
BM25 and Reranker Model, with
0.92 2,427
Recursive Token Chunker
0
ed on January 29,2026 at 15:23:30 UTC from IEEE Xplore. Restrictions apply.

=== Page 3 ===
ICIIBMS 2025, Track 1: Image Processing, Computer Scienc
The experiment results show that BM25 is the fastest (103
seconds) but has rela-tively low accuracy (0.74). The
Embedding Model alone is much slower (1,847 sec-onds) and,
without chunking, performs worse than BM25 (0.66).
Combining BM25 significantly improves retrieval speed, while
Recursive Token Chunker enhances ac-curacy (up to 0.92).
The Reranker Model achieves the highest accuracy (0.8933–
0.92) but comes with extremely high computational costs
(21,750 seconds). Overall, using both the BM25 and Reranker
model with chunking provides the best balance between
accuracy (0.92) and efficiency (2,427 seconds).
Each retrieval method has its own advantages and
limitations. BM25 has high computational efficiency, but it
relies on precise keyword matching and lacks semantic
understanding, making its performance limited when
processing synonymous or context-related queries. Embedding
Models can capture semantic similarity, but they usually
require inefficient candidate retrieval mechanisms and simple
vector aggregation strategies; therefore, their accuracy is
limited and the computational cost is high. In addition, the
limitation on the input tokens further reduces the model
performance in the absence of an effective chunking strategy.
Reranker Models perform detailed analysis of queries and
documents with large transformer models. Although they can
achieve the highest accuracy, their huge computational
resource requirements make it difficult to apply to real-time or
large-scale retrieval scenarios. Considering both accuracy and
efficiency, the ideal retrieval approach usually adopts a multi-
stage strategy, using BM25 as a preliminary candidate retrieval,
and then combining it with a Reranker Model for fine
screening to achieve the best balance between performance and
accuracy.
1
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

ce, Information Engineering, Okinawa, Japan, Dec.2-5, 2025
IV. CONCLUSION
This study evaluates the effectiveness of RAG-enhanced
LLM in financial question answering, addressing challenges in
accuracy and efficiency. The results show that BM25 is the
fastest retrieval method but has moderate accuracy (0.74),
while the Em-bedding Model alone is slower and less accurate
(0.66) due to the lack of chunking. The Reranker Model
achieves the highest accuracy (0.8933) but at a high computa-
tional cost (21,750 seconds). The optimal balance is achieved
by combining BM25, a Reranker Model, and Recursive Token
Chunker, resulting in an accuracy of 0.92 with a significantly
reduced execution time (2,427 seconds). This approach ensures
precise and efficient responses, making it a viable solution for
AI-driven financial services. Future work could explore further
optimizations in retrieval speed and response gen-eration
quality, enhancing customer support, investment analysis, and
regulatory com-pliance in the financial industry.
ACKNOWLEDGMENT
This work was supported by the National Science and
Technology Council (NSTC) of Taiwan, under grants NSTC
114-2221-E-002-055, NSTC 114-2221-E-018 -011.
REFERENCES
[1] N. Chinaksorn and D. Wanvarie, "LLM-RAG for Financial Question
Answering: A Case Study from SET50," 2025 International Conference
on Artificial Intelligence in Information and Communication (ICAIIC),
Fukuoka, Japan, 2025, pp. 0952-0957
[2] G. Perković, A. Drobnjak and I. Botički, "Hallucinations in LLMs:
Understanding and Addressing Challenges," 2024 47th MIPRO ICT and
Electronics Convention (MIPRO), Opatija, Croatia, 2024, pp. 2084-2088
[3] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N.,
Küttler, H., Lewis, M., Yih, W., Rocktäschel, T., Riedel, S., Kiela, D.
(2020). Retrieval-Augmented Generation for Knowledge-Intensive NLP
Tasks. arXiv preprint arXiv:2005.11401.
[4] https://tbrain.trendmicro.com.tw/Competitions/Details/37
[5] Robertson, S., & Zaragoza, H. (2009). The Probabilistic Relevance
Framework: BM25 and Beyond (Vol. 3, No. 4). Foundations and
Trends® in Information Retrieval, 333–389.
[6] Wang, L., Yang, N., Huang, X., Yang, L., Majumder, R., Wei, F. (2024).
Multilingual E5 Text Embeddings: A Technical Report. arXiv preprint
arXiv:2402.05672.
[7] Ma, X., Zhang, X., Pradeep, R., Lin, J. (2023). Zero-Shot Listwise
Document Reranking with a Large Language Model. arXiv preprint
arXiv:2305.02156.
[8] Gong, H., Shen, Y., Yu, D., Chen, J., Yu, D. (2020). Recurrent
Chunking Mechanisms for Long-Text Machine Reading Comprehension.
arXiv preprint arXiv:2005.08056.
1
ed on January 29,2026 at 15:23:30 UTC from IEEE Xplore. Restrictions apply.

Paper:RAGRouter_Learning to Route Queries to Multiple Retrieval‑Augmented Language Models.pdf
=== Page 1 ===
RAGRouter: Learning to R
Retrieval-Augmente
JiaruiZhang1 XiangyuLiu2 YongHu2
1ShanghaiJiaoTongUniver
Abst
Retrieval-AugmentedGeneration(RAG)
ofLargeLanguageModels(LLMs)onkn
ing response quality across LLMs unde
mechanisms,whichselectthemostsuita
retrieval-augmentedLLMsviaadedicate
documentsdynamicallyaffectLLMs’abil
ingmethods,whichrelyonstaticparam
suboptimalperformanceinRAGscenar
thenewretrieval-augmentedLLMrouti
ofretrieveddocumentsintotherouting
RAG-awareroutingdesign,whichleverag
bilityembeddingswithcontrastivelearn
shiftsandenableinformedroutingdecis
knowledge-intensivetasksandretrievals
LLMs,showthatRAGRouteroutperform
routing methods. With an extended sco
achievesstrongperformance-efficiencytra
1 Introduction
Therapidadvancementoflargelanguagemodels
landscape, with significant heterogeneity in para
training data, architectures, and learning objecti
remainlimitedbyoutdatedknowledge, hallucina
29]. Retrieval-AugmentedGeneration(RAG)[27
knowledge at inference time, effectively reducin
knowledge-intensivetasks[5,13,47,20,41].
While RAG enhances LLM performance by inc
exhibitsubstantialvariationintheirabilitytouti
that,givenidenticaldocuments,LLMsdifferinin
tonoise—reflectinginherentheterogeneityinRA
chitecture,trainingdata,andoptimization. Suchd
canyieldcomplementarystrengths,enablingper
routing, which pre-selects the most suitable mod
invokingthemall,offersanefficientandeffective
parametricknowledgeandRAGcapability,intelli
forleveragingRAGtoachievesuperiorperforman
effectivelyroutequeriestothemostcapableLLM
∗ChaoyueNiuisthecorrespondingauthor(rvince@
2Thecodeanddataareavailableathttps://githu
Preprint.Underreview.
5202
tcO
71
]LC.sc[
2v25032.5052:viXra

Route Queries to Multiple
ed Language Models
ChaoyueNiu1∗ FanWu1 GuihaiChen1
rsity 2WeChat,TencentInc
tract
)significantlyimprovestheperformance
nowledge-intensivetasks. However,vary-
er RAG necessitates intelligent routing
ablemodelforeachqueryfrommultiple
edroutermodel. Weobservethatexternal
litytoanswerqueries,whileexistingrout-
metricknowledgerepresentations,exhibit
rios. Toaddressthis,weformallydefine
ingproblem,incorporatingtheinfluence
framework. WeproposeRAGRouter, a
gesdocumentembeddingsandRAGcapa-
ningtocaptureknowledgerepresentation
sions. Extensiveexperimentsondiverse
settings,coveringopenandclosed-source
msthebestindividualLLMandexisting
ore-threshold-based mechanism, it also
ade-offsunderlow-latencyconstraints. 2
(LLMs)hasledtoanincreasinglydiversemodel
ametric knowledge stemming from variations in
ives [45, 35, 36, 3, 4, 57, 53]. However, LLMs
ations, andinsufficientdomaincoverage[17,22,
7,15]addressestheseissuesbyinjectingexternal
ng hallucinations and improving performance on
corporating external knowledge, different LLMs
ilizeretrievedcontent. Priorstudies[6,11]show
nformationextraction,integration,androbustness
AGcapabilitiesstemmingfromdifferencesinar-
diversitysuggeststhatcombiningmultiplemodels
rformancethatsurpassesanysinglemodel. LLM
del for each query from a pool of LLMs without
efusionstrategy[9]. Underdualheterogeneityin
igentqueryroutingpresentsapromisingdirection
nce,motivatingthecentralquestion: Howcanwe
undertheRAGparadigm?
@sjtu.edu.cn).
ub.com/OwwO99/RAGRouter.

=== Page 2 ===
Amram
husban
and M
include
Ro
Figure1: Left: AccuracyofvariousLLMsonth
examplequerywhereretrieveddocumentsimprov
butimpairLlama’s(answerable unanswerable
→
underRAGduetotheirinabilitytocapturesuchd
Current multi-model routing methods primarily
parametric knowledge [40, 19, 31, 43, 38, 10, 7,
construct compact vector representations of LLM
estimation. Thesemethodsallassumestaticknow
However,theseapproachesfacecriticallimitation
dynamic impact of knowledge injection. As illu
distribution of response quality across input que
abilitytoansweraquestion,renderingroutingstra
ThecoreissueliesintheStaticKnowledgeAssu
knowledge, ignoring how retrieved content dyna
RAGresponsequalitydependsontheinterplaybe
information. This leads to shortcomings: Missin
queriesandmodelembeddings,overlookingdocum
IgnoringRAGCapability—priorworkcaptureson
differingabilitytoleveragedocuments. Thesegap
strategies—theyfailtoadapttothedynamicrelati
Totacklethisissue,weproposeRAGRouter,ac
explicitlymodelsknowledgeshiftsinRAGscenari
LLMs by modeling key factors that affect post-r
RAGRouterincorporatesadocumentencoderand
andqueryinteractions,therebyaddressingmissin
RAGcapabilityembedding—alearnablevectorrep
tent—tomitigateignoringRAGcapability. Howev
due to inherent variations introduced by retrieval
employacontrastivelearningobjective,whereposi
LLMsthatcorrectlyorincorrectlyrespondtoaqu
non-RAGandRAGsettings)andIntra-Setting(wi
as an anchor, the objective encourages alignmen
pushingapartunanswerableones. ThisallowsR
behaviorshifts,movingbeyondthestaticknowled
WeevaluateRAGRouteronasuiteofknowledge
settings. ExperimentalresultsshowthatRAGRoute
LLM,highlightingitsabilitytoleveragethecompl
augmentedscenarios. Furthermore,RAGRoutersu
routingmethods,validatingtheeffectivenessofm
Our main contributions are summarized as follow
firstworkexploringLLMroutingintheRAGse
learning-basedroutingmechanismthatisawareo
and document-aware representations to effective
strategiesinRAG;(iii)Wevalidatetheeffectivenes
2

m In the Book of Exodus, Amram is the
nd of Jochebed and father of Aaron, Moses Retrieval
Miriam. Alternative spellings of the name
e . In addition to being married to ...
Generation
Who is the father of Moses?
Amram. Aaron. Route
Router
Jacob. Amram.
outing succeeded Routing Failed
(w/o RAG) (w RAG)
hePopQAtaskbeforeandafterRAG.Right: An
veQwen’sresponse(unanswerable answerable)
→
e), illustratinghowexistingroutingmethodsfail
dynamicshifts.
match queries to LLMs based on their inherent
, 33, 58, 12]. Several approaches [7, 33, 58, 12]
Ms to enable efficient query-model compatibility
wledgerepresentationsfornon-RAGscenarios.
nsinRAGsettings,astheyfailtoaccountforthe
ustrated in Figure 1, RAG dramatically shifts the
eries—external documents can reverse a model’s
ategiesdesignedfornon-RAGscenariosobsolete.
umption: existingapproachesassumefixedLLM
amically reshapes their capabilities. In practice,
etweenamodel’sinternalknowledgeandexternal
ng Doc Interaction—existing methods focus on
mentfeaturesandtheirinteractionwithmodels;and
nlystaticknowledgedifferences,neglectingLLMs’
pshighlightshortcomingsofcurrentLLMrouting
ionshipbetweenLLMandexternalknowledge.
contrastivelearning-basedroutingframeworkthat
ios. RAGRouterisdesignedtoroutequeriesacross
retrieval performance. At the architecture level,
dacrossencodertocapturedocumentsemantics
ngdocumentinteraction,andassignseachLLMa
presentingitsproficiencyinutilizingretrievedcon-
ver,directlyoptimizingsucharouterischallenging
l. To address this, at the optimization level, we
itiveandnegativesamples—i.e.,representationsof
uery—aredrawnfrombothCross-Setting(between
ithineachsetting). Takingthequeryrepresentation
nt between answerable model-query pairs while
RAGRoutertoeffectivelymodelretrieval-induced
dgeassumption.
e-intensivetasks[32,34,25,2,23]andretrieval
ersurpassestheperformanceofthebestindividual
lementarystrengthsofmultiplemodelsinretrieval-
ubstantiallyoutperformsexistingnon-RAG-aware
modelingretrieval-inducedknowledgeshifts.
ws: (i) To the best of our knowledge, this is the
etting; (ii)WeproposeRAGRouter, acontrastive
ofknowledgeshifts,incorporatingRAGcapability
ely address the failure modes of existing routing
ssofourmethodonfiveknowledge-intensivetasks
2

=== Page 3 ===
underlocalandonlineretrievalsettings,usingop
series scales from 0.5B to 72B and closed-sourc
DeepSeek-R1. ResultsshowthatRAGRouterou
non-RAG-awareroutingmethodsby1.67%–9.33%
mechanismtoRAGRouter,andresultsshowthatits
ofallbaselines,indicatingsuperiorperformance-ef
2 RelatedWork
Retrieval-AugmentedGeneration. RAGenhan
formationfromexternaldatabases[27,15]. Ittyp
generationpipelines,wheredocumentsareretrieve
itforgeneration. PriorworkhasimprovedRAGb
enhancingthegenerator’sabilitytoutilizeretriev
heterogeneousLMcapabilitiesinprocessingexter
[28, 6] and tolerating retrieval noise [11, 39]. T
tionopportunitiesforensembleapproachesthats
scenarios. Inthiswork,westudytheroutingprob
LLMRouting.ExistingLLMroutingapproaches[
settings, where routing relies solely on the input
withoutincorporatingexternalretrieveddocuments
learningtomodelquery-modelcompatibility, wh
matrix factorization to learn compact model emb
constructs a heterogeneous graph with nodes fo
interactionsasedgestocapturecontextualalignm
However,inRAGscenarios,retrieveddocumentsi
existingmethodsoverlook. Incontrast,ourpropo
LLMs’RAGcapabilities,enablingmoreeffective
3 ProblemFormulation
RAGenhancesLLMsbyintegratingexternalknow
theretrieverRet( ,q)selectsrelevantdocumentsd
D
generatesaresponseybasedonboththequeryqa
WeformulateaLLMroutingproblemunderRA
candidate LLMs. A routing policy R :
Q×D
M foreachinputpair(q,d). Toevaluateresp
R(q,d)
σ(M ,q,d) 0,1 , where σ(M ,q,d) = 1 if t
i i
referenceans ∈ we { ry∗. } Importantly,usingafixedmo
differentquery-documentpairs. Theobjectiveisto
maxE (cid:2) σ(
q∼Q
R
Notably,whennoexternaldocumentsareavailab
RAGsettingnaturallydegeneratesintotheconve
routingpolicysimplifiestoR: 1,...,N ,
Q→{ }
whichassessestheresponsebasedsolelyonthequ
maxE (cid:2) σ
q∼Q
R
Thus,theconventionalroutingproblemcanbeseen
setting,correspondingtotheboundaryconditionw
4 RAGRouter
4.1 RoutingModelArchitectureDesign
We establish a conceptual framework by constru
resentation and LLM-query matching under RA
3

pen-sourceLLMssuchastheQwenandLLaMA
ce LLMs including GPT-4o, Qwen2.5-Max, and
utperformsthebestindividualLLMandexisting
%;(iv)Weapplyanextendedscore-threshold-based
saccuracy–latencycurvegenerallyliesabovethose
fficiencytrade-offsunderlow-latencyconstraints.
nceslanguagemodelsbyintegratingretrievedin-
picallyfollowsaroundofretrievalandsequential
edbasedontheinputqueryandconcatenatedwith
byoptimizingretrievalcomponents[56,51,42]or
vedcontent[21,49,48]. Recentstudieshighlight
rnalinformation,bothinutilizingretrievedcontent
These heterogeneous capabilities reveal optimiza-
strategicallyleveragemultipleLLMswithinRAG
blemundertheRAGsetting.
[7,10,12,33,58,31]primarilyfocusonnon-RAG
t query and each model’s parametric knowledge,
s. Forexample,RouterDC[7]usesdualcontrastive
hileEmbedLLM[58]andRouteLLM[33]apply
beddings for scalable routing. GraphRouter [12]
or tasks, queries, and LLMs, and encodes their
mentbetweenqueryneedsandmodelcapabilities.
inducedynamicshiftsinmodelknowledge,which
osedRAGRoutermodelsboththedocumentsand
eroutingunderretrieval-augmentedsettings.
wledgethroughatwo-stageprocess:givenaqueryq,
dfromanexternalcorpus ,andthemodelM(q,d)
D
andthedocumentsd,i.e.,y =M(q,Ret( ,q)).
D
AGsetting. Let = M ,...,M beasetof
1 N
M { }
1,...,N selects the most suitable model
→ { }
ponsequality,wedefineanoraclescoringfunction
the response from M given q and d matches the
i
odelcanbesuboptimal,asdifferentLLMsexcelon
omaximizetheexpectedroutingperformance:
(cid:3)
(M ,q,d) (1)
R(q,d)
ble(i.e.,d = ),theLLMroutingproblemunder
∅
entionalLLMroutingproblem. Inthissetting,the
andtheoraclescoringfunctionbecomesσ(M ,q),
i
uery. Theobjectivebecomes:
(cid:3)
σ(M ,q) (2)
R(q)
nasaspecialcaseofLLMroutingundertheRAG
whered= .
∅
ucting an intuitive explanation of knowledge rep-
AG and non-RAG settings. In non-RAG settings
3

=== Page 4 ===
[7,58],eachLLMistypicallyassociatedwithaco
whichimplicitlyreflectsitsparametricknowledge;
representingtheknowledgeneededtoanswerit. A
usedtogaugetheLLM’sabilitytorespond,guidin
However, in RAG settings, the LLM is augmen
parametric knowledge. This additional informat
renderingtheoriginalknowledgerepresentationv
k
shiftsduetotheintegrationofexternalinformatio
v′ =v
k
wherev isthefusedknowledgerepresentationde
f
scenarios,thesimilaritybetweenthequeryandth
shouldserveasthenewroutingcriterion,asitmor
RAGRouterisdesignedwiththisinsightinmind
andexplicitlymodelsthefusedknowledgev to
f
dynamicallyupdatetheLLM’sknowledgerep-
resentation. Weidentifythreecorefactorsthat
influencev : (1)thenon-parametricknowledge
f
providedbythedocuments;(2)theLLM’sability
toprocessexternalinformation,includingknowl-
edgeextractionandrobustnesstonoise;and(3)
thequery’sroleinguidingknowledgeretrieval.
Basedonthese,RAGRouterconsistsofthefol-
lowingmodules,withitsarchitectureillustrated
inFigure2.
RepresentingParametricKnowledge. Toob-
taintheoriginalparametricknowledgerepresen-
tationv ,weintroducetheLLMKnowledgeEm-
k
beddingLayerϕ ,whichtakestheLLMIDM
K
andoutputsv =ϕ (M),capturinginter-model
k K
variabilityinparametricknowledge. Forquery
representation,weemployaQueryEncoderϕ ,
Q
whichencodesthequeryqasv =ϕ (q).
q Q
Representing RAG-Aware Factors. To com-
putethefusedknowledgev ,RAGRouterinte-
f
gratessignalsfromthreeperspectives. First,theno
mentsiscapturedbytheDocumentEncoderϕ ,
D
practice,thedocumentandqueryencoderssharep
space. Second,theLLM’sabilitytoprocessexte
bilityEmbeddingLayerϕ ,whichmapseachca
R
representingitsintrinsiccapacitytoutilizeretriev
knowledgeretrievalisrepresentedbytheCrossE
pair(d,q)toproduceaninteractionrepresentation
RepresentationUpdateforSimilarity-BasedRo
thenderivedviaamulti-headattentionmechanism
v =Attentio
f
Withthefusedknowledgecomputed,theRAG-awa
v′ =v +v . Thefinalroutingdecisionisbasedo
k k f
knowledgerepresentationsofcandidatemodels:
R(q,d)=arg ma
i∈{1,.
Thisformulationallowstheroutingpolicytoexpl
documentretrieval,thusmaintainingaccurateasse
4

ompactknowledgerepresentationvectorv Rdim,
k
;andmeanwhile,aqueryisencodedasv
∈Rdim,
q
∈
Aproxymetric,likesimilaritysim(v ,v )isthen
q k
ngnon-RAGroutingprocess.
nted with retrieved documents that provide non-
tion influences the model’s response generation,
insufficient. TheeffectiveknowledgeoftheLLM
k
on,resultinginanewrepresentation:
v +v (3)
k f
erivedfromthedocuments. Consequently,inRAG
heupdatedknowledgerepresentationsim(v ,v′)
q k
reaccuratelyreflectsthemodel’sabilitytorespond.
(b) Knowledge Update (c) Similarity-Based Routing
…
𝑣′𝑘
…
+
𝑣𝑓
Muti-Head
⊙ similarity
Attention
… 𝑣𝑘 … 𝑣𝑟 𝑣𝑐 𝑣𝑑 𝑣𝑞
LLM Knowledge RAG Capability Cross Document Query
Embedding Layer Embedding Layer Encoder Encoder Encoder
…
LLMs Doc Query
(a) Input Embedding
Figure2: TheinferencepipelineofRAGRouter:
(a) Encode query, document, cross interaction,
LLMknowledge,andRAGcapability;(b)Fuse
RAGcapability,document,andcrossembeddings
to update knowledge representation; (c) Route
basedonsimilaritywiththequeryembedding.
on-parametricknowledgeprovidedbythedocu-
whichencodesadocumentdintov =ϕ (d). In
d D
parameterstoensureconsistencyintheembedding
ernalinformationiscapturedbytheRAGCapa-
andidatemodelM toanembeddingv =ϕ (M),
r R
vedevidence. Third,thequery’sroleinguiding
Encoderϕ ,whichprocessesthequery-document
C
nv =ϕ (d,q).
c C
outing. Thefusedknowledgerepresentationv is
f
mthatintegratesthesesignals:
on(v ,v ,v ) (4)
r d c
areknowledgerepresentationofthemodelbecomes
onthesimilaritybetweenthequeryandtheupdated
ax sim(v ,v′ ) (5)
...,N}{ q ki }
licitlyaccountforknowledgeshiftsintroducedby
essmentofeachLLM’sabilityinRAGsettings.
4

=== Page 5 ===
Cross-Setting Contrast
RAG Setting
Intra-Setting Contrast
Who plays Mary
Jane in spiderman 2?
Non-RAG Setting
(a) Construct Positive and Negative Samples for One Q
Figure3: (a)CSCconstructspositiveandnegative
quality(e.g.,Llamaw/RAG(✓)vs. Llamaw/oRA
setting(e.g.,Llamaw/RAG(✓)vs. Qwenw/RAG
learningpullspositivesamplesclosertothequery
4.2 Optimization
InRAGsettings,theincorporationofretrieveddoc
answerability—someLLMsbecomeabletoansw
failafterretrieval. Theseshiftsinanswerability,e
changesinthemodel’sknowledgerepresentation.
andnegativepairsacross differentknowledgesta
ofcontrastivelearning[8,18], whichisparticula
knowledgerepresentationshiftsinducedbyextern
To this end, we design the Cross-Setting Contr
differences between the non-RAG and RAG sett
(ISC)mechanismtomodelrepresentationdifferen
usingthequeryrepresentationv astheanchor,C
q
selectingknowledgerepresentationswithdifferen
settings(bluearrows). ISC,ontheotherhand,se
withdifferentresponsequalitieswithinthesames
RAGRouterdistinguishbetweendifferentknowled
ISCenhancesthemodel’sdiscriminativeabilityac
CombiningCSCandISC,weconstructacomprehe
theRAGRouter. Foragivenqueryq,wedefineth
(cid:40)
V = v σ(M ,q)=1
+
{
ki
|
i
}
V = v σ(M ,q)=0
−
{
ki
|
i
}
Thecorrespondingcontrastivelossisdefinedas:
(cid:88)
(q)= log
L CT − exp(sim(v ,v
q k+
vk+∈V+
whereτ isatemperaturehyperparameter. Thislos
to positive samples and further from negative on
aresensitivetobothknowledgeshiftsandmodel
model’sresponseability—e.g.,fromunanswerab
dynamictransitions,enhancingroutingaccuracya
TofurtherenhanceLLMdiscrimination,weintro
modelM,defines =Sigmoid(sim(v ,v ));f
M,q k q
Sigmoid(sim(v′,v )). Lety =σ(M,q)andy
k q M,q
classificationlossis:
(cid:88)
(q)= [y log
CLS M,q
L −
M∈M∪M′
Thetotallossistheweightedsumofthecontras
balancinghyperparameter:
(q)= (q
CT
L L
5

Pull Close
Push Away
Query (b) Contrastive Learning
samplesunderdifferentsettingsbasedonresponse
AG(✗)),whileISCconstructsthemunderthesame
G(✗));(b)BycombiningCSCandISC,contrastive
yrepresentationandpushesnegativeonesaway.
cumentsoftenleadstosignificantchangesinLLM
werqueriestheypreviouslycouldnot,whileothers
effectivelylabeltransitions,reflectcorresponding
Suchtransitionsnaturallyyieldstructuredpositive
ates. Thissetting alignswellwith theprinciples
arlywell-suitedforcapturingandoptimizingthe
nalknowledgeinjectioninRAGRouter.
rast (CSC) mechanism to model representation
tings, and introduce the Intra-Setting Contrast
nceswithinthesamesetting. AsshowninFigure3,
CSCconstructspositiveandnegativesamplesby
ntresponsequalitiesfromthenon-RAGandRAG
electspositiveandnegativesamplesfrommodels
setting(orangearrows). ThisenablesCSCtohelp
dgetransferpatternsinducedbydocuments,while
crossLLMswithinthesamesetting.
ensivesetofpositiveandnegativesamplestotrain
hepositiveandnegativesetsasfollows:
v′ σ(M ,d,q)=1
}∪{ kj | j } (6)
v′ σ(M ,d,q)=0
}∪{ kj | j }
exp(sim(v ,v )/τ)
q k+
(cid:80) (7)
)/τ)+ exp(sim(v ,v )/τ)
+ vk−∈V− q k−
ssencouragesthequeryembeddingv tobecloser
q
nes, enabling the learning of representations that
heterogeneity. Whenretrieveddocumentsaltera
bletoanswerable—themechanismcapturesthese
andknowledgeadaptabilityinRAGRouter.
oduceabinaryclassificationloss. Fortheoriginal
fortheRAG-enhancedmodelM′,defines
M′,q
=
y M′,q =σ(M,d,q)betheground-truthlabels. The
s +(1 y )log(1 s )] (8)
M,q M,q M,q
− −
stivelossandclassificationloss, withλ > 0asa
q)+λ (q) (9)
CLS
L
5

=== Page 6 ===
4.3 Latency-AwareExtendedDesign
WhileRAGRouterdoesnotexplicitlymodelLLM’
didateLLMgivenaquery,whichcanbeexploited
andefficiency. Tothisend,weintroduceascore-th
pre-sorttheN availableLLMsas[M ,M ,...,M
1 2
smallerparametersizesandlowerlatency—meani
Given a query, suppose M receives the highest
i
performance-optimalmodel). Insteadofroutingd
andselectthefirstLLMM whosescoresatisfiess
j
marginthreshold. Thismechanismsacrificesasm
efficiency,makingRAGRouteradaptabletolatenc
5 Experiments
5.1 ExperimentalSetup
Datasets. Weselectqueriesfromfivedifferentk
open-domainquestion-answeringbenchmarkcove
domains;(ii)MedMCQA[34]isamultiple-choic
andclinicalreasoning;(iii)NaturalQuestions(N
real-worldsearchqueriesrequiringspan-levelans
(WebQ) [2] is a knowledge base-driven benchm
evaluateentity-centricfactualreasoning;and(v)Tr
centeredonfactoid-stylequestionssourcedfrom
[42],weadoptCoverExactMatchastheevaluati
Furtherdataprocessingdetailsanddatasetstatistic
CandidateLLMs. Weselected15mainstreamLL
[52, 14, 44, 55, 1] with parameter size ranging f
0.5Bto72B.Comprehensivestatisticsonmodelsc
andlatency3arepresentedinTable1,andimplem
tationdetailsareprovidedinAppendixA.1.
Retrieval Settings. Following [42], we adopt b
local and online retrieval strategies for PopQA
MedMCQAtoreflectrealisticRAGscenarios. L
retrievalusesthe2018EnglishWikipediadump
withBGE-large-en-v1.5[50]asthedenseretriever.
lineretrievalleveragestheDuckDuckGoWebSe
API4 toaccessup-to-dateexternalcontent. For
WebQ,andTriviaQA,wefollow[11]andconstruc
trievalcontextsfromWikipediapassagesaugme
withsyntheticnoise(e.g.,irrelevantdistractors,c
terfactualnoise)tosimulateimperfectretrieval. T
settingenablesevaluatetheeffectivenessofthero
Baselines. WecompareRAGRouteragainstara
not RAG-aware and exploited only query-LLM
augmentation. This series of baselines include P
modelselectionviameta-prompts;GraphRouter
aheterogeneousgraph;RouterDC[7],whichali
learning;KNNRouter,which[19]reliesonhisto
Factorization(MF)[33,58],whichreconstructsL
Wealsointroducesomerule-basedroutingmethod
OracleSingleBestthatideallyselectsthebest-pe
3LatencyreferstotheaveragetimetakenbyanLLM
timeandpotentialnetworkdelays.
4https://duckduckgo.com
6

’slatency,itoutputsarelevancescoreforeachcan-
tosupportflexibletrade-offsbetweenperformance
hreshold-basedroutingmechanism. Concretely,we
M ]basedontheirpriorefficiencyprofiles,suchas
N
ingthatM isthemostefficientandM theleast.
1 N
t predicted score from RAGRouter (i.e., it is the
directlytoM ,wetraversethelistfromM toM
i 1 i
s s θ,whereθisauser-definedscore
Mi,q
−
Mj,q
≤
mallamountofaccuracyforsignificantlyimproved
cy-constrainedorresource-limitedscenarios.
knowledge-intensivetasks: (i)PopQA[32]isan
eringdiversefactualtopicsfrombroadknowledge
cebenchmarkfocusedonbiomedicalknowledge
NQ)[25]isanopen-domainbenchmarkbasedon
swerretrievalfromWikipedia;(iv)WebQuestions
mark grounded in Freebase relations, designed to
riviaQA(TQA)[23]isanopen-domainbenchmark
triviaenthusiastsandwebdocuments. Following
ionmetricforPopQA,NQ,WebQ,andTriviaQA.
csaresummarizedinAppendixA.2.
LMs Table 1: Statistics of different LLMs and
from theirlatency.
cales LLM Params(B) Latency(ms)
men-
Qwen2.5-0.5B-Instruct 0.494 24.54
Llama-3.2-1B-Instruct 1.240 20.47
Qwen2.5-1.5B-Instruct 1.500 24.79
both
gemma-2-2b-it 2.614 31.80
and Llama-3.2-3B-Instruct 3.213 81.82
Local Qwen2.5-3B-Instruct 3.000 24.39
Yi-1.5-6B-Chat 6.061 142.67
[24]
Qwen2.5-7B-Instruct 7.616 80.83
. On-
Ministral-8B-Instruct-2410 8.020 26.13
earch Meta-Llama-3.1-8B-Instruct 8.030 177.37
NQ, Yi-1.5-9B-Chat 8.829 199.61
Qwen2.5-14B-Instruct 14.770 175.42
ctre-
Qwen2.5-32B-Instruct 32.764 156.26
ented
Qwen2.5-72B-Instruct 72.706 1610.00
coun- Llama-3.3-70B-Instruct 70.554 1970.00
This
outingmodelundernoisyconditions.
angeofbaselines. Existingroutingmethodswere
M compatibility, ignoring the impact of retrieval
Prompt LLM [12], which employs GPT-4o for
r[12],whichmodelsqueries,tasks,andLLMsin
ignsquery-LLMembeddingsthroughcontrastive
oricalperformanceofsimilarqueries;andMatrix
LLMcorrectnesspatternsvialow-ranklatentspaces.
ds,includingaSingleFixedLLMforallqueries;
erformingsingleLLMperdataset;RandomLLM
Mtocompleteasinglequery,includingbothinference
6

=== Page 7 ===
Table2: PerformancecomparisonofRAGRoute
acrossdifferentknowledge-intensivetasksandre
"Ret."indicateswhetherthemethodisretrieval-aw
bold,andthesecond-bestareunderlined.
PopQA
Method Ret.
Local Online
Qwen2.5-0.5B-Instruct - 45.19 51.11
Llama-3.2-1B-Instruct - 39.26 46.67
Qwen2.5-1.5B-Instruct - 44.44 48.89
gemma-2-2b-it - 41.11 50.74
Llama-3.2-3B-Instruct - 45.56 51.48
Qwen2.5-3B-Instruct - 41.11 47.41
Yi-1.5-6B-Chat - 46.67 51.48
Qwen2.5-7B-Instruct - 42.96 48.15
Ministral-8B-Instruct-2410 - 41.48 46.30
Meta-Llama-3.1-8B-Instruct - 46.67 51.85
Yi-1.5-9B-Chat - 46.67 52.59
Qwen2.5-14B-Instruct - 46.30 50.00
Qwen2.5-32B-Instruct - 45.93 48.52
Qwen2.5-72B-Instruct - 44.81 47.78
Llama-3.3-70B-Instruct - 46.30 50.37
OracleSingleBest - 46.67 52.59
Random - 44.30 49.56
Weighted - 46.35 50.53
PromptLLM[12] ✗ 46.67 51.48
GraphRouter[12] ✗ 47.41 51.48
RouterDC[7] ✗ 44.81 50.37
KNNRouter[19] ✗ 46.67 52.22
MF[33,58] ✗ 46.30 52.59
RAGRouter(Ours) ✓ 48.52 52.59
Oracle - 54.44 57.41
assignment;WeightedroutingtodifferentLLMs
distribution. Toshowtheidealupperboundofrouti
byroutingeachquerytoitsoptimalLLMusinggr
ImplementationDetails. FortheRAGRouterar
encoderforbothqueriesanddocuments,andms-m
resultinginatotalparametersizeofapproximately
andtheRAGcapabilityvectoraresettoadimensi
lasttwotransformerlayersinthequery/document
training. Theclassificationlossweightλissetto
0.2. TherouterisoptimizedusingAdamW[30]w
epochs. Allexperimentsareconductedonasingle
5.2 MainResults
ComparisonwithSingleNon-RoutedLLMs. Ta
knowledge-intensivetasksandretrievalsettings. R
mance,withanaverageaccuracyof64.46%. Itsu
LLM,LLaMA-3.3-70B-Instruct(60.85%),by+3
inRAG.Notably,RAGRouteralsooutperformsth
selectstheoptimalsinglemodelforeachdataset—
integrationofmultipleRAG-enhancedLLMstoa
ComparisonwithNon-RAG-AwareandOther
allnon-retrieval-awareroutingbaselines,includin
+4.21%),KNNRouter(60.44%,+4.02%),andRout
thelimitationsofmethodsthatdonotexplicitlym
7

erwithrule-basedandnon-RAG-awarebaselines
etrievalsettings. Testingaccuracy(%)isreported.
ware(✓)ornot(✗). Thebestresultsareshownin
MedMCQA
NQ WQ TQA Avg
Local Online
25.93 34.81 27.08 38.75 39.17 37.43
17.78 36.67 25.42 31.67 43.33 34.40
30.00 42.59 30.00 35.42 46.67 39.72
20.37 37.04 22.92 27.92 40.83 34.42
27.41 44.81 36.67 45.42 65.00 45.19
40.37 49.26 30.42 36.25 53.33 42.59
31.11 40.00 35.83 43.33 56.67 43.58
35.93 43.33 29.58 35.42 55.00 41.48
50.74 62.22 38.33 42.08 63.75 49.27
41.85 52.59 39.58 46.25 69.58 49.77
50.74 57.78 38.33 47.08 58.75 50.28
57.04 64.07 42.92 47.08 72.50 54.27
43.33 49.63 44.58 50.42 80.42 51.83
67.04 70.00 40.00 48.75 79.17 56.79
68.89 70.37 51.67 50.42 87.92 60.85
68.89 70.37 51.67 50.42 87.92 61.22
40.57 50.35 35.56 41.75 60.81 46.13
68.31 70.18 46.87 48.39 86.09 59.53
61.85 65.93 39.58 49.17 71.25 55.13
68.89 70.37 51.67 50.42 87.92 61.17
67.04 68.89 40.00 48.33 77.50 56.71
68.15 71.48 52.08 46.25 86.25 60.44
68.89 71.48 49.17 50.42 82.92 60.25
71.48 74.44 56.67 56.67 90.83 64.46
91.85 90.37 69.17 77.92 96.25 76.77
saccordingtoRAGRouter’sempiricalprobability
ingperformance,weintroducetheOraclebaseline
round-truthperformancedata.
rchitecture,weuseall-mpnet-base-v2[37]asthe
marco-MiniLM-L12-v2[46]asthecross-encoder,
y136M.Boththeknowledgerepresentationvector
ionalityof768. Tomitigateoverfitting,allbutthe
tencoderandthecross-encoderarefrozenduring
2.0,andthecontrastivelearningtemperatureτ to
withalearningrateof5e-5,batchsizeof64,for10
eNVIDIARTX4090DGPU.
able2presentsthetestaccuracyacrossavarietyof
RAGRouterconsistentlyachievesthehighestperfor-
urpassesthebest-performingsingleRAG-enabled
3.61%,demonstratingtheeffectivenessofrouting
heOracleSingleBestbaseline(61.22%)—which
—by+3.24%,indicatingthatroutingenablesthe
achieveperformancebeyondanyindividualmodel.
Baselines. RAGRoutersignificantlyoutperforms
ngGraphRouter(61.17%,+3.29%),MF(60.25%,
terDC(56.71%,+7.75%).Theseresultsunderscore
modeltheinteractionbetweenLLMsandretrieved
7

=== Page 8 ===
knowledge. Withoutcapturingretrieval-inducedc
effectiveroutingdecisionsinRAG.RAGRouteral
(46.13%,+18.33%)andWeighted(59.53%,+4.93%
modelingRAG-specificcapabilitiesandknowledg
InferenceCostofRAGRouter. Theinferencecha
4090DGPU.PeakGPUmemoryusageis4147M
270instancesin3secondsacross5batches,yieldi
instance. TheseresultsclearlyshowthatRAGRou
5.3 RoutingPerformanceInvolvingClosed-So
ToverifythatRAGRoutercanstilldeliverperform
gains with strong closed-source LLMs, we augm
the candidate LLMs set—comprising Qwen2.5-3
Instruct, Qwen2.5-72B-Instruct, and Llama-3.3-7
Instruct—withclosed-sourcemodelsQwen2.5-M
GPT-4o6,andreasoning-basedDeepSeek-R17.
As shown in Table 3, RAGRouter outperforms
strongest model GPT-4o by +1.67%, while in
ing GPT-4o in only 44.79% of samples. It also
passes Qwen2.5-Max (+7.71%) and the reason
based DeepSeek-R1 (+4.17%). Furthermore,
GRouter significantly outperforms non–RAG-aw
baselines such as KNN Router (+4.17%) and
(+2.92%). TheseresultsdemonstratethatRAGRo
caneffectivelyintegrateclosed-sourcemodels,le
agingtheircomplementaritytoachievesimultane
improvementsinbothperformanceandefficiency.
5.4 ExtendedResultsonLatency-AwareRout
Metrics. Three metrics are used for evaluate lat
proportion ofthe areaunder theaccuracy–latenc
efficiency;PeakAcc,thehighestaccuracyachiev
definedasthelatencyofthebest-performingsingl
methodtomatchitsaccuracy,indicatingtheeffici
Table4: Area(%),PeakAccuracy(PA,%),andL
baselines using score-threshold-based routing on
indicatesfailuretomatchthebestsingle-LLMper
MedMCQA(Local) Me
Method
Area↑ PA↑ G(s)↑ Area
RouterDC 55.87 62.22 - 57.7
MF 46.42 59.63 0.10 54.8
GraphRouter 47.96 59.30 0.18 57.8
KNNRouter 52.26 62.30 - 60.1
RAGRouter 57.12 62.59 0.24 63.1
QuantitativeResultsandVisualization. Weappl
RAGRouterandbaselinesacrossalltasks. Results
reportedinTable4andFigure4,withsupplementa
achievesthehighestAreaandPeakAccuracyacro
5https://chat.qwen.ai
6https://platform.openai.com/docs/models
7https://www.deepseek.com
8

capabilityshifts,suchapproachesstruggletomake
lsosurpassesrule-basedstrategiessuchasRandom
%). Together,thesefindingssupportourcoreclaim:
geshiftisessentialforaccurateLLMrouting.
aracteristicsareevaluatedonasingleNVIDIARTX
MiB.Withabatchsizeof64,RAGRouterprocesses
inganaverageinferencetimeof0.011secondsper
uterislightweightandefficientduringinference.
ourceLLMs
mance Table 3: Testing accuracy (%) of RA-
ment GRouterandbaselineswiththeinclusionof
32B- closed-sourceLLMs(Qwen2.5-Max,GPT-
70B- 4o,andDeepSeek-R1)onNQandWebQ,
Max5, boldindicatesbestresults.
Method NQ WQ Avg
s the
nvok- Qwen2.5-32B-Instruct 44.58 50.42 47.50
Qwen2.5-72B-Instruct 40.00 48.75 44.38
o sur-
Llama-3.3-70B-Instruct 51.67 50.42 51.05
ning-
RA- Qwen2.5-Max 51.25 52.08 51.67
ware GPT-4o 58.33 57.08 57.71
MF DeepSeek-R1 56.67 53.75 55.21
outer KNNRouter 58.75 51.67 55.21
ever- MF 57.50 55.42 56.46
eous RAGRouter 59.58 59.17 59.38
y.
ting
tency-aware routing: Area, which measures the
cycurvewithin 1second, reflecting overall time-
vedwithin1second;andLatencyGap-to-Match,
leLLMminustheminimumlatencyrequiredbya
iencymarginobtainedthroughrouting.
LatencyGap-to-Match(G,s)forRAGRouterand
n MedMCQA (Local/Online) and TriviaQA. "–"
rformance;bolddenotesthebestresult.
edMCQA(Online) TQA
a↑ PA↑ G(s)↑ Area↑ PA↑ G(s)↑
77 65.56 - 70.99 75.83 -
85 65.56 0.45 66.84 80.00 -
82 64.67 0.32 65.42 73.15 0.01
18 66.10 0.33 72.61 81.65 -
12 67.78 0.51 73.78 87.50 0.76
lythescore-threshold-basedroutingmechanismto
sonMedMCQA(Local/Online)andTriviaQAare
aryresultsinAppendixC.RAGRouterconsistently
ossalltasks. Specifically,itoutperformsbaselines
s/gpt-4o
8

=== Page 9 ===
80
60
40
20
0 500 1000 1500 2000
LatencyperQuery(ms)
)%(ycaruccAgnitseT
MedMCQA(Local)
90
RAGRouter 80
70
60
50
40
0 500
Latencyp
)%(ycaruccAgnitseT
Qwen2.5-0.5B-Instruct Llama-3.2-3B-Instruct Ministral-8B-Inst
Llama-3.2-1B-Instruct Qwen2.5-3B-Instruct Meta-Llama-3.1-
Qwen2.5-1.5B-Instruct Yi-1.5-6B-Chat Yi-1.5-9B-Chat
gemma-2-2b-it Qwen2.5-7B-Instruct Qwen2.5-14B-Ins
MedMC
R
Figure4: Accuracy–latencycurvesonMedMCQ
by4.86%–10.7%,2.94%–8.27%,and1.17%–8.36
and5.85%–14.35%inPeakAccuracyonMedMCQ
showninFigure4,itsaccuracy–latencycurvecon
superior accuracy under low-latency constraints
LatencyGap-to-Matchmargins,demonstratingitca
substantiallylowerlatency. Theseresultshighligh
spaceenabledbyretrievalaugmentation,efficientl
performanceoflargeroneswithoutincurringunne
5.5 SensitivityAnalysisandAblationStudy
Table5: AblationstudyofCrossEncoder
PopQA
Configuration
Local Online
RAGRouter 48.52 52.59
w/oCrossEncoder 47.78 52.22
w/oRAGCapabilityEmbeddingLayer 48.52 52.22
Table6: Ablationstudyofco
PopQA MedMCQA
ISC CSC
Local Online Local Onli
✓ ✓ 48.52 52.59 71.48 74.4
✓ ✗ 48.15 52.22 71.11 73.3
✗ ✓ 48.52 51.85 71.48 74.0
✗ ✗ 47.78 51.48 69.26 70.7
AblationStudyonRAGRouterArchitecture. W
butionsoftheCrossEncoderandtheRAGCapabil
Table5,removingtheCrossEncoderreducesper
ofquery-documentinteractionsforderivingfused
CapabilityEmbeddingLayerreducesperformance
explicitmodelingofRAGcapabilitiesandinstead
itsoverallbehavior,withoutdistinguishingbetwe
Thisresultdemonstratesthatexplicitlymodeling
thataccountsforeachLLM’scapacitytoexploite
Effects of Positive and Negative Sample Sele
effectiveness of contrastive learning, we perform
sampleconstructionstrategiesusedinourmethod.
ContrastorCross-SettingContrastresultsinperfo
Whenbothcomponentsareremoved—effectivelyd
9

CQA(Online)
90
RAGRouter
80
70
60
50
40
1000 1500 2000 0 500 1000 1500 2000
perQuery(ms) LatencyperQuery(ms)
)%(ycaruccAgnitseT
truct-2410 Qwen2.5-32B-Instruct Weighted KNNRouter
-8B-Instruct Qwen2.5-72B-Instruct PromptLLM RouterDC
Llama-3.3-70B-Instruct Oracle GraphRouter
struct Random EmbedLLM RAGRouter(Ours)
TriviaQA
RAGRouter
QA(Local),MedMCQA(Online),andTriviaQA.
6%inArea,andby0.29%–3.29%,1.68%–3.11%,
QA(Local/Online)andTriviaQA,respectively. As
nsistentlydominatesthoseofbaselines,indicating
s. RAGRouter also achieves the largest positive
anmatchtheaccuracyofthebestsingleLLMwith
htRAGRouter’sabilitytoexploittheoptimization
lyleveragingsmaller,fastermodelstoachievethe
ecessarylatency.
randRAGCapabilityEmbeddingLayer.
MedMCQA
NQ WQ TQA Avg ∆
Local Online
71.48 74.44 56.67 56.67 90.83 64.46 0.00
70.74 74.44 55.42 55.00 88.75 63.48 -0.98
71.11 74.44 55.42 54.17 90.00 63.70 -0.76
ontrastivelearningobjectives.
A
NQ WQ TQA Avg ∆
ine
44 56.67 56.67 90.83 64.46 0.00
33 53.75 56.25 89.58 63.49 -0.97
07 55.00 56.25 89.58 63.82 -0.64
74 53.75 53.75 89.17 62.28 -2.18
Weconductanablationstudytoevaluatethecontri-
lityEmbeddingLayerinRAGRouter. Asshownin
rformanceby0.98%,highlightingtheimportance
dknowledgerepresentations. RemovingtheRAG
eby0.76%;inthisconfiguration,weremovedthe
assignedeachLLMasingleembeddingcapturing
eenparametricknowledgeandRAGcapabilities.
RAGcapabilitiesisessentialforeffectiverouting
externalinformation.
ection in Contrastive Learning. To assess the
m an ablation study on the two positive–negative
AsshowninTable6,removingeitherIntra-Setting
ormancedropsof0.64%and0.97%,respectively.
disablingcontrastivelearning—accuracydecreases
9

=== Page 10 ===
Table7: Effectsofdiffere
PopQA
CandidateSet Method
Local Online L
OracleSingleBest 45.56 51.48 4
Small
RAGRouter 45.56 51.85 4
OracleSingleBest 46.30 50.37 6
Large
RAGRouter 47.41 50.74 7
OracleSingleBest 46.30 51.48 6
Small&Large
RAGRouter 48.15 52.22 7
PopQA(Local) 0.00 0.00 1.53 0.76 0.00 0.00 6
PopQA(Online) 0.00 2.13 0.71 0.71 0.71 0.71
4
MedMCQA(Local) 0.00 -1.04 -1.04 -1.04 0.52 0.00
MedMCQA(Online) 0.00 -0.50 -1.51 -0.50 1.00 0.00
2
NQ 0.00 2.31 3.84 2.31 4.62 2.31
WQ 0.00 4.69 0.79 5.48 6.26 -3.11 0
TQA 0.00 0.94 1.42 1.42 3.31 2.37
Avg 0.00 1.03 0.68 1.14 2.32 0.46
−2
128 256 384 512 768 1024
Dimension
)%(etaRegnahCycaruccAgnitseT
Figure5: Effectsofdimension(baseline: 128).
by2.18%. Thesefindingsunderscoretheimportan
representationshiftsandLLMheterogeneity,whic
EffectsoftheDimensionofLLMKnowledgean
theimpactofthedimensionalityofbothknowledg
showninFigure5,weobservethatperformanceim
768,afterwhichitdeclines. Accordingly,weadop
DetailedresultsareprovidedinAppendixD.
Effects of λ. We investigate the impact of the
function(Eq.9)ontestaccuracy. AsshowninFig
classificationlossesyieldsbetterperformancethan
only63.76%onaverage). Asλincreases,accura
average,beforeexperiencingaslightdecline.Based
resultsareprovidedinAppendixD.
EffectsofDifferentCandidateLLMsSets. Wein
affectsroutingperformance. Tothisend,weform
andLarge( 32B)—andevaluatethreeconfigurati
≥
setcombiningboth. AsshowninTable7,RAGR
Best in all settings, demonstrating its ability to c
Twokeyinsightsemerge. First,theroutinguppe
theLargesetyieldsasignificantlyhigherOracle
(47.68%), withRAGRouterfollowingthesamet
heterogeneousmodelsyieldsthebestperformanc
RAGRouteraverage(63.30%)andthelargestgain
thatmodeldiversityimprovescomplementarityan
6 Conclusion
In this paper, we have studied the problem of LL
(RAG)forthefirsttimeandproposeRAGRouter,th
contrastive learning, RAGRouter captures know
documents,enablingeffectiveroutingdecisions. E
demonstratethatRAGRouteroutperformsexistin
performance-efficiencytrade-offsunderlow-laten
1

entcandidateLLMssets.
MedMCQA
NQ WQ TQA Avg ∆
Local Online
40.37 49.26 36.67 45.42 65.00 47.68 0.00
40.37 51.48 36.67 47.92 65.00 48.41 +0.73
68.89 70.37 51.67 50.42 87.92 60.85 0.00
71.11 73.33 54.58 53.33 90.42 62.99 +2.14
68.89 70.37 51.67 50.42 87.92 61.01 0.00
71.48 73.33 53.33 55.00 89.58 63.30 +2.29
PopQA(Local) 0.00 0.00 -0.76 0.00 -0.76 -0.76 -0.76
4
PopQA(Online) 0.00 0.70 0.00 0.00 0.00 0.70 0.70
MedMCQA(Local) 0.00 0.53 1.58 1.58 1.58 1.58 2.10 3
MedMCQA(Online) 0.00 -0.50 -0.99 -0.50 0.00 0.00 0.00
2
NQ 0.00 0.00 0.00 0.74 2.26 3.00 0.74
1
WQ 0.00 0.00 2.31 2.31 4.62 3.84 3.06
TQA 0.00 0.00 0.00 0.45 0.45 0.00 -0.46 0
Avg 0.00 0.08 0.28 0.63 1.10 1.08 0.71
0.0 0.5 1.0 1.5 2.0 2.5 3.0
λ
)%(etaRegnahCycaruccAgnitseT
Figure6: Effectsofλ(baseline: λ=0).
nceofcontrastivelearningincapturingknowledge
chisessentialforeffectiveroutinginRAGsettings.
ndRAGCapabilityEmbeddings. Weinvestigate
gerepresentationandRAGcapabilityvectors. As
mprovesasthedimensionalityincreases,peakingat
ptadimensionalityof768inthemainexperiments.
e classification loss weight λ in the overall loss
gure6,weobservethatcombiningcontrastiveand
usingcontrastivelossalone(i.e.,λ=0,achieving
acyimproves,peakingatλ = 2with64.46%on
donthis,wesetλ=2inallexperiments.Detailed
nvestigatehowthecompositionofcandidateLLMs
mtwosubsetsofmodels—Small( 3Bparameters)
≤
ions: Smallonly,Largeonly,andaheterogeneous
RouterconsistentlyoutperformstheOracleSingle
coordinate models and achieve cumulative gains.
erboundislargelydeterminedbymodelstrength:
SingleBestaverage(60.85%)thantheSmallset
trend(62.99%vs. 48.41%). Second, combining
ce: theSmall&Largesettingachievesthehighest
noveritsOracleSingleBest(+2.29%),suggesting
ndenablesmoreeffectiverouting.
LM routing in Retrieval-Augmented Generation
hefirstRAG-awareroutingmethod. Byleveraging
wledge representation shifts induced by external
Experimentsondiverseknowledge-intensivetasks
ngnon-RAG-awaremethodsandachievesstrong
ncyconstraints.
10

=== Page 11 ===
AcknowledgmentsandDisclosureofFu
ThisworkwassupportedinpartbyNationalKey
ChinaNSFgrantNo. 62025204,No. 62202296,
andNo. 62572299,TencentWeChatResearchPr
Theopinions,findings,conclusions,andrecomm
authorsanddonotnecessarilyreflecttheviewsof
References
[1] QJiangAlbert,AlexandreSablayrolles,Arth
Chaplot. Mistral7b. arXiv,2023.
[2] JonathanBerant,AndrewChou,RoyFrostig
fromquestion-answerpairs. InProceedings
naturallanguageprocessing,pages1533–15
[3] TomBrown,BenjaminMann,NickRyder,M
ArvindNeelakantan,PranavShyam,GirishS
few-shotlearners. Advancesinneuralinform
[4] SébastienBubeck,VarunChadrasekaran,R
Kamar,PeterLee,YinTatLee,YuanzhiLi,
intelligence: Earlyexperimentswithgpt-4,2
[5] DengCai,YanWang,LemaoLiu,andShum
textgeneration. InProceedingsofthe45thi
anddevelopmentininformationretrieval,pa
[6] Jiawei Chen, Hongyu Lin, Xianpei Han, an
elsinretrieval-augmentedgeneration. InP
Intelligence,volume38,pages17754–17762
[7] ShuhaoChen,WeisenJiang,BaijiongLin,J
basedrouterbydualcontrastivelearningfor
arXiv:2409.19886,2024.
[8] TingChen,SimonKornblith,MohammadNo
for contrastive learning of visual represent
learning,pages1597–1607.PmLR,2020.
[9] ZhijunChen,JingzhengLi,PengpengChen,
DingqiYang,HailongSun,andPhilipSYu
surveyonllmensemble. arXivpreprintarXi
[10] DujianDing, AnkurMallick, ChiWang, Ro
LaksVSLakshmanan,andAhmedHassanA
awarequeryrouting. arXivpreprintarXiv:24
[11] FeitengFang,YuelinBai,ShiwenNi,MinY
noiserobustnessofretrieval-augmentedlan
arXivpreprintarXiv:2405.20978,2024.
[12] TaoFeng,YanzhenShen,andJiaxuanYou.Gr
InTheThirteenthInternationalConferenceo
[13] YunfanGao,YunXiong,XinyuGao,Kangx
HaofenWang,andHaofenWang. Retrieval-
Asurvey. arXivpreprintarXiv:2312.10997,
[14] Aaron Grattafiori, Abhimanyu Dubey, Abh
AhmadAl-Dahle,AieshaLetman,AkhilMath
3herdofmodels. arXivpreprintarXiv:2407
1

unding
yR&DProgramofChina(No. 2022ZD0119100),
No. 62272293,No. 62441236,No. U24A20326,
rogram,andSJTU-HuaweiExploreXGiftFund.
mendationsexpressedinthispaperarethoseofthe
fthefundingagenciesorthegovernment.
hurMensch,ChrisBamford,andDevendraSingh
g,andPercyLiang. Semanticparsingonfreebase
softhe2013conferenceonempiricalmethodsin
544,2013.
MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
Sastry,AmandaAskell,etal. Languagemodelsare
mationprocessingsystems,33:1877–1901,2020.
RonenEldan,JohannesGehrke,EricHorvitz,Ece
ScottLundberg,etal. Sparksofartificialgeneral
2023.
mingShi. Recentadvancesinretrieval-augmented
internationalACMSIGIRconferenceonresearch
ages3417–3419,2022.
nd Le Sun. Benchmarking large language mod-
ProceedingsoftheAAAIConferenceonArtificial
2,2024.
JamesTKwok,andYuZhang. Routerdc: Query-
assemblinglargelanguagemodels. arXivpreprint
orouzi,andGeoffreyHinton. Asimpleframework
tations. In International conference on machine
ZhuoranLi,KaiSun,YuankaiLuo,QianrenMao,
u. Harnessingmultiplelargelanguagemodels: A
iv:2502.18036,2025.
obertSim, SubhabrataMukherjee, VictorRuhle,
Awadallah. Hybridllm: Cost-efficientandquality-
404.14618,2024.
Yang,XiaojunChen,andRuifengXu. Enhancing
nguagemodelswithadaptiveadversarialtraining.
raphrouter:Agraph-basedrouterforllmselections.
onLearningRepresentations,2024.
xiangJia,JinliuPan,YuxiBi,YiDai,JiaweiSun,
-augmentedgenerationforlargelanguagemodels:
2,2023.
hinav Jauhri, Abhinav Pandey, Abhishek Kadian,
hur,AlanSchelten,AlexVaughan,etal. Thellama
7.21783,2024.
11

=== Page 12 ===
[15] Kelvin Guu, Kenton Lee, Zora Tung, Panu
augmented language model pre-training. In
pages3929–3938.PMLR,2020.
[16] HiroakiHayashi,PrashantBudania,PengWa
Neubig. Wikiasp: Adatasetformulti-domain
AssociationforComputationalLinguistics,9
[17] Hangfeng He, Hongming Zhang, and Dan
languagemodelinference. arXivpreprintar
[18] KaimingHe,HaoqiFan,YuxinWu,Saining
unsupervisedvisualrepresentationlearning.
computervisionandpatternrecognition,pag
[19] QitianJasonHu,JacobBieker,XiuyuLi,N
KurtKeutzer,andShriyashKaustubhUpad
routingsystem. arXivpreprintarXiv:2403.1
[20] GautierIzacardandEdouardGrave. Levera
opendomainquestionanswering. arXivprep
[21] GautierIzacard,PatrickLewis,MariaLom
JaneDwivedi-Yu,ArmandJoulin,Sebastian
withretrievalaugmentedlanguagemodels. a
[22] ZiweiJi,NayeonLee,RitaFrieske,Tiezhen
AndreaMadotto,andPascaleFung. Survey
ACMcomputingsurveys,55(12):1–38,2023.
[23] Mandar Joshi, Eunsol Choi, Daniel S We
scale distantly supervised challenge datas
arXiv:1705.03551,2017.
[24] VladimirKarpukhin,BarlasOguz,SewonM
DanqiChen,andWen-tauYih. Densepassa
InEMNLP(1),pages6769–6781,2020.
[25] TomKwiatkowski,JennimariaPalomaki,Oliv
Alberti,DanielleEpstein,IlliaPolosukhin,Jac
benchmarkforquestionansweringresearch.
Linguistics,7:453–466,2019.
[26] Woosuk Kwon, Zhuohan Li, Siyuan Zhuan
JosephE.Gonzalez,HaoZhang,andIonSto
guagemodelservingwithpagedattention. In
onOperatingSystemsPrinciples,2023.
[27] PatrickLewis,EthanPerez,AleksandraPik
Goyal,HeinrichKüttler,MikeLewis,Wen-tau
generation for knowledge-intensive nlp tas
systems,33:9459–9474,2020.
[28] KuanLi,LiwenZhang,YongJiang,Pengjun
Lara: Benchmarkingretrieval-augmentedgen
lcorragrouting. arXivpreprintarXiv:2502.
[29] XianzhiLi,SamuelChan,XiaodanZhu,Yul
Shah. Arechatgptandgpt-4general-purpos
severaltypicaltasks. arXivpreprintarXiv:23
[30] Ilya Loshchilov and Frank Hutter. Decoup
arXiv:1711.05101,2017.
1

upong Pasupat, and Mingwei Chang. Retrieval
n International conference on machine learning,
ang,ChrisAckerson,RajNeervannan,andGraham
naspect-basedsummarization. Transactionsofthe
9:211–225,2021.
Roth. Rethinking with retrieval: Faithful large
rXiv:2301.00303,2022.
gXie,andRossGirshick. Momentumcontrastfor
. InProceedingsoftheIEEE/CVFconferenceon
ges9729–9738,2020.
NanJiang,BenjaminKeigwin,GauravRanganath,
dhyay. Routerbench: Abenchmarkformulti-llm
12031,2024.
agingpassageretrievalwithgenerativemodelsfor
printarXiv:2007.01282,2020.
meli,LucasHosseini,FabioPetroni,TimoSchick,
nRiedel,andEdouardGrave. Few-shotlearning
arXivpreprintarXiv:2208.03299,1(2):4,2022.
ngYu,DanSu,YanXu,EtsukoIshii,YeJinBang,
yofhallucinationinnaturallanguagegeneration.
.
eld, and Luke Zettlemoyer. Triviaqa: A large
set for reading comprehension. arXiv preprint
Min,PatrickSHLewis,LedellWu,SergeyEdunov,
ageretrievalforopen-domainquestionanswering.
viaRedfield,MichaelCollins,AnkurParikh,Chris
cobDevlin,KentonLee,etal. Naturalquestions:a
TransactionsoftheAssociationforComputational
ng, Ying Sheng, Lianmin Zheng, Cody Hao Yu,
oica. Efficientmemorymanagementforlargelan-
ProceedingsoftheACMSIGOPS29thSymposium
ktus,FabioPetroni,VladimirKarpukhin,Naman
uYih,TimRocktäschel,etal.Retrieval-augmented
sks. Advances in neural information processing
nXie,FeiHuang,ShuaiWang,andMinhaoCheng.
nerationandlong-contextllms-nosilverbulletfor
.09977,2025.
longPei,ZhiqiangMa,XiaomoLiu,andSameena
sesolversforfinancialtextanalytics? astudyon
305.05862,2023.
pled weight decay regularization. arXiv preprint
12

=== Page 13 ===
[31] Keming Lu, Hongyi Yuan, Runji Lin, Juny
Zhou. Routingtotheexpert: Efficientreward
preprintarXiv:2311.08692,2023.
[32] AlexMallen,AkariAsai,VictorZhong,Ra
jishirzi. When not to trust language model
non-parametricmemories. arXivpreprintar
[33] IsaacOng,AmjadAlmahairi,VincentWu,W
MWaleedKadous,andIonStoica. Routellm
TheThirteenthInternationalConferenceon
[34] AnkitPal,LogeshKumarUmapathi,andMal
multi-subjectmulti-choicedatasetformedic
health,inference,andlearning,pages248–2
[35] AlecRadford,KarthikNarasimhan,TimSal
understandingbygenerativepre-training. Op
[36] Alec Radford, Jeffrey Wu, Rewon Child, D
Languagemodelsareunsupervisedmultitask
[37] NilsReimersandIrynaGurevych. Sentence
networks. arXivpreprintarXiv:1908.10084,
[38] MarijaŠakota,MaximePeyrard,andRobertW
modelchoiceviameta-modeling. InProceed
WebSearchandDataMining,pages606–61
[39] FredaShi,XinyunChen,KanishkaMisra,N
Schärli,andDennyZhou. Largelanguagemo
InInternationalConferenceonMachineLea
[40] TalShnitzer,AnthonyOu,MírianSilva,Kate
son,andMikhailYurochkin. Largelanguag
preprintarXiv:2309.15789,2023.
[41] Devendra Singh, Siva Reddy, Will Hamilto
trainingofmulti-documentreaderandretriev
inNeuralInformationProcessingSystems,3
[42] HuatongSong,JinhaoJiang,YingqianMin,Ji
andJi-RongWen. R1-searcher: Incentivizin
learning. arXivpreprintarXiv:2503.05592,2
[43] Dimitris Stripelis, Zijian Hu, Jipeng Zhang
YuhangYao,SalmanAvestimehr,andChaoya
forefficientllminference. arXivpreprintarX
[44] Gemma Team, Morgane Riviere, Shreya
SuryaBhupatiraju,LéonardHussenot,Thom
et al. Gemma 2: Improving open langua
arXiv:2408.00118,2024.
[45] AshishVaswani,NoamShazeer,NikiParma
ŁukaszKaiser,andIlliaPolosukhin. Attentio
processingsystems,30,2017.
[46] WenhuiWang,FuruWei,LiDong,Hangbo
self-attentiondistillationfortask-agnosticco
inneuralinformationprocessingsystems,33
[47] XiaohuaWang,ZhenghuaWang,XuanGao
Shi,ZhengyuanWang,ShizhengLi,QiQia
augmentedgeneration. arXivpreprintarXiv:
1

yang Lin, Zheng Yuan, Chang Zhou, and Jingren
d-guidedensembleoflargelanguagemodels. arXiv
ajarshiDas,DanielKhashabi,andHannanehHa-
ls: Investigating effectiveness of parametric and
rXiv:2212.10511,2022.
Wei-LinChiang,TianhaoWu,JosephEGonzalez,
m: Learningtoroutellmsfrompreferencedata. In
LearningRepresentations,2024.
laikannanSankarasubbu. Medmcqa: Alarge-scale
caldomainquestionanswering. InConferenceon
260.PMLR,2022.
limans,IlyaSutskever,etal. Improvinglanguage
penAIblog,2018.
David Luan, Dario Amodei, Ilya Sutskever, et al.
klearners. OpenAIblog,1(8):9,2019.
e-bert: Sentenceembeddingsusingsiamesebert-
,2019.
West. Fly-swatorcannon? cost-effectivelanguage
dingsofthe17thACMInternationalConferenceon
15,2024.
NathanScales,DavidDohan,EdHChi,Nathanael
odelscanbeeasilydistractedbyirrelevantcontext.
arning,pages31210–31227.PMLR,2023.
eSoule,YuekaiSun,JustinSolomon,NeilThomp-
gemodelroutingwithbenchmarkdatasets. arXiv
on, Chris Dyer, and Dani Yogatama. End-to-end
verforopen-domainquestionanswering. Advances
34:25968–25981,2021.
ieChen,ZhipengChen,WayneXinZhao,LeiFang,
ngthesearchcapabilityinllmsviareinforcement
2025.
g, Zhaozhuo Xu, Alay Dilipbhai Shah, Han Jin,
angHe. Tensoroperarouter: Amulti-modelrouter
Xiv:2408.12320,2024.
Pathak, Pier Giuseppe Sessa, Cassidy Hardin,
masMesnard,BobakShahriari,AlexandreRamé,
age models at a practical size. arXiv preprint
ar,JakobUszkoreit,LlionJones,AidanNGomez,
onisallyouneed. Advancesinneuralinformation
oBao,NanYang,andMingZhou. Minilm: Deep
ompressionofpre-trainedtransformers. Advances
3:5776–5788,2020.
o,FeiranZhang,YixinWu,ZhiboXu,Tianyuan
an,etal. Searchingforbestpracticesinretrieval-
:2407.01219,2024.
13

=== Page 14 ===
[48] ZilongWang,ZifengWang,LongLe,Huaix
YuweiZhang,AnushMattapalli,AnkurTaly
retrievalaugmentedgenerationthroughdraft
[49] ZhepeiWei,Wei-LinChen,andYuMeng. In
tionviaself-synthesizedrationales. arXivpr
[50] ShitaoXiao,ZhengLiu,PeitianZhang,Nikl
pack:Packedresourcesforgeneralchineseem
ACMSIGIRconferenceonresearchanddeve
2024.
[51] RanXu,WenqiShi,YueYu,YuchenZhuang
Zhang,andCarlYang. Bmretriever: Tuning
retrievers. arXivpreprintarXiv:2404.18443,
[52] AnYang, BaosongYang, BeichenZhang, B
Li,DayihengLiu,FeiHuang,HaoranWei,
arXiv:2412.15115,2024.
[53] Haoran Yang, Yumeng Zhang, Jiaqi Xu, H
Unveiling the generalization power of fine
arXiv:2403.09162,2024.
[54] ZhilinYang,PengQi,SaizhengZhang,Yos
dinov,andChristopherDManning. Hotpot
questionanswering. arXivpreprintarXiv:18
[55] AlexYoung,BeiChen,ChaoLi,ChengenH
HengLi,JiangchengZhu,JianqunChen,et
preprintarXiv:2403.04652,2024.
[56] PeitianZhang,ShitaoXiao,ZhengLiu,Zhich
augmentlargelanguagemodels. arXivprepr
[57] YueZhang,MingZhang,HaipengYuan,Sh
XuanjingHuang. Llmeval: Apreliminarystu
ProceedingsoftheAAAIConferenceonArtifi
2024.
[58] RichardZhuang,TianhaoWu,ZhaojinWen
dran. Embedllm: Learningcompactrepresen
arXiv:2410.02223,2024.
1

xiuStevenZheng,SwaroopMishra,VincentPerot,
y,JingboShang,etal. Speculativerag: Enhancing
ting. arXivpreprintarXiv:2407.08223,2024.
nstructrag: Instructingretrieval-augmentedgenera-
reprintarXiv:2406.13629,2024.
lasMuennighoff,DefuLian,andJian-YunNie. C-
mbeddings.InProceedingsofthe47thinternational
elopmentininformationretrieval,pages641–649,
g,YanqiaoZhu,MayDWang,JoyceCHo,Chao
glargelanguagemodelsasbetterbiomedicaltext
,2024.
BinyuanHui, BoZheng, BowenYu, Chengyuan
etal. Qwen2.5technicalreport. arXivpreprint
Hongyuan Lu, Pheng Ann Heng, and Wai Lam.
e-tuned large language models. arXiv preprint
shuaBengio,WilliamWCohen,RuslanSalakhut-
tqa: Adatasetfordiverse,explainablemulti-hop
809.09600,2018.
Huang,GeZhang,GuanweiZhang,GuoyinWang,
al. Yi: Openfoundationmodelsby01.ai. arXiv
hengDou,andJian-YunNie. Retrieveanythingto
rintarXiv:2310.07554,2023.
hichunLiu,YongyaoShi,TaoGui,QiZhang,and
udyonhowtoevaluatelargelanguagemodels. In
ficialIntelligence,volume38,pages19615–19622,
n,AndrewLi,JiantaoJiao,andKannanRamchan-
ntationsoflargelanguagemodels. arXivpreprint
14

=== Page 15 ===
AppendixContents
A ExperimentalSetupDetails
A.1 DetailsofCandidateLLMs . . . . . . .
A.2 DetailsofDatasets . . . . . . . . . . .
B ImpactofRAGonLLMPerformance
C FullResultsonLatency-AwareRouting
D FullResultsofSensitivityAnalysisandAbl
E CaseStudiesofFailuresandSuccesses
E.1 QuantitativeFailureCaseStudiesofRou
E.2 QualitativeSuccessCaseStudiesofRAG
F RoutingPerformanceunderCross-Domain
G RoutingPerformanceunderNoisyRetrieva
H RoutingPerformanceonSummarizationTa
1

16
. . . . . . . . . . . . . . . . . . . . . . . . 16
. . . . . . . . . . . . . . . . . . . . . . . . 16
16
18
lationStudy 19
19
utingDecisions . . . . . . . . . . . . . . . 19
GRouter . . . . . . . . . . . . . . . . . . . 20
nSettings 21
al 21
ask 22
15

=== Page 16 ===
A ExperimentalSetupDetails
A.1 DetailsofCandidateLLMs
TheresponsesofQwen2.5-72B-Instruct,Llama-3
tainedviaAPIcalls,whileotheropen-sourceLLMs
[26]forhigh-speedinferencefromHuggingface8.N
incorporatedbothaveragenetworklatencyandin
locallydeployedmodelsexclusivelyaccountedfo
A.2 DetailsofDatasets
Table8: Thestatisticsre
PopQA M
QueryNum
Local Online Loc
Train 2000 2000 200
Test 270 270 27
AsshowninTable8, werandomlysampledque
[32],MedMCQA[34],NQ[25],WebQ[2],andT
andtestsets. ForPopQAandMedMCQA,were
searchenginesfollowing[42],whileforNQ,WebQ
documentsbasedon[11],withanequalproportion
Noise,IrrelevantRetrievalNoise,andCounterfac
wasprocessedbythe15LLMsdescribedinSectio
thenevaluatedagainstground-truthanswerstoder
B ImpactofRAGonLLMPerforman
PerformanceShiftwithandwithoutRAG.We
andafterRAGacrossdifferentmodelsandsetting
suchasPopQA(Online), MedMCQA(Local), a
performanceandreducestheaccuracygapamong
(e.g.,Qwen2.5-0.5B-Instructachievesa17.57%i
tolargermodels(e.g.,Llama-3.3-70B-Instructim
retrievalsettings(NQ,WebQ,TriviaQA),theimp
othersdegrade(e.g.,onNQ,Llama-3.3-70B-Instru
performsbetter). Theseresultshighlightinconsis
thatRAGsignificantlyaltersthedistributionofres
ofnon-RAG-awareroutingstrategies.
AnalysisofResponseQualityReversal. Wefurt
forthesamequery. Toquantifythesereversals,we
gainrateistheproportionofqueriesthatwereunan
withretrieveddocuments,calculatedas:
#(Incorrec
PositiveGainRate=
#
Thenegativeinterferenceratemeasurestheoppos
retrievalbutbecameunanswerableduetoretrieved
#(Corr
NegativeInterferenceRate=
Figures8and9reportthesemetricsacross15LL
modelsandtasks,thepositivegainrateis30.86%,
confirmthatresponsequalityreversalsinducedb
RAG-inducedknowledgeshiftsarewidespreadam
8https://huggingface.co
1

3-70B-Instructandclosed-sourceLLMswereob-
swerelocallydeployedusingthevLLMframework
Notably,latencycalculationsforAPI-basedmodels
nferencetime,whereaslatencymeasurementsfor
orinferencetime.
esultsfordifferenttasks.
MedMCQA
NQ WQ TQA
cal Online
00 2000 1000 1000 1000
70 270 240 240 240
eriesfromfiveknowledge-intensivtasks(PopQA
TriviaQA[23])andpartitionedthemintotraining
etrieveddocumentsthroughbothlocalandonline
Q,andTriviaQA,weconstructedartificiallynoised
noffourtypes: GoldenContext,RelevantRetrieval
ctualRetrievalNoise. Eachquery-documentpair
on5.1togenerateresponses. Theseresponseswere
rivebinaryscores.
nce
eanalyzehowLLMperformancechangesbefore
gs. AsshowninFigure7,onrealretrievalsettings
andMedMCQA(Online), RAGimprovesoverall
models. Notably,small-scalemodelsbenefitmore
improvementonMedMCQA(Online))compared
mprovesbyonly0.88%). Incontrast,undernoisy
pactofRAGvaries—somemodelsimprovewhile
uctperformsworse,whileQwen2.5-0.5B-Instruct
stentperformanceshiftsacrossmodels,indicating
sponsequality,whichunderminestheassumptions
therinvestigatehowRAGcausesqualityreversals
edefinetwometricsatthetasklevel. Thepositive
nswerablewithoutretrievalbutbecameanswerable
ctw/oRAG→Correctw/RAG)
(10)
#(Incorrectw/oRAG)
siteeffect—queriesthatwereanswerablewithout
dcontent:
rectw/oRAG→Incorrectw/RAG)
(11)
#(Correctw/oRAG)
LMsandmultiplesettings. Onaverage,acrossall
negativeinterferencerateis31.46%. Theseresults
byexternaldocumentsarecommonsuggeststhat
mongLLMs.
16

=== Page 17 ===
(a) (b
(e) (f
Figure7: Accuracyof15LLMsbeforeandafterR
PopQA(Online),(b)MedMCQA(Local),(c)Med
Qwen2.5-0.5B L - l I a n m st a ru -3 c . t 2-1B Q -I w ns e t n ru 2 c .5 t -1.5B g - e I m ns m tru a- c 2 t -2b-i L t lama-3.2-3B Q -I w ns e t n ru 2 c .5 t -3B-I Y n i s - t 1 ru .5 c - t 6B-
PopQA(Local)
36.20 34.47 35.76 34.98 34.66 33.23 37.13
PopQA(Online)
45.80 42.33 44.53 43.08 41.77 41.32 45.02
MedMCQA(Local)
21.59 13.76 22.27 13.27 18.59 20.26 27.21
MedMCQA(Online)
33.86 31.64 32.96 32.08 35.82 35.24 37.39
NQ 23.99 22.01 22.59 20.65 24.21 23.24 26.92
WQ 25.87 20.19 23.96 17.17 25.87 20.17 30.09
TQA 29.23 29.29 30.39 25.13 37.22 29.23 36.67
Avg 30.94 27.67 30.35 26.62 31.16 28.95 34.35
15 20 25
Po
Figure8: PositiveGainRatesof
Qwen2.5-0.5B L - l I a n m st a ru -3 c . t 2-1B Q -I w ns e t n ru 2 c .5 t -1.5B g - e I m ns m tru a- c 2 t -2b-i L t lama-3.2-3B Q -I w ns e t n ru 2 c .5 t -3B-I Y n i s - t 1 ru .5 c - t 6B-
PopQA(Local)
22.95 21.36 16.22 28.57 20.78 21.11 19.34
PopQA(Online)
13.60 11.65 6.76 9.61 9.83 10.26 6.62
MedMCQA(Local)
55.73 62.37 50.85 70.18 57.01 45.11 64.17
MedMCQA(Online)
50.92 41.05 43.69 44.34 41.51 36.14 52.58
NQ 48.88 51.69 39.85 63.81 39.88 47.65 43.60
WQ 40.08 45.59 47.18 54.72 38.86 47.87 45.11
TQA 38.36 33.33 32.65 44.44 23.55 31.60 29.49
Avg 38.64 38.15 33.88 45.09 33.06 34.25 37.27
10 20 30
Negati
Figure9: NegativeInterferenceRates
1

b) (c)
f) (g)
RAGundervarioustasksandretrievalsettings: (a)
dMCQA(Online),(d)NQ,(e)WebQ,(f)TQA.
B-Cha Q t wen2.5-7B-I M ns i t n r i u s c tr t al-8B-I M ns e t t r a u - c L t- la 2 m 41 a 0 -3 Y .1 i- - 1 8 . B 5 - - I 9 n B st - r C u h c a t Q t wen2.5-14B Q -I w ns e t n ru 2 c .5 t -32B Q -I w ns e t n ru 2 c .5 t -72B L -I l n a s m tr a u - c 3 t .3-70B A - v I g nstruct
33.98 30.82 32.31 37.13 33.92 32.66 30.60 24.15 33.47
41.14 37.19 37.24 45.29 39.87 38.09 36.15 26.48 40.35
26.46 24.94 31.74 32.93 32.06 29.24 24.54 25.57 24.29
37.06 43.81 47.34 49.74 46.91 36.88 40.72 40.94 38.83
20.74 22.98 26.13 28.04 27.31 22.34 22.24 22.16 23.70
19.37 15.55 22.53 31.32 26.48 16.82 15.17 16.70 21.82
33.94 30.83 36.43 40.71 38.70 36.40 33.66 35.85 33.58
30.38 29.44 33.39 37.88 35.04 30.35 29.01 27.41 30.86
30 35 40 45
ositiveGainRate(%)
15candidateLLMsacrosstasks.
B-Cha Q t wen2.5-7B-I M ns i t n r i u s c tr t al-8B-I M ns e t t r a u - c L t- la 2 m 41 a 0 -3 Y .1 i- - 1 8 . B 5 - - I 9 n B st - r C u h c a t Q t wen2.5-14B Q -I w ns e t n ru 2 c .5 t -32B Q -I w ns e t n ru 2 c .5 t -72B L -I l n a s m tr a u - c 3 t .3-70B A - v I g nstruct
25.18 21.78 24.61 18.07 19.75 20.37 24.09 27.29 22.10
9.41 11.00 13.86 8.65 8.98 12.04 11.27 16.09 10.64
53.34 23.64 45.44 31.81 28.22 44.66 15.83 14.69 44.20
46.32 16.90 40.25 27.04 22.97 37.36 15.83 15.44 35.49
51.29 37.16 38.06 39.52 34.21 30.41 34.46 27.88 41.89
49.21 31.54 40.33 39.38 34.05 24.52 28.75 30.17 39.82
31.84 20.93 22.10 29.04 15.83 11.26 15.51 10.67 26.04
38.08 23.28 32.09 27.64 23.43 25.80 20.82 20.32 31.46
40 50 60 70
iveInterferenceRate(%)
sof15candidateLLMsacrosstasks.
17

=== Page 18 ===
C FullResultsonLatency-AwareRou
SettingDetails. BasedonLLMs’profiles,the15
followsinascendingorder: Qwen2.5-0.5B-Instruc
gemma-2-2b-it,Llama-3.2-3B-Instruct,Qwen2.5-3
Ministral-8B-Instruct-2410,Meta-Llama-3.1-8B-
Qwen2.5-32B-Instruct,Qwen2.5-72B-Instruct,L
selectedfromimmediatepredecessorsofthehigh
quantitativeanalysis,wediscretizedthethreshold
generatecompletehigh-precisionaccuracy–latenc
Results. Figure10illustratestheaccuracy–latenc
PopQA(Local/Online),NQ,andWebQ,whileTa
PeakAcc,andLatencyGap-to-Matchmetrics. RA
PeakAcc,withitsaccuracy–latencycurvemostl
performance-efficiencytrade-offsunderlow-laten
54
52
50
48
46
44
42
40
0 250 500 750 1000 1250 1500 1750 2000
LatencyperQuery(ms)
)%(ycaruccAgnitseT
PopQA(Local)
RAGRouter
70
60
50
40
30
0 250 500 750 1000 1250 1500 1750 2000
LatencyperQuery(ms)
)%(ycaruccAgnitseT
Qwen2.5-0.5B-Instruct Llama-3.2-3B-Instruct Ministral-8B-Instruc
Llama-3.2-1B-Instruct Qwen2.5-3B-Instruct Meta-Llama-3.1-8B-
Qwen2.5-1.5B-Instruct Yi-1.5-6B-Chat Yi-1.5-9B-Chat
gemma-2-2b-it Qwen2.5-7B-Instruct Qwen2.5-14B-Instru
NQ
RAGRouter
Figure10: Accuracy–latencycurveson
Table9: Area(%),PeakAccuracy(PA,%),andL
baselines using score-threshold-based routing on
Latency Gap-to-Match indicates failure to match
denotesmaximumroutinglatencybelow1s,exclud
PopQA(Local) PopQA(Online
Method
Area↑ PA↑ G(s)↑ Area↑ PA↑ G
RouterDC 44.58 47.04 0.02 49.80 51.85
MF - 46.30 - 49.96 52.22
GraphRouter - 47.41 -0.50 - 51.48
KNNRouter - 46.67 -0.06 - 52.22
RAGRouter 45.13 48.52 -0.48 50.13 52.59
1

uting
5candidatemodelsinSection5.4wererankedas
ct,Llama-3.2-1B-Instruct,Qwen2.5-1.5B-Instruct,
3B-Instruct,Yi-1.5-6B-Chat,Qwen2.5-7B-Instruct,
-Instruct,Yi-1.5-9B-Chat,Qwen2.5-14B-Instruct,
Llama-3.3-70B-Instruct. Substitutionmodelswere
hest-routing-scoremodelwithinthethreshold. For
parameterθover[0,1]withastepsizeof1e-4to
cytrade-offcurves.
cycurvesofRAGRouterandbaselinemethodson
Tables9presenttheirquantitativeresultsonArea,
AGRouterachievesthehighestscoresinAreaand
lysurpassingthebaselines,demonstratingstrong
ncyconstraints.
56
54
52
50
48
46
0 250 500 750 1000 1250 1500 1750 2000
LatencyperQuery(ms)
)%(ycaruccAgnitseT
PopQA(Online)
RAGRouter
80
70
60
50
40
30
0 250 500 750 1000 1250 1500 1750 2000
LatencyperQuery(ms)
)%(ycaruccAgnitseT
ct-2410 Qwen2.5-32B-Instruct Weighted KNNRouter
-Instruct Qwen2.5-72B-Instruct PromptLLM RouterDC
Llama-3.3-70B-Instruct Oracle GraphRouter
uct Random EmbedLLM RAGRouter(Ours)
WQ
RAGRouter
PopQA(Local/Online),NQandWebQ.
LatencyGap-to-Match(G,s)forRAGRouterand
n PopQA (Local/Online), NQ and WebQ. "-" in
h the best single-LLM performance; "-" in Area
dedfromcomparison;bolddenotesthebestresult.
e) NQ WQ
G(s)↑ Area↑ PA↑ G(s)↑ Area↑ PA↑ G(s)↑
- 33.10 42.92 - 46.39 49.17 -
-0.91 37.34 44.17 - - 50.42 1.66
- 35.68 38.35 0.02 45.67 48.18 0.01
- 41.13 50.63 0.72 - 50.42 1.72
0.15 43.63 55.83 1.13 - 58.33 1.84
18

=== Page 19 ===
D FullResultsofSensitivityAnalysis
Table 10 presents the impact of the dimensiona
capabilityembeddingsontestaccuracy,thebesta
Table10: Effect
PopQA MedMC
Dimension
Local Online Local O
128 48.52 52.22 71.11
256 48.52 53.33 70.37
384 49.26 52.59 70.37
512 48.89 52.59 70.37
768 48.52 52.59 71.48
1024 48.52 52.59 71.11
Table 11 presents the impact of the loss weight
observedatλ=2.
Table11: E
PopQA MedMCQA
λ
Local Online Local Onli
0.0 48.89 52.59 70.37 74.4
0.5 48.89 52.96 70.74 74.0
1.0 48.52 52.59 71.48 73.7
1.5 48.89 52.59 71.48 74.0
2.0 48.52 52.59 71.48 74.4
2.5 48.52 52.96 71.48 74.4
3.0 48.52 52.96 71.85 74.4
E CaseStudiesofFailuresandSucces
E.1 QuantitativeFailureCaseStudiesofRou
Weconductaquantitativeanalysisofcaseswhere
failtoselectthecorrectLLMorroutecorrectlyon
datasets. Inparticular,wecategorizenon-trivially
LLMsfail)intofourtypesasfollows:
• F1: Failuretoperceiveperformancedegr
performancedecreasesafterRAGisappl
• F2: Inherenttaskdifficulty,wherethemaj
confusion.
• F3: Overconfidentselectionofstronger
chosenbutstillfail,outsidethecondition
• F4: Otherfactors,suchasoutliersoramb
Table12: Type-wisefailurerates(asapercentag
Method F1(%) F2(%
KNNRouter 5.00 8.33
MF 5.51 8.21
RAGRouter 3.85 7.05
1

andAblationStudy
ality of LLM knowledge embeddings and RAG
averageaccuracyisobservedatdimension768.
tsofdimension.
CQA
NQ WQ TQA Avg
Online
73.70 54.17 53.33 87.92 63.00
73.33 55.42 55.83 88.75 63.65
72.59 56.25 53.75 89.17 63.43
73.33 55.42 56.25 89.17 63.72
74.44 56.67 56.67 90.83 64.46
73.70 55.42 51.67 90.00 63.29
λ on test accuracy, the best average accuracy is
Effectsofλ.
A
NQ WQ TQA Avg
ine
44 55.42 54.17 90.42 63.76
07 55.42 54.17 90.42 63.81
70 55.42 55.42 90.42 63.94
07 55.83 55.42 90.83 64.16
44 56.67 56.67 90.83 64.46
44 57.08 56.25 90.42 64.45
44 55.83 55.83 90.00 64.21
sses
utingDecisions
RAGRouterandnon-RAG-awareroutingmethods
nthePopQA(Local),MedMCQA(Local),andNQ
unsolvablefailures(i.e.,excludingcaseswhereall
radationcausedbyRAG(i.e.,caseswhereLLM’s
lied).
jorityofLLMs(e.g.,>80%)fail,leadingtorouting
rLLMsforcaseswherehigh-capacityLLMsare
nsofF1andF2.
biguousinputs.
geofallcases)andtheoverallfailurerate(FR).
%) F3(%) F4(%) FR(%)
1.28 1.54 16.15
1.41 1.79 16.92
1.41 0.64 12.95
19

=== Page 20 ===
AsshowninTable12,wecanobservethatcompare
KNNRouterandMF,ourRAGRouterachievesthe
failure rate. This suggests that RAGRouter more
introducedbyRAG.
We additionally analyze cases where RAGRout
succeed. Wefindthatsuchcasesarelessthan2%o
ofthesecasesfallintotheF2type,wheretheperfo
small.
E.2 QualitativeSuccessCaseStudiesofRAGR
WefurtherillustrateRAGRouter’sabilitytopercei
conditionsthroughthetwocasestudiesshownin
document contains both correct answer informat
RAGRouteridentifiesthatthedocumentprovides
Instruct and accordingly selects it as the respond
responseinthenon-RAGsettingduetoconfusion
correctstheanswerwhenthedocumentisincorp
exhibitsgreatersensitivitytodistractingcontent
leadstoreverseinterferenceandimpairsitsreaso
However,traditionalroutingstrategieswithoutR
modelperformanceandfailtoperceivethedocum
toMeta-Llama-3.1-8B-Instructandresultinginro
Table13: AcasefromPopQA(Online),demonstra
capabilityandknowledgeshift.
Query WhatisAgenthecapitalof?
Document Agen,locatedintheNouvelle
astheprefectureoftheLot-et-
positioningalongtheriverGa
southeastofBordeaux.Ithasa
buildingssuchasthetwelfth-c
includingtheMuséedesBeau
c¨apitaloftheprune,h¨ostinga
withapopulationof32,485in
toitshistoricalsignificancew
Groundtruth Lot-et-Garonne
Router RAGRouter
SelectedLLM Qwen2.5-14B-Instruct
Responsew/oRAG Lotdepartment.(✗)
Responsew/RAG Lot-et-Garonnedepartment.(
As shown in Table 14, the query itself is challe
Qwen2.5-72B-Instruct is able to provide a corre
However, when the retrieved document containin
Instruct, benefiting from its stronger capabilitie
successfullyidentifies“Poreotics”asthecorrecta
effectivelyutilizethedocumentandstillproducesa
isabletosensethedifferentialcapabilitiesofthe
androutesthequerytothemodelwithstrongerin
response. Bycontrast,non-RAG-awareroutingme
asRouterDC,failtocapturedynamicperformance
routingandresponsefailure. Thiscasefurtherhigh
capabilitydifferencesamongLLMsinretrieval-au
2

edwithnon-RAG-awareroutingmethods,including
elowestoverallfailurerateandthelowestF1-type
e correctly captures performance shifts in LLMs
ter fails while non-RAG-aware routing methods
ofalltheevaluatedcases. Notably,morethan50%
ormancegapbetweencandidateLLMsisinherently
Router
ivechangesinLLMknowledgestatesunderRAG
Table13andTable14. InTable13,theretrieved
tion and certain distracting content. In this case,
ssignificantperformancegainsforQwen2.5-14B-
der. Although this model produces an incorrect
nbetweentwoFrenchdepartments,itsuccessfully
porated. Incontrast,Meta-Llama-3.1-8B-Instruct
inthedocument,wheretheintroductionofRAG
oning,andthusitisnotprioritizedbyRAGRouter.
RAGawareness,suchasMF,relysolelyonstatic
ment-inducedperformanceshifts,ultimatelyrouting
outingfailureforthissample.
atingRAGRouter’sabilitytoperceiveLLMs’RAG
e-AquitaineregionofSouthwesternFrance,serves
-Garonnedepartment.Knownforitsgeographical
aronne,thecityliesapproximately135kilometers
arichculturalheritage,featuringvarioushistorical
centuryAgenCathedralandnumerousmuseums,
uxArts.Agenisalsocolloquiallyreferredtoasthe
apopularprunefestivaleveryAugust. Thetown,
2021,hasitsownRomanCatholicdiocese,adding
withintheregion.
MF
Meta-Llama-3.1-8B-Instruct
Lot-et-Garonnedepartment.(✓)
(✓) Theprunecapital.(✗)
enging, and neither Llama-3.3-70B-Instruct nor
ect answer without access to external documents.
ng key information is provided, Llama-3.3-70B-
es in information extraction and comprehension,
answer. Incontrast,Qwen2.5-72B-Instructfailsto
anincorrectresponse. Inthisscenario,RAGRouter
ecandidateLLMsinleveragingretrievedcontent
nformationextractionability,leadingtoacorrect
ethodsbasedonstaticmodelingassumptions,such
eshiftsinducedbyretrievalandresultinincorrect
hlightstheadvantageofRAGRouterinperceiving
ugmentedsettings.
20

=== Page 21 ===
Table14: AcasefromNQ,demonstratingRAGR
andknowledgeshift.
Query whoarethedancersinthelaz
Document tenthwastheonechosen.The
Duddy,producedbyNickTab
chimpanzee masks; it was re
presentedinasalonecontinu
singingandhangingoutinab
masksandMarsdressesinb
singswhathefeelstodoonad
typicalofaboy-band,
Groundtruth Poreotics
Router RAGRouter
SelectedLLM Llama-3.3-70B-Instruct
Responsew/oRAG BrunoMarsanddancers.(✗)
Responsew/RAG Poreoticsdancers.(✓)
F RoutingPerformanceunderCross-
ToevaluateRAGRouter’sgeneralizationacrossdo
• Setting1: TrainedonMedMCQA(Loca
newdatasetHotpotQA[54]forWikipedi
• Setting2: TrainedonNQthatcontainsre
(Local).
Table15: Testingaccuracy(%)ofRAGRouter
Setting1
Method
MedMCQA(Local)→H
KNNRouter 18.60
MF 20.20
RAGRouter 21.20
AsshowninTable15,RAGRouteroutperforms
5.56%intwocross-domainsettings,demonstratin
tagecanbeattributedtoRAGRouter’scontrastive
relativeRAGcapabilitydifferencesamongcandid
G RoutingPerformanceunderNoisy
We further partition the TriviaQA dataset into
noise—Golden Context, Relevant Noise, Irrelev
atetheroutingeffectivenessofRAGRouterunde
variousbaselinemethods. AsshowninTable16,
manceacrossallsubsets,outperformingboththe
demonstratingstrongrobustnesstodifferenttypes
Notably,ontheRelevantNoise,IrrelevantNoise,a
baselinesexhibitsignificantperformancegapscom
inhigh-noiseretrievalscenarios. Wehypothesizeth
retrieval,whichaffectsdifferentLLMsinheteroge
onfixedmodelrepresentationsandignoreRAG
strategiesundersuchconditions.
2

Router’sabilitytoperceiveLLMs’RAGcapability
zysongvideo?
eofficialvideowasdirectedbyMarsandCameron
briandDaraSiegel,andfeaturesPoreoticswearing
eleased on April 15, 2011. The whole video is
uousanduninterruptedshot,itbeginswithMars
bedroomwithfivedancers,theyallwearmonkey
blacksunglassesandaflannelshirt. WhileMars
dayoff,heandthemonkeysperformdancemoves
RouterDC
Qwen2.5-72B-Instruct
Fivedancers.(✗)
BrunoMarsandhisbackupdancers.(✗)
-DomainSettings
omains,wedesigntwocross-domainsettings:
al)thatfallsintomedicaldomain,buttestedona
ia-basedmulti-hopQA.
eal-worldsearchqueries,buttestedonMedMCQA
andbaselinesundertwocross-domainsettings.
Setting2
HotpotQA NQ→MedMCQA(Local)
63.33
58.89
68.89
non–RAG-awareroutingmethodsby1.00%and
ngstronggeneralization. Thisperformanceadvan-
elearningframework,whicheffectivelycaptures
dateLLMs,evenwhentransferredacrossdomains.
Retrieval
o four subsets with manually injected retrieval
vant Noise, and Counterfactual Noise—to evalu-
erdifferentnoiseconditions, incomparisonwith
RAGRouterconsistentlyachievesthebestperfor-
eOracleSingleBestandotherroutingstrategies,
sofretrievalnoise.
andCounterfactualNoisesubsets,non-RAG-aware
mparedtoRAGRouter,highlightingtheirlimitations
hatthisisduetoknowledgeshiftinducedbynoisy
eneousways. Asaresult,routingmethodsthatrely
capabilitiesstruggletoaccuratelymodelrouting
21

=== Page 22 ===
Table16: Testaccuracy(%)ofRAGRouterandbas
retrievalnoise,boldindicatesbestresults.
Method GoldenContext RelevantNois
OracleSingleBest 95.00 83.33
KNNRouter 96.67 78.33
GraphRouter 95.00 83.33
RouterDC 80.00 78.33
MF 95.00 76.67
RAGRouter 98.33 86.67
H RoutingPerformanceonSummariz
ToverifythatRAGRouterisalsoapplicabletoRA
experiments on WikiASP [16], a summarization
Specifically,WikiASPaimstogenerateaspect-bas
supportstheuseofretrievedevidence. WeadoptR
ForcontinuousperformancemetricssuchasROUG
byscore-basedprobabilisticsampling. Giveneach
labelitas“cananswer”(positive,label1)withpro
withprobability1 s. Thisstrategyisappliedin
−
trainingbasedoncontinuousperformancesignals
Table17: PerformanceofRAGRouterandbas
Method
Qwen2.5-0.5B-Ins
Qwen2.5-1.5B-Ins
Llama-3.2-3B-Inst
Qwen2.5-3B-Instr
Qwen2.5-7B-Instr
Llama-3.1-8B-Inst
KNNRouter
MF
RAGRouter
AsshowninTable17, RAGRouteroutperforms
5.3%. ThesefindingsdemonstratethatRAGRoute
2

selinesonTriviaQAsubsetswithdifferenttypesof
se IrrelevantNoise CounterfactualNoise Avg
90.00 83.33 87.92
83.33 86.67 86.25
90.00 83.33 87.92
73.33 78.33 77.50
80.00 80.00 82.92
90.00 88.33 90.83
zationTask
AGtasksbeyondquestionanswering,weconduct
n task commonly used in RAG-related research.
sedsummariesforWikipediaentitiesandnaturally
ROUGE-Lastheevaluationmetric.
GE-L,weextendRAGRouter’scontrastivelearning
hcandidateLLM’snormalizedscores [0,1],we
∈
obabilitys,and“cannotanswer”(negative,label0)
nourWikiASPexperiments,enablingcontrastive
ratherthandiscretecorrectnesslabels.
selinesonWikiASPmeasuredbyROUGE-L.
WikiASP
struct 0.1388
struct 0.1411
truct 0.1768
ruct 0.1528
ruct 0.1897
truct 0.1455
0.1881
0.1865
0.1981
non-RAG-awareroutingbaselinesbymorethan
ergeneralizeswelltodifferentRAGtasks.
22

Paper:Retrieval-Augmented_Generation_RAG_and_LLM_Integration.pdf
=== Page 1 ===
Retrieval-Augmented Gen
Integr
Büşra Tural Zeynep
Research & Development Center, Vakıf Research & Develop
Participation Partici
Istanbul, Turkey Istanbul,
busra.tural@vakifkatilim.com.tr zeynep.orpek@va
Abstract— Advances in Natural Language Processing (NLP)
have led to the emergence of complex structures such as Large
Language Models (LLM). LLMs are highly successful in
understanding the subtleties of language and processing context
by being trained on large datasets. However, the difficulties
encountered in Information Retrieval (IR) processes have
created an awareness that these models are not sufficient on
their own. Traditional IR methods have generally been
insufficient in understanding the complexity of natural language
in responding to specific queries and retrieving appropriate
information from documents or databases. Since this process is
based only on keywords, it cannot fully capture the semantic
meaning of the language. For this reason, it has been necessary
to go beyond traditional IR methods for more precise
information creation based on context and meaning. As a result
of these requirements, the Retrieval-Augmented Generation
(RAG) architecture has come to the fore. RAG offers the ability
to create richer and contextually meaningful answers to user
queries by integrating LLMs with information retrieval
processes. This architecture allows the language model to
instantly access external information sources; thus, it generates
more accurate and contextual responses armed with existing
information. These features of RAG provide appropriate
solutions to users' information-based demands by better
understanding the complexity of natural language. In this study,
it is emphasized that the integration of RAG architecture with
information retrieval systems and LLMs provides more
sensitive and accurate solutions in information-intensive tasks.
This study emphasizes that the RAG architecture's ability to
retrieve information by dynamically using the learnings
obtained from large datasets of LLMs strengthens applications
in the field of NLP.
Keywords— Retrieval-Augmented Generation, Large
Language Models, Information Retrieval, Natural Language
Processing
I. INTRODUCTION
In studies conducted on NLP, LLM has demonstrated
superior performance compared to other models. LLM has
achieved this success with large datasets on which it was
trained with larger parameters. Nevertheless, the lack of
resources, insufficient current data, and inadequate number of
datasets on which LLMs were trained may impede the model's
success from reaching the desired level. LLMs are constrained
to generating text based on the datasets from which they were
trained. Nevertheless, the quantity of data generated on a daily
basis, both in the physical world and in the digital domain, is
rapidly expanding. The inability of LLMs to adapt to an
increase and updates in data has an adverse effect on the
success of the model. The content, quantity, and caliber of the
979-8-3315-4010-4/24/$31.00 ©2024 IEEE
80354801.4202.13346SASI/9011.01
:IOD
|
EEEI
4202©
00.13$/42/4-0104-5133-8-979
|
)SASI(
seigolonhceT
tramS
ni
sehcaorppA
evitavonnI
no
muisopmyS
lanoitanretnI
ht8
4202
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

neration (RAG) and LLM
ration
p Örpek Zeynep Destan
pment Center, Vakıf Research & Development Center, Vakıf
ipation Participation
, Turkey Istanbul, Turkey
akifkatilim.com.tr zeynep.destan@vakifkatilim.com.tr
data utilized for model training become less significant over
time, leading to a decline in model performance. As a
preliminary solution, it was proposed that the models be
retrained with new data. Nevertheless, retraining the models
with each new dataset is not The studies conducted by Gerard
Salton in the 1970s constituted the foundation for
contemporary IR methodologies. Information retrieval (IR)
can be defined as the process of finding keywords within a
given text. The process entails identifying the desired message
and the sought information through a systematic examination
of the text. Salton used TF-IDF to calculate the frequency of
terms in the text or document they appear in [1]. This approach
has informed the development of new IR models, including
the Best Match 25 and Vector Space Model. The objective of
IR models is to identify the document that is most relevant to
a given query, and this is achieved through the use of
techniques such as pre-indexing documents and vectorizing
both documents and queries. Despite the efficacy of IR models
in identifying the most pertinent document on a given subject,
they have not yet reached the desired level of success due to
the intricate structure of natural language. It has been
demonstrated that IR models are unable to meet the requisite
demands in isolation.
The efficacy of generative language models in generating
text has declined over time due to the static and limited nature
of the datasets. In response to these challenges, the Facebook
artificial intelligence research team unveiled an architectural
framework, designated as the RAG model, in 2020. The RAG
model is based on the integration of text generation
capabilities inherent to generative artificial intelligence
models with the ability of IR models to identify the most
pertinent text. The RAG architecture represents an approach
that combines generative models with information retrieval
models. The advantageous aspects of the two language models
have been combined for a common purpose.
The primary characteristic of the RAG architecture is its
capacity to draw upon external sources of information in real
time, extending beyond the confines of the dataset utilized
during the text generation process. In this manner, the model
is distinct from traditional, large language models that are
based on static datasets, as it is capable of generating texts that
are supported by dynamic, up-to-date information. The RAG
model employs IR systems to identify the most pertinent
documents for a given query and then synthesizes these
documents to generate accurate and comprehensive responses.
This approach enables the model to obtain more dynamic and
precise results by leveraging data that emerges after the
training period. Consequently, the RAG model seeks to
ed on January 29,2026 at 14:42:40 UTC from IEEE Xplore. Restrictions apply.

=== Page 2 ===
address the limitation of LLMs being dependent on a static
database.
The RAG architecture has enabled LLMs to ingest not
only the dataset they are trained on, but also larger and more
current data sources, including the internet and archives.
Large language models, such as GPT (Generative Pre-trained
Transformer) and Bert (Bidirectional Encoder
Representations from Transformers), have been supported by
IR models, including VSM (Vector Space Model) and DRM
(Dense Retrieval Model). This has enabled them to produce
more meaningful and accurate results. The RAG architecture
has enhanced the capabilities of LLM by integrating external
information sources. The RAG architecture is predominantly
employed in the context of complex language models. In
particular, the RAG architecture is commonly employed in
question-answering models to scan documents and identify
the pertinent subject matter, thereby facilitating the generation
of responses. In addition, the RAG architecture is employed
in the generation of meaningful text, such as blog posts and
news articles, due to the enhanced efficacy of generative
language models when supported by information retrieval
models. The structure of the RAG architecture is also
employed in dialogic applications, such as chatbots, as it
enables the establishment of a continuous dialogue with the
user and the scanning of the internet or large archives for up-
to-date answers. As can be observed, RAG architecture is a
methodology employed primarily in applications that
necessitate current and sophisticated information.
II. RELATED WORKS
NLP is a subfield of machine learning and deep learning
that deals with the processing and interpretation of language.
Transformer-based models (e.g. GPT, BERT) have directly
influenced the development of architectures such as RAG.
These models have formed the basis for systems that can
understand and create human language [2].
Transformer-based models such as BERT and GPT can
capture the meaning of language in more depth by being
trained on large datasets. While BERT can effectively capture
the context of words in text thanks to its bidirectional
approach, models such as GPT have become more effective in
text generation by using forward language modeling
(unidirectional). These models have strengthened the
language generation and understanding capabilities that form
the basis of systems such as RAG [3] [4].
As the complexity of language models increases, the need
for knowledge retrieval and access systems to improve the
accuracy of models in knowledge-intensive tasks has also
increased. Open-Domain Question Answering (ODQA)
systems have been developed to meet this need. These systems
scan large document collections and retrieve relevant
documents to increase the accuracy of the answer created by
the language model when answering a user question. In such
systems, instead of directly creating knowledge, language
models first retrieve the necessary documents and then create
an answer using these documents. Important steps have been
taken in the development of ODQA systems. First-generation
systems, such as DrQA, access large knowledge bases such as
Wikipedia, find text fragments that are relevant to a particular
question and create an answer using this information [5] .
RAG-like architectures are based on information retrieval
systems. These systems aim to retrieve the most relevant
information from large datasets (documents, articles, web
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

pages, etc.) based on a specific query. Traditional information
retrieval systems (such as search engines) are based on
keywords. However, with the development of artificial
intelligence and deep learning methods, more sophisticated
systems have emerged [6].
Lewis et al. (2020) introduced their pioneering work, the
RAG architecture, to develop a model that improves the
performance of LLMs using external data sources in
knowledge-intensive natural language processing tasks. This
work demonstrates how RAG creates more accurate results by
accessing external databases without relying solely on the
parameters of language models [7].
In this context, the RAG architecture emerges as a
remarkable innovation in natural language processing. This
architecture has the capacity to provide more contextual and
meaningful answers to users by integrating LLM and
information retrieval processes. When the literature is
examined, various studies are conducted to understand the
effectiveness and application areas of RAG.
RAG architecture integrates with LLM’s information
retrieval systems. Before answering a question, these systems
retrieve relevant documents and use this information to create
answers. This provides a great advantage, especially in
providing accurate and up-to-date information. RAG is an
advanced system with models such as Google’s T5 and offers
more efficient information creation processes [8].
While RAG combines knowledge retrieval and generation,
similar approaches have also been developed. For example,
the FiD (Fusion-in-Decoder) architecture combines retrieved
documents to create an answer. RePAQ (Retrieval-enhanced
Pretrained Autoregressive Query) offers a more compact
structure, allowing faster knowledge retrieval and response
generation [8] [9].
In his study, Reimers (2019) developed sentence-level
embedding techniques to make retrieval-based systems such
as RAG work more efficiently. This method is frequently used
in the retrieval phase of RAG [10].
The ColBERT system developed by Khattab and Zaharia
(2020) has made the document retrieval process more efficient
with a BERT-based bidirectional attention mechanism. This
model has greatly contributed to the development of systems
where retrieval and generative models work together, such as
RAG [11].
Nogueira et al. (2020) studied pre-trained sequence-to-
sequence models for ranking retrieved documents. This work
plays a critical role in the ranking and evaluation processes of
retrieved information in RAG systems [12].
Guu, K. et al. (2020) Analyzing the effects of RAG on
information retrieval and language modeling, this study
provides important findings on real-time information
integration [13].
In their work, Karpukhin et al. (2020) examine the
transitive document retrieval methods based on the RAG
architecture [14].
In their work, Xiong et al. (2021) examine the effects of
transformer-based models on information sorting and discuss
its relationship with the RAG architecture [15].
Gao et al. (2023) examine the development of the RAG
paradigm in their work, addressing the Naive RAG, Advanced
ed on January 29,2026 at 14:42:40 UTC from IEEE Xplore. Restrictions apply.

=== Page 3 ===
RAG, and Modular RAG models, and analyzing in detail the
three core components of RAG systems: retrieval, generation,
and augmentation techniques. They also introduce current
evaluation frameworks and benchmarks, highlighting the
current challenges and future research areas of RAG [16].
In their study, Fan et al. (2024) systematically examine the
ability of LLMs integrated with RAG to improve content
quality by leveraging external knowledge sources. The
research reviews the existing RAG and LLM literature from
three main technical perspectives, evaluating the advantages
offered by RAG to overcome the models' inherent knowledge
constraints and ensure knowledge timeliness; it also discusses
current challenges and potential directions for future research
[17].
Salemi et al. (2024) propose a new method for evaluating
RAG systems, called eRAG, introducing an approach where
each retrieved document is individually used by the large
language model. eRAG provides more accurate evaluations at
the document level while providing higher correlation and
significantly less computational resource consumption
compared to traditional methods [18].
There are a significant number of studies in the literature
on the applications and impacts of RAG architecture. These
studies deeply examine the advantages and solutions provided
by the integration of RAG with knowledge retrieval processes
in natural language processing tasks. For example, the
research conducted by Lewis et al. (2020) reveals how the
RAG architecture creates more effective answers in
knowledge-intensive NLP tasks [7].
Studies conducted on the RAG architecture in the
literature reveal the potential and effectiveness of this system
in information-intensive natural language processing tasks.
RAG's ability to create more contextual and accurate answers
by combining large language models with information
retrieval processes is supported by various studies. In the
future, further development of this architecture will contribute
to the further strengthening of applications in the field of
information retrieval systems and natural language
processing. Therefore, the RAG architecture stands out as an
important innovation in terms of accelerating and increasing
the accuracy of information-based processes, and research in
this area will enable the development of more reliable and
effective systems. Such studies are critical to understanding
the interaction between information retrieval and language
creation in RAG, and research conducted in this context plays
an important role in the future development of the field.
III. LANGUAGE MODELS AND RAG ARCHITECTURE
The intense interest in NLP and the artificial intelligence
ecosystem, which includes a large number of developers, has
provided the basis for the spread of new technologies and
architectures. Language models trained on large datasets stand
out with their human-like abilities, such as creating text,
editing, answering questions, summarizing, and translating by
learning the mathematical structure of the language.
Language models work by probabilistically modeling the
distribution of words in sentences and their use together, and
thus can predict the next word.
Language models are classified according to the dataset
they are trained on and the parameter size used. While LLMs
contain more than 100 million parameters, small language
models (SLM) contain less than 100 million parameters.
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

LLMs face difficulties in accessing accurate and up-to-
date information during the real-time usage processes of the
model when they are trained on huge datasets. Language
models can create answers based on the dataset on which they
are trained. This requires the model training process to be run
again after a data update in the dataset. This process is
insufficient in terms of time benefit and cost when the LLM
training process and costs are taken into consideration.
The training process of LLMs raises serious concerns
about sustainability and a green environment. The equipment
used in model training are devices with high energy
requirements and operate with high energy consumption for
long periods. This situation creates a significant
environmental impact in terms of sustainability and causes
excessive consumption of energy resources. The necessity of
retraining the model with each new data update turns into a
process that harms the environment and has negative effects
on energy resources over time. Alternative and more
sustainable solutions have been sought for this problem.
RAG architecture has become widespread as a flexible and
robust architecture against changes in the information in the
dataset that is effective in the IR processes of LLMs. An
update on the data in the dataset provides access to up-to-date
data in real-time without the need to retrain the language
model. RAG architecture prevents the model from being
dependent on fixed data with the IR layer and enables the
creation of responses based on dynamic data. With these
features, RAG architecture has a flexible and robust structure
against changing data.
Fig. 1. RAG Architecture.
RAG architecture consists of Retrieval Document Search,
Augmentation, and Generation stages. These stages are as
follows.
• Retrieval Document Search: This stage includes
the process of finding and retrieving documents
related to the question asked by the user from the
dataset.
• Augmentation: This stage includes the processes
of making sense of the documents returned
regarding the question, increasing the value of
the data by adding additional data and documents
and updating the data. In this way, the answer is
created by adding not only the trained field but
also the current data.
• Generation: This stage covers the process of
producing the final answer to the question asked
by the user. Answers to the relevant question are
ed on January 29,2026 at 14:42:40 UTC from IEEE Xplore. Restrictions apply.

=== Page 4 ===
created through data collected from different
channels.
The RAG architecture is used as the basis for many
architectures customized to different needs and domains. Its
main variations are Standard RAG, Corrective RAG,
Speculative RAG, Fusion RAG, Agentic RAG, Self RAG,
Graph RAG, Modular RAG, and RadioRAG.
Standard RAG, the basis for other variations of this
architecture, is highly successful in question-and-answer
systems, and in summarizing large texts. Despite its
widespread use, it may fail to retrieve data related to the user's
question in the IR step. This may cause the answer created to
be incorrect or insufficient.
Corrective RAG proposes to add an additional verification
layer to check and correct the accuracy of the generated
answer after the IR and answer generation stages. In this way,
in cases where the answer created by standard RAG is
incomplete or insufficient, it can create more successful
answers because it includes a re-improvement and answer
generation phase [19].
Speculative RAG offers a solution to provide correct
answers in cases where the returned data is insufficient. It
involves the process of the model predicting the answer based
on the information in the returned data and other information
in the language model. However, the answer created as a result
of these studies may not be correct [20].
Fusion RAG is an architecture that allows a holistic
response to be created from data obtained by collecting data
from different sources. In particular, it aims to create
successful outputs by combining relevant data in cases where
data from different data sources contradict each other.
However, one of the biggest challenges to be encountered is
when there is a lot of data that contradict each other, in which
case it becomes difficult to ensure accuracy in the response
[21].
Agentic RAG enables the model to decide independently
which type of data it needs. In this way, it adds decision-
making ability to the model, allowing the model to be used by
prioritizing the most appropriate data in case of different types
of data. However, an error or failure in the prediction
mechanism can directly affect the answer to be created and
may cause incorrect outputs to be created [22].
Self RAG stands out with its feature of evaluating the
model's performance. The model contributes to the model's
consistency with the dataset by evaluating the quality of the
answer it creates while producing an answer to the relevant
question. However, since the model's performance evaluation
depends on the accuracy of the data, it will create wrong
answers if the data is incorrect, incomplete, or insufficient
[23].
Graph RAG enables understanding and organizing
information through relationships by incorporating graph-
based data structures into IR processes. It is suitable for use in
areas that require relational understanding, such as biological
research in understanding the relationships between genes,
proteins, and diseases. However, outdated, incorrect, or
incomplete graphs affect the accuracy of the answer to be
created. Therefore, graph structures must be kept correct and
up-to-date [24].
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

Modular RAG is an approach to optimizing all
components separately and independently. The process of
separating them into modules makes the system more flexible
and customizable. In this way, improvements and fine-tuning
can be done only to a certain module. However, it can be
difficult to ensure that different modules work seamlessly and
are fully compatible with each other [25].
Radio RAG was developed to integrate real-time and
radiology information into LLM. It was tested using a dataset
called RadioQA and strengthened LLM’s ability to diagnose
diseases with real-time radiological information. According to
the results of the tests conducted, it was observed that some
models increased the diagnostic accuracy by up to 54%. The
test results demonstrate the potential of Radio RAG to
improve and change disease diagnosis processes [26].
Each variation of the RAG architecture has been shaped
according to different needs and challenges in different areas.
The standard RAG is the basis for most architectures. On the
other hand, widely used and known variations such as
Corrective RAG, Speculative RAG, Fusion RAG, Agentic
RAG, Self RAG, Graph RAG, Modular RAG, and RadioRAG
have been developed to provide architecture suitable for the
main requirements. As these models develop and prove their
success more accurately and contextually, their developer
base and usage areas will increase day by day.
IV. RESULT
The RAG architecture is an advanced solution that
overcomes the current limitations of LLMs, offering
significant advantages in effects-based missions. The
shortcomings of LLMs, such as the inability to pass training
data and the inaccessibility of external data sources, are
effectively addressed by RAG's IR data forwarding. Thanks to
this architecture's ability to pull information from external
data sources, more contextual, up-to-date, and highly accurate
solutions can be created. Especially for applications with large
datasets and updated data, RAG revolutionizes the field of
NLP by providing more efficient and flexible partitioning of
language models. RAG has provided a solution to LLM's
concerns about maintainability. The model of ensuring that
datasets are updated has been an alternative solution to the
problem of re-training process. Architectures like RAG are
everywhere, with rich contributions from the accessibility of
IR and language partitions, sustainability information, and
smarter and more dynamic systems.
ACKNOWLEDGMENT
This research was made possible thanks to the support and
infrastructure provided by Vakıf Participation Bank R&D
Center. The valuable contributions of Vakıf Participation
Bank R&D Center were effective in the successful completion
of this study. We would like to express our sincere gratitude
to Vakıf Participation Bank R&D Center for their support.
REFERENCES
[1] K. Spärck Jones, " IDF term weighting and IR research lessons,"
Journal of documentation, pp. 60(5), 521-523., 2004.
[2] A. Vaswani, "Attention is all you need.," Advances in Neural
Information Processing Systems, 2017.
[3] J. Devlin, "Bert: Pre-training of deep bidirectional transformers for
language understanding," arXiv preprint arXiv:1810.04805., 2018.
[4] J. &. R. S. Howard, "Universal language model fine-tuning for text
classification," arXiv preprint arXiv:1801.06146., 2018.
ed on January 29,2026 at 14:42:40 UTC from IEEE Xplore. Restrictions apply.

=== Page 5 ===
[5] D. Chen, "Reading Wikipedia to answer open‐domain questions.," [20] Z. W. Z. L. L. Z. H. S. M. S. P. V. .. &. P. T. Wang, "Speculative rag:
arXiv preprint arXiv:1704.00051., 2017. Enhancing retrieval augmented generation through drafting.," arXiv
preprint arXiv:2407.08223., 2024.
[6] C. D. Manning, "Introduction to information retrieval.," 2008.
[21] Z. Rackauckas, "Rag-fusion: a new take on retrieval-augmented
[7] P. P. E. P. A. P. F. K. V. G. N. .. &. K. D. Lewis, "Retrieval-augmented
generation.," arXiv preprint arXiv:2402.03367., 2024.
generation for knowledge-intensive nlp tasks," Advances in Neural
Information Processing Systems, pp. 33, 9459-9474., 2020. [22] C. S. S. S. &. R. V. Ravuru, "Agentic Retrieval-Augmented
Generation for Time Series Analysis.," arXiv preprint
[8] G. &. G. E. Izacard, "Leveraging passage retrieval with generative
arXiv:2408.14484., 2024.
models for open domain question answering," arXiv preprint
arXiv:2007.01282., 2020. [23] A. W. Z. W. Y. S. A. &. H. H. Asai, "Self-rag: Learning to retrieve,
generate, and critique through self-reflection.," arXiv preprint
[9] P. W. Y. L. L. M. P. K. H. P. A. .. &. R. S. Lewis, "PAQ: 65 million
arXiv:2310.11511., 2023.
probably-asked questions and what you can do with them,"
Transactions of the Association for Computational Linguistics, pp. 9, [24] B. Z. Y. L. Y. B. X. S. H. H. C. .. &. T. S. Peng, "Graph retrieval-
1098-1115., 2021. augmented generation: A survey.," arXiv preprint arXiv:2408.08921.,
2024.
[10] N. (. Reimers, "Sentence-BERT: Sentence Embeddings using Siamese
BERT-Networks.," arXiv preprint arXiv:1908.10084., 2019. [25] Y. X. Y. W. M. &. W. H. Gao, "Modular RAG: Transforming RAG
Systems into LEGO-like Reconfigurable Frameworks.," arXiv
[11] O. &. Z. M. Khattab, "Colbert: Efficient and effective passage search
preprint arXiv:2407.21059., 2024.
via contextualized late interaction over bert.," In Proceedings of the
43rd International ACM SIGIR conference on research and [26] S. T. L. M. B. K. S. R. F. D. K. C. .. &. T. D. Arasteh, " RadioRAG:
development in Information, pp. Retrieval (pp.39-48), 2020, July. Factual Large Language Models for Enhanced Diagnostics in
Radiology Using Dynamic Retrieval Augmented Generation.," arXiv
[12] F. N. R. &. L. R. (. .. B. 2. R. G. B. ,. P. .. S. Souza, "BERTimbau:
preprint arXiv:2407.15, 2024.
pretrained BERT models for Brazilian Portuguese. In Intelligent
Systems: 9th Brazilian Conference,," Springer International
Publishing., pp. Part I 9 (pp. 403-417), October 20–23, 2020.
[13] K. L. K. T. Z. P. P. &. C. M. Guu, "Retrieval augmented language
model pre-training.," In International conference on machine
learning, vol. PMLR, pp. pp. 3929-3938, 2020, November.
[14] V. O. B. M. S. L. P. W. L. E. S. .. &. Y. W. T. Karpukhin, "Dense
passage retrieval for open-domain question answering.," arXiv
preprint arXiv:2004.04906., 2020.
[15] A. N. R. &. L. J. Yates, "Pretrained transformers for text ranking:
BERT and beyond.," In Proceedings of the 14th ACM International
Conference on web search and data mining, pp. pp. 1154-1156, 2021,
March.
[16] Y. X. Y. G. X. J. K. P. J. B. Y. .. &. W. H. Gao, "Retrieval-augmented
generation for large language models: A survey," arXiv preprint
arXiv:2312.10997., 2023.
[17] W. D. Y. N. L. W. S. L. H. Y. D. .. &. L. Q. Fan, "A survey on rag
meeting llms: Towards retrieval-augmented large language models.,"
In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining, pp. pp. 6491-6501, 2024, August.
[18] A. &. Z. H. Salemi, "Evaluating retrieval quality in retrieval-
augmented generation," In Proceedings of the 47th International ACM
SIGIR Conference on Research and Development in Information
Retrieval , pp. pp. 2395-2400, 2024, July.
[19] S. Q. G. J. C. Z. Y. &. L. Z. H. Yan, "Corrective retrieval augmented
generation.," arXiv preprint arXiv:2401.15884., 2024.
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloaded on January 29,2026 at 14:42:40 UTC from IEEE Xplore. Restrictions apply.

Paper:Retrieval_Augmented_Generation_RAG_using_LLMs.pdf
=== Page 1 ===
2025 Annual International Conference on Data Science, M
Retrieval Augmented Gene
Madiha Vahaj Syed Mehran Raza
Computer Science and Computer Science and
Engineering Engineering
Amity School of Engineering Amity School of Engineering
and Technology and Technology
Noida, India Noida, India
madiha.vahaj@gmail.com razamehran451@gmail.com
Abstract— Large Language Models (LLMs) have transformed
AI with their ability to generate human-like text, but challenges
such as hallucinations, outdated knowledge, and contextual
inaccuracies persist. Retrieval-Augmented Generation (RAG)
systems address these limitations by integrating real-time
information retrieval with LLMs, enhancing the accuracy,
relevance, and trustworthiness of generated content. This research
aims to develop and evaluate RAG systems to improve the
reliability of AI-generated outputs. To assess the impact of RAG,
a comparative analysis is conducted on LLMs, including Llama,
Mistral, Falcon, and T5. Performance has been evaluated by
comparing results on similar tasks with and without RAG,
focusing on metrics such as accuracy, contextual understanding,
and domain-specific relevance. The findings demonstrate that
RAG significantly enhances content generation, resolving critical
challenges associated with LLMs. This study underlines the
potential of retrieval-augmented techniques to improve the
reliability and applicability of AI-generated content across various
domains.
Keywords—Retrieval Augmented Generation, LLM, Llama,
Falcon, Mistral, T5
I. INTRODUCTION
A. Background
Large language models (LLMs) have transformed natural
language processing (NLP), allowing artificial intelligence
systems to produce human-like text, summarize data, and
respond to inquiries spanning multiple domains [8]. These
models—Llama, Mistral, Falcon, and T5—rely on pre-trained
information gleaned from enormous databases. Though they
have great capacity, LLMs also face major difficulties like
hallucinations—where the models generate false or erroneous
information—and an incapacity to access real-time or domain-
specific knowledge [9]. Dealing with these constraints becomes
essential as artificial intelligence applications call for precise,
current, context-aware outputs.
Three basic stages define the RAG: retrieval, in response to user
inquiries, where pertinent information is extracted from outside
sources; augmentation, in which the retrieved data is fluidly
combined with the input prompt; and generation, in which an
LLM processes augmented input to generate the final response.
Emerging as a potential solution to these problems is Retrieval-
Augmented Generation (RAG) [6]. RAG systems can provide
979-8-3315-4216-0/25/$31.00 ©2025 IEEE
29677211.5202.95346BMDCIA/9011.01
:IOD
|
EEEI
5202©
00.13$/52/0-6124-5133-8-979
|
)BMDCIA(
ygolonhceT
niahckcolB
dna
gninraeL
enihcaM
,ecneicS
ataD
no
ecnerefnoC
lanoitanretnI
launnA
5202
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

Machine Learning and Blockchain Technology (AICDMB)
eration (RAG) using LLMs
Vibha Nehra
Computer Science and
Engineering
Amity School of Engineering
and Technology
Noida, India
vnehra@amity.edu
more accurate, relevant, anchored in factual data replies by
combining external knowledge retrieval techniques with the
generating capacity of LLMs. Particularly useful for
applications needing real-time or domain-specific knowledge,
this hybrid method combines the benefits of information
retrieval systems and LLMs [6][7].
B. Problem Statement
This work intends to solve these problems by means of a
Retrieval-Augmented Generation (RAG) system that improves
LLM results by external knowledge retrieval. This work also
intends to evaluate, with and without RAG integration, the
performance of several LLMs: Llama, Mistral, Falcon, and T5.
Evaluation of their respective strengths and shortcomings in
producing accurate, context-aware, and domain-relevant
replies, therefore illustrating the additional benefit of retrieval-
based augmentation.
C. Project Objectives
The primary objective of this project is to design, develop, and
evaluate a Retrieval-Augmented Generation (RAG) system that
improves the accuracy, relevance, and contextual
understanding of AI-generated content. Specifically, the project
aims to:
• Implement an efficient retrieval mechanism for
extracting relevant information from external
resources.
• Develop methods to seamlessly integrate retrieved
data with LLM inputs.
• Compare the performance of various LLMs (Llama,
Mistral, Falcon, and T5) with and without RAG
enhancement.
• Evaluate the effectiveness of the RAG system across
different domains and task types using metrics such as
accuracy, contextual relevance, and factual reliability.
D. Scope
This research sets the foundation for developing Retrieval-
Augmented Generation (RAG) systems that enhance the
capabilities of Large Language Models (LLMs). Future
improvements to this model can focus on several key areas:
• Advanced Retrieval Mechanisms: Developing more
sophisticated algorithms to improve the efficiency and
ed on January 29,2026 at 15:09:24 UTC from IEEE Xplore. Restrictions apply.

=== Page 2 ===
accuracy of retrieving relevant information, including
handling ambiguous or incomplete queries.
• Multi-Modal Integration: Extending the model to
handle multi-modal data, enabling it to process and
integrate information from text, images, audio, and
video sources.
• Robust Evaluation Metrics: Introducing more
comprehensive evaluation frameworks that account
for domain-specific nuances, robustness to noisy data,
and long-context comprehension.
• Human-in-the-Loop Systems: Incorporating
mechanisms for user feedback to refine and validate
model outputs, ensuring greater reliability and
trustworthiness.
These advancements will further enhance the model’s ability to
deliver accurate, context-aware, and domain-specific
responses, solidifying its applicability across diverse industries
and use cases.
II. LITERATURE REVIEW
Table 1: Literature Review
S.No Summary Gaps
[1] LLMs face challenges in Lacks analysis on retrieval
arithmetic, common sense, and algorithm limitations, LLM
ethics, but skilled prompting, selection and fine-tuning,
quality control, and integration augmentation strategies,
with live data and domain- handling ambiguous prompts,
specific models offer potential and comprehensive evaluation
for their future development. metrics.
[2] Retrieval-based LMs offer Lacks evaluation on domain-
superior performance, specific performance,
adaptability, and potential scalability, LLM selection, and
across diverse domains, with a handling ambiguous queries,
focus on practical applications which are crucial for real-
and future directions. world applications.
[3] RAG-Ex, framework for LLMs, Lacks discussion on retrieval
shows high accuracy and user mechanism efficiency, data
agreement, though further integration with LLMs, fine-
optimizations are needed for tuning, comparison with
efficiency and robustness. traditional LLMs, and
addressing common LLM
limitations, all of which are
key aspects of your project.
[4] Retrieval-augmented LLMs Lacks evaluation on system
enhance query-answering performance (accuracy,
systems by addressing multi- fluency, relevance), LLM
modal contexts, leveraging selection, handling ambiguous
contrastive learning, and queries, and comparison with
offering flexibility and state-of-the-art systems.
scalability through advanced
indexing.
[5] Introduces the Retrieval- Over looks varied retrieval
Augmented Generation methods, real-world scenarios,
Benchmark (RGB) to evaluate smaller models, temporal
LLMs, finding that while RAG knowledge, human-in-the-loop
improves accuracy, challenges feedback, and scalability
remain in rejecting irrelevant concerns in RAG systems.
information and integrating
data.
[6] RAG enhances LLMs by Research on RAG lacks
integrating external data for comprehensive synthesis,
more accurate answers, evolving effective evaluation methods,
through three paradigms, and robustness to noise, integration
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

requiring future improvements with long contexts, hybrid
in robustness, fine-tuning, approaches, and exploration of
multi-modal data handling, and scaling laws, along with
evaluation metrics. practical engineering
challenges.
As highlighted in Table I, the field of Retrieval-Augmented
Generation (RAG) represents a significant advancement in
overcoming the critical limitations of large language models
(LLMs). LLMs face challenges in arithmetic precision,
common sense reasoning, and ethical considerations, often
struggling with simple computational tasks and maintaining
consistent logic across domains [1].
Retrieval-based language models offer a promising solution,
outperforming parametric LLMs with fewer parameters. These
models enhance performance and flexibility in updating
knowledge, adapting effectively to tasks like dialogue systems,
semantic parsing, and machine translation, with potential in
multilingual, multimodal, and code retrieval domains [2].
Frameworks like RAG-Ex [3] enable approximate explanations
correlated with downstream performance, improving response
accuracy and transparency.
However, significant gaps persist in retrieval algorithm
limitations, handling out-of-domain queries, and integrating
noisy data [4]. Integrating retrieved information with LLM
inputs remains complex, with limited studies on optimal
strategies [3]. RAG systems also face challenges in rejecting
irrelevant information, integrating diverse data sources,
managing factual inconsistencies, and showing sensitivity to
query complexity [5].
The research community calls for better evaluation metrics,
robust retrieval mechanisms, and improved information
integration techniques [6]. Future directions include developing
efficient retrieval algorithms, seamless integration methods,
and comprehensive evaluation frameworks to create
contextually aware and reliable language systems that address
current LLM limitations [1][6].
III. METHODOLOGY
This section outlines the methodology for developing and
evaluating the Retrieval-Augmented Generation (RAG)
system, detailing the preparatory phase, model implementation,
and comparative analysis framework.
A. Preparatory Phase
1) Comprehensive Literature Review: A thorough
literature review on Retrieval-Augmented Generation (RAG)
and Large Language Models (LLMs) was conducted. This
review encompassed recent advances, challenges, and existing
approaches in the field, informing the design and
implementation of the RAG system. Key themes included the
limitations of current LLMs, the role of retrieval mechanisms,
and successful integration strategies.
2) Selection of LLM Architectures: Four state-of-the-art
LLM architectures, specifically Mistral [10][11], Falcon [12],
Llama[11], and T5 [13], were selected for comparative
ed on January 29,2026 at 15:09:24 UTC from IEEE Xplore. Restrictions apply.

=== Page 3 ===
analysis. The selection criteria focused on availability,
performance benchmarks, and adaptability to RAG integration.
B. Model Implementation
1) Development of Non-RAG Baseline Models: Non-RAG
baseline models were developed to establish a performance
benchmark for each selected LLM.
2) Integration of RAG Techniques: RAG techniques were
integrated with each selected language model. This involved
augmenting the model’s input with the retrieved information
before generating the output, ensuring that the LLM could
leverage real-time data or domain-specific knowledge to
improve the quality of responses.
C. Comparative Analysis Framework
1) Definition of Performance Metrics: Comprehensive
performance metrics were established to evaluate the outputs of
both RAG-augmented and non-RAG models. Metrics included:
a) Accuracy: Assessment of factual correctness using
precision and recall.
b) Contextual Relevance: Measured through metrics
such as BLEU, ROUGE, or human evaluations.
2) Creation of Diverse Questions: A diverse set of
questions was created across multiple domains, including
general knowledge, technical subjects, and industry-related
queries. This diverse question set benchmarked the models'
performance in generating accurate and contextually relevant
responses under varying conditions.
3) Systematic Comparative Analysis: A systematic
approach was adopted to compare the performance of RAG-
enhanced models with their non-RAG counterparts. Each
LLM's responses to the same questions were analyzed to
evaluate improvements achieved through RAG integration.
IV. RESULT AND DISCUSSIONS
This study compared the performance of four large language
models (LLMs)—Mistral, Llama, T5, and Falcon—were
evaluated in this work on a set of domain-specific questions.
Each model was assigned three questions and given the
identical reference text. Standard metrics for assessing
language generation tasks, rouge-l (roule) and bleu scores were
used to evaluate the models' performances. The aim was to
evaluate their capacity to provide three separate questions
covering many knowledge domains accurate and fluid replies.
Three domain-specific questions were chosen to test the
models' performance:
• Q1: "What are the main features of Alzheimer's
disease, and what structures are involved?" (Medical
Domain)
• Q2: "What are the time complexities of the Quick Sort
algorithm?" (Computer Science Domain)
• Q3: "What are the qualifications required to become
the President of India?" (Civic Knowledge Domain)
Two widely used metrics were employed to evaluate the quality
of the generated responses:
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

• RouLE ( ROUGE-L Recall-Oriented Understudy
of Gisting Evaluation): It measures the longest
common word sequence, computed by the Longest
Common Subsequence (LCS) algorithm [15]. This
metric assesses the relative length of the model-
generated output, with higher values indicating better
performance in estimating the expected output length
relative to the input. RouLE is measured for multiple
queries (Q1, Q2, and Q3).
• BLEU (Bilingual Evaluation Understudy): BLEU is
a widely used metric in machine translation and text
generation, which evaluates the n-gram precision
between the generated output and reference texts.
Higher BLEU scores indicate better alignment with
the reference outputs [14]. BLEU scores are also
calculated for different questions (Q1, Q2, and Q3).
Table 2: Model Performance Comparison (LLM With RAG)
Model Q1 Q2 Q3
RouLe BLEU RouLe BLEU RouLe BLEU
Mistral 0.6486 0.2852 0.8888 0.1431 0.9692 0.7256
Falcon 0.6486 0.2852 0.3950 0.0140 0.8474 0.7121
Llama 0.2790 0.0731 0.2077 0.0108 0.9218 0.8292
T5 0.3157 0.0073 0.8888 0.6391 0.9137 0.7997
Fig. 1. RouLe score comparison across models for multiple questions
Fig. 2. BLEU score comparison across models for multiple questions
Table II shows the LLM models' performance when RAG is
implemented. With high results on all three questions (Q1:
0.648, Q2: 0.889, Q3: 0.969), Mistral shows an excellent
ed on January 29,2026 at 15:09:24 UTC from IEEE Xplore. Restrictions apply.

=== Page 4 ===
RouLE performance. These numbers show Mistral's consistency
in producing outputs with rather aligned lengths with regard for
expectations. The BLEU ratings for Mistral vary, nevertheless
(Q1: 0.285, Q2: 0.143, Q3: 0.726). The Q3 BLEU score is
especially higher, indicating in that case greater alignment with
reference outputs. While the BLEU scores in Fig. 2 indicate a
more varying trajectory, culminating at Q3, Mistral's RouLE
values in Fig. 1 show a consistent rise from Q1 to Q3. These
findings show its capacity to properly address factual as well as
domain-specific queries.
Llama shows a significant drop in performance across the
RouLE metrics, with lower values for Q1 (0.279) and Q2
(0.208), though it reaches a relatively higher value for Q3
(0.922). BLEU scores for Llama are also low for Q1 (0.073) and
Q2 (0.011), but the BLEU score for Q3 is 0.829, demonstrating
an improvement and suggesting better handling of complex,
non-technical questions. Fig.1 and Fig.2 illustrates this drop in
performance, with Llama’s RouLE and BLEU scores both
starting low and showing a noticeable rise at Q3.
T5 exhibits a balanced performance with relatively strong
RouLE scores (Q1: 0.316, Q2: 0.889, Q3: 0.914), indicating that
it performs well in length estimation. However, its BLEU scores
are inconsistent, with very low values for Q1 (0.007) and Q3
(0.0006), though it shows a somewhat higher BLEU score for
Q2 (0.639), suggesting it generates better text for that query. In
Fig.2, T5’s BLEU scores display a steep contrast between Q1
and Q2, with a dramatic drop at Q3, while its RouLE scores
remain relatively consistent. T5 showed the lowest performance
overall, particularly in BLEU scores. These results suggest
difficulties in generating precise and fluent responses to domain-
specific queries, indicating a need for further optimization.
Falcon performs similarly to Mistral, showing high RouLE
scores for Q1 (0.649) and Q3 (0.847), but a noticeable dip for
Q2 (0.395). Its BLEU scores mirror the RouLE trend to some
extent, with higher values for Q3 (0.712) compared to Q1
(0.285) and Q2 (0.014). Fig.1 and Fig.2 illustrates the dip in Q2
for both RouLE and BLEU, with Q3 showing the highest values
across both metrics. Falcon’s performance declined in Q2,
indicating potential limitations in handling algorithmic or
technical queries.
Table 3: RouLe and BLEU Similarity Index of LLM Models with RAG and
without RAG
S.No. Model Question RouLE_ BLEU_
Similarity Similarity
Q1 0.226804 0.027918
1 Mistral
Q2 0.277108 0.019496
Q3 0.318182 0.117017
Q1 0.736842 0.258984
2 Falcon
Q2 0.337349 0.016208
Q3 0.400000 0.178185
Q1 0.600000 0.086334
3 T5
Q2 0.740741 0.022678
Q3 0.187500 0.000597
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

Fig. 3. BLEU similarity comparison across models (with vs without RAG)
Fig. 4. RouLe similarity comparison across models (with vs without RAG)
Analyzed for three questions (Q1, Q2, Q3) across three
models—Mistral, T5, and Falcon—were the similarity scores
for produced outputs. Two aspects were the alignment of
produced content with reference outputs depending on RouLE
scores (measuring length estimate) and BLEU scores
(evaluating n-gram accuracy). The findings are presented in
Table III and graphically shown in Figures 3 and 4, therefore
offering a thorough comparison of the performance of the
models with and without Retrieval-Augmented Generation
(RAG) application.
Mistral shows moderate similarity to the reference outputs in
terms of RouLE and BLEU across all questions. For Q1, the
RouLE score is 0.227 and the BLEU score is 0.028, indicating a
lower alignment with both length expectations and reference
text. For Q2, the values are 0.277 (RouLE) and 0.019 (BLEU),
which remain low. For Q3, Mistral shows an increase in RouLE
(0.318) and BLEU (0.117), but these scores are still below those
of some other models, suggesting that while Mistral's generated
text length aligns reasonably with the reference, the precision of
its generated content (as measured by BLEU) could be
improved.
T5 has higher similarity scores in both metrics compared to
Mistral. For Q1, it has a strong RouLE score of 0.600 and a
BLEU score of 0.086, suggesting that T5's generated text has a
better length estimation and n-gram alignment with reference
outputs. In Q2, T5 further improves, achieving a RouLE score
of 0.741 and a BLEU score of 0.023. However, in Q3, the model
sees a slight drop in both metrics (RouLE: 0.188, BLEU:
0.0006), indicating lower alignment for that particular question.
Fig.3 and Fig.4 shows the steady improvement in T5’s RouLE
ed on January 29,2026 at 15:09:24 UTC from IEEE Xplore. Restrictions apply.

=== Page 5 ===
scores, while its BLEU scores peak at Q1, before declining at
Q3.
Falcon stands out in terms of similarity scores, particularly for
Q1, where it achieves a high RouLE score of 0.737 and a
relatively higher BLEU score of 0.259. Falcon also performs
well for Q2 with RouLE at 0.337 and BLEU at 0.016. For Q3,
Falcon demonstrates a solid alignment with reference content,
with RouLE at 0.400 and BLEU at 0.178, suggesting its strength
in producing more accurate and relevant outputs. Fig.3 and Fig.4
highlights Falcon’s consistent performance, with high similarity
scores at Q1 and a moderate drop for Q2 before stabilizing for
Q3.
In summary, Mistral, Falcon, and T5 exhibit distinct strengths
across the experiments. Mistral performs well in terms of output
length estimation (RouLE) but has varying BLEU scores that
suggest it might struggle with precise content generation. T5
demonstrates solid performance with good length estimation
and stronger BLEU scores in certain cases, while Falcon
maintains consistent performance with higher alignment to
reference outputs, particularly for Q1 and Q3. Llama, on the
other hand, generally lags behind in both RouLE and BLEU
across most questions, highlighting areas where its performance
could be improved.
V. CONCLUSION AND FUTURE SCOPE
This study demonstrates the significant potential of Retrieval-
Augmented Generation (RAG) systems in overcoming Large
Language Models' (LLMs') constraints like hallucinations, out-
of-date knowledge, and contextually shallow outputs. Real-
time retrieval systems help RAG systems improve the
dependability, contextual relevance, and correctness of AI-
generated replies. Using measures like BLEU and ROUGE, a
comparison of LLMs—including Mistral, Falcon, Llama, and
T5—showcases how consistently RAG integration improves
performance across several domains. Among the models,
Mistral and Falcon showed very strong accuracy in facts and
contextual alignment. These results show the need of coupling
the generating capacity of LLMs with external knowledge
retrieval, so providing a viable route for creating AI systems
that provide dependable, context-aware, and domain-specific
solutions for a wide spectrum of uses. The promising outcomes
of this study open several avenues for future exploration in
Retrieval-Augmented Generation (RAG) systems. Further
research can focus on integrating multi-modal retrieval to
enhance LLMs with data from visual, audio, or structured
sources. Improving real-time retrieval efficiency and
optimizing fusion strategies for retrieved knowledge with
prompts can yield more contextually aware outputs.
Authorized licensed use limited to: Volvo Cars Vehicle Engineering. Downloade

Additionally, deploying human-in-the-loop frameworks for
iterative feedback and refinement will boost reliability and user
trust. Finally, expanding the evaluation across diverse domains
and languages will validate the scalability and generalizability
of RAG-enhanced models in real-world applications.
REFERENCES
[1] Teubner, Timm, Christoph M. Flath, Christof Weinhardt, Wil van der
Aalst, and Oliver Hinz. "Welcome to the era of chatgpt et al. the prospects
of large language models." Business & Information Systems
Engineering 65, no. 2 (2023): 95-101.
[2] Asai, Akari, Sewon Min, Zexuan Zhong, and Danqi Chen. "Retrieval-
based language models and applications." In Proceedings of the 61st
Annual Meeting of the Association for Computational Linguistics
(Volume 6: Tutorial Abstracts), pp. 41-46. 2023.
[3] Sudhi, Viju, Sinchana Ramakanth Bhat, Max Rudat, and Roman Teucher.
"Rag-ex: A generic framework for explaining retrieval augmented
generation." In Proceedings of the 47th International ACM SIGIR
Conference on Research and Development in Information Retrieval, pp.
2776-2780. 2024.
[4] Wang, Mengzhao, Haotian Wu, Xiangyu Ke, Yunjun Gao, Xiaoliang Xu,
and Lu Chen. "An Interactive Multi-modal Query Answering System with
Retrieval-Augmented Large Language Models." arXiv preprint
arXiv:2407.04217 (2024).
[5] Chen, Jiawei, Hongyu Lin, Xianpei Han, and Le Sun. "Benchmarking
large language models in retrieval-augmented generation."
In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38,
no. 16, pp. 17754-17762. 2024.
[6] Gao, Yunfan, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi,
Yi Dai, Jiawei Sun, and Haofen Wang. "Retrieval-augmented generation
for large language models: A survey." arXiv preprint
arXiv:2312.10997 (2023).
[7] Lewis, Patrick, et al. "Retrieval-augmented generation for knowledge-
intensive nlp tasks." Advances in neural information processing
systems 33 (2020): 9459-9474.
[8] Naveed, Humza, et al. "A comprehensive overview of large language
models." arXiv preprint arXiv:2307.06435 (2023).
[9] Hadi, Muhammad Usman, et al. "A survey on large language models:
Applications, challenges, limitations, and practical usage." Authorea
Preprints (2023).
[10] Hamzah, Farizal, and Nuraini Sulaiman. "Multimodal integration in large
language models: A case study with mistral llm." (2024).
[11] Hou, Guangyu, and Qin Lian. "Benchmarking of commercial large
language models: Chatgpt, mistral, and llama." (2024).
[12] Almazrouei, Ebtesam, et al. "The falcon series of open language
models." arXiv preprint arXiv:2311.16867 (2023).
[13] Mastropaolo, Antonio, et al. "Studying the usage of text-to-text transfer
transformer to support code-related tasks." 2021 IEEE/ACM 43rd
International Conference on Software Engineering (ICSE). IEEE, 2021.
[14] Reiter, Ehud. "A structured review of the validity of
BLEU." Computational Linguistics 44.3 (2018): 393-401.
[15] Barbella, Marcello, and Genoveffa Tortora. "Rouge metric evaluation for
text summarization techniques." Available at SSRN 4120317 (2022).
ed on January 29,2026 at 15:09:24 UTC from IEEE Xplore. Restrictions apply.