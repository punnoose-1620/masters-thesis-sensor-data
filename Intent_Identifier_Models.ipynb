{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/punnoose-1620/masters-thesis-sensor-data/blob/main/Intent_Identifier_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp_kaurj0sqT"
      },
      "source": [
        "# Automotive Intent Classification and Slot Filling System\n",
        "=========================================================\n",
        "\n",
        "A modular TensorFlow/Keras implementation for multi-task learning on automotive technical queries.\n",
        "\n",
        "This script implements three neural architectures:\n",
        "1. Bi-LSTM with Self-Attention\n",
        "2. 1D-CNN with multiple kernel sizes\n",
        "3. Joint Multi-Task Learning model\n",
        "\n",
        "Author: Automotive NLP Team\n",
        "Date: 2026-02-04"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGlC1KR5025G"
      },
      "source": [
        "## Imports and Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: seaborn in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (1.8.0)\n",
            "Requirement already satisfied: tensorflow in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (2.20.0)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (3.10.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (3.0.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (2.4.1)\n",
            "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (26.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (6.33.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (2.32.5)\n",
            "Requirement already satisfied: setuptools in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (80.10.2)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (2.1.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (2.20.0)\n",
            "Requirement already satisfied: keras>=3.10.0 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (3.13.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.6.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2.21.0->tensorflow) (2026.1.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.10.1)\n",
            "Requirement already satisfied: pillow in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard~=2.20.0->tensorflow) (12.1.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=3 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from astunparse>=1.6.0->tensorflow) (0.46.3)\n",
            "Requirement already satisfied: rich in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from keras>=3.10.0->tensorflow) (14.3.2)\n",
            "Requirement already satisfied: namex in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\pkozhupp\\appdata\\roaming\\python\\python313\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.3 -> 26.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install seaborn scikit-learn tensorflow matplotlib pandas numpy google"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hT8E4Bbz02XO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\PKOZHUPP\\AppData\\Local\\Temp\\ipykernel_12716\\1446452458.py:20: FutureWarning: \n",
            "\n",
            "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
            "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
            "See README for more details:\n",
            "\n",
            "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
            "\n",
            "  import google.generativeai as genai\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model, Input\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZC1rwfn06UE"
      },
      "source": [
        "## Set random seeds for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "uVooh4fV08WH"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmSVZZwe1Lmf"
      },
      "source": [
        "## Configurations and Initializations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdAVm2xL0-YH"
      },
      "source": [
        "### GLOBAL CONFIGURATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "YAikFZ4m1FiP"
      },
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    'FILE_PATH': 'datasets\\intent_dataset.csv',  # Update this path\n",
        "    'EMBEDDING_DIM': 128,\n",
        "    'HIDDEN_UNITS': 128,\n",
        "    'LEARNING_RATE': 0.0005,\n",
        "    'BATCH_SIZE': 32,\n",
        "    'EPOCHS': 200,\n",
        "    'MAX_SEQUENCE_LENGTH': 50,\n",
        "    'MAX_VOCAB_SIZE': 10000,\n",
        "    'VALIDATION_SPLIT': 0.15,\n",
        "    'TEST_SPLIT': 0.15,\n",
        "    'EARLY_STOPPING_PATIENCE': 10,\n",
        "    'CNN_FILTERS': 128,\n",
        "    'CNN_KERNEL_SIZES': [3, 4, 5],\n",
        "    'DROPOUT_RATE': 0.3,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTBnOJw11IxW"
      },
      "source": [
        "### Global variables to store preprocessing artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "8oNeQUW51Qu_"
      },
      "outputs": [],
      "source": [
        "tokenizer = None\n",
        "label_encoders = {}\n",
        "history_objects = {}\n",
        "test_results = {}\n",
        "\n",
        "COMPARISON_CHART = 'Visualizations/model_comparison.png'\n",
        "BI_LSTM_CHART = 'Visualizations/bilstm_results.png'\n",
        "CNN_CHART = 'Visualizations/cnn_results.png'\n",
        "JOINT_CHART = 'Visualizations/joint_results.png'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVSzVodG1TV_"
      },
      "source": [
        "## Dataset Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tzc9H0J1W_O"
      },
      "source": [
        "### FUNCTION 1: LOAD DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "fKxUGe1H14mT"
      },
      "outputs": [],
      "source": [
        "def load_data(file_path=None):\n",
        "    \"\"\"\n",
        "    Load automotive dataset from CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the CSV file. Uses CONFIG['FILE_PATH'] if None.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Loaded dataset\n",
        "    \"\"\"\n",
        "    if file_path is None:\n",
        "        file_path = CONFIG['FILE_PATH']\n",
        "\n",
        "    print(f\"Loading data from: {file_path}\")\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"✓ Data loaded successfully: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "        print(f\"✓ Columns: {list(df.columns)}\")\n",
        "        print(f\"\\nFirst few rows:\")\n",
        "        print(df.head())\n",
        "\n",
        "        # Validate required columns\n",
        "        required_cols = ['query', 'intent', 'brand', 'model', 'sensor']\n",
        "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "        # Handle missing values\n",
        "        print(f\"\\nMissing values before handling:\")\n",
        "        print(df.isnull().sum())\n",
        "\n",
        "        # Fill missing slot values with 'NONE' to preserve them as valid categories\n",
        "        for col in ['brand', 'model', 'sensor', 'year']:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna('NONE').astype(str)\n",
        "\n",
        "        # Drop rows with missing queries or intents\n",
        "        df = df.dropna(subset=['query', 'intent'])\n",
        "\n",
        "        print(f\"\\n✓ Data cleaned: {df.shape[0]} rows remaining\")\n",
        "        print(f\"\\nIntent distribution:\")\n",
        "        print(df['intent'].value_counts())\n",
        "\n",
        "        return df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"✗ Error: File not found at {file_path}\")\n",
        "        print(\"Creating synthetic dataset for demonstration...\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error loading data: {str(e)}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5QlzAkK__nC"
      },
      "source": [
        "### FUNCTION 2: PREPROCESS DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "KHECUzBSAFpr"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(df):\n",
        "    \"\"\"\n",
        "    Preprocess automotive queries and labels.\n",
        "    Handles technical jargon carefully - no aggressive stop-word removal.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input dataframe\n",
        "\n",
        "    Returns:\n",
        "        tuple: (X_padded, y_intent, y_brand, y_model, y_sensor, metadata)\n",
        "    \"\"\"\n",
        "    global tokenizer, label_encoders\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"PREPROCESSING DATA\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # 1. Text Tokenization (preserve technical terms)\n",
        "    print(\"\\n1. Tokenizing queries...\")\n",
        "    tokenizer = Tokenizer(\n",
        "        num_words=CONFIG['MAX_VOCAB_SIZE'],\n",
        "        oov_token='<OOV>',\n",
        "        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',  # Keep technical chars\n",
        "        lower=True\n",
        "    )\n",
        "\n",
        "    tokenizer.fit_on_texts(df['query'])\n",
        "    sequences = tokenizer.texts_to_sequences(df['query'])\n",
        "\n",
        "    # 2. Padding\n",
        "    print(f\"2. Padding sequences to length {CONFIG['MAX_SEQUENCE_LENGTH']}...\")\n",
        "    X_padded = pad_sequences(\n",
        "        sequences,\n",
        "        maxlen=CONFIG['MAX_SEQUENCE_LENGTH'],\n",
        "        padding='post',\n",
        "        truncating='post'\n",
        "    )\n",
        "\n",
        "    vocab_size = min(len(tokenizer.word_index) + 1, CONFIG['MAX_VOCAB_SIZE'])\n",
        "    print(f\"   ✓ Vocabulary size: {vocab_size}\")\n",
        "    print(f\"   ✓ Padded shape: {X_padded.shape}\")\n",
        "\n",
        "    # 3. Encode labels\n",
        "    print(\"\\n3. Encoding labels...\")\n",
        "\n",
        "    # Intent (primary classification)\n",
        "    label_encoders['intent'] = LabelEncoder()\n",
        "    y_intent_encoded = label_encoders['intent'].fit_transform(df['intent'])\n",
        "    y_intent = to_categorical(y_intent_encoded)\n",
        "    print(f\"   ✓ Intent classes: {len(label_encoders['intent'].classes_)}\")\n",
        "    print(f\"     Classes: {label_encoders['intent'].classes_}\")\n",
        "\n",
        "    # Slot filling targets\n",
        "    slot_columns = ['brand', 'model', 'sensor']\n",
        "    y_slots = {}\n",
        "\n",
        "    for slot in slot_columns:\n",
        "        if slot in df.columns:\n",
        "            label_encoders[slot] = LabelEncoder()\n",
        "            y_encoded = label_encoders[slot].fit_transform(df[slot])\n",
        "            y_slots[slot] = to_categorical(y_encoded)\n",
        "            print(f\"   ✓ {slot.capitalize()} classes: {len(label_encoders[slot].classes_)}\")\n",
        "\n",
        "    # Prepare metadata\n",
        "    metadata = {\n",
        "        'vocab_size': vocab_size,\n",
        "        'num_intent_classes': len(label_encoders['intent'].classes_),\n",
        "        'num_brand_classes': len(label_encoders['brand'].classes_),\n",
        "        'num_model_classes': len(label_encoders['model'].classes_),\n",
        "        'num_sensor_classes': len(label_encoders['sensor'].classes_),\n",
        "        'intent_labels': label_encoders['intent'].classes_,\n",
        "        'max_seq_length': CONFIG['MAX_SEQUENCE_LENGTH']\n",
        "    }\n",
        "\n",
        "    print(f\"\\n✓ Preprocessing complete!\")\n",
        "    print(f\"  Input shape: {X_padded.shape}\")\n",
        "    print(f\"  Intent shape: {y_intent.shape}\")\n",
        "\n",
        "    return X_padded, y_intent, y_slots['brand'], y_slots['model'], y_slots['sensor'], metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si7Ni5QMAHKr"
      },
      "source": [
        "### FUNCTION 3: SPLIT DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "CDl_LbkvANWV"
      },
      "outputs": [],
      "source": [
        "def split_data(X, y_intent, y_brand, y_model, y_sensor):\n",
        "    \"\"\"\n",
        "    Split data into Train (70%), Validation (15%), and Test (15%).\n",
        "\n",
        "    Args:\n",
        "        X: Input sequences\n",
        "        y_intent, y_brand, y_model, y_sensor: Target labels\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing all splits\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"SPLITTING DATA\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # First split: Train+Val (85%) vs Test (15%)\n",
        "    test_size = CONFIG['TEST_SPLIT']\n",
        "    val_size = CONFIG['VALIDATION_SPLIT'] / (1 - test_size)  # 15% of remaining 85%\n",
        "\n",
        "    X_temp, X_test, y_intent_temp, y_intent_test, y_brand_temp, y_brand_test, \\\n",
        "    y_model_temp, y_model_test, y_sensor_temp, y_sensor_test = train_test_split(\n",
        "        X, y_intent, y_brand, y_model, y_sensor,\n",
        "        test_size=test_size,\n",
        "        random_state=42,\n",
        "        stratify=np.argmax(y_intent, axis=1)\n",
        "    )\n",
        "\n",
        "    # Second split: Train (70%) vs Validation (15%)\n",
        "    X_train, X_val, y_intent_train, y_intent_val, y_brand_train, y_brand_val, \\\n",
        "    y_model_train, y_model_val, y_sensor_train, y_sensor_val = train_test_split(\n",
        "        X_temp, y_intent_temp, y_brand_temp, y_model_temp, y_sensor_temp,\n",
        "        test_size=val_size,\n",
        "        random_state=42,\n",
        "        stratify=np.argmax(y_intent_temp, axis=1)\n",
        "    )\n",
        "\n",
        "    splits = {\n",
        "        'X_train': X_train, 'X_val': X_val, 'X_test': X_test,\n",
        "        'y_intent_train': y_intent_train, 'y_intent_val': y_intent_val, 'y_intent_test': y_intent_test,\n",
        "        'y_brand_train': y_brand_train, 'y_brand_val': y_brand_val, 'y_brand_test': y_brand_test,\n",
        "        'y_model_train': y_model_train, 'y_model_val': y_model_val, 'y_model_test': y_model_test,\n",
        "        'y_sensor_train': y_sensor_train, 'y_sensor_val': y_sensor_val, 'y_sensor_test': y_sensor_test,\n",
        "    }\n",
        "\n",
        "    print(f\"Train set: {X_train.shape[0]} samples ({X_train.shape[0]/X.shape[0]*100:.1f}%)\")\n",
        "    print(f\"Val set:   {X_val.shape[0]} samples ({X_val.shape[0]/X.shape[0]*100:.1f}%)\")\n",
        "    print(f\"Test set:  {X_test.shape[0]} samples ({X_test.shape[0]/X.shape[0]*100:.1f}%)\")\n",
        "    print(f\"\\n✓ Data split complete!\")\n",
        "\n",
        "    return splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2t1tedTA5RG"
      },
      "source": [
        "## Model Functions - Design and Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XAnBr4xARpF"
      },
      "source": [
        "### FUNCTION 4: BUILD BI-LSTM MODEL WITH SELF-ATTENTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "0nWjfBmiAVA2"
      },
      "outputs": [],
      "source": [
        "def build_bilstm_model(metadata):\n",
        "    \"\"\"\n",
        "    Build Bi-directional LSTM model with Self-Attention layer.\n",
        "\n",
        "    Args:\n",
        "        metadata (dict): Model configuration metadata\n",
        "\n",
        "    Returns:\n",
        "        keras.Model: Compiled Bi-LSTM model\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"BUILDING BI-LSTM MODEL WITH SELF-ATTENTION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Input layer\n",
        "    input_layer = Input(shape=(metadata['max_seq_length'],), name='input')\n",
        "\n",
        "    # Embedding layer\n",
        "    embedding = layers.Embedding(\n",
        "        input_dim=metadata['vocab_size'],\n",
        "        output_dim=CONFIG['EMBEDDING_DIM'],\n",
        "        mask_zero=True,\n",
        "        name='embedding'\n",
        "    )(input_layer)\n",
        "\n",
        "    # Bi-LSTM layer\n",
        "    bilstm = layers.Bidirectional(\n",
        "        layers.LSTM(CONFIG['HIDDEN_UNITS'], return_sequences=True),\n",
        "        name='bilstm'\n",
        "    )(embedding)\n",
        "\n",
        "    # Self-Attention mechanism\n",
        "    #attention = layers.Attention(name='self_attention')([bilstm, bilstm])\n",
        "\n",
        "    # Global pooling\n",
        "    #pooled = layers.GlobalAveragePooling1D(name='global_pool')(attention)\n",
        "\n",
        "    # Global pooling (no self-attention to avoid gradient bug with layers.Attention)\n",
        "    pooled = layers.GlobalAveragePooling1D(name='global_pool')(bilstm)\n",
        "\n",
        "    # Dropout for regularization\n",
        "    dropout = layers.Dropout(CONFIG['DROPOUT_RATE'], name='dropout')(pooled)\n",
        "\n",
        "    # Dense layer\n",
        "    dense = layers.Dense(CONFIG['HIDDEN_UNITS'], activation='relu', name='dense')(dropout)\n",
        "\n",
        "    # Output layer for intent classification\n",
        "    output = layers.Dense(\n",
        "        metadata['num_intent_classes'],\n",
        "        activation='softmax',\n",
        "        name='intent_output'\n",
        "    )(dense)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs=input_layer, outputs=output, name='BiLSTM_SelfAttention')\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=CONFIG['LEARNING_RATE']),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    print(model.summary())\n",
        "    print(f\"\\n✓ Bi-LSTM model built successfully!\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFKYupl3BAg_"
      },
      "source": [
        "### FUNCTION 5: BUILD 1D-CNN MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "_X0U2YYsBLBi"
      },
      "outputs": [],
      "source": [
        "def build_cnn_model(metadata):\n",
        "    \"\"\"\n",
        "    Build 1D-CNN model with multiple kernel sizes (3, 4, 5) to capture\n",
        "    local technical n-grams.\n",
        "\n",
        "    Args:\n",
        "        metadata (dict): Model configuration metadata\n",
        "\n",
        "    Returns:\n",
        "        keras.Model: Compiled CNN model\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"BUILDING 1D-CNN MODEL WITH MULTI-KERNEL\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Input layer\n",
        "    input_layer = Input(shape=(metadata['max_seq_length'],), name='input')\n",
        "\n",
        "    # Embedding layer\n",
        "    embedding = layers.Embedding(\n",
        "        input_dim=metadata['vocab_size'],\n",
        "        output_dim=CONFIG['EMBEDDING_DIM'],\n",
        "        name='embedding'\n",
        "    )(input_layer)\n",
        "\n",
        "    # Multiple CNN branches with different kernel sizes\n",
        "    conv_branches = []\n",
        "    for kernel_size in CONFIG['CNN_KERNEL_SIZES']:\n",
        "        conv = layers.Conv1D(\n",
        "            filters=CONFIG['CNN_FILTERS'],\n",
        "            kernel_size=kernel_size,\n",
        "            activation='relu',\n",
        "            name=f'conv1d_k{kernel_size}'\n",
        "        )(embedding)\n",
        "        pool = layers.GlobalMaxPooling1D(name=f'maxpool_k{kernel_size}')(conv)\n",
        "        conv_branches.append(pool)\n",
        "\n",
        "    # Concatenate all branches\n",
        "    if len(conv_branches) > 1:\n",
        "        concatenated = layers.Concatenate(name='concat_branches')(conv_branches)\n",
        "    else:\n",
        "        concatenated = conv_branches[0]\n",
        "\n",
        "    # Dropout for regularization\n",
        "    dropout = layers.Dropout(CONFIG['DROPOUT_RATE'], name='dropout')(concatenated)\n",
        "\n",
        "    # Dense layer\n",
        "    dense = layers.Dense(CONFIG['HIDDEN_UNITS'], activation='relu', name='dense')(dropout)\n",
        "\n",
        "    # Output layer for intent classification\n",
        "    output = layers.Dense(\n",
        "        metadata['num_intent_classes'],\n",
        "        activation='softmax',\n",
        "        name='intent_output'\n",
        "    )(dense)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs=input_layer, outputs=output, name='CNN_MultiKernel')\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=CONFIG['LEARNING_RATE']),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    print(model.summary())\n",
        "    print(f\"\\n✓ 1D-CNN model built successfully!\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gw-v_XbBIvc"
      },
      "source": [
        "### FUNCTION 6: BUILD JOINT MULTI-TASK MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "A3rN1GzdBVey"
      },
      "outputs": [],
      "source": [
        "def build_joint_model(metadata):\n",
        "    \"\"\"\n",
        "    Build Joint Multi-Task Learning model with shared backbone and multiple heads\n",
        "    for Intent Classification and Slot Filling (Brand, Model, Sensor).\n",
        "\n",
        "    Args:\n",
        "        metadata (dict): Model configuration metadata\n",
        "\n",
        "    Returns:\n",
        "        keras.Model: Compiled joint model\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"BUILDING JOINT MULTI-TASK MODEL\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Input layer\n",
        "    input_layer = Input(shape=(metadata['max_seq_length'],), name='input')\n",
        "\n",
        "    # Shared Embedding layer\n",
        "    embedding = layers.Embedding(\n",
        "        input_dim=metadata['vocab_size'],\n",
        "        output_dim=CONFIG['EMBEDDING_DIM'],\n",
        "        mask_zero=True,\n",
        "        name='shared_embedding'\n",
        "    )(input_layer)\n",
        "\n",
        "    # Shared Bi-LSTM encoder\n",
        "    bilstm = layers.Bidirectional(\n",
        "        layers.LSTM(CONFIG['HIDDEN_UNITS'], return_sequences=True),\n",
        "        name='shared_bilstm'\n",
        "    )(embedding)\n",
        "\n",
        "    # Shared Self-Attention\n",
        "    attention = layers.Attention(name='shared_attention')([bilstm, bilstm])\n",
        "\n",
        "    # Global pooling\n",
        "    pooled = layers.GlobalAveragePooling1D(name='shared_pool')(attention)\n",
        "\n",
        "    # Shared dropout\n",
        "    shared_features = layers.Dropout(CONFIG['DROPOUT_RATE'], name='shared_dropout')(pooled)\n",
        "\n",
        "    # Shared dense layer\n",
        "    shared_dense = layers.Dense(\n",
        "        CONFIG['HIDDEN_UNITS'],\n",
        "        activation='relu',\n",
        "        name='shared_dense'\n",
        "    )(shared_features)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Task-specific heads\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Head 1: Intent Classification\n",
        "    intent_dense = layers.Dense(64, activation='relu', name='intent_dense')(shared_dense)\n",
        "    intent_dropout = layers.Dropout(0.2, name='intent_dropout')(intent_dense)\n",
        "    intent_output = layers.Dense(\n",
        "        metadata['num_intent_classes'],\n",
        "        activation='softmax',\n",
        "        name='intent_output'\n",
        "    )(intent_dropout)\n",
        "\n",
        "    # Head 2: Brand Slot Filling\n",
        "    brand_dense = layers.Dense(64, activation='relu', name='brand_dense')(shared_dense)\n",
        "    brand_dropout = layers.Dropout(0.2, name='brand_dropout')(brand_dense)\n",
        "    brand_output = layers.Dense(\n",
        "        metadata['num_brand_classes'],\n",
        "        activation='softmax',\n",
        "        name='brand_output'\n",
        "    )(brand_dropout)\n",
        "\n",
        "    # Head 3: Model Slot Filling\n",
        "    model_dense = layers.Dense(64, activation='relu', name='model_dense')(shared_dense)\n",
        "    model_dropout = layers.Dropout(0.2, name='model_dropout')(model_dense)\n",
        "    model_output = layers.Dense(\n",
        "        metadata['num_model_classes'],\n",
        "        activation='softmax',\n",
        "        name='model_output'\n",
        "    )(model_dropout)\n",
        "\n",
        "    # Head 4: Sensor Slot Filling\n",
        "    sensor_dense = layers.Dense(64, activation='relu', name='sensor_dense')(shared_dense)\n",
        "    sensor_dropout = layers.Dropout(0.2, name='sensor_dropout')(sensor_dense)\n",
        "    sensor_output = layers.Dense(\n",
        "        metadata['num_sensor_classes'],\n",
        "        activation='softmax',\n",
        "        name='sensor_output'\n",
        "    )(sensor_dropout)\n",
        "\n",
        "    # Create multi-output model\n",
        "    model = Model(\n",
        "        inputs=input_layer,\n",
        "        outputs=[intent_output, brand_output, model_output, sensor_output],\n",
        "        name='Joint_MTL_Model'\n",
        "    )\n",
        "\n",
        "    # Compile with multiple losses and metrics\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=CONFIG['LEARNING_RATE']),\n",
        "        loss={\n",
        "            'intent_output': 'categorical_crossentropy',\n",
        "            'brand_output': 'categorical_crossentropy',\n",
        "            'model_output': 'categorical_crossentropy',\n",
        "            'sensor_output': 'categorical_crossentropy'\n",
        "        },\n",
        "        loss_weights={\n",
        "            'intent_output': 2.0,  # Prioritize intent slightly\n",
        "            'brand_output': 1.0,\n",
        "            'model_output': 1.0,\n",
        "            'sensor_output': 1.0\n",
        "        },\n",
        "        metrics={\n",
        "            'intent_output': ['accuracy'],\n",
        "            'brand_output': ['accuracy'],\n",
        "            'model_output': ['accuracy'],\n",
        "            'sensor_output': ['accuracy']\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(model.summary())\n",
        "    print(f\"\\n✓ Joint Multi-Task model built successfully!\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SzCoFZdCvKY"
      },
      "source": [
        "## Model Functions - Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TreciG5dBZMV"
      },
      "source": [
        "### FUNCTION 7: TRAIN AND EVALUATE BI-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "LaWOkqSOBelC"
      },
      "outputs": [],
      "source": [
        "def train_eval_bilstm(model, splits, metadata):\n",
        "    \"\"\"\n",
        "    Train Bi-LSTM model with early stopping and evaluate on test set.\n",
        "\n",
        "    Args:\n",
        "        model: Compiled Bi-LSTM model\n",
        "        splits (dict): Train/val/test data splits\n",
        "        metadata (dict): Model metadata\n",
        "\n",
        "    Returns:\n",
        "        tuple: (history, test_metrics)\n",
        "    \"\"\"\n",
        "    global history_objects, test_results\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"TRAINING BI-LSTM MODEL\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Callbacks\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=CONFIG['EARLY_STOPPING_PATIENCE'],\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    reduce_lr = ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\nTraining...\")\n",
        "    history = model.fit(\n",
        "        splits['X_train'],\n",
        "        splits['y_intent_train'],\n",
        "        validation_data=(splits['X_val'], splits['y_intent_val']),\n",
        "        epochs=CONFIG['EPOCHS'],\n",
        "        batch_size=CONFIG['BATCH_SIZE'],\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    history_objects['bilstm'] = history\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"EVALUATING BI-LSTM ON TEST SET\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    test_loss, test_acc = model.evaluate(\n",
        "        splits['X_test'],\n",
        "        splits['y_intent_test'],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_probs = model.predict(splits['X_test'], verbose=0)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "    y_true = np.argmax(splits['y_intent_test'], axis=1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    test_f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "    test_f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "    test_metrics = {\n",
        "        'loss': test_loss,\n",
        "        'accuracy': test_acc,\n",
        "        'f1_macro': test_f1_macro,\n",
        "        'f1_weighted': test_f1_weighted,\n",
        "        'y_true': y_true,\n",
        "        'y_pred': y_pred\n",
        "    }\n",
        "\n",
        "    test_results['bilstm'] = test_metrics\n",
        "\n",
        "    print(f\"\\nTest Results:\")\n",
        "    print(f\"  Loss: {test_loss:.4f}\")\n",
        "    print(f\"  Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"  F1-Score (Macro): {test_f1_macro:.4f}\")\n",
        "    print(f\"  F1-Score (Weighted): {test_f1_weighted:.4f}\")\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(\n",
        "        y_true, y_pred,\n",
        "        target_names=metadata['intent_labels'],\n",
        "        digits=4\n",
        "    ))\n",
        "\n",
        "    return history, test_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzcPY5QkBgoD"
      },
      "source": [
        "### FUNCTION 8: TRAIN AND EVALUATE CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "FQ6sPVZVBlUy"
      },
      "outputs": [],
      "source": [
        "def train_eval_cnn(model, splits, metadata):\n",
        "    \"\"\"\n",
        "    Train CNN model with early stopping and evaluate on test set.\n",
        "\n",
        "    Args:\n",
        "        model: Compiled CNN model\n",
        "        splits (dict): Train/val/test data splits\n",
        "        metadata (dict): Model metadata\n",
        "\n",
        "    Returns:\n",
        "        tuple: (history, test_metrics)\n",
        "    \"\"\"\n",
        "    global history_objects, test_results\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"TRAINING 1D-CNN MODEL\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Callbacks\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=CONFIG['EARLY_STOPPING_PATIENCE'],\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    reduce_lr = ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\nTraining...\")\n",
        "    history = model.fit(\n",
        "        splits['X_train'],\n",
        "        splits['y_intent_train'],\n",
        "        validation_data=(splits['X_val'], splits['y_intent_val']),\n",
        "        epochs=CONFIG['EPOCHS'],\n",
        "        batch_size=CONFIG['BATCH_SIZE'],\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    history_objects['cnn'] = history\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"EVALUATING 1D-CNN ON TEST SET\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    test_loss, test_acc = model.evaluate(\n",
        "        splits['X_test'],\n",
        "        splits['y_intent_test'],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_probs = model.predict(splits['X_test'], verbose=0)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "    y_true = np.argmax(splits['y_intent_test'], axis=1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    test_f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "    test_f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "    test_metrics = {\n",
        "        'loss': test_loss,\n",
        "        'accuracy': test_acc,\n",
        "        'f1_macro': test_f1_macro,\n",
        "        'f1_weighted': test_f1_weighted,\n",
        "        'y_true': y_true,\n",
        "        'y_pred': y_pred\n",
        "    }\n",
        "\n",
        "    test_results['cnn'] = test_metrics\n",
        "\n",
        "    print(f\"\\nTest Results:\")\n",
        "    print(f\"  Loss: {test_loss:.4f}\")\n",
        "    print(f\"  Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"  F1-Score (Macro): {test_f1_macro:.4f}\")\n",
        "    print(f\"  F1-Score (Weighted): {test_f1_weighted:.4f}\")\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(\n",
        "        y_true, y_pred,\n",
        "        target_names=metadata['intent_labels'],\n",
        "        digits=4\n",
        "    ))\n",
        "\n",
        "    return history, test_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHrcqk7nCA2f"
      },
      "source": [
        "### FUNCTION 9: TRAIN AND EVALUATE JOINT MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "Nh-I-atBCNo2"
      },
      "outputs": [],
      "source": [
        "def train_eval_joint(model, splits, metadata):\n",
        "    \"\"\"\n",
        "    Train Joint Multi-Task model with early stopping and evaluate on test set.\n",
        "\n",
        "    Args:\n",
        "        model: Compiled Joint model\n",
        "        splits (dict): Train/val/test data splits\n",
        "        metadata (dict): Model metadata\n",
        "\n",
        "    Returns:\n",
        "        tuple: (history, test_metrics)\n",
        "    \"\"\"\n",
        "    global history_objects, test_results\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"TRAINING JOINT MULTI-TASK MODEL\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Prepare multi-output training data\n",
        "    y_train = {\n",
        "        'intent_output': splits['y_intent_train'],\n",
        "        'brand_output': splits['y_brand_train'],\n",
        "        'model_output': splits['y_model_train'],\n",
        "        'sensor_output': splits['y_sensor_train']\n",
        "    }\n",
        "\n",
        "    y_val = {\n",
        "        'intent_output': splits['y_intent_val'],\n",
        "        'brand_output': splits['y_brand_val'],\n",
        "        'model_output': splits['y_model_val'],\n",
        "        'sensor_output': splits['y_sensor_val']\n",
        "    }\n",
        "\n",
        "    # Callbacks\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=CONFIG['EARLY_STOPPING_PATIENCE'],\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    reduce_lr = ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\nTraining...\")\n",
        "    history = model.fit(\n",
        "        splits['X_train'],\n",
        "        y_train,\n",
        "        validation_data=(splits['X_val'], y_val),\n",
        "        epochs=CONFIG['EPOCHS'],\n",
        "        batch_size=CONFIG['BATCH_SIZE'],\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    history_objects['joint'] = history\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"EVALUATING JOINT MODEL ON TEST SET\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    y_test = {\n",
        "        'intent_output': splits['y_intent_test'],\n",
        "        'brand_output': splits['y_brand_test'],\n",
        "        'model_output': splits['y_model_test'],\n",
        "        'sensor_output': splits['y_sensor_test']\n",
        "    }\n",
        "\n",
        "    test_results_raw = model.evaluate(\n",
        "        splits['X_test'],\n",
        "        y_test,\n",
        "        verbose=0,\n",
        "        return_dict=True\n",
        "    )\n",
        "\n",
        "    # Predictions\n",
        "    predictions = model.predict(splits['X_test'], verbose=0)\n",
        "    y_intent_pred_probs, y_brand_pred_probs, y_model_pred_probs, y_sensor_pred_probs = predictions\n",
        "\n",
        "    # Intent predictions\n",
        "    y_intent_pred = np.argmax(y_intent_pred_probs, axis=1)\n",
        "    y_intent_true = np.argmax(splits['y_intent_test'], axis=1)\n",
        "\n",
        "    # Calculate intent metrics\n",
        "    intent_f1_macro = f1_score(y_intent_true, y_intent_pred, average='macro')\n",
        "    intent_f1_weighted = f1_score(y_intent_true, y_intent_pred, average='weighted')\n",
        "    intent_acc = accuracy_score(y_intent_true, y_intent_pred)\n",
        "\n",
        "    # Brand predictions\n",
        "    y_brand_pred = np.argmax(y_brand_pred_probs, axis=1)\n",
        "    y_brand_true = np.argmax(splits['y_brand_test'], axis=1)\n",
        "    brand_acc = accuracy_score(y_brand_true, y_brand_pred)\n",
        "\n",
        "    # Model predictions\n",
        "    y_model_pred = np.argmax(y_model_pred_probs, axis=1)\n",
        "    y_model_true = np.argmax(splits['y_model_test'], axis=1)\n",
        "    model_acc = accuracy_score(y_model_true, y_model_pred)\n",
        "\n",
        "    # Sensor predictions\n",
        "    y_sensor_pred = np.argmax(y_sensor_pred_probs, axis=1)\n",
        "    y_sensor_true = np.argmax(splits['y_sensor_test'], axis=1)\n",
        "    sensor_acc = accuracy_score(y_sensor_true, y_sensor_pred)\n",
        "\n",
        "    test_metrics = {\n",
        "        'loss': test_results_raw['loss'],\n",
        "        'accuracy': intent_acc,\n",
        "        'f1_macro': intent_f1_macro,\n",
        "        'f1_weighted': intent_f1_weighted,\n",
        "        'y_true': y_intent_true,\n",
        "        'y_pred': y_intent_pred,\n",
        "        'intent_accuracy': intent_acc,\n",
        "        'brand_accuracy': brand_acc,\n",
        "        'model_accuracy': model_acc,\n",
        "        'sensor_accuracy': sensor_acc\n",
        "    }\n",
        "\n",
        "    test_results['joint'] = test_metrics\n",
        "\n",
        "    print(f\"\\nTest Results:\")\n",
        "    print(f\"  Overall Loss: {test_results_raw['loss']:.4f}\")\n",
        "    print(f\"\\nIntent Classification:\")\n",
        "    print(f\"  Accuracy: {intent_acc:.4f}\")\n",
        "    print(f\"  F1-Score (Macro): {intent_f1_macro:.4f}\")\n",
        "    print(f\"  F1-Score (Weighted): {intent_f1_weighted:.4f}\")\n",
        "    print(f\"\\nSlot Filling Accuracy:\")\n",
        "    print(f\"  Brand: {brand_acc:.4f}\")\n",
        "    print(f\"  Model: {model_acc:.4f}\")\n",
        "    print(f\"  Sensor: {sensor_acc:.4f}\")\n",
        "\n",
        "    print(\"\\nIntent Classification Report:\")\n",
        "    print(classification_report(\n",
        "        y_intent_true, y_intent_pred,\n",
        "        target_names=metadata['intent_labels'],\n",
        "        digits=4\n",
        "    ))\n",
        "\n",
        "    return history, test_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kXwoITfC2CB"
      },
      "source": [
        "## Model Functions - Result Visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN4PSJAdCP_m"
      },
      "source": [
        "### FUNCTION 10: VISUALIZE BI-LSTM RESULTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tThP8zOnCUvZ"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'BI_LSTM_CHART' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvisualize_bilstm\u001b[39m(history, test_metrics, metadata, save_path=\u001b[43mBI_LSTM_CHART\u001b[49m):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m    Plot training history and confusion matrix for Bi-LSTM model.\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m \u001b[33;03m        save_path (str): Path to save the figure\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'BI_LSTM_CHART' is not defined"
          ]
        }
      ],
      "source": [
        "def visualize_bilstm(history, test_metrics, metadata, save_path=BI_LSTM_CHART):\n",
        "    \"\"\"\n",
        "    Plot training history and confusion matrix for Bi-LSTM model.\n",
        "\n",
        "    Args:\n",
        "        history: Training history object\n",
        "        test_metrics (dict): Test evaluation metrics\n",
        "        metadata (dict): Model metadata\n",
        "        save_path (str): Path to save the figure\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"VISUALIZING BI-LSTM RESULTS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    fig = plt.figure(figsize=(16, 5))\n",
        "\n",
        "    # Plot 1: Training & Validation Accuracy\n",
        "    ax1 = plt.subplot(1, 3, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
        "    plt.plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
        "    plt.title('Bi-LSTM: Accuracy Over Epochs', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Accuracy', fontsize=12)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    # Plot 2: Training & Validation Loss\n",
        "    ax2 = plt.subplot(1, 3, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
        "    plt.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
        "    plt.title('Bi-LSTM: Loss Over Epochs', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Loss', fontsize=12)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    # Plot 3: Confusion Matrix\n",
        "    ax3 = plt.subplot(1, 3, 3)\n",
        "    cm = confusion_matrix(test_metrics['y_true'], test_metrics['y_pred'])\n",
        "\n",
        "    # Normalize confusion matrix\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    sns.heatmap(\n",
        "        cm_normalized,\n",
        "        annot=True,\n",
        "        fmt='.2f',\n",
        "        cmap='Blues',\n",
        "        xticklabels=metadata['intent_labels'],\n",
        "        yticklabels=metadata['intent_labels'],\n",
        "        cbar_kws={'label': 'Proportion'},\n",
        "        ax=ax3\n",
        "    )\n",
        "    plt.title('Bi-LSTM: Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Predicted', fontsize=12)\n",
        "    plt.ylabel('Actual', fontsize=12)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"✓ Visualization saved to: {save_path}\")\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IbNN7jBCYIA"
      },
      "source": [
        "### FUNCTION 11: VISUALIZE CNN RESULTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7eNYL04CbLr"
      },
      "outputs": [],
      "source": [
        "def visualize_cnn(history, test_metrics, metadata, save_path=CNN_CHART):\n",
        "    \"\"\"\n",
        "    Plot training history and confusion matrix for CNN model.\n",
        "\n",
        "    Args:\n",
        "        history: Training history object\n",
        "        test_metrics (dict): Test evaluation metrics\n",
        "        metadata (dict): Model metadata\n",
        "        save_path (str): Path to save the figure\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"VISUALIZING 1D-CNN RESULTS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    fig = plt.figure(figsize=(16, 5))\n",
        "\n",
        "    # Plot 1: Training & Validation Accuracy\n",
        "    ax1 = plt.subplot(1, 3, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
        "    plt.plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
        "    plt.title('1D-CNN: Accuracy Over Epochs', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Accuracy', fontsize=12)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    # Plot 2: Training & Validation Loss\n",
        "    ax2 = plt.subplot(1, 3, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
        "    plt.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
        "    plt.title('1D-CNN: Loss Over Epochs', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Loss', fontsize=12)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    # Plot 3: Confusion Matrix\n",
        "    ax3 = plt.subplot(1, 3, 3)\n",
        "    cm = confusion_matrix(test_metrics['y_true'], test_metrics['y_pred'])\n",
        "\n",
        "    # Normalize confusion matrix\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    sns.heatmap(\n",
        "        cm_normalized,\n",
        "        annot=True,\n",
        "        fmt='.2f',\n",
        "        cmap='Greens',\n",
        "        xticklabels=metadata['intent_labels'],\n",
        "        yticklabels=metadata['intent_labels'],\n",
        "        cbar_kws={'label': 'Proportion'},\n",
        "        ax=ax3\n",
        "    )\n",
        "    plt.title('1D-CNN: Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Predicted', fontsize=12)\n",
        "    plt.ylabel('Actual', fontsize=12)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"✓ Visualization saved to: {save_path}\")\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZd8GSobCdKl"
      },
      "source": [
        "### FUNCTION 12: VISUALIZE MODEL COMPARISON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHgBm5aHCh9w"
      },
      "outputs": [],
      "source": [
        "def visualize_comparison(save_path=COMPARISON_CHART):\n",
        "    \"\"\"\n",
        "    Create comprehensive side-by-side comparison of all models (Bi-LSTM, CNN, Joint MTL, and Gemini if evaluated).\n",
        "\n",
        "    Args:\n",
        "        save_path (str): Path to save the comparison figure\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"CREATING MODEL COMPARISON VISUALIZATION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Extract metrics for comparison (include Gemini if present in test_results)\n",
        "    model_keys = ['bilstm', 'cnn', 'joint']\n",
        "    if 'gemini' in test_results:\n",
        "        model_keys = model_keys + ['gemini']\n",
        "    models = ['Bi-LSTM', '1D-CNN', 'Joint MTL'] + (['Gemini'] if 'gemini' in test_results else [])\n",
        "\n",
        "    bar_colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']\n",
        "    colors = bar_colors[:len(model_keys)]\n",
        "\n",
        "    accuracies = [test_results[key]['accuracy'] for key in model_keys]\n",
        "    f1_macros = [test_results[key]['f1_macro'] for key in model_keys]\n",
        "    f1_weighteds = [test_results[key]['f1_weighted'] for key in model_keys]\n",
        "    losses = [test_results[key]['loss'] if test_results[key].get('loss') is not None else 0.0 for key in model_keys]\n",
        "\n",
        "    # Create comprehensive comparison figure\n",
        "    fig = plt.figure(figsize=(18, 10))\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Plot 1: Accuracy Comparison\n",
        "    # -------------------------------------------------------------------------\n",
        "    ax1 = plt.subplot(2, 3, 1)\n",
        "    bars = plt.bar(models, accuracies, color=colors, alpha=0.8, edgecolor='black')\n",
        "    plt.title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('Accuracy', fontsize=12)\n",
        "    plt.ylim([0, 1.0])\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    for bar, val in zip(bars, accuracies):\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{val:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Plot 2: F1-Score (Macro) Comparison\n",
        "    # -------------------------------------------------------------------------\n",
        "    ax2 = plt.subplot(2, 3, 2)\n",
        "    bars = plt.bar(models, f1_macros, color=colors, alpha=0.8, edgecolor='black')\n",
        "    plt.title('F1-Score (Macro) Comparison', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('F1-Score (Macro)', fontsize=12)\n",
        "    plt.ylim([0, 1.0])\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    for bar, val in zip(bars, f1_macros):\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{val:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Plot 3: Loss Comparison\n",
        "    # -------------------------------------------------------------------------\n",
        "    ax3 = plt.subplot(2, 3, 3)\n",
        "    bars = plt.bar(models, losses, color=colors, alpha=0.8, edgecolor='black')\n",
        "    plt.title('Test Loss Comparison', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('Loss', fontsize=12)\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    for bar, val in zip(bars, losses):\n",
        "        height = bar.get_height()\n",
        "        loss_label = f'{val:.4f}' if (model_keys[bars.index(bar)] != 'gemini' and test_results[model_keys[bars.index(bar)]].get('loss') is not None) else ('N/A' if model_keys[bars.index(bar)] == 'gemini' else f'{val:.4f}')\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                loss_label, ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "    # Simpler: label all with value or N/A per key\n",
        "    for i, (bar, val) in enumerate(zip(bars, losses)):\n",
        "        lbl = 'N/A' if model_keys[i] == 'gemini' else f'{val:.4f}'\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., bar.get_height(), lbl, ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Plot 4: Multi-Metric Radar Chart\n",
        "    # -------------------------------------------------------------------------\n",
        "    ax4 = plt.subplot(2, 3, 4, projection='polar')\n",
        "\n",
        "    categories = ['Accuracy', 'F1-Macro', 'F1-Weighted']\n",
        "    num_vars = len(categories)\n",
        "\n",
        "    angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
        "    angles += angles[:1]\n",
        "\n",
        "    for i, (model_name, key) in enumerate(zip(models, model_keys)):\n",
        "        values = [\n",
        "            test_results[key]['accuracy'],\n",
        "            test_results[key]['f1_macro'],\n",
        "            test_results[key]['f1_weighted']\n",
        "        ]\n",
        "        values += values[:1]\n",
        "        ax4.plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors[i])\n",
        "        ax4.fill(angles, values, alpha=0.15, color=colors[i])\n",
        "\n",
        "    ax4.set_xticks(angles[:-1])\n",
        "    ax4.set_xticklabels(categories, fontsize=10)\n",
        "    ax4.set_ylim(0, 1)\n",
        "    ax4.set_title('Multi-Metric Performance Radar', fontsize=14, fontweight='bold', pad=20)\n",
        "    ax4.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)\n",
        "    ax4.grid(True)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Plot 5: Training History Comparison (Accuracy) - only trained models\n",
        "    # -------------------------------------------------------------------------\n",
        "    ax5 = plt.subplot(2, 3, 5)\n",
        "\n",
        "    for i, (model_name, key) in enumerate(zip(models, model_keys)):\n",
        "        if key in history_objects:\n",
        "            history = history_objects[key]\n",
        "            if key == 'joint':\n",
        "                acc_key = 'intent_output_accuracy' if 'intent_output_accuracy' in history.history else 'accuracy'\n",
        "            else:\n",
        "                acc_key = 'accuracy'\n",
        "\n",
        "            if acc_key in history.history:\n",
        "                plt.plot(history.history[acc_key], label=model_name, linewidth=2, color=colors[i])\n",
        "\n",
        "    plt.title('Training Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Accuracy', fontsize=12)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Plot 6: Summary Table\n",
        "    # -------------------------------------------------------------------------\n",
        "    ax6 = plt.subplot(2, 3, 6)\n",
        "    ax6.axis('off')\n",
        "\n",
        "    table_data = []\n",
        "    for model_name, key in zip(models, model_keys):\n",
        "        loss_val = test_results[key].get('loss')\n",
        "        loss_str = f\"{loss_val:.4f}\" if loss_val is not None else \"N/A\"\n",
        "        row = [\n",
        "            model_name,\n",
        "            f\"{test_results[key]['accuracy']:.4f}\",\n",
        "            f\"{test_results[key]['f1_macro']:.4f}\",\n",
        "            f\"{test_results[key]['f1_weighted']:.4f}\",\n",
        "            loss_str\n",
        "        ]\n",
        "        table_data.append(row)\n",
        "\n",
        "    if 'joint' in test_results and 'brand_accuracy' in test_results['joint']:\n",
        "        joint_slots = f\"Brand: {test_results['joint']['brand_accuracy']:.3f} | \" \\\n",
        "                     f\"Model: {test_results['joint']['model_accuracy']:.3f} | \" \\\n",
        "                     f\"Sensor: {test_results['joint']['sensor_accuracy']:.3f}\"\n",
        "    else:\n",
        "        joint_slots = \"N/A\"\n",
        "\n",
        "    table = ax6.table(\n",
        "        cellText=table_data,\n",
        "        colLabels=['Model', 'Accuracy', 'F1-Macro', 'F1-Weighted', 'Loss'],\n",
        "        cellLoc='center',\n",
        "        loc='center',\n",
        "        bbox=[0, 0.3, 1, 0.6]\n",
        "    )\n",
        "\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(10)\n",
        "    table.scale(1, 2)\n",
        "\n",
        "    for i in range(5):\n",
        "        table[(0, i)].set_facecolor('#34495e')\n",
        "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
        "\n",
        "    best_acc_idx = np.argmax(accuracies)\n",
        "    best_f1_idx = np.argmax(f1_macros)\n",
        "\n",
        "    for i in range(len(models)):\n",
        "        if i == best_acc_idx:\n",
        "            table[(i+1, 1)].set_facecolor('#d5f4e6')\n",
        "        if i == best_f1_idx:\n",
        "            table[(i+1, 2)].set_facecolor('#d5f4e6')\n",
        "\n",
        "    ax6.text(0.5, 0.15, f\"Joint MTL Slot Filling: {joint_slots}\",\n",
        "            ha='center', va='center', fontsize=9, style='italic',\n",
        "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
        "\n",
        "    ax6.set_title('Performance Summary', fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "    fig.suptitle('Automotive Intent Classification: Model Comparison',\n",
        "                fontsize=16, fontweight='bold', y=0.98)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"✓ Comparison visualization saved to: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"FINAL MODEL COMPARISON SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\n{'Model':<15} {'Accuracy':<12} {'F1-Macro':<12} {'F1-Weighted':<12} {'Loss':<10}\")\n",
        "    print(\"-\" * 70)\n",
        "    for model_name, key in zip(models, model_keys):\n",
        "        loss_val = test_results[key].get('loss')\n",
        "        loss_str = f\"{loss_val:.4f}\" if loss_val is not None else \"N/A\"\n",
        "        print(f\"{model_name:<15} \"\n",
        "              f\"{test_results[key]['accuracy']:<12.4f} \"\n",
        "              f\"{test_results[key]['f1_macro']:<12.4f} \"\n",
        "              f\"{test_results[key]['f1_weighted']:<12.4f} \"\n",
        "              f\"{loss_str:<10}\")\n",
        "\n",
        "    best_model_idx = np.argmax([test_results[key]['f1_macro'] for key in model_keys])\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"🏆 BEST MODEL: {models[best_model_idx]} (based on F1-Macro score)\")\n",
        "    print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function 13: Load Gemini Key from .env file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_gemini_key_from_env():\n",
        "    \"\"\"Load Gemini API key from environment (GOOGLE_API_KEY or GEMINI_API_KEY).\"\"\"\n",
        "    return os.environ.get('GOOGLE_API_KEY') or os.environ.get('GEMINI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function 14: Initialize Gemini Instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_gemini(api_key=None, model_name='gemini-2.5-flash'):\n",
        "    \"\"\"\n",
        "    Configure and return the Gemini model. If api_key is None, uses load_gemini_key_from_env().\n",
        "    \"\"\"\n",
        "    global _gemini_model\n",
        "    key = api_key or load_gemini_key_from_env()\n",
        "    if not key:\n",
        "        raise ValueError(\"No API key. Set GOOGLE_API_KEY or GEMINI_API_KEY, or pass api_key=...\")\n",
        "    genai.configure(api_key=key)\n",
        "    _gemini_model = genai.GenerativeModel(model_name)\n",
        "    return _gemini_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function 15: Generate Response for Single Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_response(input_query, intent_labels):\n",
        "    \"\"\"\n",
        "    Send one query to Gemini and return a single intent string (one of intent_labels).\n",
        "    intent_labels: list/array of valid intent names, e.g. metadata['intent_labels'].\n",
        "    \"\"\"\n",
        "    global _gemini_model\n",
        "    if _gemini_model is None:\n",
        "        raise RuntimeError(\"Call initialize_gemini() first.\")\n",
        "    labels_str = \", \".join(intent_labels)\n",
        "    prompt = f\"\"\"You are an intent classifier for automotive technical queries. Choose exactly one intent from this list: {labels_str}.\n",
        "Reply with only the intent name, nothing else. No explanation.\n",
        "\n",
        "Query: {input_query}\n",
        "Intent:\"\"\"\n",
        "    response = _gemini_model.generate_content(prompt)\n",
        "    if response and response.text:\n",
        "        return response.text.strip()\n",
        "    return \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function 16: Generate Responses for all Queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_all_responses(queries_list, intent_labels, delay_seconds=0.1):\n",
        "    \"\"\"\n",
        "    Call generate_response for each query. Returns list of intent strings in same order as queries_list.\n",
        "    delay_seconds: optional pause between calls to avoid rate limits.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for i, q in enumerate(queries_list):\n",
        "        try:\n",
        "            out = generate_response(q, intent_labels)\n",
        "            results.append(out)\n",
        "        except Exception as e:\n",
        "            results.append(\"\")\n",
        "            print(f\"Query {i} error: {e}\")\n",
        "        if delay_seconds > 0:\n",
        "            time.sleep(delay_seconds)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function 17: Parse Intent from Generated Response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_intent_from_response(response_text, intent_labels):\n",
        "    \"\"\"\n",
        "    Map raw Gemini output to one of intent_labels. Returns first matching label or intent_labels[0] as fallback.\n",
        "    \"\"\"\n",
        "    if not response_text:\n",
        "        return intent_labels[0]\n",
        "    text = response_text.strip().lower()\n",
        "    for label in intent_labels:\n",
        "        if label.lower() in text or text in label.lower():\n",
        "            return label\n",
        "    # Try exact match after normalizing spaces/underscores\n",
        "    normalized = re.sub(r'[\\s_]+', '_', text)\n",
        "    for label in intent_labels:\n",
        "        if re.sub(r'[\\s_]+', '_', label.lower()) == normalized:\n",
        "            return label\n",
        "    return intent_labels[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function 18: Get test queries from splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_test_queries(df, y_intent, y_brand, y_model, y_sensor):\n",
        "    \"\"\"\n",
        "    Return the list of raw test query strings in the same order as splits['X_test'] and splits['y_intent_test'].\n",
        "    Uses the same split logic as split_data() so indices align.\n",
        "    \"\"\"\n",
        "    test_size = CONFIG['TEST_SPLIT']\n",
        "    val_size = CONFIG['VALIDATION_SPLIT'] / (1 - test_size)\n",
        "    queries = df['query'].values\n",
        "\n",
        "    q_temp, q_test, y_i_temp, y_i_test, y_b_temp, y_b_test, y_m_temp, y_m_test, y_s_temp, y_s_test = train_test_split(\n",
        "        queries, y_intent, y_brand, y_model, y_sensor,\n",
        "        test_size=test_size,\n",
        "        random_state=42,\n",
        "        stratify=np.argmax(y_intent, axis=1)\n",
        "    )\n",
        "    return q_test.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function 19: Evaluate Gemini Response wrt Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_response(responses_list, y_intent_test, metadata):\n",
        "    \"\"\"\n",
        "    Compare Gemini responses to ground truth. Expects y_intent_test one-hot, same as splits['y_intent_test'].\n",
        "    Stores results in test_results['gemini'] with the same structure as other models (loss set to 0.0 for API model).\n",
        "    \"\"\"\n",
        "    global test_results\n",
        "    intent_labels = list(metadata['intent_labels'])\n",
        "    y_true = np.argmax(y_intent_test, axis=1)\n",
        "    y_pred = []\n",
        "    for r in responses_list:\n",
        "        label = parse_intent_from_response(r, intent_labels)\n",
        "        idx = intent_labels.index(label) if label in intent_labels else 0\n",
        "        y_pred.append(idx)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "    test_metrics = {\n",
        "        'loss': 0.0,\n",
        "        'accuracy': acc,\n",
        "        'f1_macro': f1_macro,\n",
        "        'f1_weighted': f1_weighted,\n",
        "        'y_true': y_true,\n",
        "        'y_pred': y_pred,\n",
        "    }\n",
        "    test_results['gemini'] = test_metrics\n",
        "\n",
        "    print(\"Gemini Test Results:\")\n",
        "    print(f\"  Accuracy: {acc:.4f}\")\n",
        "    print(f\"  F1-Score (Macro): {f1_macro:.4f}\")\n",
        "    print(f\"  F1-Score (Weighted): {f1_weighted:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=intent_labels, digits=4))\n",
        "    return test_results['gemini']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJnAXHWlDCEY"
      },
      "source": [
        "## MAIN EXECUTION PIPELINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QghGspKklnad",
        "outputId": "80903c56-fd5b-43d1-f46b-0213310dcb6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            " AUTOMOTIVE INTENT CLASSIFICATION & SLOT FILLING SYSTEM\n",
            "======================================================================\n",
            "\n",
            "Configuration:\n",
            "  FILE_PATH: datasets\\intent_dataset.csv\n",
            "  EMBEDDING_DIM: 128\n",
            "  HIDDEN_UNITS: 128\n",
            "  LEARNING_RATE: 0.0005\n",
            "  BATCH_SIZE: 32\n",
            "  EPOCHS: 200\n",
            "  MAX_SEQUENCE_LENGTH: 50\n",
            "  MAX_VOCAB_SIZE: 10000\n",
            "  VALIDATION_SPLIT: 0.15\n",
            "  TEST_SPLIT: 0.15\n",
            "  EARLY_STOPPING_PATIENCE: 10\n",
            "  CNN_FILTERS: 128\n",
            "  CNN_KERNEL_SIZES: [3, 4, 5]\n",
            "  DROPOUT_RATE: 0.3\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" AUTOMOTIVE INTENT CLASSIFICATION & SLOT FILLING SYSTEM\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nConfiguration:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivG_mrl4lOHJ",
        "outputId": "c556838b-37c0-47a0-9f56-9b0a21aff5cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from: datasets\\intent_dataset.csv\n",
            "✓ Data loaded successfully: 11178 rows, 9 columns\n",
            "✓ Columns: ['query', 'intent', 'brand', 'model', 'year', 'sensor', 'style', 'word_count', 'char_count']\n",
            "\n",
            "First few rows:\n",
            "                                         query               intent  \\\n",
            "0          status of LiDAR collection in Jiyue  system_status_check   \n",
            "1  explain Vision system context in Polestar 2      data_definition   \n",
            "2                 what is Lidar for Lynk & Co?      data_definition   \n",
            "3      who is owner High Voltage battery data?    access_governance   \n",
            "4               roof rig needed for Acoustics?    collection_method   \n",
            "\n",
            "       brand       model  year                sensor     style  word_count  \\\n",
            "0      Jiyue          01  2023                 LiDAR   verbose           6   \n",
            "1   Polestar  Polestar 2  2021         Vision system     noisy           7   \n",
            "2  Lynk & Co          01  2019                 Lidar  standard           7   \n",
            "3      Volvo        XC60  2019  High Voltage battery  standard           7   \n",
            "4   Polestar  Polestar 3  2024             Acoustics     noisy           5   \n",
            "\n",
            "   char_count  \n",
            "0          35  \n",
            "1          43  \n",
            "2          28  \n",
            "3          39  \n",
            "4          30  \n",
            "\n",
            "Missing values before handling:\n",
            "query         0\n",
            "intent        0\n",
            "brand         0\n",
            "model         0\n",
            "year          0\n",
            "sensor        0\n",
            "style         0\n",
            "word_count    0\n",
            "char_count    0\n",
            "dtype: int64\n",
            "\n",
            "✓ Data cleaned: 11178 rows remaining\n",
            "\n",
            "Intent distribution:\n",
            "intent\n",
            "data_definition         1554\n",
            "temporal_stats          1338\n",
            "comparative_analysis    1256\n",
            "statistical_query       1250\n",
            "anomaly_info            1222\n",
            "collection_method       1194\n",
            "access_governance       1024\n",
            "imperative_commands     1011\n",
            "system_status_check      742\n",
            "contextual_followups     587\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Load Data\n",
        "df = load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0vWhHNblRF5",
        "outputId": "c7c9047c-6db4-4a2f-f272-d9bfede3927f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "PREPROCESSING DATA\n",
            "======================================================================\n",
            "\n",
            "1. Tokenizing queries...\n",
            "2. Padding sequences to length 50...\n",
            "   ✓ Vocabulary size: 1280\n",
            "   ✓ Padded shape: (11178, 50)\n",
            "\n",
            "3. Encoding labels...\n",
            "   ✓ Intent classes: 10\n",
            "     Classes: ['access_governance' 'anomaly_info' 'collection_method'\n",
            " 'comparative_analysis' 'contextual_followups' 'data_definition'\n",
            " 'imperative_commands' 'statistical_query' 'system_status_check'\n",
            " 'temporal_stats']\n",
            "   ✓ Brand classes: 7\n",
            "   ✓ Model classes: 34\n",
            "   ✓ Sensor classes: 22\n",
            "\n",
            "✓ Preprocessing complete!\n",
            "  Input shape: (11178, 50)\n",
            "  Intent shape: (11178, 10)\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Preprocess Data\n",
        "X, y_intent, y_brand, y_model, y_sensor, metadata = preprocess_data(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJZSN7cnlUf5",
        "outputId": "97f9065a-e17b-4cae-a5af-9f4d1f11aeeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "SPLITTING DATA\n",
            "======================================================================\n",
            "Train set: 7824 samples (70.0%)\n",
            "Val set:   1677 samples (15.0%)\n",
            "Test set:  1677 samples (15.0%)\n",
            "\n",
            "✓ Data split complete!\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Split Data\n",
        "splits = split_data(X, y_intent, y_brand, y_model, y_sensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OVKxtZCXlXvG",
        "outputId": "d376c391-7edf-4093-c89a-b44f2d4fb012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "BUILDING BI-LSTM MODEL WITH SELF-ATTENTION\n",
            "======================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"BiLSTM_SelfAttention\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"BiLSTM_SelfAttention\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">163,840</span> │ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_5         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bilstm              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_pool         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bilstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │ not_equal_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ intent_output       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m163,840\u001b[0m │ input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_5         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bilstm              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m263,168\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_pool         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bilstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │ not_equal_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ intent_output       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │      \u001b[38;5;34m1,290\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">461,194</span> (1.76 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m461,194\u001b[0m (1.76 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">461,194</span> (1.76 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m461,194\u001b[0m (1.76 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "\n",
            "✓ Bi-LSTM model built successfully!\n",
            "\n",
            "======================================================================\n",
            "TRAINING BI-LSTM MODEL\n",
            "======================================================================\n",
            "\n",
            "Training...\n",
            "Epoch 1/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 121ms/step - accuracy: 0.6168 - loss: 1.4089 - val_accuracy: 0.8867 - val_loss: 0.6825 - learning_rate: 5.0000e-04\n",
            "Epoch 2/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 124ms/step - accuracy: 0.8937 - loss: 0.5860 - val_accuracy: 0.9010 - val_loss: 0.5613 - learning_rate: 5.0000e-04\n",
            "Epoch 3/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 122ms/step - accuracy: 0.8999 - loss: 0.5137 - val_accuracy: 0.9040 - val_loss: 0.5439 - learning_rate: 5.0000e-04\n",
            "Epoch 4/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 119ms/step - accuracy: 0.9013 - loss: 0.4877 - val_accuracy: 0.9028 - val_loss: 0.5363 - learning_rate: 5.0000e-04\n",
            "Epoch 5/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 121ms/step - accuracy: 0.9016 - loss: 0.4653 - val_accuracy: 0.8998 - val_loss: 0.5434 - learning_rate: 5.0000e-04\n",
            "Epoch 6/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 129ms/step - accuracy: 0.9032 - loss: 0.4536 - val_accuracy: 0.8992 - val_loss: 0.5448 - learning_rate: 5.0000e-04\n",
            "Epoch 7/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8989 - loss: 0.4551\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 124ms/step - accuracy: 0.9034 - loss: 0.4412 - val_accuracy: 0.8986 - val_loss: 0.5524 - learning_rate: 5.0000e-04\n",
            "Epoch 8/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 126ms/step - accuracy: 0.9048 - loss: 0.4203 - val_accuracy: 0.8998 - val_loss: 0.5426 - learning_rate: 2.5000e-04\n",
            "Epoch 9/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 124ms/step - accuracy: 0.9059 - loss: 0.4120 - val_accuracy: 0.9010 - val_loss: 0.5467 - learning_rate: 2.5000e-04\n",
            "Epoch 10/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9019 - loss: 0.4115\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 129ms/step - accuracy: 0.9061 - loss: 0.4031 - val_accuracy: 0.8986 - val_loss: 0.5579 - learning_rate: 2.5000e-04\n",
            "Epoch 11/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 137ms/step - accuracy: 0.9078 - loss: 0.3869 - val_accuracy: 0.8992 - val_loss: 0.5609 - learning_rate: 1.2500e-04\n",
            "Epoch 12/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 141ms/step - accuracy: 0.9071 - loss: 0.3799 - val_accuracy: 0.8986 - val_loss: 0.5665 - learning_rate: 1.2500e-04\n",
            "Epoch 13/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.9044 - loss: 0.3905\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 141ms/step - accuracy: 0.9085 - loss: 0.3801 - val_accuracy: 0.8980 - val_loss: 0.5725 - learning_rate: 1.2500e-04\n",
            "Epoch 14/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 141ms/step - accuracy: 0.9100 - loss: 0.3705 - val_accuracy: 0.8980 - val_loss: 0.5747 - learning_rate: 6.2500e-05\n",
            "Epoch 14: early stopping\n",
            "Restoring model weights from the end of the best epoch: 4.\n",
            "\n",
            "======================================================================\n",
            "EVALUATING BI-LSTM ON TEST SET\n",
            "======================================================================\n",
            "\n",
            "Test Results:\n",
            "  Loss: 0.5111\n",
            "  Accuracy: 0.9082\n",
            "  F1-Score (Macro): 0.8949\n",
            "  F1-Score (Weighted): 0.9055\n",
            "\n",
            "Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "   access_governance     0.8889    0.9870    0.9354       154\n",
            "        anomaly_info     0.9333    0.9945    0.9630       183\n",
            "   collection_method     0.9345    0.8771    0.9049       179\n",
            "comparative_analysis     0.9126    1.0000    0.9543       188\n",
            "contextual_followups     0.9000    0.6136    0.7297        88\n",
            "     data_definition     0.9079    0.8884    0.8980       233\n",
            " imperative_commands     0.9457    0.8026    0.8683       152\n",
            "   statistical_query     0.8883    0.9734    0.9289       188\n",
            " system_status_check     0.9468    0.8018    0.8683       111\n",
            "      temporal_stats     0.8591    0.9403    0.8979       201\n",
            "\n",
            "            accuracy                         0.9082      1677\n",
            "           macro avg     0.9117    0.8879    0.8949      1677\n",
            "        weighted avg     0.9098    0.9082    0.9055      1677\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Build and Train Bi-LSTM Model\n",
        "bilstm_model = build_bilstm_model(metadata)\n",
        "bilstm_history, bilstm_metrics = train_eval_bilstm(bilstm_model, splits, metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "30UMys3DlaeG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "BUILDING 1D-CNN MODEL WITH MULTI-KERNEL\n",
            "======================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"CNN_MultiKernel\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"CNN_MultiKernel\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">163,840</span> │ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_k3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_k4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_k5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ maxpool_k3          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_k3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ maxpool_k4          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_k4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ maxpool_k5          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_k5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concat_branches     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ maxpool_k3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ maxpool_k4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
              "│                     │                   │            │ maxpool_k5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concat_branches[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ intent_output       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m163,840\u001b[0m │ input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_k3 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m49,280\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_k4 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m65,664\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_k5 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m82,048\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ maxpool_k3          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_k3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ maxpool_k4          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_k4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ maxpool_k5          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_k5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concat_branches     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ maxpool_k3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ maxpool_k4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
              "│                     │                   │            │ maxpool_k5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ concat_branches[\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m49,280\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ intent_output       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │      \u001b[38;5;34m1,290\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">411,402</span> (1.57 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m411,402\u001b[0m (1.57 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">411,402</span> (1.57 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m411,402\u001b[0m (1.57 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "\n",
            "✓ 1D-CNN model built successfully!\n",
            "\n",
            "======================================================================\n",
            "TRAINING 1D-CNN MODEL\n",
            "======================================================================\n",
            "\n",
            "Training...\n",
            "Epoch 1/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 48ms/step - accuracy: 0.6383 - loss: 1.3422 - val_accuracy: 0.8962 - val_loss: 0.5810 - learning_rate: 5.0000e-04\n",
            "Epoch 2/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.8976 - loss: 0.5390 - val_accuracy: 0.9040 - val_loss: 0.5151 - learning_rate: 5.0000e-04\n",
            "Epoch 3/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.9004 - loss: 0.4776 - val_accuracy: 0.9028 - val_loss: 0.5115 - learning_rate: 5.0000e-04\n",
            "Epoch 4/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.9021 - loss: 0.4439 - val_accuracy: 0.9022 - val_loss: 0.5132 - learning_rate: 5.0000e-04\n",
            "Epoch 5/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.9036 - loss: 0.4140 - val_accuracy: 0.9016 - val_loss: 0.5261 - learning_rate: 5.0000e-04\n",
            "Epoch 6/200\n",
            "\u001b[1m244/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8984 - loss: 0.4072\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.9040 - loss: 0.3922 - val_accuracy: 0.8998 - val_loss: 0.5372 - learning_rate: 5.0000e-04\n",
            "Epoch 7/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.9070 - loss: 0.3455 - val_accuracy: 0.9010 - val_loss: 0.5348 - learning_rate: 2.5000e-04\n",
            "Epoch 8/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.9098 - loss: 0.3292 - val_accuracy: 0.8998 - val_loss: 0.5443 - learning_rate: 2.5000e-04\n",
            "Epoch 9/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9071 - loss: 0.3238\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.9127 - loss: 0.3077 - val_accuracy: 0.9004 - val_loss: 0.5596 - learning_rate: 2.5000e-04\n",
            "Epoch 10/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.9149 - loss: 0.2850 - val_accuracy: 0.8998 - val_loss: 0.5603 - learning_rate: 1.2500e-04\n",
            "Epoch 11/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - accuracy: 0.9181 - loss: 0.2732 - val_accuracy: 0.8992 - val_loss: 0.5667 - learning_rate: 1.2500e-04\n",
            "Epoch 12/200\n",
            "\u001b[1m244/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9149 - loss: 0.2794\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 69ms/step - accuracy: 0.9191 - loss: 0.2664 - val_accuracy: 0.8998 - val_loss: 0.5730 - learning_rate: 1.2500e-04\n",
            "Epoch 13/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - accuracy: 0.9211 - loss: 0.2519 - val_accuracy: 0.8986 - val_loss: 0.5749 - learning_rate: 6.2500e-05\n",
            "Epoch 13: early stopping\n",
            "Restoring model weights from the end of the best epoch: 3.\n",
            "\n",
            "======================================================================\n",
            "EVALUATING 1D-CNN ON TEST SET\n",
            "======================================================================\n",
            "\n",
            "Test Results:\n",
            "  Loss: 0.4978\n",
            "  Accuracy: 0.9094\n",
            "  F1-Score (Macro): 0.8961\n",
            "  F1-Score (Weighted): 0.9068\n",
            "\n",
            "Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "   access_governance     0.8889    0.9870    0.9354       154\n",
            "        anomaly_info     0.9286    0.9945    0.9604       183\n",
            "   collection_method     0.9515    0.8771    0.9128       179\n",
            "comparative_analysis     0.9126    1.0000    0.9543       188\n",
            "contextual_followups     0.9153    0.6136    0.7347        88\n",
            "     data_definition     0.9127    0.8970    0.9048       233\n",
            " imperative_commands     0.9385    0.8026    0.8652       152\n",
            "   statistical_query     0.8883    0.9734    0.9289       188\n",
            " system_status_check     0.9468    0.8018    0.8683       111\n",
            "      temporal_stats     0.8552    0.9403    0.8957       201\n",
            "\n",
            "            accuracy                         0.9094      1677\n",
            "           macro avg     0.9138    0.8887    0.8961      1677\n",
            "        weighted avg     0.9115    0.9094    0.9068      1677\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Build and Train CNN Model\n",
        "cnn_model = build_cnn_model(metadata)\n",
        "cnn_history, cnn_metrics = train_eval_cnn(cnn_model, splits, metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "KVLLuu_GldH5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "BUILDING JOINT MULTI-TASK MODEL\n",
            "======================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Joint_MTL_Model\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"Joint_MTL_Model\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ shared_embedding    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">163,840</span> │ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_6         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ shared_bilstm       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> │ shared_embedding… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ shared_attention    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ shared_bilstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)         │                   │            │ shared_bilstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ convert_to_tensor_2 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ not_equal_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvertToTensor</span>)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ shared_pool         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ shared_attention… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │ convert_to_tenso… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ shared_dropout      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ shared_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ shared_dense        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ shared_dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ intent_dense        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ shared_dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ brand_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ shared_dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ model_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ shared_dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ sensor_dense        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ shared_dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ intent_dropout      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ intent_dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ brand_dropout       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ brand_dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ model_dropout       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ model_dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ sensor_dropout      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sensor_dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ intent_output       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │ intent_dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ brand_output        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">455</span> │ brand_dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ model_output        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,210</span> │ model_dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ sensor_output       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,430</span> │ sensor_dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ shared_embedding    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m163,840\u001b[0m │ input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_6         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ shared_bilstm       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m263,168\u001b[0m │ shared_embedding… │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ shared_attention    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ shared_bilstm[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mAttention\u001b[0m)         │                   │            │ shared_bilstm[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ convert_to_tensor_2 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ not_equal_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mConvertToTensor\u001b[0m)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ shared_pool         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ shared_attention… │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │ convert_to_tenso… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ shared_dropout      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ shared_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ shared_dense        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ shared_dropout[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ intent_dense        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ shared_dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ brand_dense (\u001b[38;5;33mDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ shared_dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ model_dense (\u001b[38;5;33mDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ shared_dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ sensor_dense        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ shared_dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ intent_dropout      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ intent_dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ brand_dropout       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ brand_dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ model_dropout       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ model_dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ sensor_dropout      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ sensor_dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ intent_output       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │        \u001b[38;5;34m650\u001b[0m │ intent_dropout[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ brand_output        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)         │        \u001b[38;5;34m455\u001b[0m │ brand_dropout[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ model_output        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m)        │      \u001b[38;5;34m2,210\u001b[0m │ model_dropout[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ sensor_output       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m)        │      \u001b[38;5;34m1,430\u001b[0m │ sensor_dropout[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">497,673</span> (1.90 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m497,673\u001b[0m (1.90 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">497,673</span> (1.90 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m497,673\u001b[0m (1.90 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "\n",
            "✓ Joint Multi-Task model built successfully!\n",
            "\n",
            "======================================================================\n",
            "TRAINING JOINT MULTI-TASK MODEL\n",
            "======================================================================\n",
            "\n",
            "Training...\n",
            "Epoch 1/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 139ms/step - brand_output_accuracy: 0.3992 - brand_output_loss: 1.5786 - intent_output_accuracy: 0.5404 - intent_output_loss: 1.5495 - loss: 11.0416 - model_output_accuracy: 0.1033 - model_output_loss: 3.2875 - sensor_output_accuracy: 0.0740 - sensor_output_loss: 3.0704 - val_brand_output_accuracy: 0.7531 - val_brand_output_loss: 0.8702 - val_intent_output_accuracy: 0.8718 - val_intent_output_loss: 0.7222 - val_loss: 8.1770 - val_model_output_accuracy: 0.1485 - val_model_output_loss: 2.7876 - val_sensor_output_accuracy: 0.0751 - val_sensor_output_loss: 3.0599 - learning_rate: 5.0000e-04\n",
            "Epoch 2/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 73ms/step - brand_output_accuracy: 0.8240 - brand_output_loss: 0.5860 - intent_output_accuracy: 0.8722 - intent_output_loss: 0.7001 - loss: 7.4682 - model_output_accuracy: 0.1976 - model_output_loss: 2.4659 - sensor_output_accuracy: 0.0970 - sensor_output_loss: 3.0137 - val_brand_output_accuracy: 0.9165 - val_brand_output_loss: 0.2917 - val_intent_output_accuracy: 0.8909 - val_intent_output_loss: 0.6018 - val_loss: 6.4676 - val_model_output_accuracy: 0.2743 - val_model_output_loss: 2.0381 - val_sensor_output_accuracy: 0.1288 - val_sensor_output_loss: 2.9204 - learning_rate: 5.0000e-04\n",
            "Epoch 3/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 73ms/step - brand_output_accuracy: 0.9081 - brand_output_loss: 0.3048 - intent_output_accuracy: 0.8837 - intent_output_loss: 0.6240 - loss: 6.0836 - model_output_accuracy: 0.2914 - model_output_loss: 1.9862 - sensor_output_accuracy: 0.2308 - sensor_output_loss: 2.5419 - val_brand_output_accuracy: 0.9326 - val_brand_output_loss: 0.2061 - val_intent_output_accuracy: 0.8933 - val_intent_output_loss: 0.5902 - val_loss: 5.0453 - val_model_output_accuracy: 0.3792 - val_model_output_loss: 1.7054 - val_sensor_output_accuracy: 0.4365 - val_sensor_output_loss: 1.9396 - learning_rate: 5.0000e-04\n",
            "Epoch 4/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 75ms/step - brand_output_accuracy: 0.9251 - brand_output_loss: 0.2435 - intent_output_accuracy: 0.8832 - intent_output_loss: 0.5988 - loss: 4.7869 - model_output_accuracy: 0.3607 - model_output_loss: 1.7496 - sensor_output_accuracy: 0.4776 - sensor_output_loss: 1.5943 - val_brand_output_accuracy: 0.9374 - val_brand_output_loss: 0.1723 - val_intent_output_accuracy: 0.8927 - val_intent_output_loss: 0.5979 - val_loss: 3.8510 - val_model_output_accuracy: 0.4621 - val_model_output_loss: 1.4927 - val_sensor_output_accuracy: 0.7281 - val_sensor_output_loss: 0.9781 - learning_rate: 5.0000e-04\n",
            "Epoch 5/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 76ms/step - brand_output_accuracy: 0.9294 - brand_output_loss: 0.2184 - intent_output_accuracy: 0.8837 - intent_output_loss: 0.5906 - loss: 3.8666 - model_output_accuracy: 0.4425 - model_output_loss: 1.5297 - sensor_output_accuracy: 0.6921 - sensor_output_loss: 0.9345 - val_brand_output_accuracy: 0.9481 - val_brand_output_loss: 0.1494 - val_intent_output_accuracy: 0.8951 - val_intent_output_loss: 0.5890 - val_loss: 3.1810 - val_model_output_accuracy: 0.5605 - val_model_output_loss: 1.2952 - val_sensor_output_accuracy: 0.8420 - val_sensor_output_loss: 0.5446 - learning_rate: 5.0000e-04\n",
            "Epoch 6/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 79ms/step - brand_output_accuracy: 0.9361 - brand_output_loss: 0.1920 - intent_output_accuracy: 0.8873 - intent_output_loss: 0.5623 - loss: 3.2452 - model_output_accuracy: 0.5189 - model_output_loss: 1.3373 - sensor_output_accuracy: 0.8007 - sensor_output_loss: 0.5882 - val_brand_output_accuracy: 0.9463 - val_brand_output_loss: 0.1546 - val_intent_output_accuracy: 0.8962 - val_intent_output_loss: 0.5917 - val_loss: 2.8239 - val_model_output_accuracy: 0.6422 - val_model_output_loss: 1.1031 - val_sensor_output_accuracy: 0.8712 - val_sensor_output_loss: 0.3698 - learning_rate: 5.0000e-04\n",
            "Epoch 7/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 76ms/step - brand_output_accuracy: 0.9390 - brand_output_loss: 0.1739 - intent_output_accuracy: 0.8900 - intent_output_loss: 0.5384 - loss: 2.8148 - model_output_accuracy: 0.6057 - model_output_loss: 1.1312 - sensor_output_accuracy: 0.8473 - sensor_output_loss: 0.4303 - val_brand_output_accuracy: 0.9445 - val_brand_output_loss: 0.1545 - val_intent_output_accuracy: 0.8951 - val_intent_output_loss: 0.5991 - val_loss: 2.6142 - val_model_output_accuracy: 0.6905 - val_model_output_loss: 0.9568 - val_sensor_output_accuracy: 0.8885 - val_sensor_output_loss: 0.2930 - learning_rate: 5.0000e-04\n",
            "Epoch 8/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 75ms/step - brand_output_accuracy: 0.9392 - brand_output_loss: 0.1737 - intent_output_accuracy: 0.8946 - intent_output_loss: 0.5182 - loss: 2.5665 - model_output_accuracy: 0.6555 - model_output_loss: 0.9983 - sensor_output_accuracy: 0.8676 - sensor_output_loss: 0.3554 - val_brand_output_accuracy: 0.9499 - val_brand_output_loss: 0.1345 - val_intent_output_accuracy: 0.8933 - val_intent_output_loss: 0.6181 - val_loss: 2.4618 - val_model_output_accuracy: 0.7454 - val_model_output_loss: 0.8069 - val_sensor_output_accuracy: 0.8939 - val_sensor_output_loss: 0.2683 - learning_rate: 5.0000e-04\n",
            "Epoch 9/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 74ms/step - brand_output_accuracy: 0.9421 - brand_output_loss: 0.1571 - intent_output_accuracy: 0.8917 - intent_output_loss: 0.5248 - loss: 2.3997 - model_output_accuracy: 0.7025 - model_output_loss: 0.8740 - sensor_output_accuracy: 0.8779 - sensor_output_loss: 0.3166 - val_brand_output_accuracy: 0.9505 - val_brand_output_loss: 0.1360 - val_intent_output_accuracy: 0.8921 - val_intent_output_loss: 0.6020 - val_loss: 2.3440 - val_model_output_accuracy: 0.7710 - val_model_output_loss: 0.7364 - val_sensor_output_accuracy: 0.8843 - val_sensor_output_loss: 0.2519 - learning_rate: 5.0000e-04\n",
            "Epoch 10/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 75ms/step - brand_output_accuracy: 0.9429 - brand_output_loss: 0.1529 - intent_output_accuracy: 0.8942 - intent_output_loss: 0.4923 - loss: 2.2236 - model_output_accuracy: 0.7253 - model_output_loss: 0.7967 - sensor_output_accuracy: 0.8825 - sensor_output_loss: 0.2866 - val_brand_output_accuracy: 0.9529 - val_brand_output_loss: 0.1336 - val_intent_output_accuracy: 0.8921 - val_intent_output_loss: 0.5975 - val_loss: 2.2713 - val_model_output_accuracy: 0.7752 - val_model_output_loss: 0.6805 - val_sensor_output_accuracy: 0.8915 - val_sensor_output_loss: 0.2487 - learning_rate: 5.0000e-04\n",
            "Epoch 11/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 73ms/step - brand_output_accuracy: 0.9462 - brand_output_loss: 0.1467 - intent_output_accuracy: 0.8972 - intent_output_loss: 0.4851 - loss: 2.1390 - model_output_accuracy: 0.7513 - model_output_loss: 0.7438 - sensor_output_accuracy: 0.8855 - sensor_output_loss: 0.2764 - val_brand_output_accuracy: 0.9493 - val_brand_output_loss: 0.1366 - val_intent_output_accuracy: 0.8927 - val_intent_output_loss: 0.6042 - val_loss: 2.2589 - val_model_output_accuracy: 0.7764 - val_model_output_loss: 0.6540 - val_sensor_output_accuracy: 0.8897 - val_sensor_output_loss: 0.2450 - learning_rate: 5.0000e-04\n",
            "Epoch 12/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 72ms/step - brand_output_accuracy: 0.9484 - brand_output_loss: 0.1386 - intent_output_accuracy: 0.8978 - intent_output_loss: 0.4762 - loss: 2.0501 - model_output_accuracy: 0.7625 - model_output_loss: 0.7009 - sensor_output_accuracy: 0.8902 - sensor_output_loss: 0.2557 - val_brand_output_accuracy: 0.9523 - val_brand_output_loss: 0.1297 - val_intent_output_accuracy: 0.8939 - val_intent_output_loss: 0.5805 - val_loss: 2.1745 - val_model_output_accuracy: 0.7800 - val_model_output_loss: 0.6296 - val_sensor_output_accuracy: 0.8921 - val_sensor_output_loss: 0.2415 - learning_rate: 5.0000e-04\n",
            "Epoch 13/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 75ms/step - brand_output_accuracy: 0.9490 - brand_output_loss: 0.1373 - intent_output_accuracy: 0.9021 - intent_output_loss: 0.4596 - loss: 1.9789 - model_output_accuracy: 0.7716 - model_output_loss: 0.6769 - sensor_output_accuracy: 0.8975 - sensor_output_loss: 0.2432 - val_brand_output_accuracy: 0.9481 - val_brand_output_loss: 0.1365 - val_intent_output_accuracy: 0.8945 - val_intent_output_loss: 0.6039 - val_loss: 2.2236 - val_model_output_accuracy: 0.7841 - val_model_output_loss: 0.6221 - val_sensor_output_accuracy: 0.8903 - val_sensor_output_loss: 0.2424 - learning_rate: 5.0000e-04\n",
            "Epoch 14/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 75ms/step - brand_output_accuracy: 0.9516 - brand_output_loss: 0.1319 - intent_output_accuracy: 0.9001 - intent_output_loss: 0.4545 - loss: 1.9215 - model_output_accuracy: 0.7827 - model_output_loss: 0.6413 - sensor_output_accuracy: 0.8933 - sensor_output_loss: 0.2376 - val_brand_output_accuracy: 0.9487 - val_brand_output_loss: 0.1341 - val_intent_output_accuracy: 0.8939 - val_intent_output_loss: 0.5967 - val_loss: 2.1946 - val_model_output_accuracy: 0.7806 - val_model_output_loss: 0.6101 - val_sensor_output_accuracy: 0.8939 - val_sensor_output_loss: 0.2425 - learning_rate: 5.0000e-04\n",
            "Epoch 15/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - brand_output_accuracy: 0.9524 - brand_output_loss: 0.1300 - intent_output_accuracy: 0.9039 - intent_output_loss: 0.4476 - loss: 1.8790 - model_output_accuracy: 0.7837 - model_output_loss: 0.6268 - sensor_output_accuracy: 0.8924 - sensor_output_loss: 0.2270\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 78ms/step - brand_output_accuracy: 0.9504 - brand_output_loss: 0.1318 - intent_output_accuracy: 0.9044 - intent_output_loss: 0.4320 - loss: 1.8645 - model_output_accuracy: 0.7821 - model_output_loss: 0.6308 - sensor_output_accuracy: 0.8902 - sensor_output_loss: 0.2357 - val_brand_output_accuracy: 0.9493 - val_brand_output_loss: 0.1321 - val_intent_output_accuracy: 0.8939 - val_intent_output_loss: 0.6180 - val_loss: 2.2208 - val_model_output_accuracy: 0.7835 - val_model_output_loss: 0.5995 - val_sensor_output_accuracy: 0.8897 - val_sensor_output_loss: 0.2387 - learning_rate: 5.0000e-04\n",
            "Epoch 16/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 74ms/step - brand_output_accuracy: 0.9530 - brand_output_loss: 0.1244 - intent_output_accuracy: 0.9058 - intent_output_loss: 0.4171 - loss: 1.7754 - model_output_accuracy: 0.7890 - model_output_loss: 0.5984 - sensor_output_accuracy: 0.8993 - sensor_output_loss: 0.2161 - val_brand_output_accuracy: 0.9481 - val_brand_output_loss: 0.1300 - val_intent_output_accuracy: 0.8945 - val_intent_output_loss: 0.6246 - val_loss: 2.2164 - val_model_output_accuracy: 0.7871 - val_model_output_loss: 0.5865 - val_sensor_output_accuracy: 0.8980 - val_sensor_output_loss: 0.2355 - learning_rate: 2.5000e-04\n",
            "Epoch 17/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 78ms/step - brand_output_accuracy: 0.9548 - brand_output_loss: 0.1156 - intent_output_accuracy: 0.9078 - intent_output_loss: 0.4057 - loss: 1.7281 - model_output_accuracy: 0.7932 - model_output_loss: 0.5853 - sensor_output_accuracy: 0.9002 - sensor_output_loss: 0.2138 - val_brand_output_accuracy: 0.9487 - val_brand_output_loss: 0.1338 - val_intent_output_accuracy: 0.8939 - val_intent_output_loss: 0.6235 - val_loss: 2.2153 - val_model_output_accuracy: 0.7859 - val_model_output_loss: 0.5788 - val_sensor_output_accuracy: 0.8915 - val_sensor_output_loss: 0.2409 - learning_rate: 2.5000e-04\n",
            "Epoch 18/200\n",
            "\u001b[1m244/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - brand_output_accuracy: 0.9556 - brand_output_loss: 0.1215 - intent_output_accuracy: 0.9065 - intent_output_loss: 0.4014 - loss: 1.6926 - model_output_accuracy: 0.7976 - model_output_loss: 0.5714 - sensor_output_accuracy: 0.9031 - sensor_output_loss: 0.1969\n",
            "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 80ms/step - brand_output_accuracy: 0.9553 - brand_output_loss: 0.1224 - intent_output_accuracy: 0.9082 - intent_output_loss: 0.3872 - loss: 1.6837 - model_output_accuracy: 0.7960 - model_output_loss: 0.5788 - sensor_output_accuracy: 0.9017 - sensor_output_loss: 0.2060 - val_brand_output_accuracy: 0.9493 - val_brand_output_loss: 0.1331 - val_intent_output_accuracy: 0.8933 - val_intent_output_loss: 0.6288 - val_loss: 2.2227 - val_model_output_accuracy: 0.7823 - val_model_output_loss: 0.5778 - val_sensor_output_accuracy: 0.8921 - val_sensor_output_loss: 0.2404 - learning_rate: 2.5000e-04\n",
            "Epoch 19/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 78ms/step - brand_output_accuracy: 0.9568 - brand_output_loss: 0.1133 - intent_output_accuracy: 0.9117 - intent_output_loss: 0.3748 - loss: 1.6305 - model_output_accuracy: 0.7988 - model_output_loss: 0.5627 - sensor_output_accuracy: 0.9013 - sensor_output_loss: 0.2029 - val_brand_output_accuracy: 0.9475 - val_brand_output_loss: 0.1345 - val_intent_output_accuracy: 0.8945 - val_intent_output_loss: 0.6237 - val_loss: 2.2063 - val_model_output_accuracy: 0.7823 - val_model_output_loss: 0.5745 - val_sensor_output_accuracy: 0.8915 - val_sensor_output_loss: 0.2359 - learning_rate: 1.2500e-04\n",
            "Epoch 20/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 81ms/step - brand_output_accuracy: 0.9573 - brand_output_loss: 0.1113 - intent_output_accuracy: 0.9112 - intent_output_loss: 0.3744 - loss: 1.6168 - model_output_accuracy: 0.8013 - model_output_loss: 0.5518 - sensor_output_accuracy: 0.9009 - sensor_output_loss: 0.2025 - val_brand_output_accuracy: 0.9451 - val_brand_output_loss: 0.1326 - val_intent_output_accuracy: 0.8951 - val_intent_output_loss: 0.6364 - val_loss: 2.2277 - val_model_output_accuracy: 0.7835 - val_model_output_loss: 0.5744 - val_sensor_output_accuracy: 0.8897 - val_sensor_output_loss: 0.2339 - learning_rate: 1.2500e-04\n",
            "Epoch 21/200\n",
            "\u001b[1m244/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - brand_output_accuracy: 0.9589 - brand_output_loss: 0.1110 - intent_output_accuracy: 0.9092 - intent_output_loss: 0.3879 - loss: 1.6284 - model_output_accuracy: 0.7988 - model_output_loss: 0.5524 - sensor_output_accuracy: 0.9085 - sensor_output_loss: 0.1892\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 78ms/step - brand_output_accuracy: 0.9587 - brand_output_loss: 0.1098 - intent_output_accuracy: 0.9121 - intent_output_loss: 0.3742 - loss: 1.6105 - model_output_accuracy: 0.8023 - model_output_loss: 0.5507 - sensor_output_accuracy: 0.9045 - sensor_output_loss: 0.1996 - val_brand_output_accuracy: 0.9469 - val_brand_output_loss: 0.1344 - val_intent_output_accuracy: 0.8968 - val_intent_output_loss: 0.6358 - val_loss: 2.2287 - val_model_output_accuracy: 0.7835 - val_model_output_loss: 0.5743 - val_sensor_output_accuracy: 0.8921 - val_sensor_output_loss: 0.2346 - learning_rate: 1.2500e-04\n",
            "Epoch 22/200\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 77ms/step - brand_output_accuracy: 0.9585 - brand_output_loss: 0.1095 - intent_output_accuracy: 0.9128 - intent_output_loss: 0.3642 - loss: 1.5761 - model_output_accuracy: 0.8016 - model_output_loss: 0.5422 - sensor_output_accuracy: 0.9052 - sensor_output_loss: 0.1944 - val_brand_output_accuracy: 0.9469 - val_brand_output_loss: 0.1345 - val_intent_output_accuracy: 0.8939 - val_intent_output_loss: 0.6371 - val_loss: 2.2306 - val_model_output_accuracy: 0.7812 - val_model_output_loss: 0.5727 - val_sensor_output_accuracy: 0.8861 - val_sensor_output_loss: 0.2351 - learning_rate: 6.2500e-05\n",
            "Epoch 22: early stopping\n",
            "Restoring model weights from the end of the best epoch: 12.\n",
            "\n",
            "======================================================================\n",
            "EVALUATING JOINT MODEL ON TEST SET\n",
            "======================================================================\n",
            "\n",
            "Test Results:\n",
            "  Overall Loss: 2.2294\n",
            "\n",
            "Intent Classification:\n",
            "  Accuracy: 0.9004\n",
            "  F1-Score (Macro): 0.8880\n",
            "  F1-Score (Weighted): 0.8980\n",
            "\n",
            "Slot Filling Accuracy:\n",
            "  Brand: 0.9428\n",
            "  Model: 0.7633\n",
            "  Sensor: 0.9112\n",
            "\n",
            "Intent Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "   access_governance     0.8869    0.9675    0.9255       154\n",
            "        anomaly_info     0.9326    0.9836    0.9574       183\n",
            "   collection_method     0.9451    0.8659    0.9038       179\n",
            "comparative_analysis     0.9082    1.0000    0.9519       188\n",
            "contextual_followups     0.9153    0.6136    0.7347        88\n",
            "     data_definition     0.8760    0.9099    0.8926       233\n",
            " imperative_commands     0.9037    0.8026    0.8502       152\n",
            "   statistical_query     0.8966    0.9681    0.9309       188\n",
            " system_status_check     0.9368    0.8018    0.8641       111\n",
            "      temporal_stats     0.8483    0.8905    0.8689       201\n",
            "\n",
            "            accuracy                         0.9004      1677\n",
            "           macro avg     0.9050    0.8804    0.8880      1677\n",
            "        weighted avg     0.9018    0.9004    0.8980      1677\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 6: Build and Train Joint Model\n",
        "joint_model = build_joint_model(metadata)\n",
        "joint_history, joint_metrics = train_eval_joint(joint_model, splits, metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7: Connect to Gemini and test model\n",
        "_gemini_model = None\n",
        "\n",
        "# Get raw test queries in same order as splits['X_test'] / splits['y_intent_test']\n",
        "query_test = get_test_queries(df, y_intent, y_brand, y_model, y_sensor)\n",
        "# print(f\"Test queries count: {len(query_test)}\")\n",
        "\n",
        "# Initialize Gemini (uses GOOGLE_API_KEY or GEMINI_API_KEY if api_key not set)\n",
        "initialize_gemini()  # or initialize_gemini(api_key=\"your-key\")\n",
        "\n",
        "# Generate intents for all test queries (small delay to reduce rate limits)\n",
        "gemini_responses = generate_all_responses(query_test, metadata['intent_labels'], delay_seconds=0.1)\n",
        "# print(f\"Generated {len(gemini_responses)} responses.\")\n",
        "\n",
        "# Evaluate against splits['y_intent_test'] and print report\n",
        "evaluate_response(gemini_responses, splits['y_intent_test'], metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lbj_g0ZlgAI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "VISUALIZING BI-LSTM RESULTS\n",
            "======================================================================\n",
            "✓ Visualization saved to: Visualizations/bilstm_results.png\n",
            "\n",
            "======================================================================\n",
            "VISUALIZING 1D-CNN RESULTS\n",
            "======================================================================\n",
            "✓ Visualization saved to: Visualizations/cnn_results.png\n",
            "\n",
            "======================================================================\n",
            "CREATING MODEL COMPARISON VISUALIZATION\n",
            "======================================================================\n",
            "✓ Comparison visualization saved to: Visualizations/model_comparison.png\n",
            "\n",
            "======================================================================\n",
            "FINAL MODEL COMPARISON SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Model           Accuracy     F1-Macro     F1-Weighted  Loss      \n",
            "----------------------------------------------------------------------\n",
            "Bi-LSTM         0.9082       0.8949       0.9055       0.5111    \n",
            "1D-CNN          0.9094       0.8961       0.9068       0.4978    \n",
            "Joint MTL       0.9004       0.8880       0.8980       2.2294    \n",
            "\n",
            "======================================================================\n",
            "🏆 BEST MODEL: 1D-CNN (based on F1-Macro score)\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Step 8: Visualizations\n",
        "visualize_bilstm(bilstm_history, bilstm_metrics, metadata)\n",
        "visualize_cnn(cnn_history, cnn_metrics, metadata)\n",
        "visualize_comparison()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "GQs2o6MdljiD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "✓ ALL TASKS COMPLETED SUCCESSFULLY!\n",
            "======================================================================\n",
            "\n",
            "Generated files:\n",
            "  1. /home/claude/bilstm_results.png\n",
            "  2. /home/claude/cnn_results.png\n",
            "  3. /home/claude/model_comparison.png\n",
            "\n",
            "You can now use these models for automotive query classification!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✓ ALL TASKS COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nGenerated files:\")\n",
        "print(\"  1. /home/claude/bilstm_results.png\")\n",
        "print(\"  2. /home/claude/cnn_results.png\")\n",
        "print(\"  3. /home/claude/model_comparison.png\")\n",
        "print(\"\\nYou can now use these models for automotive query classification!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyP2Xsn3dOQ+hhhtlpSHxYDS",
      "collapsed_sections": [
        "pGlC1KR5025G",
        "_ZC1rwfn06UE",
        "EdAVm2xL0-YH",
        "lTBnOJw11IxW",
        "TVSzVodG1TV_",
        "_tzc9H0J1W_O",
        "m5QlzAkK__nC",
        "Si7Ni5QMAHKr",
        "8XAnBr4xARpF",
        "SFKYupl3BAg_",
        "3gw-v_XbBIvc",
        "TreciG5dBZMV",
        "rzcPY5QkBgoD",
        "gHrcqk7nCA2f",
        "7kXwoITfC2CB",
        "NN4PSJAdCP_m",
        "3IbNN7jBCYIA",
        "kZd8GSobCdKl"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
